Assessing Metadata Quality and Terminology Coverage of a Federally
Sponsored Health Data Repository
A Dissertation
SUBMITTED TO THE FACULTY OF
UNIVERSITY OF MINNESOTA
BY

David Terrence Marc

IN PARTIAL FULFILLMENT OF THE REQUIREMENTS
FOR THE DEGREE OF
DOCTOR OF PHILOSOPHY

Laël Gatewood, PhD, FACMI [advisor]
Rui Zhang, PhD [co-advisor]

February 2016

ProQuest Number: 10076337

All rights reserved
INFORMATION TO ALL USERS
The quality of this reproduction is dependent upon the quality of the copy submitted.
In the unlikely event that the author did not send a complete manuscript
and there are missing pages, these will be noted. Also, if material had to be removed,
a note will indicate the deletion.

ProQuest 10076337
Published by ProQuest LLC (2016). Copyright of the Dissertation is held by the Author.
All rights reserved.
This work is protected against unauthorized copying under Title 17, United States Code
Microform Edition © ProQuest LLC.
ProQuest LLC.
789 East Eisenhower Parkway
P.O. Box 1346
Ann Arbor, MI 48106 - 1346

© David Terrence Marc 2016

i
ACKNOWLEDGEMENTS
I would like to acknowledge the expertise of my friend and colleague, Dr. Rui Zhang. Dr.
Zhang always forced me to think about a problem in a novel way. He is encouraging and
engaged in my work, and is truly an important pillar in my academic career. I would also
like to acknowledge Dr. Laël Gatewood. I am forever grateful for the opportunities and
experiences offered by Dr. Gatewood’s mentorship. Next, I need to acknowledge the
mentorship and support of Dr. Saif Khairat. Dr. Khairat has always challenged me and
motivated me to be a better informaticist. I would also like to thank Dr. Vitaly
Herasevich for his continual support and mentorship as I’ve progressed with my
dissertation work. In addition, I would like to thank Mr. James Beattie for sharing his
expertise and enthusiasm with my research. Finally, I would like to thank my colleagues
at the College of St. Scholastica for their support and encouragement as I’ve worked on
completing my dissertation.

ii
DEDICATION
This dissertation is dedicated to my wife, Cassie, my son, Ephram, and my daughter,
Luella. It is through their unyielding encouragement that I find myself here today. My
wife, Cassie, is my biggest cheerleader. My children, Ephram and Luella, are the shining
stars in my life. They endowed me the motivation to succeed by striving to set a great
example as their father.

iii
ABSTRACT
The Open Government Initiative began an era of information sharing by publishing data
that is accessible to the public. HealthData.gov is a data portal that was developed by the
U.S. Federal Government to publish metadata to disseminate information about
healthcare datasets to the American people. Despite the growth in the number of datasets
published, there has been limited public participation in the use of the data, which has
been attributed to the currently implemented methods for data storage and retrieval. An
automated assessment of the HealthData.gov metadata was conducted to assess
completeness, accuracy, and consistency of metadata published from 2012 to 2014. Also,
a method for indexing the datasets using Medical Subject Headings (MeSH) was
evaluated using a term coverage study. The results of these studies demonstrated that
metadata published in earlier years were less complete, lower quality, and less consistent.
Also, metadata that underwent modifications following their original creation were of
higher quality. MeSH offered adequate coverage of the metadata concepts, thereby
lending support for the adoption of the terminology for indexing purposes. The results
suggested that greater standardization is needed when publishing metadata. This research
contributed to the development of automated metrics for assessing metadata quality,
design recommendations for a framework to supports high quality metadata, and
recommendations for expanding MeSH to offer greater coverage of concepts from
HealthData.gov.

iv
TABLE OF CONTENTS
LIST OF TABLES........................................................................................................................ vi
LIST OF FIGURES .................................................................................................................... viii
CHAPTER 1 INTRODUCTION ..................................................................................................1
CHAPTER 2 LITERATURE REVIEW ......................................................................................5
2.1 OPEN GOVERNMENT INITIATIVE .................................................................................................... 5
2.2 PUBLICLY AVAILABLE HEALTHCARE DATA .................................................................................... 8
2.3 CHALLENGES WITH PUBLICLY AVAILABLE HEALTHCARE DATA ...............................................15
2.4 METADATA QUALITY .....................................................................................................................19
2.5 METHODS FOR ASSESSING METADATA QUALITY ........................................................................24
2.6 INITIATIVES FOR METADATA STANDARDS ..................................................................................25
2.7 MEDICAL SUBJECT HEADINGS (MESH) .......................................................................................28
2.8 THE EVOLUTION OF MEDICAL SUBJECT HEADINGS ...................................................................29
2.9 MODERNIZATION OF MEDICAL SUBJECT HEADINGS ...................................................................31
2.10 CURRENT PRACTICE FOR MESH INDEXING ..............................................................................32
2.11 INDEXING PUBLIC DATA WITH MESH .......................................................................................34
CHAPTER 3 METHODS .......................................................................................................... 37
3.1 DATA EXTRACTION ........................................................................................................................37
3.2 SUMMARY OF HEALTHDATA.GOV METADATA ............................................................................38
3.2.1 Datasets by Year and Modification Status ............................................................ 38
3.2.2 Datasets by Author .......................................................................................................... 39
3.2.3 Duplicate Metadata ........................................................................................................ 39
3.3 QUALITY OF HEALTHDATA.GOV METADATA...............................................................................39
3.3.1 Automated Evaluation of Completeness ................................................................ 39
3.3.2 Automated Evaluation of Accuracy ......................................................................... 42
3.3.3 Automated Evaluation of Consistency .................................................................... 50
3.4 INDEXING METADATA WITH MESH .............................................................................................53
3.5 STATISTICAL METHODS .................................................................................................................57
CHAPTER 4 RESULTS ............................................................................................................. 59
4.1 METADATA SUMMARY RESULTS ...................................................................................................59
4.2 QUALITY RESULTS ..........................................................................................................................61
4.2.1 Completeness Results ..................................................................................................... 61
4.2.2 Accuracy Results............................................................................................................... 68
4.2.2.1 Spelling ....................................................................................................................... 68
4.2.2.2 Broken Links ............................................................................................................ 69
4.2.2.3 Semantic Distance of Metadata instance to Source................................ 70
4.2.3 Consistency Results ......................................................................................................... 74
4.2.3.1 CKAN Requirements ............................................................................................. 74
4.2.3.2 The Dublin Core Standards................................................................................ 78

v
4.3 INVESTIGATING TERM COVERAGE WITH MESH.................................................................................. 79
CHAPTER 5 DISCUSSION ...................................................................................................... 83
5.1 METADATA SUMMARY ...................................................................................................................84
5.2 METADATA QUALITY .....................................................................................................................88
5.2.1 Metadata Completeness ................................................................................................ 89
5.2.2 Metadata Accuracy ......................................................................................................... 91
5.2.3 Metadata Consistency .................................................................................................... 94
5.3 TERM COVERAGE WITH MESH .....................................................................................................96
5.4 CONTRIBUTIONS TO HEALTH INFORMATICS................................................................................98
5.4.1 Formal Recommendations ........................................................................................... 98
5.4.1.1 Data Portal Recommendations........................................................................ 98
5.4.1.2 Recommendations for Expanding MeSH .................................................. 104
5.4.2 Implementing Recommendations .......................................................................... 106
5.4.2.1 Metadata Framework to Support Modifications .................................. 106
5.4.2.2 Automated MeSH Indexing ............................................................................. 109
5.4.2.3 Automated Assessment of Metadata Quality .......................................... 110
5.5 DATA QUALITY IN OTHER DOMAINS ......................................................................................... 111
5.6 LIMITATIONS OF THE RESEARCH ............................................................................................... 117
5.6.1 Limits to Data ................................................................................................................. 117
5.6.2 Limits to Methods.......................................................................................................... 117
5.7 FUTURE RESEARCH ..................................................................................................................... 118
5.7.1 Usability and Usefulness of MeSH .......................................................................... 118
5.7.2 Ontological Development for MeSH Expansion ............................................... 119
5.7.3 Development of Data Selection and Quality Tools ......................................... 120
CHAPTER 6 RESEARCH SUMMARY & CONTRIBUTIONS ......................................... 123
REFERENCES.......................................................................................................................... 126

vi
LIST OF TABLES
Table 2.1. Metrics for Health Data Initiative strategic goals ............................................10
Table 2.2. Required metadata fields for HealthData.gov...................................................15
Table 2.3. Optional metadata fields for HealthData.gov ...................................................16
Table 2.4. Characteristics of metadata quality. Abstracted from Bruce and
Hillman [25] ......................................................................................................................21
Table 3.1. Weights for Qwcomp measure ..........................................................................41
Table 3.2. Type of errors evaluated for accuracy of the metadata ....................................43
Table 3.3. CKAN metadata requirements for data types that were evaluated ...................51
Table 3.4. Dublin Core metadata requirements that were evaluated .................................54
Table 4.1. Datasets published each year by modification status........................................60
Table 4.2. Frequency of datasets published by U.S. health agencies ................................63
Table 4.3. Frequency of duplicate metadata entries published by U.S. health
agencies ..............................................................................................................................64
Table 4.4. Results from Qcomp scores. A) Comparison of Qcomp by modification status
within each creation year; B) Comparison of Qcomp by modification status between each
year. ....................................................................................................................................64
Table 4.5. Results from Qwcomp scores. A) Comparison of Qwcomp by modification
status within each creation year; B) Comparison of Qwcomp by modification status
between each year ..............................................................................................................67
Table 4.6. Frequency of HTTP Status Codes (Note: status code 200 indicated a working
URL) .................................................................................................................................69
Table 4.7. Results from Qaccu scores. A) Comparison of Qaccu by modification status
within each creation year; B) Comparison of Qaccu by modification status between each
year. ....................................................................................................................................73
Table 4.8. Metadata compliance with CKAN requirements ..............................................76

vii
Table 4.9. Results from Qcons scores. A) Comparison of Qcons by modification status
within each creation year; B) Comparison of Qcons by modification status between each
year .....................................................................................................................................77
Table 4.10. Metadata compliance with Dublin Core standards .........................................79
Table 4.11. Top 10 MeSH Concepts from HealthData.gov ...............................................80
Table 4.12. Frequency of matching HealthData.gov nouns to MeSH terms. ...................81
Table 4.13. Frequency of HealthData.gov terms by classification that were not covered
by MeSH. ..........................................................................................................................82
Table 5.1. Recommendations for non-functional and functional system requirements for
managing metadata at HealthData.gov. Template adapted from CDC [75]. Shading
indicates author’s contributions. ......................................................................................101
Table 5.2. Proposed MeSH concepts, entry terms, and summaries to fill the gap in terms
to describe HealthData.gov datasets. Concepts and summaries were abstracted from
DCMI, and the entry terms were added by the author [77] .............................................105

viii
LIST OF FIGURES
Figure 4.1. Dotplot of the distribution and median values (red line) for Qcomp scores by
the year the metadata was created and modification status (n=1,632). .............................65
Figure 4.2. Dotplot of the distribution and median values (red line) for Qwcomp scores by
the year the metadata was created and modification status (n=1,632). .............................67
Figure 4.3. Histogram of Qaccu scores across all years the data was published
(n=1,529). ...........................................................................................................................70
Figure 4.4. Dotplot of the distribution and median (red line) of Qaccu scores by the year
the metadata was created and modification status (n=1,529). ...........................................73
Figure 4.5. Dotplot of the distribution and median (red line) of Qcons scores by the year
the metadata was created and modification status (n=1,632). ...........................................78
Figure 5.1. Framework of metadata changes to support knowledge management.
Simplified from Zavalina and colleagues [32] ................................................................108

1
CHAPTER 1
INTRODUCTION
As part of the Open Government Initiative, the United States Federal Government
published important datasets to the public using an online portal to increase collaboration,
transparency, consumer participation, and research. Federally sponsored healthcare data
is published online at HealthData.gov. Despite the motivation to increase governmental
transparency and accountability by publishing important healthcare datasets on
HealthData.gov, there are considerable limitations to data access and retrieval. The lack
of consistent standards to data storage and retrieval has been shown to inhibit citizen
engagement in utilizing these resources for scholarly purposes [1]. Numerous studies
have shown that although the government is adopting technology to increase
transparency, the current state of the tools have limited citizen participation [2-5]. Martin,
Foulonneau, and Turki [6] conducted a study to examine if data that is considered open
and public is actually accessible. They found that there is limited openness of datasets
advertised as open data, and that there is considerable heterogeneity in the metadata
elements that are intended to represent data sources. Shah and colleagues [7] explained
how publicly available biomedical data are annotated with unstructured text and are
rarely described with ontology concepts available in the domain. The challenge is to
create a consistent terminology for the public data to index the resource and allow for
search methods that can identify the resource and related resources. Together, these
issues challenge users in the discovery and access of public data. The limited public
participation in using public healthcare data may be overcome through improvements to

2
the quality of the metadata and adoption of standard annotation principles to index the
publicly available data. Of particular interest is identifying the points of failure in the
quality of the HealthData.gov metadata and investigating an indexing strategy that may
help to overcome such limitations.

In HealthData.gov, metadata is used to summarize the available datasets and act as the
sole source of information for searching and retrieving relevant data. The HealthData.gov
metadata is maintained with the Comprehensive Knowledge Archive Network (CKAN),
which is open-source data repository software. The CKAN platform specifies
requirements to standardize the storage of metadata. Winn [8] formally evaluated CKAN
and found that there were limitations to the software including the inability to browse
metadata history and workflow challenges around data storage and management.
However, there were advantages including the flexibility for organizations to adapt the
use of CKAN with the opportunity to support data exchange using the Resource
Description Framework (RDF) to facilitate data merging under a common schema.

Until now, the quality and efficacy of storing publicly available healthcare data on
HealthData.gov with CKAN has not been formally evaluated. There are documented
challenges around the usability of HealthData.gov leading to accessibility issues, but the
underlying causes of the challenges are not well explained [2-5]. Therefore, a primary
aim of this dissertation is to formally examine the quality of the metadata of
HealthData.gov to determine the failure points that may be contributing to usability
challenges.

3
One possible method for overcoming the usability challenges with regards to searching
and retrieving public healthcare data from HealthData.gov is the adoption of a
standardized terminology for indexing datasets. Given the commonalities between the
metadata from HealthData.gov and MEDLINE, Medical Subject Headings (MeSH) may
offer an indexing solution. MeSH has historically been used as a controlled vocabulary
for indexing biomedical literature in MEDLINE. In order to conclude if MeSH can be
adapted to indexing datasets, there needs to be a formal evaluation of the efficacy of
MeSH for covering the concepts that are available in the datasets from HealthData.gov.
Therefore, another primary aim of the dissertation is to determine if MeSH is an
appropriate terminology for indexing publicly available healthcare datasets from
HealthData.gov

The overall goal of the research is to investigate the quality of the HealthData.gov
metadata and explore the use of the MeSH terminology for indexing publicly available
data from HealthData.gov. Therefore, the specific aims of the research include the
following:
1. Illustrate the corpus of data available from HealthData.gov
2. Investigate the quality of the metadata instances in HealthData.gov
3. Evaluate if MeSH is an appropriate terminology for indexing publicly available
healthcare datasets from HealthData.gov

4
The remaining chapters of this dissertation will closely examine the aforementioned
aims. Chapter 2 is a literature review that explores existing literature around metadata
quality, standards, and standard indexing practices, which offers rationale for exploring
the aims. Chapter 3 offers the methodology that was adopted to address each aim.
Chapter 4 includes the results from the methods employed. Chapter 5 offers a discussion
on the interpretation of the results and how they relate back to existing literature.
Additionally, Chapter 5 includes recommendations regarding the structure and content of
metadata for publishing publicly available healthcare data. Chapter 6 is the final chapter
and offers concise conclusions regarding the rationale, methods, results and interpretation
of the dissertation.

5
CHAPTER 2
LITERATURE REVIEW
In the following literature review, the rationale for exploring the research aims will be
detailed. As part of an initiative enacted by the United States Federal Government,
important datasets are published to the public to increase collaboration, transparency,
consumer participation, and research. This initiative is referred to as the Open
Government Initiative. The federally sponsored healthcare data is published online at
HealthData.gov. Currently, HealthData.gov does not adequately support the accessibility
goal of the Open Government Initiative due to issues around inconsistent data storage
methods leading to challenges with retrieving relevant data. Specifically, the metadata
may be suffering from limitations in completeness, accuracy, and consistency in
representing the source datasets. Additionally, the metadata is not indexed with a
standardized terminology. Given the commonalities between the metadata published on
HealthData.gov and MEDLINE metadata, Medical Subject Headings (MeSH) may offer
an indexing solution. The remainder of this chapter will review the current state of the
literature regarding the open government initiative, metadata quality assessment, and the
MeSH terminology.

2.1 OPEN GOVERNMENT INITIATIVE
The United States federal government collects and stores massive amounts of data, yet
that data has largely been inaccessible by the American people [9]. In an effort to change
the culture regarding governmental transparency and information sharing, then

6
Presidential-candidate Barack Obama pledged to create a transparent and connected
democracy [10]. On January 21, 2009, the day President Obama was sworn into office,
the “Memorandum for the Heads of Executive Departments and Agencies: Transparency
and Open Government” was signed [11]. This Memorandum outlined the Obama
Administration’s “Open Government Directive” that detailed actionable steps and
deadlines that federal agencies were required to take to promote the principles of
transparency, participation and collaboration. This directive initiated the Data.gov
website [12] for publishing and disseminating federally-sponsored datasets. The
memorandum specified that each federal agency must identify and publish in an open
format at least three high-value datasets and register those datasets via Data.gov [13].

High value information was defined as “information that can be used to increase agency
accountability and responsiveness; improve public knowledge of the agency and its
operations; further the core mission of the agency; create economic opportunity; or
respond to need and demand as identified through public consultation” [13]. In the U.S.
National Action Plan, a policy was enacted regarding “smart disclosure,” or releasing
high quality, complex information in standardized, machine-readable formats that can
improve decisions and aid scientific research [14]. The action plan noted the
government’s intent to develop Federal guidelines on scientific data [14]. Federal
agencies were encouraged to release a variety of “smart disclosure” data that was
accessible, machine-readable, standardized, timely, adaptive and innovative to markets,
in an interoperable, and de-identified format [15]. Ultimately, the Open Government

7
Initiative resulted in tremendous growth of governmental data that is available on the
Internet and can be viewed, downloaded, and analyzed by anyone via the online data
repository, Data.gov.

The Data.gov webpage is an online data repository that includes tools and resources,
communities, and education on how to utilize the site and its data. A community portal,
HealthData.gov [16], was specifically developed to support the dissemination of
healthcare data including what is derived and aggregated from electronic health records
(EHRs), medical devices, clinical trials, clinical registries, public health surveillance data,
consumer product data, community health performance information, government
spending data, and genomic data [16].

Following the publication of the Open Data Directive, the U.S. Department of Health and
Human Services (HHS) began to actively promote wide distribution of healthcare data
through the Health Data Initiative (HDI). In May 2012, the Health Data Consortium
(HDC) was developed to oversee the HDI and offer a structured approach to accelerate
the availability and use of health data. The HDC is a public-private partnership that
drives innovative uses of health information to improve health outcomes for individuals,
enhance the effectiveness of the health care system, and build healthier communities. In
October 2013, HDC created a strategic plan for the HDI, outlining the methods for
advancing the distribution of health data [17]. This plan defined the mission of HDI as:

8
“help[ing] improve health, healthcare, and the delivery of human services by harnessing
the power of data and fostering a culture of innovative uses of data in public and private
sector institutions, communicates, research groups and policy making arenas” [17]
By making health data publicly available, the HDI intended to describe how the public
can responsibly support healthcare innovations through creative, cost-effective, and
efficient solutions [17]. As described in the HDC strategic plan [17], the primary focus of
the HDI was to achieve the following five goals:
1. Advance the HealthData.gov site to be a more efficient, user-friendly, technicallyadvanced platform for data discovery
2. Highlight assets that support the HHS strategic initiatives that focus on data
liberation
3. Educate new and existing participants within the health and human services
ecosystem on data availability and dissemination for problem solving
4. Enable and incentivize the health data ecosystem to utilize data assets in
innovative ways
5. Implement administrative and departmental policies for fostering openness and
transparency
Under HDC’s strategic plan, various metrics were developed to determine the level of
achievement regarding the aforementioned goals. The corresponding metrics are outlined
in Table 2.1. The strategic plan suggested that if the results from these metrics were not
meeting the set expectations, appropriate changes to the initiative would be put forth.

2.2 PUBLICLY AVAILABLE HEALTHCARE DATA
As of December 2014, the overall impact of the Open Government Initiative on
publishing healthcare data resulted in the release of almost 2000 high value data sets on
HealthData.gov.

9
In both Data.gov and HealthData.gov, users have the ability to filter datasets by subject,
description, publisher, keywords, and other relevant metadata (Table 2.2 & Table 2.3).
Although the dissemination of publicly available data is a step forward in promoting
governmental transparency, there exist significant limitations in actually acquiring the
datasets. Specifically, the limitations in the annotation principles adopted by
HealthData.gov may result in suboptimal data searches thereby leading to issues around
accessibility. Also, there is no oversight to ensure that the required metadata was
included in a format that would facilitate indexing practices.

Data.gov and HealthData.gov adopted a data management system constructed by CKAN,
an open-source data portal software [18]. The CKAN search application-programming
interface (API) is based on keyword searches for querying and accessing data. The
searches are based on metadata that is supplied by the data publisher. Research suggests
that there are limitations with CKAN, yet the search API has never been formally
evaluated [8].

Strategic Goal
1) Advance the
HealthData.gov
site to be a more
efficient, userfriendly,
technicallyadvanced
platform for data
discovery

Accept feedback from users
about how the platform should
be altered to better suit their
uses and needs

Initiative
Determine and secure funding
to support infrastructure
upgrades and maintenance
Improve the metadata and
processes to generate metadata

10

Metrics
Secure commitment from departmental leadership for
infrastructure funding
Craft a two-year investment plan for HealthData.gov infrastructure
Solicit feedback from users about issues with metadata
Implement changes and re-survey users to validate improvements
Issue guidance to all health data leads with timeframes and
instructions for improving metadata
Recraft the descriptions and metadata for existing assets to use as
examples of plan language writing for better metadata.
Perform random assessment of data assets
Increase the number of data assets updated to utilize common
core metadata
Monitor user feedback on updated metadata template
Prioritize user feedback on the platform to inform our approaches
to the user experience
Aggregate the most popular data asset requests from the platform
and inform the respective department
Create a module to invite suggestions for additional data sets

Table 2.1. Metrics for Health Data Initiative strategic goals.

Strategic Goals
2) Highlight
assets that
support the HHS
strategic
initiatives that
focus on data
liberation

Create communications and
social media strategies to
highlight the availability of the
strategically relevant data
assets (SRDA)
Develop a pipeline of data
assets that are not publicly
available but are strategic
targets for focused data
liberation efforts

Initiative
Highlight departmental assets
that support achieving strategic
goals

Develop a plan and pipeline for liberating new datasets

Metrics
Define the characteristics of strategically relevant data assets and
identify data assets that meet the definitions of SRDA
Identify data assets that are not available in machine-readable
formats, and convert them to machine-readable formats
Identify examples of innovative uses of data assets for impactful
and innovative healthcare solutions
Identify examples of innovative uses of data assets that are not
strategically relevant and how these can impact innovative
solutions in healthcare
Measure the increases in HealthData.gov traffic to SRDAs
Measure the efficacy of the social media outreach by tracking
website statistics on source websites

Table 2.1. Metrics for Health Data Initiative strategic goals (continued)

11

Strategic Goals
3) Educate new
and existing
participants
within the health
and human
services
ecosystem on
data availability
and
dissemination
for problem
solving

Develop use cases and internal
marketing approaches to
enhance engagement

Increase the percentage of data
assets with machine-readable
outputs
Expanding external outreach
including conference
attendance, speaking
arrangements, social media
presence, and online outlets

Initiative
Highlight the value of openly
available health data

Obtain feedback from HHS employees about their awareness of
the HealthData.gov platform
Implement changes and re-survey HHS employees to validate
internal communication campaigns to improve awareness

Health data leads will post educational modules
Obtain feedback about user ratings for each educational module
Improve communication through the HealthData.gov blog with at
least one post per week
Increase the HealthData.gov twitter followers to 15,000 and
increase number of retweets to 60 per month
HDI representatives will speak at conferences
Visitors to HealthData.gov can score datasets
Visitors to HealthData.gov can append linked media

12

Metrics
Collaborate with stakeholders to offer examples of how data assets
are being used by healthcare systems to impact operations for cost
containment and quality improvement
Offer examples of how data has been utilized to stem the tide of
waste, fraud, and abuse
Highlight examples where data is utilized for program evaluation
Increase to percentage of data assets on HealthData.gov available
in machine-readable formats

Table 2.1. Metrics for Health Data Initiative strategic goals (continued)

Strategic Goals
4) Enable and
incentivize the
health data
ecosystem to
utilize data
assets in
innovative ways

Initiative
Publicize the availability of data
and draw attention to uses of
the data as a driver in the
transformation of health,
healthcare, and delivery of
human services
Seek new ways to engage
entrepreneurs who may use the
data as fuel for their business

Develop and cultivate private sector and community relationships
Provide insights about the data to the business community to
highlight the myriad opportunities the data provide
Generate market analysis for the department about how the data
are being used, what additional data the market desires, and
accumulate recommendations that will help prioritize HDI’s work

Metrics
Develop a series of educational and training events with nongovernmental organizations

Table 2.1. Metrics for Health Data Initiative strategic goals (continued)

13

Strategic Goals
5) Implement
administrative
and
departmental
policies for
fostering
openness and
transparency.

Draft plans to address the
dissemination of data from
federally funded research
Continue to lead the execution
of disseminating open and
machine-readable data

Initiative
Draft a charter to
institutionalize the HDI as a
vehicle for transparency, public
access to health data, and
feedback to senior leadership
Implement the Open Data
Policy which supports
information processing and
dissemination activities

Contribute feedback agencies representing HHS’s strategy for
meeting cross agency performance goals
Have a strategic plan for meeting performance goals including
metrics, milestones, and priorities for the initiative
14

Create and maintain an enterprise data inventory
Create a public data listing on HHS.gov according to a metadata
schema
Create a process to engage with customers to prioritize data
releases
Document if data cannot be released because it is private
Clarify roles and responsibilities for promoting efficient and
effective data releases
Disseminate federal award language that supports open data as a
direct output of acquisitions, contracts, and grants
Incorporate new interoperability and openness requirements into
core agency processes
Strengthen measures to ensure that privacy and confidentiality are
fully protected
Submit a strategy describing compliance with the initiative

Metrics
Obtain feedback on, and complete departmental clearance for a
charter to institutionalize the HDI

Table 2.1. Metrics for Health Data Initiative strategic goals (continued)

15
Table 2.2. Required metadata fields for HealthData.gov.
Field

Type

Description

id

GUID

The unique identifier for the dataset

title

plain text

The title name for the dataset

notes

plain text

The description of the dataset

notes_r HTML text
endered

The description of the dataset rendered in HTML using
Markdown

author

plain text

The name of the federal agency that submitted the
dataset

url

url

The URL to the home page for the dataset, which may
link to downloadable files

tags

array of strings

Tags associated with the dataset

2.3 CHALLENGES WITH PUBLICLY AVAILABLE HEALTHCARE DATA
There are obvious reasons why the federal government is making high quality healthcare
data available for research and use by the public, including benefits related to the goals of
the Open Government Initiation of increased collaboration, transparency, participation,
and research. The overarching benefit of these data is the potential to improve decisions
based upon knowledge [15]. However, the impact of the Open Government Initiative on
supporting research has been minimal. A primary reason why the Open Government
Initiative is failing to support research is due in part to the fact that transparency and
access standards are inconsistently applied within federal communication policymaking
[19]. Thus, the lack of consistency may be inhibiting citizen engagement in utilizing
these resources for scholarly purposes [1]. In fact, studies have shown that although the
government is adopting technology to increase transparency, these tools have been
ineffective in engaging citizen participation [2-5].

16
Table 2.3. Optional metadata fields for HealthData.gov.
Field
author_id

Type
uri

Description
A URI uniquely identifying the agency submitting
the data. The URI is in the
http://healthdata.gov/id/agency space and while it
does not currently resolve to a resource it can be
used as a canonical identifier for the agency.

Group Name

plain text

A display name shared across datasets that are
related.

Agency

plain text

The name of the federal department submitting the
data. Generally “Health and Human Services.”

Subject Area 1

string

A subject area. Subjects come from a fixed
vocabulary, currently: Administrative, Biomedical
Research, Children's Health, Epidemiology,
Health Care Cost, Health Care Providers,
Medicaid, Medicare, Other, Population Statistics,
Quality Measurement, Safety, Treatments.

Subject Area 2

string

A subject area. See above.

Subject Area 3

string

A subject area. See above.

Date Released

date

The date the dataset was first made available to
the public (possibly before it was posted on
HealthData.gov). Format: YYYY-MM-DD.

Date Updated

date

The date the dataset was last changed, i.e. the last
change to the data itself and not necessarily the
metadata record. Format: YYYY-MM-DD.

Agency
Program URL

url

The URL of the agency program responsible for
the data.

Collection
Frequency

string

The frequency with which the data was collected,
which is sometimes different from the frequency
at which the data is published. Possible values are
Annually, Semi-Annually, Quarterly, Monthly,
Weekly, Daily.

17
Table 2.3. Optional metadata fields for HealthData.gov. (continued)
Field

Type

Description

Coverage Period
Start

date

The start of the coverage period, i.e. the date range
that the data pertains to. Format: YYYY-MM-DD.

Coverage Period
End

date

The end of the coverage period, i.e. the date range
that the data pertains to. Format: YYYY-MM-DD.

Coverage Period
Fiscal Year Start

year

For coverage periods that are based on fiscal years
rather than calendar years, the starting fiscal year of
the coverage period. Format: YYYY.

Coverage Period
Fiscal Year End

year

For coverage periods that are based on fiscal years
rather than calendar years, the ending fiscal year of
the coverage period. If the coverage period end
fiscal year is omitted, the dataset may cover the
period from the starting fiscal year to the present
time. Format: YYYY.

Unit of Analysis

plain text

The unit of analysis, i.e. the object of study.
Examples are “recalled food items” and “renal
dialysis facility”.

Geographic
Scope

plain text

The geographic region covered by the dataset. If
omitted, the dataset is typically national in scope.

Geographic
Granularity

string

The granularity of the geographic coverage. Possible
values are Latitude/Longitude Coordinate, Street
Address, Census Tract, City, metropolitan statistical
area, ZIP Code, County, State, Sub-National
Region, and Country.

Technical
Documentation

url

The URL to technical documentation for the dataset.

Data Dictionary

url

The URL to a data dictionary for the dataset.

Collection
Instrument

url

The URL to information about the data collection
instrument.

License
Agreement
Required

integer

Whether a license agreement must be agreed to
before using the data (1 if yes, 0 if no, omitted if not
known).

License
Agreement

url

The URL to a license agreement that must be agreed
to before using the data.

18
Although the websites Data.gov and HealthData.gov have the potential to support
governmental transparency, the success of these websites rests upon how well they can
accommodate consumer accessibility [20]. Robinson and colleagues [21] discussed the
challenges with complying and coordinating with government-wide policy requirements
for federal websites. Chang and Kannan [22] and Dawes and Helbig [3] suggested that
more research is needed to better understand the needs of citizens in order to enhance the
use of the data tools. Similarly, others have suggested a need for a more integrated
approach to investigate tools to support data accessibility, dissemination, and inventory
management to solve the problems around consumer participation [5, 22].

Information accessibility remains a challenge for citizens regardless of the growth of
Data.gov and HealthData.gov. Research is needed to investigate practices that can
accommodate consumers in accessing and using publicly available healthcare data.
Studies suggested that to promote widespread use of public data the government should
allow third party, private entities to play larger roles in the delivery of data and
information [5, 21]. Feasibly, other federal efforts can be leveraged to support
accessibility and usability of publicly available data. As Data.gov and HealthData.gov
expand, the ability for end-users to find relevant data sources will become paramount for
the sustainability of the websites and the Open Government Initiative. Particularly, the
metadata quality, data structure and data extraction methods that were adopted in the
construction of Data.gov and HealthData.gov need to be evaluated so that the most
relevant data is queried. Adopted indexing methods of datasets should be systematic,

19
structured, and not too labor-intensive to allow for long-term sustainability and
usability of publicly available data.

2.4 METADATA QUALITY
The quality of the metadata is paramount for supporting effective data retrieval. Research
has demonstrated that the quality of metadata instances is related to the operation of
retrieving error-free, relevant resources [23]. Beall [23] explains how quality control of
metadata is critical for digital library management. Despite the time-intensive cost of
retaining high quality metadata, the benefits include accurate, error-free data, which
supports consistent access to that data.

A widely cited study on metadata quality is a paper by Bruce and Hillmann [24] where
they defined general characteristics of metadata quality including completeness,
accuracy, provenance, conformance to expectations, logical consistency and coherence,
timeliness, and accessibility (Table 2.4). Methods have been developed to empirically
evaluate each of these defined characteristics. Until now, the quality of metadata for
Data.gov and Healthdata.gov has not been investigated. However, there is a need to
determine how the quality of the metadata impacts the indexing and retrieval of the data
instances in hopes of developing metadata best practices [24].

Poor quality metadata can impact the indexing and storage of data [25]. Hillmann [25]
explained how the development of infrastructure to support the creation and maintenance

20
of high quality metadata is a required, albeit difficult process. Ochoa and Duval [26]
showed how the quality of the metadata could impact the ability to index data instances.
Ochoa and Duval [26] demonstrated the use of automated measures of metadata quality,
which can inform developers of problems around metadata standards and definitions. By
correcting such issues, improvements in metadata quality can improve the storage and
retrieval of relevant data instances [26].

The National Information Standards Organization (NISO) suggests six principles of what
is termed “good” metadata: (1) conforms to community standards; (2) supports
interoperability; (3) uses authority control and content standards; (4) includes a clear
statement of the conditions and terms of use; (5) supports long-term curatorship and
preservation; and (6) should have the qualities of good objects, including authority,
authenticity, archivability, persistence, and unique identification [27]. The purpose of the
NISO principles is to provide a framework to guide organizations in building robust
digital records that support interoperability by adopting standards related to semantic
factors to ensure higher metadata quality. Despite the principles outlined by NISO, most
research exploring metadata quality has focused on the role in bibliographic functions
[24, 26, 28, 29]. In the presented discussion, the quality of metadata reflects the degree to
which the metadata in question perform the core functions of data discovery, use,
provenance, currency, and authenticity. That is, the purpose of the metadata is to
facilitate finding, identifying, selecting, and obtaining datasets.

21
Table 2.4. Characteristics of metadata quality. Abstracted from Bruce and Hillman
[25].
Quality
Measure
Completeness

Provenance

Accuracy

Quality Criteria
Does the element set completely
describe the objects?
Are all relevant elements used
for each object?
Who is responsible for creating,
extracting, or transforming the
metadata?
How was the metadata created or
extracted?

Application profile;
documentation
Visual view*; sample
OAI server info†; File info,
TEI Header‡
OAI Provenance; colophon
or file description

What transformations have been
OAI About
done on the data since its
creation?
Have accepted methods been used OAI About; documentation
for creation or extraction?
What has been done to ensure
valid values and structure?

Are default values appropriate,
and have they been appropriately
used?
Conformance to
expectations

Compliance indicators

Does metadata describe what it
claims to?
Are controlled vocabularies
aligned with audience
characteristics and understanding
of the objects?
Are compromises documented
and in line with community
expectations?

OAI About; visual view;
sample; knowledge of
source provider practices;
documentation for creatorprovided metadata; knownitem search tests
Known-item search tests;
visual view
Visual view; external
documentation; high ratio of
populated elements per
record
Visual view, sample,
documentation; expert
review
Documentation; user
assessment studies

22
Table 2.4. Characteristics of metadata quality. Abstracted from Bruce and Hillman
[25]. (continued)
Quality
Measure
Logical
consistency and
coherence

Timeliness

Accessibility

Quality Criteria

Compliance indicators

Is data in elements consistent
throughout?

Visual view

How does it compare with other
data within the community?
Is metadata regularly updated as
the resources change?

Research or knowledge of
other community data;
documentation
Sample or date sort of
administrative information

Are controlled vocabularies
updated when relevant?

Test against known changes
in relevant vocabularies

Is an appropriate element set for
audience and community being
used?
Is it affordable to use and
maintain?

Research or knowledge of
other community data;
documentation
Experience of other
implementers; evidence of
licensing or other costs.
Standard format; extensible
schema

Does it permit further valueadds?

* “visual view” means the process of evaluating metadata using visual graphical analysis
tools, as described in the Dushay and Hillmann [25].
† Open Archives Initiative (home page)
‡ Text Encoding Initiative (home page), http://www.tei-c.org/ (accessed 28 July 2003)
Park [29] suggested that the most common criteria used in measuring metadata quality
include completeness, accuracy, and consistency. Completeness is related to two factors.
First, the metadata that is used should describe the target objects as completely as
possible. Second, the metadata should be applied to the target object population as
completely as possible, without including elements that are rarely used [24]. Accuracy is
related to the correctness of metadata and the representation of the metadata to the
source. That is, the metadata should be correct and factual. Additionally, typographical

23
errors and nonconformity of names and abbreviations are related to accuracy.
Consistency, also known as comparability or coherence, is measured by looking at the
relationship of metadata on the conceptual level to the metadata format on the structural
level. Conceptual consistency includes the degree to which the same data elements are
used for delivering similar concepts in the description of a resource. Structural
consistency explains the extent to which the same structure or format is used for
presenting similar data elements. For instance, the conceptual differences for formatting
the data (e.g., MM-DD-YYY or DD-MM-YYY) may lead to inconsistencies at the
structural level.

A common practice in digital repositories is changing metadata in order to offer the most
up to date representation of the data source. Research has shown that modifications to
metadata can lead to improvements in the overall quality [30-32]. Tarver and colleagues
[30] found that with modifications to metadata, there is an improvement to the overall
completeness of the metadata, and thereby led to higher quality. Zavalina and others [31]
saw that modifications include more than completions, but also addition of fields,
removal of variables, removal of fields, and changes to existing data elements, which all
led to improvements in the quality of the metadata. Interestingly, authors that supply data
to HealthData.gov have the option to update the metadata . However, the frequency such
updates are made is unknown and the impact these updates have on the quality of the
metadata is unknown. Winn [8] found that with modifications to the metadata using the
CKAN software, historical metadata records are not archived. Therefore, there is no way
to know what modifications were made to the metadata overtime.

24
2.5 METHODS FOR ASSESSING METADATA QUALITY
Metadata quality assessment must address attributes of the metadata at several levels: the
semantic structure (the “format” of the metadata) the syntactic structure (the metadata
“schema”), and the data values themselves. These three levels can be validated for quality
using automated methods [25].

Automated metadata quality assessments are usually more cost effective and less labor
intensive than manual human methods. Ochoa [26] explained how manual evaluations
require humans to review a statistically significant sample of metadata instances against a
predefined set of quality parameters. Typically, more than one human reviews the
metadata instances and the quality parameters are averaged across all evaluators. As
described by Ochoa [26], this method has three disadvantages: 1) the manual quality
estimation is only valid at the sampling time. If a considerable amount of new resources
are inserted in the repository, the assessment may no longer be an appropriate estimate of
quality and must be redone; 2) only the average quality can be inferred with these
methods; 3) obtaining the quality estimation in this way is costly and labor intensive. To
overcome these limitations, automated quality evaluations have been suggested as
preferred methods.

Automated quality evaluations collect statistical information from all the metadata
instances in a repository to obtain an estimation of the quality. There is evidence of using
simple statistical methods for an automated quality evaluation. One study calculated

25
automated quality metrics of all data repositories in the Open Language Archive [33].
Bui and Park [34] performed a study to evaluate the completeness of more than one
million metadata instances. Najjar, Ternier, and Duval [35] used automated methods to
examine the quality of the metadata created by ARIADNE, a system that automatically
generates and extracts metadata from sources. All these studies used automatic methods
to obtain a basic estimation of the quality of each individual metadata instance without
incurring the cost involved in manual quality review. However, automated methods do
need to be validated in order to provide a similar level of “meaningfulness” as a human
generated estimation. Ochoa and Duval [26] found that automated methods of evaluating
data quality were not as effective as human’s manually evaluating the metadata, but were
comparable. They suggested a method that combines automation of certain quality
metrics while using human review with other metrics may be the best way to manage
quality assessment of metadata.

2.6 INITIATIVES FOR METADATA STANDARDS
Initiatives have been set forth to offer standards for the names and formats of metadata
fields. The most widely recognized initiative is the Dublin Core Metadata Initiative
(DCMI) where a framework was designed to specify metadata requirements. The name of
the framework is “The Dublin Core”, otherwise known as the Dublin Core Metadata
Element Set. The original specifications were created in 1995 and included fifteen
generic elements for describing the following: Creator, Contributor, Publisher, Title,
Date, Language, Format, Subject, Description, Identifier, Relation, Source, Type,

26
Coverage, and Rights [36]. Today, the Dublin Core is a formal standard for the
development of metadata corresponding to web applications [37]. The Dublin Core is
now incorporated as part of the Semantic Web developed by the World Wide Web
Consortium (W3C). The Semantic Web offers standards for formatting data and
exchange protocols on the Web [38].

The Dublin Core Metadata Vocabulary terms offer formatting and structural requirements
for encoding metadata terms [36]. The metadata terms are distinguished by four types:
properties, classes, datatypes, and vocabulary encoding schemes. The properties element
is the core attribute of a resource and used for defining uniform structured resource
descriptions (e.g., title, creator). The set of fifteen generic elements created in 1995 is the
basis of the properties element. Classes are groups of resources that have common
properties and are therefore put together as a member of one concept (e.g., collection,
dataset). Each property may relate to one or more class by a ‘has domain relationship’
where the domain specifies the class of resources that the property should be used to
describe. Alternatively, a property may be related to one or more classes by a ‘has range
relationship’ where the range indicated the class of resources that should be used as
values for that property. Datatypes are rules that specify how a value should be structured
(e.g., formatting a date field). Finally, vocabulary encoding scheme (VES) identifies
controlled vocabularies, such as thesauri, classifications, subject headings, and
taxonomies that may be used as values [36].

27
Interestingly, HealthData.gov uses a metadata schema called the “Project Open Data
Metadata Schema” [39]. This schema is extensible where metadata elements from Dublin
Core can be included, however, Dublin Core is not the basis of the metadata schema.
Rather, the Project Open Data Metadata Schema is based on the Data Catalog Vocabulary
(DCAT), a hierarchical vocabulary specific to datasets. This specification defines three
types of metadata elements: Required, Required-if (conditionally required), and
Expanded fields. Table 2.2 displays the metadata elements that are required when
published metadata to HealthData.gov while Table 2.3 demonstrates the expanded
optional fields. There are very few metadata elements that are of the “Required-if” type.
Those elements that fit this type include a bureauCode, which must be provided by
United States Federal Government Agencies and includes the agency code and bureau
code. Also, if the dataset is available for public download, any accessibility requirements
must be included (e.g., file type) [39].

The Dublin Core VES element is of particular interest when discussing metadata
standards for HealthData.gov. The fact that a standardized vocabulary has not been
adopted for indexing may be a key factor leading to the challenges with data
accessibility. The purpose of the following information presented hereto is to explore a
standardized vocabulary that may have the potential to support the indexing of publicly
available healthcare data. The terminology that will be explored is Medical Subject
Headings (MeSH).

28
2.7 MEDICAL SUBJECT HEADINGS (MESH)
A federal initiative that may be leveraged to support the usability and accessibility of
relevant data and offer a standardized vocabulary for indexing purposes is Medical
Subject Headings (MeSH). In the 1960s, the National Library of Medicine (NLM) dealt
with information retrieval issues around biomedical literature due to poor indexing and
search mechanisms for articles in MEDLINE. Due to problems in retrieving relevant
medical literature, NLM produced MeSH. The goal of MeSH is “to provide a
reproducible partition of concepts relevant to biomedicine for purposes of organization of
medical knowledge and information” [40].

MeSH is one of several biomedical vocabularies that are available in the Unified Medical
Language System (UMLS) Metathesaurus. The UMLS Metathesaurus is a large, multipurpose, and multi-lingual vocabulary database that contains information about
biomedical and health related concepts, their various names, and the relationships among
them [41].

MeSH is based off of a concept-oriented framework. The framework has a three-level
structure: descriptor class, concept, and term. A concept is a common idea expressed by
synonymous words or terms. For example, “Cardiac Disease” and “Heart Disease”
express the same concept. A concept is a term for each of these synonymous terms. A
term is a way of describing the concept. The concept will often have a preferred term,
which has been adopted as the name of the concept. The descriptor is a class of one or
more related concepts where the name of the descriptor is the preferred concept.

29
The main headings and entry terms are the names of the concepts in a descriptor class.
In MEDLINE the descriptor name is often attached to the citation record. An entry term
may be the descriptor name or a concept in that descriptor class. The role of entry terms
is to provide a guide for choosing proper concepts. Therefore, searchers most often use
entry terms for determining the most appropriate concept class.

Interestingly, there are very few studies evaluating the performance of the MeSH
framework for retrieving resources other than medical literature [42, 43]. Of particular
interest is the possible influence of the MeSH vocabulary in indexing and retrieving
publicly available healthcare data that is published under the Data.gov community site,
HealthData.gov.

2.8 THE EVOLUTION OF MEDICAL SUBJECT HEADINGS
The National Library of Medicine (NLM) evolved from a small collection of books and
journals to the world’s largest biomedical library. The NLM began in 1836 when the
United States Surgeon General, Joseph Lovell, requested $150 in funds to buy books
[44]. In 1840, the very first Catalogue of Books in the Library of the Surgeon General
was created, which consisted of a handwritten list of 134 titles. This collection was called
the “Library of the Surgeon General’s Office” [45]. In 1862, a medical museum was
established to maintain a collection of medical reports, statistics, and specimens in order
to document the history of the war, which resulted in the first printed catalog in 1864 of
around 2,100 volumes [45]. In 1871, then Surgeon General, Joseph K. Barnes, and a

30
physician by the name of John Shaw Billings agreed that the collection should be
developed into a “National Medical Library”. By 1875, almost 75% of all medical
periodicals published were contained within this library. Before this time, the method for
cataloging the periodicals consisted of arranging books by subject and then alphabetically
by author; journals were arranged alphabetically by title. However, there wasn’t a
formalized subject guide for searching journal contents by article.

In 1874, Billings began the subject indexing of journals in the library, which a year later
resulted in the Specimen Fasciculus of a Catalogue of the National Medical Library [46].
The Specimen Fasciculus offered a cataloguing method that would list books by author
and subject, journals by title, and journal articles by subject, all in a single alphabet. This
would later lead to volume one of the Index-Catalogue of the Library of the Surgeon
General’s Office, United States Army, which appeared in 1880. By 1895, the complete
series was completed and consisted of sixteen volumes [46].

To keep up with the output of medical books and journal articles, Billings recruited the
help of the bibliographic publisher, Frederick Leypoldt, and instituted the Index Medicus
in 1879 [45]. Due to the lack of government funding the Index Medicus ceased
publication in 1899, but was later revived in 1903 with the support of the Carnegie
Institution of Washington. In 1927, Index Medicus was merged into the American
Medical Association’s similar publication known as the Quarterly Cumulative Index
Medicus. In 1940, Atherton Seidal of the American Documentation Institute along with

31
the backing of Harold Wellington Jones, the library director, formed the Current List
of Medical Literature, which was put together similar to the Index Medicus. The Current
List was a weekly pamphlet that listed recently created medical books and articles. This
effort eventually resulted in the modernization of the library. The Current List of Medical
Literature was later revamped as a monthly publication that included that month’s authors
as well as cumulative author and subject indexes. The obsolete Index-Catalogue was
finally stopped in 1950 [45].

As the library continued to grow, debate began on whether the library should remain in
the military or be transferred to another department. In 1956, the National Library of
Medicine Act, sponsored by Senators Lister Hill and John F. Kennedy, placed the library
in the federal government’s health agency to serve not only the military, but also the
nation’s health professionals. At this time, the Current List of Medical Literature and the
Quarterly Cumulative Index Medicus were concurrently published. In 1959, the two
methods were combined and a modern Index Medicus was developed [45]. This version
was published in 1960 and was the first rendition of what is now known as Medical
Subject Headings (MeSH).

2.9 MODERNIZATION OF MESH
The MeSH thesaurus is a controlled vocabulary developed and maintained by the
National Library of Medicine (NLM). MeSH was developed for indexing, cataloging, and
searching biomedical and health-related information and documents [47]. In 1960, there

32
were a total of sixty-seven medical subject subheadings. The subheadings covered
general topics such as “therapy”, which was used to mean “therapy of”, “therapeutic use
of”, or “therapeutic aspects”. Additionally, articles were catalogued under just one
subheading. Due to the generalized nature of the early subheadings and the limitations
with the cataloguing process, many articles were not easy to find. In 1963, the second
edition of MeSH was created and included a categorized lists of terms that were printed
for the very first time with the title of “Medical Subject Headings” [48]. The second
edition of MeSH contained greater coverage of concepts for indexing, thereby leading to
increased specificity of the descriptors [49]. Additionally, an article could be found by
searching for the single subheading, but also the specific terms that were comprehended
in the meaning of the larger term [49]. The second edition facilitated more thorough
searches due to the sorting of terms into broad categories, and these lists were published
to enable a user to find related terms. Also, more specific terms and headings were
introduced to accommodate topics where there were a growing number of citations [49].

As computer technology became more prevalent, there were great changes in the way that
MeSH was managed and revised to make a more practical and systematic approach when
compared to printed indexes and catalogues.

2.10 CURRENT PRACTICE FOR MESH INDEXING
There are two processes that have been adopted to index with MeSH. These processes are
currently employed for indexing biomedical literature in MEDLINE and include: 1)

33
human cataloguers determine the appropriate MeSH terms for representing a published
article; 2) Automatic methods are used to determine the appropriate MeSH terms for
representing a published article.

NLM currently employs human indexers to determine the most applicable MeSH terms
for describing an article that is published in MEDLINE. The human indexers must
undergo special training to independently catalogue literature using MeSH. NLM
indexers examine articles and assign the most specific MeSH heading(s) that
appropriately describes the concept(s) discussed in the paper. NLM's MEDLINE indexers
use the following steps to determine the subject content of an article [50]:
1. Read carefully and understand the title.
2. Read the introduction, looking for the purpose of the article.
3. Scan the body of the article, focus on the Materials & Methods section and the
Results section.
4. Note section headings, paragraph headings; italics, boldface; charts, plates, tables,
illustrations; laboratory methods, case reports, etc.
5. Select for indexing only those subjects actually discussed as opposed to those
subjects merely mentioned.
6. Read the summary or conclusions of the author to determine whether the stated
purpose was achieved. Do not index implications or suggested future applications.
Do not index conclusive statements not supported by the text.
7. Scan the abstract for items missed, verifying that the text supports indexing these
concepts.
8. Scan the author's own indexing or the keywords supplied by the publisher to see
whether the concepts chosen are actually discussed in the text.
9. Scan the bibliographic references supplied by the author for clues and further
corroboration.
NLM indexers use the MeSH Browser [51], an online vocabulary look-up aid with
complete MeSH records, to find the term that best describes the concept present in the
article that will be indexed. Indexers use the most specific term available to describe a

34
concept. The indexer assigns as many MeSH headings as appropriate to cover the
topics of the article (generally 5 to 15). When there is no specific heading for a concept,
the indexer will use the closest general heading that is available. Coletti and Bleich [52]
explain that the manual indexing of MeSH terms is an incredibly accurate method for
indexing MEDLINE articles.

An alternative method for indexing is using automated methods. Aronson [53-56] has
conducted a series of studies to develop an automated indexing method. The Medical
Text Indexer (MTI) was developed as a result of this initiative [56]. Research suggested
that biomedical literature in MEDLINE was successfully indexed with MTI when only a
title and abstract were provided and that the system assigned MeSH terms with similar
accuracy as a human indexer, but with far greater efficiency [56]. Therefore, MTI was
shown to be a substitute to human indexing, which supports a more efficient indexing
process. The MTI may also prove to be an efficient and accurate tool for indexing
datasets that are available through HealthData.gov. However, before an automated
indexing engine can be adopted and applied to the HealthData.gov metadata, there is a
need to evaluate the alignment of MeSH entry terms to the concepts that are present in
the HealthData.gov metadata.

2.11 INDEXING PUBLIC DATA WITH MESH
The metadata from CKAN is similar to metadata associated with biomedical literature
published on MEDLINE, including a title, author, and description/abstract. CKAN

35
recommends that publishers provide specific metadata to describe the datasets (Table
2.2). However, CKAN does not offer recommendations around the format and content of
the metadata, or support a standardized vocabulary for indexing purposes. Tags are a
metadata element provided in CKAN that are keywords self-prescribed by the authors of
the datasets. In contrast, MEDLINE uses MeSH entry terms for indexing biomedical
literature. Currently, MeSH entry terms are assigned to each article that is accessible
through MEDLINE by trained indexers hired by the U.S. NLM.

Efforts for automating the indexing process in MEDLINE have shown promise. The
Medical Text Indexer (MTI) was developed for the sole purpose of offering MeSH
concepts by automatically processing the MEDLINE metadata [56]. The MTI may also
prove to be an efficient and accurate tool for indexing datasets that are available through
HealthData.gov. However, before an automated indexing engine can be adopted and
applied to the HealthData.gov metadata, there is a need to evaluate the alignment of
MeSH entry terms to the concepts that are present in the HealthData.gov metadata.

To support the adoption of MeSH for indexing and retrieving healthcare data from
HealthData.gov, empirical research is required to determine if MeSH is an appropriate
vocabulary for data indexing and retrieval. Also, research should examine best practices
around the metadata standards and storage practices of publicly available healthcare data
to ensure sustainability of the Open Government Initiative. If MeSH is supported as a
viable terminology for indexing high-quality metadata, and automatic methods can be

36
adopted to ensure the high quality of the metadata to support the deployment of MeSH,
this may improve the ability of consumers to find relevant data to answer questions of
interest.

Currently, HealthData.gov lacks public participation due to challenges in searching and
retrieving relevant data. These challenges may be attributed to low metadata quality and
the lack of an indexing standard for storing the metadata. In addition, authors of the data
that is published in HealthData.gov conduct frequent updates to the metadata. The
frequency of such updates and the impact that these updates have on the overall quality of
the metadata is unknown. Therefore, the purpose of this research is to first examine the
corpus of the data that is available in healthdata.gov, conduct a formal assessment of the
quality of the metadata based on the year the metadata was created and the modification
status using automated, and examine if MeSH may be a feasible controlled vocabulary
for indexing the HealthData.gov metadata.

The next chapter will explain the methods employed for examining the corpus of the
metadata published in HealthData.gov, the quality of the metadata, and MeSH coverage
of the HealthData.gov concepts.

37
CHAPTER 3
METHODS
The following methods will address the three aims: (1) examine the corpus of data
available from HealthData.gov; (2) investigate the quality of the metadata instances in
HealthData.gov; (3) evaluate if MeSH is an appropriate terminology for indexing
publicly available healthcare datasets from HealthData.gov. The goal of these methods is
to determine the scope and quality of the metadata from HealthData.gov and suggest
guidelines for data publishing and standardization to support consumer participation in
data retrieval and use. The methods associated with each of the aforementioned aims will
be explicitly detailed throughout this chapter. The method of data extraction from the
HealthData.gov will precede the specific methods that were employed to investigate the
aims.

3.1 DATA EXTRACTION
In August 2015 the complete HealthData.gov metadata catalogue from datasets that were
published from January 2012 through December 2014 was queried from the CKAN
engine in using an HTTP GET script. There were a total of 1,632 datasets published at
this time. The HealthData.gov metadata was exported from CKAN in a JSON file format.
The json Python library was used to parse the JSON file to extract the primary metadata
fields shown in Table 2.2. The extracted metadata were written to a CSV file.

38
3.2 SUMMARY OF HEALTHDATA.GOV METADATA
The first aim of the dissertation is to summarize the data that is made available on
HealthData.gov. The methods will be explained for summarizing the frequency that
metadata was published by year, underwent modifications, and the authors that published
the metadata.

3.2.1 Datasets by Year and Modification Status
A contingency table was constructed to count the number of datasets that were originally
generated in the years of 2012, 2013, and 2014. In addition, a cross tabulation with
modification status identified how many datasets were modified following their original
publication date. A modification was defined as a metadata instance that had undergone
changes since its original creation. Modifications were determined by examining
differences in the date the metadata was created compared to the date of the last edit to
the metadata. A field denoted the date of original publication of the metadata instance.
The metadata was queried in August 2015 and if a metadata instance was found to have
undergone any modifications since the original creation date, the modification status was
categorized as “modified”. If a metadata instance was not found to have undergone
modifications since the creation date, the modification status was categorized as
“unmodified”. A Pearson’s Chi-squared test of goodness of fit was used to determine if
any of the observed years when metadata was created had fewer or greater than expected
datasets that had undergone modifications.

39
3.2.2 Datasets by Author
The frequency of datasets by author was calculated as a contingency table. The
percentage of the total datasets that each author published was also computed. The
authors were further categorized based on whether they were a federal, state, or city
agency. Additionally, the frequency of federal agencies that were established under the
U.S. Department of Health and Human Services (HHS) was calculated.

3.2.3 Duplicate Metadata
The frequency that each author published duplicate metadata for the notes field was
determined. The blank entries for the notes field were omitted from this analysis. The
proportion of duplication for the notes field relative to the total number of datasets was
also calculated for each of the offending authors.

3.3 QUALITY OF HEALTHDATA.GOV METADATA
The quality of the HealthData.gov metadata was evaluated using automated methods. The
following quality measures were evaluated: completeness, accuracy, and consistency.
The quality of the metadata was compared across years based on the date that the
metadata was created and whether the metadata had undergone modifications or not.

3.3.1 Automated Evaluation of Completeness
The completeness of the HealthData.gov metadata was measured in relation to the CKAN
metadata guidelines. CKAN specifies certain metadata fields as primary for describing

40
the healthcare datasets (Table 2.2). According to HealthData.gov [57], these metadata
fields are required for publishing the HealthData.gov datasets.

The most direct approach to measure completeness of an instance is to use the number of
completed metadata fields as a proxy. The basic completeness metric was a count of the
number of primary fields in each metadata instance that contained a no-null value. In the
case of multi-valued fields, such as the required tags field, the field was considered
complete if at least one instance existed. Equation 1 expressed how this metric was
determined.
N

Qcomp

å P(i)
=

i=1

N

(1)

Where P(i) is 1 if the ith field has a no-null value, 0 otherwise. N is the number of fields
defined in the metadata standard. The maximum value of this metric is 1 (in the case all
the fields contain information) and the minimum value is 0 (all metadata primary fields
are empty). Due to a skewed distribution to Qcomp, the Kruskal-Wallis statistical
procedure was used to test the null hypothesis that the medians of each year the data was
created are equal, and the alternative hypothesis that at least one median from one year
the data was created is different from the median of at least one other year. To calculate
pairwise multiple comparisons between each year the Nemenyi post hoc test for multiple
comparisons of mean rank sums of independent samples was used. For each year, the
Mann-Whitney U test was used to compare the median values of Qcomp based on the
independent variables of modification status to test the null hypothesis that for a specific

41
year the median Qcomp for each modification status each are equal, and the alternative
hypothesis that the median Qcomp is different based on each modification status.

Table 3.1. Weights for Qwcomp measure.
Field

Weights
(α)

Description

id

0

The unique identifier for the dataset

title

1.0

The title name for the dataset

notes

1.0

The description of the dataset

notes_rendered 0

The description of the dataset rendered in HTML
using Markdown

author

0.5

The name of the federal agency that submitted the
dataset

url

1.0

The URL to the home page for the dataset, which
may link to downloadable files

tags

0.5

Tags associated with the dataset

While straightforward, the simple completeness metric does not reflect how humans
measure the completeness of an instance. Not all data elements are relevant for all
resources. Moreover, not all metadata elements are equally relevant to all contexts. To
account for this phenomenon, a weighting factor was multiplied to the presence or
absence of a metadata field. This factor represents the importance of the field. Table 3.1
displays the weights that were assigned for the primary metadata fields.

42
This weighting factor can easily be included in the calculation of the completeness
metric as shown in the Equation 2.
N

Qwcomp

=

å P(i)*a

i

i=1

N

åa

i

i=1

(2)

Where αi is the weight of the ith field, which are specified in Table 3.1. The maximum
value for Qwcomp is 1 (all fields with an importance weight different from 0 are filled)
and a minimum value of 0 (all fields with an importance weight different from 0 are
empty). The α was a positive value that represented the importance (or relevance) of the
metadata field for some context or task (Table 3.1).
The weighted completeness measures the completeness of a metadata instance against the
current needs of a community. Therefore some fields that currently are not important for
the community were given a value of zero and have no impact in the Qwcomp metric
(i.e., id, notes_rendered). The Kruskal-Wallis statistical procedure was used to test if the
median Qwcomp score of each year the data was created are equal or not. To calculate
pairwise multiple comparisons between each year the Nemenyi post hoc test for multiple
comparisons of mean rank sums of independent samples was used. For each year, the
Mann-Whitney U test was used to compare the median values of Qwcomp based on the
independent variables of modification status.

3.3.2 Automated Evaluation of Accuracy
Accuracy is the degree to which metadata values are “correct” (i.e. how well they
describe the object). For objective information, such as the correct data type, the output is

43
a binary value, either “right” or “wrong”. In the case of subjective information, such as
the notes or title, the evaluation of correctness is a more complex process.

Although humans are able to assess the accuracy of a metadata instance with relative
ease, computers require complex artificial intelligence algorithms to simulate the same
level of understanding. Nevertheless, there exists accuracy metrics that are easy to
calculate, such as typographical errors and broken links. These metrics establish the
number of easy-to-spot errors present in metadata instances. Table 3.2 shows the types of
errors that were assessed for each metadata field.

Table 3.2. Type of errors evaluated for accuracy of the metadata.
Field

Description

id

NA

title

Typographical errors

notes

Typographical errors

notes_rendered

NA

author

Typographical errors

url

Broken links

tags

Typographical errors

44
Typographical errors were determined using the GSpell SPECIALIST Spelling
Suggestion Tool [58]. GSpell is a spelling suggestion tool that uses a variety of
algorithms to retrieve close neighbors. The algorithms include the NGrams, metaphone,
common misspellings, and homophone retrieval tools. The Levenshtein edit distance is
used to determine correct candidate terms, and similarly ranked candidates are reordered
by use of word based corpus frequencies. GSpell is run through a Java API and shell
scripts are provided to process a tokenized list of terms.

The metadata had to be tokenized into individual words to be processed by GSpell.
Tokenization was achieved in the R statistical software package by first removing
punctuation from all metadata instances in a metadata field element. Next, any extra
white space beyond a single space was removed from each metadata instance for each
metadata field element type. A for-loop statement was used to split each word in each
metadata instance for each metadata field element type by splitting strings based on the
location of a single space. In addition, the for-loop assigned an ID number to each
metadata instance, which was paired with each tokenized individual word found in the
dataset. For each metadata field element, the tokenized individual words that were found
in that metadata field were exported as a CSV file. This CSV file was imported into
GSpell and using shell scripts was processed for misspellings. The GSpell output
included an ID for each individual tokenized word, the original word, suggested
candidate words, the ID of the metadata instance where the word was located, the
Levenshtein edit distance (0 indicated an exact match), the ranking of candidate terms

45
(1.0 indicated the most likely match), the method used to find candidate terms, and a
message as to whether the word was spelled correctly. The GSpell output was written to a
CSV file and imported into R for further analysis.

The total number of tokenized individual words was determined by counting the number
of times a unique word ID occurred. The total number of misspellings was determined by
counting the frequency that the rank was listed as “NA” or that the number one ranked
candidate term had a Levenshtein edit distance greater than 0. The overall count of
misspellings for each of the metadata field elements that are shown in Table 3.2 was
determined. In addition, the proportion of words with typographical errors in each of
these data fields was calculated. The proportion was determined by taking the number of
misspellings in a data field divided by the total number of words found in that data field.

Links were classified as broken or working using the urllib.request python library, which
uses the online markup validation service by the World Wide Web Consortium (W3C) to
check the syntax and connection of HTML documents. URLs had to pass a preliminary
syntax test to verify the common elements of an HTML file extension were included
before the connection was assessed. Following the syntax test, the URL content was
queried. The prevalence of HTTP response status codes was recorded for each URL. A
response code of 200 indicated a successful URL. A responses code in the 400 and 500
ranges indicated URLs with errors. The frequency of the error response codes were
summed together and reported as “broken links.” The proportion of URLs that resulted in

46
errors was calculated. Only metadata instances where a URL was provided were
included in the calculation. The proportion of URL errors that occurred in modified and
unmodified metadata instances was compared using a two-sample Z-test of proportions.
A three-sample Z-test of proportions was used to compare the proportion of URL errors
across each of the three years the metadata instances were created. In addition, the
number of broken links for each of the metadata creation years and the modification
status was also compared using a Pearson’s chi-squared test.

Another measure of accuracy is the semantic distance between the information a user
could extract from the metadata instance and the information the same user could obtain
from the resource itself and its context: the shorter the distance, the higher the accuracy
of the metadata instance. Ochoa and Duval [26] proposed an approach to calculate the
semantic difference between the metadata instance and resources that contain textual
information. Using the metadata and the resource can provide a better estimation of the
accuracy of the record than just counting the number of errors in the metadata. This
method builds upon Vector Space model techniques used in Information Retrieval to
calculate the distance between texts [59]. Formula 3 presents the method used to measure
semantic distance. A multi-dimensional space is constructed in which each word present
in the text of the original resource defines a dimension. The source of the information for
the HealthData.gov metadata is found on the webpage of the respective agency that
authored the dataset. Source information was derived using the URL metadata field that
was specified for each dataset using the BeautifulSoup Python Library. Using the

47
urllib.request python library only valid URLs were included in the analysis, as
determined by an HTTP status code of 200 (i.e., indicating a successfully working URL).
BeautifulSoup was used to query the contents of the HTML files by working with a
parser to provide idiomatic ways of navigating, searching, and modifying the parse tree.
All of the content located on the specified URL landing page was used as the original
resource text.

The tm R package was used for processing the text for each metadata instance and the
corresponding URL landing page text. All of the text was transformed to lowercase,
punctuation was removed, the resulting white space was stripped, and common English
words (i.e., stopwords) were removed from the text. Two vectors were created based on
the distinct words that appeared in the text of the original resource and the text of the
metadata instance, respectively. The vectors were preprocessed before measuring the
distance by removing any words that are not found in biomedical text using the
biomedical dictionary GSpell. Any word that was not recognized by GSpell was removed
from the vectors. The reason this was done is to remove aberrant words that may have
been a result of text that is embedded within URLs found on the source page (e.g., .html,
https). The semantic distance was determined based on the comparison of nouns that
appear in the metadata instance that also occurred in the original resource text. A rule was
generated that created a binary output where 1 indicated that a word appeared in the
metadata text and also appeared in the source text while 0 indicated a word appeared in
the metadata text and not the source text. A vector distance metric is applied to find the

48
quantitative semantic distance between both texts. In Equation 3 the distance formula
is presented.
N

Qaccu

=

åtfresource
tfmetadata
i=1

i

i

(3)

Where tfresourcei is the binary output that indicated whether a word appearing in the
metadata text also appeared in the source text of the ith dataset and tfmetadatai is the
relative frequency of nouns of the ith dataset. N is the total number of distinct words in
the metadata text.

The minimum value (lower quality) is 0, meaning that the text in the metadata does not
exist in the source text. The maximum value (higher quality) is 1, meaning that all the
words from the metadata appear in the source text. Therefore, a value closer to 1
represents metadata that offers a more accurate description of the actual resources.

To validate the Qaccu scores, 2 human reviewers were used to independently assess the
level of agreement between the metadata and source text using a manual review process.
The metadata was presented to the reviewers and the content on the URL was presented
to the reviewers. The reviewers were asked to rate the level of agreement between the
information that was presented in the URL to the information present in the metadata.
The ratings were captured on a 7-point Likert scale from absolutely inappropriate to
absolutely appropriate. The ratings were converted to a percentage by dividing the ranked
response by 6, with the lowest ranked response equal to 0 and the highest to 1.

49
An initial calibration session was conducted between the two reviewers to evaluate
inter-rater reliability for rating the agreement of the two data sources. In the calibration
session, each reviewer reviewed the same 20 datasets. These datasets were randomly
chosen. The reviewer’s responses were assessed using an intraclass correlation
coefficient (ICC) as an index of interrater reliability of quantitative data. ICC was used to
measure interrater reliability, rather than Cohen’s kappa, because the variables were
ordinal in nature [60]. Additionally, F-test and confidence interval were computed.
Following the initial review, the reviewers discussed the rationale for their independent
ratings. Minor adjustments to their method of independently rating the level of agreement
between the source and metadata were made by making a consensus.

Once the ICC reached a value greater than 0.8, each reviewer independently reviewed a
random sampling of 100 different datasets. There were a total of 220 manually reviewed
datasets including the overlapping 20 datasets. To determine if the reviewer’s ratings
corresponded to the semantic distance equation, ICC was used to measure reliability
between the Qaccu scores and the mean reviewer scores. The Kruskal-Wallis statistical
procedure was used to test if the median Qaccu score of each year the data was created
are equal or not. To calculate pairwise multiple comparisons between each year the
Nemenyi post hoc test for multiple comparisons of mean rank sums of independent
samples was used. For each year, the Mann-Whitney U test was used to compare the
median values of Qaccu based on the independent variables of modification status.

50
3.3.3 Automated Evaluation of Consistency
The logical consistency of a metadata instance can be estimated as the degree to which it
matches metadata standard definitions. There were two consistency issues that were
measured: 1) Whether the fields of the metadata instances followed the CKAN
requirements for the data type; 2) Whether the fields of the metadata instances followed
the Dublin Core requirements.

The first consistency issue was measured by first determining if each metadata field met
the requirements of the data type as specified by CKAN [61]. There were five different
data types included in the metadata fields including GUID, plain text, HTML text, URL,
and array of text. The GUID field must be in lowercase with hyphens used for separation.
There should be no spaces in the GUID. CKAN required that the field with plain text and
array of text include only Unicode characters. CKAN required that all HTML documents
must use two spaces for indentation and there should be no trailing whitespace. The URL
field should include a reference to a webpage indicated by the inclusion of “http”, must
specify the domain name, and the specific web page [61]. A complete list of the number
of rules evaluated is shown in Table 3.3. Given that some rules were evaluated for more
than one metadata field, there were a total number of 9 rules evaluated as shown in Table
3.3 (note: some rules apply to more than one field). With the exception of the ‘Plain text
characters’ rule, the rules were assessed using regular expressions (regex) in the R
statistical software package using the grep1 function to determine if the format of the
metadata field followed the CKAN requirements. The ‘Plain text characters’ rule was

51
assessed by identifying the metadata instances that raised an exception to UTF-8
encoding. If a character was not encoding in UTF-8, an exception is raised by revealing
the following replacement characters “�” or “Â£”. A logical vector was returned (i.e.,
TRUE or FALSE) indicating whether the regex found a match in the corresponding
element in the input vector. The total number and proportion of TRUE values were added
across each metadata field and for each metadata instance.
Table 3.3. CKAN metadata requirements for data types that were evaluated.
Rule

Fields

Definition

GUID case

id

Are fields with GUID data type in lowercase?

GUID spacing

id

Do fields with GUID data type include hyphens
in place of whitespace?

Plain text characters

title,
notes,
author

Do fields with a plain text data type only uses
unicode characters encoded as UTF-8?

HTML characters

notes_re
ndered

Do fields with HTML text as the data type
include two spaces for indentation and no trailing
whitespace?

URL http

url

Do fields with the URL data type include a
reference to a webpage indicated by the inclusion
of “http”?

URL domain

url

Do fields with the URL data type specify the
domain name?

URL page

url

Do fields with the URL data type specify the
specific web page?

52
The consistency metric is equal to 1 minus the average of the fraction of problems
found, for each type of problem. Equation 4 was adapted from Ochoa and Duval [26] and
present the calculation for the type 1 consistency.
N

Qcons

=1-

åbrokenRule

i

i=1

N

(4)

where brokeRule is a binary variable where 0 indicates compliance with a rule and 1
indicates noncompliance with a rule for each ith metadata instance. N is the number of
rules in the metadata standard. The minimum value for the consistency metric is 0 (all
possible errors were made) and the maximum value is 1 (there were no consistency
problems).

An estimation of the consistency of the metadata instance was inversely proportional to
the number of problems found in the instance. That is, the more problems that were
found, the less consistent the data was and vice versa; the fewer problems, the more
consistent the data was. The Kruskal-Wallis statistical procedure was used to test if the
median Qcons score of each year the data was created are equal or not. To calculate
pairwise multiple comparisons between each year the Nemenyi post hoc test for multiple
comparisons of mean rank sums of independent samples was used. For each year, the
Mann-Whitney U test was used to compare the median values of Qcons based on the
independent variables of modification status.

53
The second measure of consistency was determined based on the metadata
requirements set by the DCMI. The consistency measures evaluated the level of
agreement between the HealthData.gov metadata field names and the standard field
names based on the Dublin Core. Also, the agreement with the field properties was
evaluated including whether the HealthData.gov fields properly aligned with the expected
range of values and data type set forth by DCMI. A complete list of the number of rules
evaluated is shown in Table 3.4. Each requirement was assessed by assessing the CKAN
metadata framework and whether the rule was affirmed or negated. Each rule was
constructed based on the standards set forth through The Dublin Core Metadata Element
Set, Version 1.1. There were a total of 9 rules evaluated. A Dublin Core consistency
score was calculated by counting the number of affirmations and negations from each of
the 9 rules.

3.4 INDEXING METADATA WITH MESH
The prevalence of the represented concepts in the metadata was evaluated using a term
coverage study. To determine the concepts that were covered by the metadata, the noun
and noun phrases from the metadata fields were mapped to MeSH. The noun and noun
phrases were determined based on the UMLS Metathesaurus by using MetaMap. The
metadata text was parsed into simple noun phrases using the SPECIALIST minimal
commitment parser [62]. Variants were generated for each phrase using the SPECIALIST
lexicon. A variant consisted of phrase words together with acronyms, abbreviations,
synonyms, derivational variants, and meaningful combinations of these, including

54
inflectional and spelling variants [54]. A candidate set of all Metathesaurus strings
containing at least one of the variants was retrieved. Each candidate was evaluated
against the input text by mapping the phrase words to the candidate’s words and then,
calculating the strength of the mapping using a linguistically principled evaluation
function [47].
Table 3.4. Dublin Core metadata requirements that were evaluated [34].
Rule

Definition

Identifier field

Is the field representing the unambigous reference to the
resource labeled “identifier”?

Identifier content

Is the unique identifier identified by a string conforming to a
formal identification system?

Title field

Is the field representing the name of a given resource labeled
“title”?

Description field

Is the field representing the description of the resource labeled
term “description”?

Contributor field

Is the field representing the entity responsible for making
contributions to the resource labeled “contributor”?

Date field

Do date fields use the encoding scheme W3CDTF profile of ISO
8601?

Source field

Is the field representing the related source from which the
descired resource was derived labeled “source”?

Subject field

Is the field representing the topic of the resource identified by
the label “subject”?

Subject content

Does the field representing the topic of the resource represented
by keywords from a controlled vocabulary?

To determine the overall frequency of all noun phrases that occurred in the
HealthData.gov notes, all UMLS concepts were indexed with MetaMap and the output
was selected to reveal noun phrases, nouns, and acronyms/abbreviations. The

55
MedPost/Semantic Knowledge Representation (SKR) part-of- speech tagger broke the
original notes for each dataset into sentences, tokenized each sentence and then further
tagged tokens as nouns, verbs, prepositions, adjectives, and punctuations. Candidates
were identified from the list of variants for each concept. Once the nouns and noun
phrases were identified, MetaMap provides UMLS concept candidates with concept
unifier identifier (CUI), preferred names and semantic types. All of the mapping results
were printed and exported in a machine-readable output. The machine-readable output
was further processed by extracting nouns from the summary while omitting verbs,
prepositions, adjectives, and punctuation using a regular expression function in the R
statistical software package. The frequency of nouns, noun phrases, and distinct nouns
that was present in the HealthData.gov metadata were then reported.

The complete MeSH library was downloaded as an XML file from the U.S. NLM
webpage [47]. The XML file was parsed using the Python-based Element Tree XML API
to create a Python dictionary of all the MeSH concepts, related concepts, and entry terms.
The dictionary was written to a CSV file and uploaded to the R statistical software
package to calculate MeSH term coverage.

In order to determine the level of coverage that MeSH offered for indexing the
HealthData.gov metadata, the nouns that were extracted from the metadata using
MetaMap were mapped to MeSH entry terms. The nouns were mapped to MeSH by
developing a query in the R statistical software package to search for partial or exact

56
matches between the nouns extracted using MetaMap and the complete MeSH library.
The frequency of partial, exact, and no matches between the MeSH terms and the
extracted MetaMap nouns were calculated. If a noun was contained as part of a MeSH
entry term, it was considered a partial match. A noun that exactly matched a MeSH entry
term was considered an exact match. Any noun that was not part of a MeSH entry term
was considered not matching.

Further analysis was conducted by restricting the comparison of MeSH terms to the
nouns extracted using MetaMap for those nouns that accounted for 75% of all of the
identified nouns. The 75% threshold was determined based on the observation that this
accounted for nouns that occurred at least 10 times in the HealthData.gov metadata. This
was completed in order to evaluate the efficacy of indexing the data with MeSH while
minimizing the effects of nouns that do not occur frequently in the HealthData.gov
metadata. The frequencies of partial, exact and no matches were calculated to determine
the efficacy of MeSH for indexing the most common occurring nouns in HealthData.gov.

A final analysis was carried out to determine the most common MeSH concepts that
occurred in the HealthData.gov metadata. The HealthData.gov metadata was processed a
second time using MetaMap, but restricting the library source to only MeSH. The phrases
that were mapped to MeSH were returned along with the MetaMap summary, which was
exported to show the associated MeSH candidate concepts that map to the noun phrases.
A mapping score was provided that revealed the level of agreement between a noun

57
phrase and the MeSH candidate concept. A mapping score of 1000 indicated an exact
match. In addition to the MeSH concept, the MeSH category was also provided. The
MetaMap summary was uploaded to the R statistical software package and a regular
expression function was used to extract the noun phrases, mapping scores, candidate
MeSH terms, and MeSH categories. MeSH terms were only included in the analysis if the
mapping scores were greater than or equal to 900. The frequency that each MeSH
concept and category occurred in the HealthData.gov metadata was calculated.

Chi-squared tests of independence were used to determine if the frequency of matching
types (partial, exact, and no match) were significantly different between the mapped
nouns and MeSH entry terms. The frequency of the partial matches was calculated as the
number of nouns that matched at least part of a MeSH entry terms. Exact matches were
the frequency of nouns that exactly matched a MeSH entry term. The frequency of no
matches was calculated as the sum of the nouns that did not appear in a MeSH entry term.
Pairwise comparisons, with a Bonferroni correction were also carried out.

3.5 STATISTICAL METHODS
The R Statistical Software package was used to carry out the statistical analyses. The
specific inferential statistical method used for evaluation is explained above for each
specific analysis.

58
The results from these methods are presented in the subsequent chapter and presented
for each specific aim. The findings from the evaluation of the corpus of metadata
available in HealthData.gov are shown first, followed by the results of the quality
assessment, and ending with the results from the assessment of MeSH coverage of the
HealthData.gov terms.

59
CHAPTER 4
RESULTS
The quality of the HealthData.gov metadata was formally evaluated using validated
automated quality assessment methods. Additionally, the coverage of terms offered by
MeSH for representing the concepts in HealthData.gov was formally evaluated. The
results of these analysis is the focus of this chapter. In Chapter 5, these results will be
interpreted with regards to recommendations for improving data quality around
completeness, accuracy, and consistency and offer an indexing solution by adopting a
standard terminology.

4.1 METADATA SUMMARY RESULTS
HealthData.gov began publishing datasets in 2012. There were a total of 290 new
datasets published in 2012, 597 published in 2013, and 745 published in 2014. Of the
1,632 datasets that were published, the metadata for 1,369 (83.9%) of the datasets have
undergone modifications. The reason for the modifications is that the datasets from many
federal agencies are published on a quarterly or annual basis. Therefore, the metadata
may need to be modified to represent the most current data. However, such modification
may impact the quality of the metadata. That is, there may be more or fewer errors in the
metadata following a modification. Table 4.1 compares the year metadata was originally
published and the count of metadata for those years that have undergone a modification
versus those that have not undergone a modification.

60
Table 4.1. Datasets published each year by modification status
Year

Never Been Modified

Been Modified

Total

2012

21 (7%)

269 (93%)

290

2013

46 (8%)

551 (92%)

597

2014

196 (26%)

549 (74%)

745

Total

263

1369

1632

When comparing the count of datasets that have been modified versus those that have not
been modified across the years the datasets were created, the Pearson’s Chi-squared test
revealed significance (χ2=105.4, df=2, p-value<0.001). A test of goodness of fit revealed
that there were greater than expected datasets with modifications for metadata created in
2012 and 2013. However, for 2014 there were fewer than expected datasets with
modifications.

As shown in Table 4.2, the federal agency that published the most datasets on
HealthData.gov is the Centers for Medicare & Medicaid Services, which accounted for
35.4% of all the datasets. The State of NY and the State of IL published the second most
datasets, each contributing to around 9% of all the datasets. The Centers for Disease
Control and Prevention accounted for 6.4% of all the datasets. When broken down by the
category of publisher, federal agencies published 63.2%, state agencies published 30.8%,
and cities published 4.8% of the datasets. There were 1.2% of the datasets with an
unknown category.

61
The frequency of repeated entries for the notes field was calculated. This was
conducted as a measure of metadata duplication. When discounting blank entries, there
were a total of 235 (14.4%) metadata instances where the notes field was repeated in
more than one entry. When examining the agencies that published duplication for the
notes field, CMS was the largest offender followed by the State of NY (Table 4.3).

4.2. QUALITY RESULTS
The results for the quality assessment will be presented based on the completeness,
accuracy, and consistency of the HeathData.gov metadata.

4.2.1 Completeness Results
When evaluating the frequency that key metadata fields were left blank, the title did not
have any blank entries. The author field was left blank in 19 (1.16%) instances. Tags
were left blank in 157 (9.62%) instances. The URL field was left blank in 33 (2.02%)
instances. Finally, notes were blank in 57 (3.49%) instances.

As shown in Figure 4.1, the median Qcomp score from metadata created in 2012 was 1.0
(IQR= 1.0-1.0) for the unmodified metadata and 1.0 (IQR =1.0-1.0) for the modified
metadata. For metadata created in 2012, only 1 (4.8%) unmodified dataset and 20 (7.4%)
modified datasets had a Qcomp score less than 1. The median Qcomp score from 2013
was 1.0 (IQR =0.86-1.0) for the unmodified metadata and 1.0 (IQR =1.0-1.0) for the
modified metadata. There were 17 (37.0%) unmodified datasets and 74 (13.4%) modified
datasets that had a Qcomp score less than 1 from 2013. The median Qcomp score from

62
2014 was 1.0 (IQR =1.0-1.0) for the unmodified metadata and 1.0 (IQR =1.0-1.0) for
the modified metadata. There were 48 (24.4%) unmodified datasets and 56 (10.2%)
modified datasets that had a Qcomp score less than 1 for metadata created in 2014.

When comparing the rank sum differences in Qcomp scores, unmodified metadata had
significantly lower values compared to modified metadata (W = 154590.0, pvalue<0.001) and there were significant differences based on the year the metadata was
created (Kruskal-Wallis χ2= 13.5; df= 2; p-value=0.002). Pairwise comparisons revealed
that metadata created in 2012 had significantly higher Qcomp scores compared to the
years 2013 (p=0.001) and 2014 (p=0.02). As shown in Table 4.4A, when comparing
average ranked sum differences in Qcomp for each year between each modification
status, there were no differences in metadata created in 2012 (W= 2900; p-value 0.65),
but unmodified metadata had significantly lower Qcomp scores than modified metadata
for 2013 (W= 9924; p-value<0.001) and 2014 (W= 45750; p-value<0.001). As shown in
Table 4.4B, when comparing average ranked summed differences in Qcomp for each
modification status between each year, the year 2012 had significantly higher Qcomp
scores for metadata created in 2013 for both the unmodified (W=641.5; p-value=0.006)
and modified metadata (W=79062; p-value=0.005). Metadata from the year 2012 had
significantly higher Qcomp scores than 2014 for unmodified metadata (W=2470.5; pvalue=0.039), but there were no differences in modified metadata (W=75882; pvalue=0.201). Metadata created in 2013 had significantly lower Qcomp scores than 2014
for modified metadata (W=144940; p-value=0.032), but no differences were seen for
unmodified metadata (W=3948.5; p-value=0.091).

63
Table 4.2. Frequency of datasets published by U.S. health agencies.
Author
Centers for Medicare & Medicaid Services
State of New York
State of Illinois
Centers for Disease Control and Prevention
Administration for Children and Families
U.S. Food and Drug Administration
Department of Health & Human Services
State of Hawaii
State of Maryland
City of Chicago
State of California
National Library of Medicine
State of Oklahoma
City of San Francisco
Health Resources and Services Administration
State of Missouri
BLANK
State of Colorado
National Institutes of Health
Substance Abuse & Mental Health Services
Administration
Agency for Healthcare Research and Quality
City of Boston
USDA
Administration for Community Living
Office of Inspector General
State of Oregon
State of Washington
National Cancer Institute
National Institute on Drug Abuse
Office of the National Coordinator
Assistant Secretary for Planning & Evaluation
Indian Health Service
Total

Category
Federal
State
State
Federal
Federal
Federal
Federal
State
State
City
State
Federal
State
City
Federal
State
Unknown
State
Federal
Federal

Freq
578
145
143
104
87
81
56
50
45
40
38
34
33
28
21
21
19
18
14
14

Prop
0.354
0.089
0.088
0.064
0.053
0.050
0.034
0.031
0.028
0.025
0.023
0.021
0.020
0.017
0.013
0.013
0.012
0.011
0.009
0.009

Federal
City
Federal
Federal
Federal
State
State
Federal
Federal
Federal
Federal
Federal

13
10
10
6
6
5
5
2
2
2
1
1
1632

0.008
0.006
0.006
0.004
0.004
0.003
0.003
0.001
0.001
0.001
0.001
0.001
1.000

64
Table 4.3. Frequency of duplicate metadata entries published by U.S. health agencies.
Author
Centers for Medicare & Medicaid Services
State of New York
Administration for Children and Families
State of Hawaii
Department of Health & Human Services
State of Illinois
U.S. Food and Drug Administration
Centers for Disease Control and Prevention
State of California
Administration for Community Living
Substance Abuse & Mental Health Services Administration
State of Maryland
City of Chicago
National Institutes of Health
State of Colorado
Total

Freq
79
28
23
18
17
15
14
13
8
5
5
4
2
2
2
235

Prop
0.048
0.017
0.014
0.011
0.010
0.009
0.009
0.008
0.005
0.003
0.003
0.002
0.001
0.001
0.001
0.144

Table 4.4. Results from Qcomp scores. A) Comparison of Qcomp by modification status
within each creation year; B) Comparison of Qcomp by modification status between each
year.
A.
Metadata
Creation Year

Ranked Sum Diff
Unmodified - Modified

2012
2013
2014

B.
Years
compared
2012 - 2013
2012 - 2014
2013 - 2014

p-value
(95% CI)

0.00007

0.65 (-0.00003, 0.00005)

-0.00003

<0.001 (-0.00002, -0.00003)

-0.00002

<0.001 (-0.00001, -0.00002)

Group

Average Ranked Diff

p-value
(95% CI)

Unmodified
Modified
Unmodified
Modified
Unmodified
Modified

0.00003
0.00002
0.00007
0.00002
-0.00001
-0.00003

0.01 (0.00002, 0.014)
0.01 (-0.00005, 0.00004)
0.04 (-0.00004, 0.00003)
0.20 (-0.0004, 0.00005)
0.09 (-0.00002, 0.00001)
0.03 (-0.00003, -0.00005)

65
Figure 4.1. Dotplot of the distribution and median values (red line) for Qcomp scores
by the year the metadata was created and modification status (n=1,632).

As shown in Figure 4.2, the median Qwcomp score for metadata created 2012 was 1.0
(IQR=1.0-1.0) for the unmodified metadata and 1.0 (IQR =0.88-1.0) for the modified
metadata. For metadata created in 2012, only 1 (4.8%) unmodified dataset and 20 (7.4%)
modified datasets had a Qwcomp score less than 1. The median Qwcomp score for
metadata created in 2013 was 1.0 (IQR =1.0-1.0) for the unmodified metadata and 1.0
(IQR =1.0-1.0) for the modified metadata. There were 17 (37.0%) unmodified datasets
and 74 (13.4%) modified datasets that had a Qwcomp score less than 1 from metadata
created in 2013. The median Qcons score from 2014 was 1.0 (IQR =1.0-1.0) for the
unmodified metadata and 1.0 (IQR =1.0-1.0) for the modified metadata. There were 48
(24.5%) unmodified datasets and 56 (10.2%) modified datasets that had a Qwcomp score
less than 1 when created in 2014.

66
When comparing the average rank sum differences in Qwcomp scores, unmodified
metadata had significantly lower Qwcomp scores compared to modified metadata (W =
153510.0, p-value<0.001) and there were significant differences based on the year the
metadata was created (Kruskal-Wallis χ2= 13.; df= 2; p-value=0.001). Pairwise
comparisons revealed that metadata created in 2012 had significantly higher Qwcomp
scores compared to the years 2013 (p=0.001) and 2014 (p=0.02). As shown in Table
4.5A, when comparing average ranked sum differences in Qwcomp for each creation year
between each modification status, there were no differences in 2012 (W= 2901; p-value
0.65), but unmodified metadata had significantly lower Qwcomp scores than modified
metadata in 2013 (W= 9663; p-value<0.001) and 2014 (W= 45478; p-value<0.001). As
shown in Table 4.5B, when comparing average ranked summed differences in Qwcomp
for each modification status between years, the metadata from 2012 had significantly
higher Qwcomp scores than 2013 for both the unmodified (W=644; p-value=0.005) and
modified metadata (W=79012; p-value=0.005). Metadata created in 2012 had
significantly higher Qwcomp scores than 2014 for unmodified metadata (W=2476; pvalue=0.037), but there were no differences in modified metadata (W=75846; pvalue=0.209). Metadata created in 2013 had significantly lower Qwcomp scores than
2014 for modified metadata (W=144970; p-value=0.033), but no differences were seen
for unmodified metadata (W=3913; p-value=0.074).

67
Table 4.5. Results from Qwcomp scores. A) Comparison of Qwcomp by modification
status within each creation year; B) Comparison of Qwcomp by modification status
between each year.
A.
Metadata
Creation Year
2012
2013
2014

B.
Years
compared
2012 - 2013
2012 - 2014
2013 - 2014

Ranked Sum Diff
Unmodified - Modified

p-value
(95% CI)*

0.00006

0.65 (-0.00006, 0.00004)

-0.00005

<0.001 (-0.00008, -0.00002)

-0.00003

<0.001 (-0.00005, -0.00001)

Group

Average Ranked Diff

p-value
(95% CI)

Unmodified
Modified
Unmodified
Modified
Unmodified
Modified

0.00002
0.00001
0.00007
0.000002
-0.00005
-0.00004

0.005 (0.00004, 0.111)
0.005 (-0.00002, 0.00005)
0.037 (-0.00004, 0.00005)
0.209 (-0.00002 0.00004)
0.074 (-0.00001, 0.00003)
0.033 (-0.00006, -0.00003)

Figure 4.2. Dotplot of the distribution and median values (red line) for Qwcomp scores by
the year the metadata was created and modification status (n=1,632).

68
4.2.2 Accuracy Results
Accuracy was assessed based on the frequency of misspellings, broken URL links, and a
measure of similarity regarding the metadata to the source data.

4.2.2.1 Spelling
The metadata field for the author included 7,755 terms. There was not a single
misspelling discovered in the author metadata field. The metadata field for the tags
included 22,721 terms. The only misspelling was a single tag that occurred in two
different datasets. This tag was “sfclimatehealth”. The metadata field for the notes
included 125,905 terms. Misspellings occurred 161 times. The primary cause of the
misspellings was due to the inclusion of the dataset’s ID in the notes section. The
metadata field for the title included 12,228 terms. Misspellings occurred 4 times. Three of
the misspellings were due to the inclusion of the name of a health program. That is,
“QuitNowTXT” appeared in one title and “SuperTracker” appeared in two different titles.
A legitimate misspelling occurred in a title where two words were combined into a single
word. This particular error included the word “afterschool” rather than the words “after
school”. Misspellings occurred so infrequently that there wasn’t a need to see if there
were statistically significant differences in the frequency of misspellings across years.

69
4.2.2.2 Broken Links
There were 1,601 metadata instances where a URL was provided. Of those, 71 (4.43%)
URLs resulted in an error. The breakdown of the type of errors is shown in Table 4.6.
Table 4.6. Frequency of HTTP Status Codes (Note: status code 200 indicated a working
URL).
HTTP Status
Code
400
403
404
500
504
200
Total

Code Definition
Bad Request
Forbidden
Not Found
Internal Server Error
Gateway Timeout
Success

Frequency
2
7
60
1
1
1503
1601

Percentage of Total
(n= 1601)
0.1%
0.4%
3.8%
0.1%
0.1%
95.6%
100%

When considering the prevalence of HTTP errors and whether the metadata instance had
been modified, in the 241 metadata instances that were not modified, 32 (13.28%) URLs
resulted in an error. In the 1,360 metadata instances that were modified, 39 (2.87%)
URLs resulted in an error. A two-sample Z-test revealed that the difference in proportions
of URL errors was significantly greater for those metadata instances that were not
modified (χ2=52.35, df=1, p-value<0.001).

When considering the prevalence of HTTP errors based on the year the metadata instance
was created, in 2012 there were 290 metadata instances that had a URL included and 29
(10.00%) URLs resulted in an error. In 2013, there were 579 metadata instances that had
a URL included and 6 (1.04%) URLs resulted in an error. In 2014, there were 732
metadata instances that had a URL included and 36 (4.92%) URLs resulted in an error. A

70
three-sample Z-test revealed that the difference in the proportions of URL errors was
significantly different across the three years (χ2=37.38, df=2, p-value<0.001) with 2012
having the greatest proportion of URLs with errors and 2013 having the lowest
proportion.
4.2.2.3 Similarity of Metadata instance to Source
The total number of metadata instances included in the semantic distance analysis was
1,529. A total of 103 metadata instances were removed due to invalid or missing URLs or
completely blank entries to the metadata or source data. The median across all datasets
was 0.96 (IQR=0.73-1.0). The range of Qaccu scores was 0.0 to 1. The distribution of
Qaccu scores is shown in Figure 4.3, which is negatively skewed.

Figure 4.3. Histogram of Qaccu scores across all years the data was published (n=1,529).

To validate the Qaccu scores, 2 human reviewers independently rated metadata instances.
From the calibration phase of the 20 datasets, the ICC statistic for reliability was 0.978,

71
indicating strong agreement between raters (F= 89.7; CI= 0.946-0.991; pvalue<0.001). When measuring reliability between the human reviewers and the Qaccu
scores, the ICC statistic for reliability was 0.30 (F= 1.8; CI= 0.135-0.394; pvalue<0.001), indicating moderate agreement. Further analysis was conducted to
determine where there was the greatest level of disagreement. This was determined by
creating a rule that a Qaccu score greater than or equal to 0.5 indicated high accuracy,
while anything lower indicated low accuracy. The same was done for the human
reviewers, a proportion greater than or equal to 0.5 indicated high accuracy, while
anything lower indicated low accuracy. A cross tabulation of the Qaccu scores and
human reviewers revealed that the greatest level of disagreement occurred when the
human reviewers rated the accuracy as highly accurate while the Qaccu score indicated
low accuracy (23/200= 11.5%). There were fewer disagreements when the human
reviewers rated low accuracy while the Qaccu score indicated high accuracy (8/200=
4.0%). Both the human reviewers and Qaccu scores showed high accuracy in 81%
(162/200) of the datasets and both showed low accuracy in 3.5% (7/200) of the datasets.
This indicates that the Qaccu score offers a conservative estimate of the accuracy of the
metadata for representing the source data when compared to human reviewers.

As shown in Figure 4.7, the median Qaccu score for metadata created in 2012 was 0.77
(IQR=0.50-0.79) for the unmodified metadata and 0.64 (IQR =0.40-0.80) for the
modified metadata. The median Qaccu score for metadata created in 2013 was 0.74 (IQR
=0.43-0.90) for the unmodified metadata and 0.94 (IQR =0.83-1.0) for the modified

72
metadata. The median Qaccu score for metadata created in 2014 was 0.67 (IQR =0.430.90) for the unmodified metadata and 1.0 (IQR =1.0-1.0) for the modified metadata.
When comparing the average sum ranked differences in Qaccu scores, unmodified
metadata had significantly lower Qaccu scores compared to modified metadata (W =
65899.0, p-value<0.001) and there were significant differences based on the year the
metadata was created (Kruskal-Wallis χ2= 368.04; df= 2; p-value=0.001). Pairwise
comparisons revealed that the metadata created in 2012 had a significantly lower Qaccu
scores compared to the years 2013 (p<0.001) and 2014 (p<0.001). Also, the metadata
created in 2013 had a significantly lower Qaccu scores than 2014 (p<0.001). As shown in
Table 4.7A, when comparing average ranked sum differences in Qaccu for each year
between each modification status, there were no differences for 2012 (W= 2279.5; pvalue 0.48), but unmodified metadata had significantly lower Qaccu scores than modified
metadata in 2013 (W= 4457; p-value<0.001) and 2014 (W= 10308; p-value<0.001). As
shown in Table 4.7B, when comparing average ranked summed differences in Qaccu for
each modification status between years, metadata created from 2012 had significantly
lower Qaccu scores than 2013 for modified metadata (W=24984; p-value<0.001) and no
difference in the unmodified metadata (W=249.5; p-value=0.644). Metadata created in
2012 had significantly lower Qaccu scores than 2014 for modified metadata (W=8062; pvalue<0.001), but there were no differences in unmodified metadata (W=1232.5; pvalue=0.858). Metadata created in 2013 had significantly lower Qaccu scores than 2014
for modified metadata (W=75800; p-value<0.001), but no differences were seen for
unmodified metadata (W=2450.5; p-value=0.806).

73
Table 4.7. Results from Qaccu scores. A) Comparison of Qaccu by modification status
within each creation year; B) Comparison of Qaccu by modification status between each
year.
A.
Metadata
Creation Year
2012
2013
2014

B.
Years
compared
2012 - 2013
2012 - 2014
2013 - 2014

Group
Unmodified
Modified
Unmodified
Modified
Unmodified
Modified

Ranked Diff

Unmodified - Modified

p-value
(95% CI)

0.03

0.48 (-0.1, 0.2)

-0.16

<0.001 (-0.25, -0.08)

-0.30

<0.001 (-0.33, -0.25)

Average Ranked Diff

p-value
(95% CI)

-0.05
-0.235
-0.008
-0.313
0.005
-0.042

0.644 (-0.167, 0.118)
<0.001 (-0.273, -0.214)
0.858 (-0.143, 0.147)
<0.001 (-0.364, -0.288)
0.806 (-0.081, 0.129)
<0.001 (-0.056, -0.028)

Figure 4.4. Dotplot of the distribution and median (red line) of Qaccu scores by the year
the metadata was created and modification status (n=1,529).

74
4.2.3 Consistency Results
Consistency was assessed using two methods: 1) the frequency that authors published
metadata that abided by CKAN requirements and 2) the status of CKAN abiding by the
Dublin Core Standard for storing metadata.

4.2.3.1 CKAN Requirements
As displayed in Table 4.8, the GUID field was an alphanumeric string where the
characters were lowercase in 100% of the data instances. There were no spaces in the
GUID, but rather hyphens in 100% of the data instances. CKAN required that the field
with plain text and array of text include only Unicode UTF-8 characters. There were no
exceptions raised when examining UTF-8 character violations. Therefore, 100% of the
title, notes, and author fields were encoded in UTF-8. CKAN required that all HTML
documents include two spaces for indentation and there should be no trailing whitespace.
This was found true in 97.7% of data instances. The reason for the discrepancy was that
the URLs that were left blank violated this rule. The URL fields that included a reference
to a webpage indicated by the inclusion of “http” were found true in 97.7% of data
instances. Again, this rule was only violated when the URL field was left blank. The
domain name of the webpage was included in 97.6% of the data instances. This rule was
violated for blank URL entries as well as two data instances that included an incomplete
URL of only “http://”. Finally, the specific web page was included in 96.2% of the URLs.
This rule was violated for those URL fields that were left blank, as well as those where a
domain name was specified, but the specific webpage was not properly included.

75
Given that some rules were evaluated for more than one metadata field, there were a
total number of 9 rules evaluated. With the exception of the ‘Plain text characters’ rule,
the rules were assessed using regular expressions (regex) in the R statistical software
package using the grep1 function to determine if the format of the metadata field follows
the CKAN requirements. The ‘Plain text characters’ rule was assessed by identifying
which metadata instances raised an exception to UTF-8 encoding. A logical vector is
returned (i.e., TRUE or FALSE) indicating whether the regex could find a match in the
corresponding element in the input vector. The total number of FALSE values was added
across each metadata field and for each metadata instance.

As shown in Figure 4.5, the median Qcons score for metadata created in 2012 was 1.0
(IQR=1.0-1.0) for the unmodified metadata and 1.0 (IQR =1.0-1.0) for the modified
metadata. For metadata created in 2012, only 1 (4.8%) unmodified dataset and 12 (4.5%)
modified datasets had a Qcons score less than 1. The median Qcons score for metadata
created in 2013 was 1.0 (IQR =0.89-1.0) for the unmodified metadata and 1.0 (IQR =1.01.0) for the modified metadata. There were 16 (34.8%) unmodified datasets and 10
(1.8%) modified datasets that had a Qcons score less than 1 for 2013. The median Qcons
score in 2014 was 1.0 (IQR =1.0-1.0) for the unmodified metadata and 1.0 (IQR =1.01.0) for the modified metadata. There were 13 (6.6%) unmodified datasets and 3 (0.5%)
modified datasets that had a Qcons score less than 1 for 2014.

76
Table 4.8. Metadata compliance with CKAN requirements.

Rule

Definition

Proportion
of TRUE
(n= 1749)

GUID case

Fields with GUID data type in lowercase

1.0

GUID spacing

Fields with GUID data type include hyphens in
place of whitespace

1.0

Title plain text

Title field include unicode characters encoded
as UTF-8

1.0

Notes plain text

Notes field include unicode characters encoded
as UTF-8

1.0

Author plain
text

Author field include unicode characters encoded
as UTF-8

1.0

HTML
characters

URL field includes two spaces for indentation
and no trailing whitespace

0.977

URL http

URL field includes a reference to a webpage
indicated by the inclusion of “http”

0.977

URL domain

URL field specifies the domain name

0.976

URL page

URL field specifies the specific web page

0.962

When comparing the average ranked sum differences in Qcons scores, unmodified
metadata had significantly lower Qcons scores compared to modified metadata (W =
162640.0; p-value<0.001) and there were not significant differences based on the year the
metadata was created (Kruskal-Wallis χ2= 6.11; df= 2; p-value=0.05). As shown in Table
4.9A, comparing average ranked sum differences in Qcons for each creation year based
on modification status, there were no differences in metadata created in 2012 (W= 2817;
p-value 0.96), but unmodified metadata had significantly lower Qcons scores than
modified metadata in 2013 (W= 8496; p-value<0.001) and 2014 (W= 50524; pvalue<0.001). As shown in Table 4.9B, when comparing average ranked summed

77
differences in Qcons for each modification status between years, the metadata created
2012 had significantly higher Qcons scores than 2013 for unmodified (W=633.5; pvalue=0.008) and significantly lower Qcons scores than 2013 for modified metadata
(W=721088; p-value=0.031). Metadata created in 2012 had significantly lower Qcons
scores than 2014 for modified metadata (W=70961; p-value<0.001), but there were no
differences in unmodified metadata (W=2102; p-value=0.709). Metadata created in 2013
had significantly higher Qcons scores than 2014 for unmodified metadata (W=3255.5; pvalue<0.001), but no differences were seen for modified metadata (W=149330; pvalue=0.052).

Table 4.9. Results from Qcons scores. A) Comparison of Qcons by modification status
within each creation year; B) Comparison of Qcons by modification status between each
year.
A.
Metadata
Creation Year
2012
2013
2014

B.
Years
compared
2012 - 2013
2012 - 2014
2013 - 2014

Ranked Diff

Unmodified-Modified

p-value
(95% CI)

0.00005

0.96 (-0.00002, 0.00002)

-0.00007

<0.001 (-0.00003, -0.00008)

-0.00004

<0.001 (-0.00001, -0.00004)

Group

Average Ranked Diff

p-value
(95% CI)

Unmodified
Modified
Unmodified
Modified
Unmodified
Modified

0.00005
-0.00002
0.00007
-0.00003
0.00001
-0.00004

0.008 (0.00005, 0.00003)
0.031 (-0.00001, -0.00008)
0.709 (-0.00004, 0.00001)
<0.001 (-0.000006, -0.0000001)
<0.001 (-0.00002, 0.00001)
0.052 (-0.00002, 0.00004)

78
Figure 4.5. Dotplot of the distribution and median (red line) of Qcons scores by the
year the metadata was created and modification status (n=1,632).

4.2.3.2 The Dublin Core Standards
The HealthData.gov metadata followed 3 of the 9 rules set forth by the Dublin Core
standard. Therefore, there was an affirmation rate of 33.3% and a negation rate of 66.6%.
Table 4.10 displays the rules that were affirmed and negated. The most common
negations were to rules related to the naming schema of fields – only 1 (16.7%) of the 6
rules related to field names were affirmations. However, when examining rules related to
the format of the content contained in fields, 2 (66.7%) of the 3 rules were affirmations.

79
Table 4.10. Metadata compliance with Dublin Core standards [80].
Rule

Definition

Answer

Identifier field

Is the field representing the unambigous No, the field is
reference to the resource labeled labeled “id”
“identifier”?

Identifier content

Is the unique identifier identified by a Yes, the string is
string
conforming
to
a
formal made of 32
identification system?
alphanumeric
characters separated
into the following
segments 8-4-4-4-12

Title field

Is the field representing the name of a Yes, the field is
given resource labeled “title”?
labeled “title”

Description field

Is the field representing the description of No, the field is
the resource labeled term “description”?
labeled “notes”

Contributor field

Is the field representing the entity No, the field is
responsible for making contributions to labeled “author”
the resource labeled “contributor”?

Date field

Do date fields use the encoding scheme Yes, the date field
W3CDTF profile of ISO 8601 (YYYY- follows W3CDTF
MM-DDThh:mmTZD)?
standards

Source field

Is the field representing the related source No, the field is
from which the descired resource was labeled “URL”
derived labeled “source”?

Subject field

Is the field representing the topic of the No, the field is
resource identified by the label “subject?
labeled ‘tags’

Subject content

Does the field representing the topic of No, the field is
the resource represented by keywords determined by the
from a controlled vocabulary?
contributor and does
not represent a
controlled
vocabulary

4.3 INVESTIGATING TERM COVERAGE WITH MESH
There were 30,132 nouns identified from 17,677 noun phrases from the 1,003 datasets
using MetaMap, of which 2,975 nouns and 9,772 noun phrases were considered unique.

80
When restricted to the MeSH library, there were a total of 18,839 concepts identified
of which 1,336 were unique. The number of unique MeSH concepts that had a mapping
score greater than or equal to 900 was 947. The 10 most frequent MeSH concepts that
had a mapping score greater or equal to 900 are shown in Table 4.11.

Table 4.11. Top 10 MeSH Concepts from HealthData.gov.
Concepts
Utilization

Frequency
(Percentage)
238 (3.6%)

Medicare

134 (2.0%)

Collections

87 (1.3%)

Report

84 (1.3%)

Programs

83 (1.3%)

File

74 (1.1%)

United States

74 (1.1%)

Medicaid

68 (1.0%)

Measures

61 (0.9%)

Publishing

59 (0.9%)

Table 4.12 shows the frequency of nouns that matched partially, exactly, or did not at all
match a MeSH entry terms. In addition, Table 3 indicates the frequency of these
matching groups when the nouns were restricted to those covering 75% of all nouns,
totaling 480 nouns.

There were significant differences between the matching types (χ2=477.3; df=2; pvalue<0.001). Pairwise comparisons demonstrated that there were significantly fewer

2

81

nouns that did not match to MeSH terms when compared to partial (χ =195.1; df=1; pvalue<0.001) and exact matches (χ2=46.9; df=1; p-value<0.001). There were significantly
more nouns with partial matches to MeSH entry terms compared to nouns that exactly
matched MeSH entry terms (χ2=417.8; df=1; p-value<0.001).

These differences were observed when the nouns were restricted to the top 75%
(χ2=209.0; df=2; p-value<0.001). Pairwise comparisons demonstrated that there were
significantly fewer nouns that did not match a MeSH terms compared to partial
(χ2=185.4; df=1; p-value<0.001) and exact matches (χ2=40.4; df=1; p-value<0.001).
Also, partial matches were significantly greater than exact matches (χ2=66.0; df=1; pvalue<0.001).

Table 4.12. Frequency of matching HealthData.gov nouns to MeSH terms.
Match
Type*
No Match
Partial
Exact
Total

Medical Subject Headings (MeSH)
All Terms
Frequency**
851 (28.6%)
1533 (51.5%)
591 (19.9%)
2975 (100%)

Restricted to top 75%
Frequency**
41 (8.6%)
302 (63.3%)
134 (28.1%)
477 (100%)

*No match is the frequency where the noun is present in the HealthData.gov metadata, but not covered by the MeSH library. A partial
match is the frequency of nouns that partially match a MeSH term. An exact match is the frequency of nouns that exactly match a
MeSH term.
** There were significant differences in the frequency of each of the matching types (df=1; p-value<0.001).

82
Table 4.13. Frequency of HealthData.gov terms by classification that were not covered
by MeSH.
Classification
Document

Freq
12

Miscellaneous

9

Human

8

Temporal
Organization
Equipment
Geography

5
3
3
1

Total

41

Terms*
appeals, chartbooks, datasets, downloads, HTTP,
MDS, MMWR, PDF, ratings, TEDS, toolkit,
website, XML
amounts, arrivals, categories, category,
compendium, equals, listing, million, percentage
authority, beneficiaries, consumers, enrollees,
individuals, intermediaries, panel, residents
January, July, June, month, weeks
bureau, OIG, SNF
dashboard, warehouse, WWW
counties

* HTTP= Hypertext Transfer Protocol; MDS= Minimum Data Set; MMWR= Morbidity and Mortality Weekly Report; PDF=
Portable Document Format; TEDS= Treatment Episode Data Set; XML= Extensible Markup Language; OIG= Office of Inspector
General; SNF= Skilled nursing facility; WWW= World Wide Web

The nouns that did not have a match to a MeSH term when restricted to the top 75% were
classified into various categories based on a proper noun scheme that was previously
developed [63]. The classifications included document, human, miscellaneous, temporal,
organization, equipment, and geography. The document classification covered the
greatest proportion of nouns that did not match MeSH terms (Table 4.13).

The next chapter will offer an interpretation of the results and how the findings contribute
to the advancement of the field of health informatics. Recommendations for developing a
data portal that supports high quality metadata will be explained. Additionally, proposed
additions to MeSH will be offered along with methods for implementing automated
indexing. The chapter will conclude with an explanation of the limitations to the research
and recommendations for future studies.

83
CHAPTER 5
DISCUSSION
The goal of this research was to investigate the quality of the HealthData.gov metadata
and explore the use of the MeSH terminology for indexing publicly available data from
HealthData.gov. Data was first published to HealthData.gov in 2012 and since that time
there has been considerable growth in the number of datasets released. In 2012 there were
a total of 290 datasets published, which grew by 597 in 2013, and 745 in 2014. The
observed growth suggested increased participation by agencies on publishing important
datasets in an open, accessible format. However, there was evidence of existing problems
regarding the quality of the metadata representing the datasets. The metadata is the
foundation of HealthData.gov. The metadata contains the information that is used for
storage and retrieval of the datasets. If problems exist in the metadata, this can impact the
ability to search and retrieve data. Ultimately, consumer participation may wane if users
are unable to easily retrieve data. To date, research has noted a lack of citizen
participation in utilizing the public data [2-5]. The lack of citizen participation has been
attributed to inconsistent access standards. However, until now research has not
addressed the specific causes of inconsistent data standards that may be impacting the
quality of the data [1, 19]. Thus, the overall goal of this research was to evaluate the
quality of the HealthData.gov metadata that is used to represent healthcare datasets, and
explore best practices for storage and retrieval of the data. There were three specific aims
of the research. The first aim was to illustrate the corpus of data available from
HealthData.gov. The second aim was to investigate the quality of the metadata instances

84
in HealthData.gov. The final aim was to evaluate if MeSH is an appropriate
terminology for indexing publicly available healthcare datasets from HealthData.gov.

The remainder of this chapter will explore the results of the dissertation and their
implications across each aim followed by a discussion of the contributions to the field of
health informatics, the limitations of the research, and proposed future research.

5.1 METADATA SUMMARY
The first aim of this dissertation was to illustrate the corpus of data available from
HealthData.gov. The first assessment for this aim was to investigate if there were
differences in the proportion of metadata that underwent modifications when comparing
each of the three years the metadata was created. The metadata that was evaluated was
queried from HealthData.gov in August 2015. At the time of this analysis, modifications
to the metadata could have occurred at any point after the metadata was published to
HealthData.gov. Table 4.1 demonstrated that the majority of the metadata published in
2012 had undergone a modification at some point (92.7%). In metadata created in 2013,
the proportion of modified metadata was also high (92.2%). However, for metadata
created in 2014 there were considerably less metadata instances that had undergone a
modification (73.7%). The reason for low rates of modification of metadata created in
2014 may be attributed to the currency of the data. That is, the data from 2014 was still
too new to undergo a modification. A post hoc assessment of the range of distances
between the creation time and modification time was conducted and summarized for each

85
year. The average time between the creation date of the metadata and the last
modification for data published in 2012 was 548 (sd= 191) days, in 2013 was 467
(sd=122) days, and in 2014 was 206 (sd=133) days. This evaluation demonstrated that the
impact that unmodified data can have on the quality of the corpus of data is important as
a large proportion of the metadata is in an unmodified state for at least a year. Research
has shown that changes to metadata are common and desired. The most common type of
changes are amendments to current metadata instances and populating empty fields [31].
Tarver and colleagues [30] explained that metadata changes lead to improvements in the
quality of the metadata because changes increased completeness of records and offered a
more descriptive summary of the source data.

Another evaluation to address the first aim was to summarize the primary publishers of
the data. Federal agencies published the largest proportion of datasets followed by state
and city agencies (Table 4.2). All federal agencies that supplied data, with the exception
of the United States Department of Agriculture (USDA), are part of the U.S. Health and
Human Services (HHS). Of the 1,032 federal datasets published, HHS accounted for 99%
of all federal datasets. There were also state agencies that published data. When the state
agencies that published data are ranked based upon state population estimates for 2015
[64], California was the largest state, followed by New York, Illinois, Washington,
Missouri, Maryland, Colorado, Oregon, Oklahoma, and Hawaii. New York and Illinois
were the states that published the most datasets followed by Maryland and Hawaii.
Interestingly, Maryland was ranked as the 6th most populous state and Hawaii the 10th

86
most populous state out of the 10 states that published data. Therefore, the population
of the state does not appear to be the driving force to participation with supplying data to
HealthData.gov.

Drees [65] developed a scoring system based on the presence and quality of open data
policies and portals for each state. All of the states that published data on
HealthData.gov, except for Washington and Colorado, have open data policies
established. This is likely a factor contributing to greater participation with publishing
data to the public, but may not lead to publishing the data on HealthData.gov. For
example, several states such as Connecticut, New Hampshire, Rhode Island, and Texas
have open data policies established, but are not publishing data on HealthData.gov. A
common component of the open data policies include a method for disseminating data.
Many states adopted an approach to develop and maintain their own data portals.
Interestingly, all of the states that published data on HealthData.gov also created their
own data portal. Additionally, despite the fact that some states do not have open data
policies established, every state in the U.S. has developed a data portal.

There are several cities that are publishing data on HealthData.gov including Boston,
Massachusetts; Chicago, Illinois; and San Francisco, California. Each of these cities have
executive orders to establish open data policies and procedures [66-68]. There are other
cities, however, that have developed open data policies and adopted portals other than
HealthData.gov including Burlington, Vermont; Philadelphia, Pennsylvania; and South

87
Bend, Indiana [65]. Despite state and some city initiatives to develop these policies and
portals, presently very few are actually publishing their data on HealthData.gov. The
reason is largely unknown and warrants further investigation.

Whether a federal, state, or city agency is publishing public data, open data policies are
incredibly important to outline methods for disseminating data, establish quality
standards, outline a metadata framework, create accessibility standards, and develop
indexing practices. To ensure data portals are designed to publish high quality data that is
searchable and retrievable, these policies are needed. The fact that many states and cities
are publishing data without open data policies stimulates questions regarding data quality,
accessibility, and long-term sustainability of open data initiatives.

As shown in Table 4.2, the agencies participating the most in publishing data on
HealthData.gov includes the Centers for Medicare and Medicaid Services, which was the
primary publisher followed by the State of New York and the State of Illinois. Together,
the top three publishers accounted for 53.1% of all the datasets published in
HealthData.gov. This is significant because the behavior of these agencies can strongly
influence the quality of the metadata. That is, if one of these agencies used imperfect
methods when publishing the metadata, a significant proportion of the datasets in
HealthData.gov will be affected. To determine if these agencies adversely impacted the
quality of the metadata, an analysis was conducted to see how frequently each agency
published duplicate information, which was measured based on the frequency that an

88
agency published the same description in the notes field for more than one entry. As
displayed in Table 4.3, duplicate entries were discovered in 14.4% of the metadata
instances. Interestingly, CMS published 4.8% and the State of New York published 1.7%
of the duplicate metadata instances. These were the top two offenders of metadata
duplication. The State of Illinois ranked as the 6th greatest offender of metadata
duplication by publishing 0.9% of the metadata duplication. Granted, duplicate metadata
wasn’t the only measure used to assess the quality of the metadata in HealthData.gov.
The quality of the metadata was also assessed for completeness, accuracy, and
consistency. These metrics were compared based on the years the metadata was created
and whether the metadata had undergone a modification.

5.2 METADATA QUALITY
The second aim of this dissertation was to quantitatively assess the quality of the
metadata instances in HealthData.gov. Quality was measured based on the completeness,
accuracy, and consistency of the metadata. These measures were chosen based on the
recommendations of past research [26]. With regards to the quality of the metadata, each
measure of quality was compared between datasets based on the year the metadata was
initially created and whether the metadata had undergone modifications. Therefore, any
impact that unmodified data had on the quality of the data was empirically investigated.

89
5.2.1 Metadata Completeness
With regards to the completeness of the metadata, the fields found to be left blank most
often were tags (9.62%), notes (3.49%), URL (2.02%), and author (1.16%). The tags
represented keywords to describe the dataset. These keywords were used to search for
data based on a topic. With the absence of the tags, there is less information for a user to
find the dataset. The fact that tags were the metadata field with the most missing data is
not surprising as authors are required to identify keywords to describe the dataset.
Research has shown that when authors are required to supply keywords they are often left
blank or are ineffective at describing the source [69, 70]. The notes included the most
content and were used as the basis for describing a dataset. Without the notes included,
there was limited information for understanding the content of a dataset. Importantly, if
the notes were not provided, this would adversely impact the ability to index the datasets
due to insufficient information provided. In fact, studies have found that lower quality
abstracts lead to less readership and difficulty in finding a study [71]. A similar premise
may apply for data whereby lower quality notes due to missing information may lead to
less use of the dataset due to difficulty in finding the data. The URL was the third ranked
field with missing data. The URL is incredibly important, as this is the field that linked
the metadata to the source data. Without the URL, a user was unable to retrieve the actual
dataset thereby completely defeating the purpose of the data portal – retrieval of public
data. Finally, the author field described the agency that published the dataset. If the
author field was left blank, a user was unable to know what agency supplied and
controlled the dataset. Tarver and colleagues [30] suggested that a primary purpose for

90
changing metadata is to add information to blank fields to improve the quality of the
data. Whether blank entries occur more frequently in modified or unmodified metadata
was the purpose of the subsequent analysis.

An analysis was conducted to determine if completeness differed based on the year the
data was published and the modification status of the data using calculated completeness
measures. The calculated measures included the Qcomp and Qwcomp. The Qcomp was
measured for all metadata instances and calculated based on the proportion of fields that
included data. The Qwcomp was very similar to Qcomp, except metadata fields were
weighted based on the importance of the metadata field for describing the dataset (the
URL, title, and notes were weighted as the most important). Overall, the Qcomp and
Qwcomp scores were relatively high across each year the metadata was created as evident
by the median value of 1.0 for each year. However, there were trends in the analysis that
suggested growing issues around completeness. The same trends were noted for both the
Qcomp and Qwcomp scores (Figure 4.1; Figure 4.2). Metadata that had undergone a
modification at the time of the analysis had significantly higher scores than the
unmodified metadata, which indicated greater completeness of the modified metadata.
Interestingly, the Qcomp and Qwcomp scores for the metadata that was left unmodified
since being published in 2012 were higher than the unmodified metadata published in
2013 and 2014 , and no differences were present in these scores between the years 2013
and 2014. Additionally, metadata that had undergone a modification had higher Qcomp
and Qwcomp scores for metadata created in 2012 and 2014 compared to 2013, with no

91
differences in the modified metadata scores between 2012 and 2014 (Table 4.4; Table
4.5). These findings were significant as they revealed a growing trend of lower
completeness in metadata created in recent years. That is, given the fact that the majority
of the metadata created in 2012 had undergone a modification, the completeness score
should be comparable to metadata created in 2013 where the majority of the metadata
also underwent modifications. Given there was a lower proportion of metadata created in
2014 that were modified when compared to other years suggested the possibility that as
more metadata from 2014 is modified there will be a less than desirable completion
score. This prediction is based on the observation that for creation years where the
majority of the metadata had undergone a modification there is declining completion
rates. That is, when comparing the modified metadata that originated in 2013 there were
lower rates of completion than modified metadata that originated in 2012. This prediction
is further supported by the fact that the completeness scores were very similar for the
unmodified metadata created in both 2013 and 2014.

5.2.2 Metadata Accuracy
The second method for assessing the quality of the metadata was related to accuracy.
Accuracy was measured by evaluating the frequency of misspellings, broken links, and
the similarity of the metadata to the source data. Misspellings were very infrequent. The
author, tags, notes, and title included a total of 168,609 terms and there were only 167
misspellings (0.1%). Therefore, misspellings did not greatly impact the quality of the
metadata. When examining the URL field for broken links (Table 4.6), there were a total

92
of 71 URLs that resulted in an error (4.43%). The majority of these errors were a
HTTP status code of 404 where the URL was not found. This occurred when the URL
was not properly updated with changes to the source webpage, which is supported by the
observation that HTTP errors occurred more frequently in the metadata that had not
undergone modifications (13.28%) versus the metadata that had undergone modifications
(2.87%). Also, the metadata that was created in 2012 had the highest frequency of URL
errors (10.0%) followed by 2014 (4.92%) and then 2013 (1.04%). The reason why there
were more HTTP errors for 2014 when compared to 2013 is likely due to the fact that
there were a greater number of unmodified metadata instances from 2014.

To better examine the accuracy of the metadata, an evaluation was conducted to
determine how well the metadata represented the source data. That is, determine if the
metadata offered a good representation of the actual datasets the publisher intended to
share with the public. The Qaccu score is a quality metric that was calculated for each
metadata instance and demonstrated how well the metadata represented the source data.
The information regarding the source data was derived from text that was available on the
website supplied through the URL for each metadata instance. The score was based on
whether the words from the metadata instance appeared in the source text. A Qaccu score
of 1 represented complete coverage (i.e., all metadata text appears in source text). A
Qaccu score of 0 meant the text in the metadata did not exist in the source text. Overall,
the median Qaccu score across the metadata instances was high (0.96); however, there
was considerable variation in the overall scores, which ranged from 0 to 1 (Figure 4.3).

93
As displayed in Table 4.7, when comparing the years the metadata were created and
the modification status, it was found that based on the average ranked sum differences,
Qaccu scores significantly increased each year metadata was created, with the year 2014
having the highest Qaccu scores and 2012 the lowest. There wasn’t a difference in the
Qaccu scores in the unmodified metadata instances across each creation year, but there
were significant differences in the Qaccu scores for the modified metadata instances. The
unmodified metadata consistently had lower scores than the modified metadata. The
metadata that had undergone a modification and originated in 2012 had the lowest Qaccu
scores, while the metadata that was modified and originated in 2014 had the highest
Qaccu scores (Figure 4.4). Together, these findings suggest that the modified metadata
offered a better representation of the source data compared to the unmodified metadata;
however, as the metadata aged the accuracy of the modified and unmodified metadata
decreased. Given the observation that there was a lower proportion of the metadata that
was modified when published in 2014, there was an increased risk of lower accuracy of
the metadata. In addition, the metadata from the older metadata instances appeared to
decrease in accuracy over time, which was most likely due to outdated metadata (Figure
4.4). Therefore, the long-term sustainability of offering accurate metadata to represent the
source data is questionable. Based on the data currently available, evidence suggested
that the accuracy of the metadata is likely to decrease overtime. To remedy this situation,
frequent updates to the metadata through further modifications is recommended.
Research supports this recommendation by showing improvements in the quality of the

94
metadata with modifications [30, 31]. However, to support changes to metadata a
digital framework needs to be adopted to support various types of changes.

5.2.3 Metadata Consistency
A final quality evaluation of the HealthData.gov metadata included a comparison of
consistency. The consistency was measured based on the requirements specified by
CKAN, the data portal where the HealthData.gov was published, as well as the standard
requirements defined by the Dublin Core, a standard for metadata design and
infrastructure. The CKAN requirements were closely followed, with the lowest
compliance in the URL field (Table 4.8). CKAN required that the source data URL
include a domain name and the specific web page. In 2.4% of the metadata instances the
URL lacked a domain name, which was largely due to blank or incomplete URLs. In
3.8% of the metadata instances the URL lacked a specific webpage. Therefore, the
domain name may have been included but because the specific webpage was not part of
the URL there was not a direct link to the data source. Users had to navigate through the
webpage from the provided URL in order to find the source data. If only the domain
name was provided and specific web page was omitted, additional navigation was
required to find the source data.

The consistency metric, Qcons, was calculated for each metadata instance based on the
compliance with the 9 CKAN requirements for the key metadata fields. Overall, the
Qcons scores were quite high, with the majority of datasets consistently following the

95
CKAN requirements. However, there were trends to suggest declining consistency
overtime with these requirements. Overall, unmodified metadata had significantly lower
Qcons scores compared to the modified metadata that originated in 2013 and 2014. This
suggests that as data is modified there is greater consistency with the CKAN
requirements. The Qcons score for the unmodified metadata was significantly higher in
metadata created in 2012 when compared to the unmodified metadata created in 2013.
Additionally, the unmodified metadata from 2013 had a significantly higher Qcons score
compared to the unmodified metadata that was published in 2014 (Table 4.9). Together,
these findings suggested that the unmodified metadata were less consistent in following
CKAN requirements for the key metadata fields and this consistency is increasingly
getting worse in recent years. However, with modifications to the metadata, there
appeared to be greater conformance with the CKAN requirements thereby increasing
consistency (Figure 4.5).

When measuring the consistency of the key metadata fields with the Dublin Core
standards there was low compliance (Table 4.10). Largely, the field names had violated
the naming schema outlined in the Dublin Core standards, where only one of the six key
metadata fields (i.e., title) followed the standard. Regarding the formatting of the data, the
metadata did conform closer to the Dublin Core standards. That is, the unique identifier
and date were formatted appropriately; however, there was not a field representing the
topic of the metadata using a controlled vocabulary. Together, the findings regarding
discordance between the HealthData.gov metadata and the Dublin Core standards is

96
concerning given the fact that the Dublin Core was developed to offer generic
metadata standards and is the most widely adopted vocabulary for a the Resource
Description Framework (RDF). RDF is a family of World Wide Web Consortium (W3C)
specifications originally designed as a metadata data model enabling resource discovery
and cataloguing to enable users to deal with information with efficiency and certainty
[72]. Of particular interest is the role of a controlled vocabulary for representing the topic
of metadata to help facilitate the search and retrieval of results. Although HealthData.gov
included tags (i.e., keywords) as a required metadata field, these tags did not conform to a
controlled vocabulary. The authors that published the data on HealthData.gov had the
liberty to choose keywords that may have described the dataset. Without adopting a
controlled vocabulary, HealthData.gov does not support hierarchical search strategies
where a search term is mapped to synonymous terms to offer greater specificity of
searches. Given the lack of adopting a controlled vocabulary in the current metadata, a
subsequent assessment was carried out to determine the viability of Medical Subject
Headings (MeSH) for representing the HealthData.gov concepts. This was the focus of
the final aim of the dissertation.

5.3 TERM COVERAGE WITH MESH
The final aim of this dissertation was to evaluate if MeSH is an appropriate terminology
for indexing publicly available healthcare datasets from HealthData.gov. A study was
conducted to determine the coverage of terms offered by MeSH when mapped to the
nouns found in the HealthData.gov metadata.

97
As shown in Table 4.12, the results of this aim demonstrated that there were
significantly greater nouns that partially or exactly matched MeSH entry terms than
nouns that didn’t have any matches to MeSH entry terms. These results demonstrated that
MeSH may offer adequate coverage of the concepts that are present in the
HealthData.gov metadata. Although additional research is needed, this study suggested
that MeSH may be a suitable controlled vocabulary to adopt for indexing HealthData.gov
datasets.

Thibault and colleagues [73] discussed the need to develop data models and dictionaries
that can support sharing of biomedical data. They suggested that the development of
terminologies and ontologies could support this goal. Based on the results of the present
experiment regarding term coverage, MeSH has the potential to support data sharing and
data searching using HealthData.gov. However, in order to adopt MeSH any gaps in the
terminology need to be addressed to offer the most complete and accurate indexing.

Although MeSH partially or exactly matched approximately 91.4% of the most
commonly occurring nouns in HealthData.gov, there is an opportunity to offer greater
coverage by expanding the MeSH terminology. In particular, Table 4.13 displays the
terms that were used to represent topics that were not covered by MeSH. Terms related to
documents were the largest category of terms where there was not coverage. This
included terms such as appeals, chartbooks, datasets, downloads, hypertext transfer
protocol (HTTP), minimum dataset (MDS), morbidity and mortality weekly report
(MMWR), portable document format (PDF), ratings, treatment episode data set (TEDS),

98
toolkit, website, and extensible markup language (XML). Given these gaps, there is an
opportunity to expand the MeSH terminology further to ensure effective and complete
coverage to index datasets published in HealthData.gov.

5.4 CONTRIBUTIONS TO HEALTH INFORMATICS
This section will explore the contributions that this dissertation has to the field of health
informatics. First, formal recommendations will be suggested regarding methods for
improving the quality of the metadata for publicly available healthcare data including
improvements to a data portal and expanding the MeSH terminology.

Second, the

methods for implementing these recommendations will explored.

5.4.1. Formal Recommendations
The first recommendation is regarding requirements of implementing data portal to
facilitate the sharing of high quality metadata will be explored. These requirements may
be implemented using CKAN or a new data portal application could be designed.
Additionally, recommendations regarding the expansion of MeSH for indexing publicly
available healthcare data will be explored.

5.4.1.1. Data Portal Requirements
Based on the results of this dissertation, there is a strong need to update the CKAN
platform or adopt a novel data repository application to address the quality issues of the
HealthData.gov metadata. The Centers for Disease Control and Prevention (CDC)
developed the Unified Process (UP), which offers a framework, methodology, and tools

99
to assist project managers and teams in following best practices in project management
[74]. The UP was developed based on Federal regulatory requirements, the HHS
Enterprise Performance Life Cycle (EPLC) framework, requirements of CDC's
Information Resources (IR) Governance processes, Program Management, Project
Management, Product Development, and other relevant standards, policies, and
requirements. The UP is a collection of practices, processes, tools, artifacts, and
information that any project can use to structure, track, and manage activities and
deliverables to achieve high-quality outcomes. Under the UP, the CDC developed
templates, which are standardized project management documents that project teams can
use as a starting point for their own project management documents, customizing them to
meet the unique needs of the project. Each template includes content commonly used in
such a document, boilerplate text, and instructions to the author to assist them in
completing and adapting the template for use on their project. To establish system
requirements for a data repository application that address the quality issues the currently
exist with HealthData.gov, the CDC UP functional and non-functional system
requirement templates were adopted [75]. As shown in Table 5.1, the specific categories
and description where functional and non-functional requirements were specified is based
from the CDC UP template guidelines. The identified quality issues discovered from the
research conducted in this dissertation offered a rationale for the development of the
functional and non-functional requirements. Closer examination of functional and nonfunctional requirements of the system is required in order to accommodate consumers
and researchers in finding and retrieving relevant health data.

100
A functional requirement relates to something the system must be able to do while a
non-functional requirement relates to performance characteristics for a system [75].
Regarding, the functional requirements, the system that is being described is the data
repository. Currently, CKAN is the software used to manage and store the data. However,
CKAN or a related software platform may be used to satisfy these requirements.

CKAN [18] and HealthData.gov [16] identified the intended audience for the data portal,
which was the basis for developing the following functional requirements. The functional
requirements need to be specified for two groups of users, the publishers of the data and
the users of the data. The publishers may include federal and local governments, research
institutions, and other organizations. For the publishers of the data, HealthData.gov [16]
infers the following functional requirements:
Manage and publish collections of data
Make the dataset description complete, accurate and consistent
Describe the datasets using keywords (Tags) that are mapped to MeSH
The users of the data may include web developers, journalists, researchers, nongovernment organizations, government organizations, or citizens. For the users of the
data, HealthData.gov [16] infers the following functional requirements:
Search for datasets with standard terminology
Filter datasets by certain criteria
Preview datasets using maps, graphs and tables

101
Table 5.1. Recommendations for non-functional and functional system requirements
for managing metadata at HealthData.gov. Template adapted from CDC [75]. Shading
indicates author’s contributions.
Category

Description

Software

Describe software
requirements and any
related processes.
Include a detailed
description of specific
software requirements
and associate them to a
specific functionality

Performance

Support

Describe performance
requirements and any
related processes.
Include a detailed
description of specific
performance
requirements and
associate them to a
specific project
functionality.
Describe all of the
technical requirements
that affect supportability
and maintainability such
as coding standards,
naming conventions,
maintenance access, and
required utilities.

Non-Functional
Requirement
Require all
metadata fields
are included
Tags should be
mapped to MeSH
entry terms

Search results
should be
immediate and
should not
contain irrelevant
datasets

Dublin Core
Standards

Functional
Requirement
Manage and
publish
collections of
data
Describe the
datasets using
Tags that are
mapped to a
standard
terminology;
Search for
datasets
Search for
datasets

Manage and
publish
collections of
data
Index data using
Describe the
MeSH entry terms datasets using
Tags that are
mapped to a
standard
terminology

102
Table 5.1. Recommendations for non-functional and functional system requirements
for managing metadata at HealthData.gov. Template adapted from CDC [75]. Shading
indicates author’s contributions. (continued)
Category

Description

Security

Describe all of the
technical requirements
that affect security such
as security audits,
cryptography, user data,
system
identification/authentica
tion, and resource
utilization.
Describe all of the
technical requirements
that affect interfaces
such as protocol
management, scheduling,
directory services,
broadcasts, message
types, error and buffer
management, and
security
Describe all of the
technical requirements
that affect availability
such as hours of
operation, level of
availability required,
downtime impact, and
support availability.

Interface

Availability

Non-Functional
Requirement
The publisher
should be
authenticated in
order to submit a
dataset

Functional
Requirement
Manage and
publish
collections of
data

The interface
should indicate
required fields

Manage and
publish
collections of
data
Preview
datasets using
maps, graphs
and tables

The interface
should present
the source data

The website
should be
available at all
times to users

Manage and
publish
collections of
data; Describe
the datasets
using Tags that
are mapped to a
standard
terminology;
Search for
datasets;
Preview
datasets using
maps, graphs
and tables

103
In order to realize the functional requirements, the system must have certain
characteristics (i.e., non-functional requirements) to perform successfully. The nonfunctional requirements need to address the quality concerns to ensure the metadata is
complete, accurate, and consistent. As shown in Table 5.1, the non-functional
requirements are characterized based on whether they are related to software,
performance, supportability, security, interface, or availability requirements [75]. The
non-functional requirements are also related to a functional requirement. To ensure
complete metadata, a non-functional requirement can include mandatory fields. That is, it
is recommended that a metadata instance cannot be submitted to HealthData.gov, unless
specific fields are completed. To address the accuracy concerns the metadata should be
published using a standardized terminology for indexing purposes. In addition, the
publisher of the dataset should verify the accuracy of the data at the time of the initial
publication and on a quarterly basis. The system could incorporate a quality assurance
process using an automated auditing tool to determine if submitted metadata meet the
minimum quality requirements to ensure the data is complete, accurate, and consistent.
To satisfy the consistency concerns, the metadata fields should follow the Dublin Core
standards for the field names and formats. This includes not just the key metadata fields,
but all metadata fields should abide by Dublin Core standards. ClinicalTrials.gov
currently uses a quality audit where incomplete or inadequate submissions are denied and
the submitter is informed of the identified issues [76]. The submitter needs to address the
issues with the submission before the data will be posted to the public. A similar
approach can be adopted by HealthData.gov.

104
At present, publicly available healthcare data is difficult to find and retrieve. The
reason for such difficulties may be related to poor data quality and inconsistent use of
standards. Policies and procedures regarding data storage that accompany the adoption of
a data repository application may aid in addressing existing challenges. The intent of the
recommendations in Table 5.1 is to encourage the development or revision of a data
repository application that leads to complete, accurate, consistent metadata and facilitates
a search process by indexing the metadata using a standardized vocabulary.

5.4.1.2. Recommendations for Expanding MeSH
Given the findings from this dissertation that there were gaps in MeSH coverage
regarding certain terms relevant to HealthData.gov, there is a requirement to fill these
gaps. The DCMI Type Vocabulary offers a potential framework to support categories of
terms related to documents, which is the category of terms with the greatest gaps in
MeSH covered. The DCMI Type Vocabulary provides a list of approved terms that may
be used to identify the genre of a resource [77]. The terms are categorized as a class and
are included in the more comprehensive Dublin Core Metadata Initiative (DCMI)
Metadata Terms. The DCMI Type Vocabulary includes the following classes: collection,
dataset, event, image, interactive resource, moving image, physical object, service,
software, sound, still image, text. For long-term sustainability of the MeSH terminology
for indexing HealthData.gov metadata, further expansion of the terminology to support
document terms is recommended. The recommendation is to use the DCMI Type
Vocabulary as MeSH concepts and the examples listed in the DCMI comments as MeSH
entry terms. These entry terms can be further expanded to include nouns that were

105
identified as current gaps in MeSH. Table 5.2 specifies the proposed concepts, entry
terms with a summary. These proposed concepts are based on the DCMI Type
Vocabulary.
Table 5.2. Proposed MeSH concepts, entry terms, and summaries to fill the gap in terms
to describe HealthData.gov datasets. Concepts and summaries were abstracted from
DCMI, and the entry terms were added by the author [77].
Concepts
Dataset

Entry Terms
Lists, tables, databases, chartbooks,
minimum dataset, data report,
treatment episode data set, toolkit
Event
Exhibition, webcast, conference,
workshop, open day, performance,
battle, trial, wedding, tea party,
conflagration
Interactive Web pages, applets, multimedia
Resource
learning objects, chat services, virtual
reality environments, dashboard
Moving
Image
Service

Animation, movies, television
program, videos, zoetropes,
simulations

Photocopying service, banking
service, authentication service,
interlibrary loans, web server
Software
C source file, MS-Windows, .exe
executable, Perl script, extensible
markup language, hypertext transfer
protocol
Sound
Music playback file format, audio
compact disc, recorded speech,
recorded sound
Still Image Paintings, drawings, graphic designs,
plans, maps
Text
Books, letters, dissertations, poems,
newspapers, articles, appeals, ratings,
portable document format

Summary
Data encoded in a defined
structure
A non-persistent, timebased occurrence.
A resource requiring
interaction from the user to
be understood, executed, or
experienced.
A series of visual
representations imparting
an impression of motion
when shown in succession.
A system that provides one
or more function.
A computer program in
source or compiled form or
a computing language or
protocol.
A resource primarily
intended to be heard.
A static visual
representation
A resource consisting
primarily of words for
reading

106
5.4.2 Implementing Recommendations
Careful consideration needs to be taken to successfully implement the recommendations
to revise or develop a new framework that can accommodate higher quality metadata and
adopt MeSH as a controlled vocabulary for indexing. The purpose of this section is to
closely examine the recommendations and propose methods for implementation.

5.4.2.1 Metadata Framework to Support Modifications
A metadata framework needs to be able to accommodate high quality standards for
publication and dissemination. Modifications to metadata were shown to significantly
improve the overall quality of the metadata [32]. The recommendations outlined in Table
5.1 would require significant revisions to the current CKAN requirements if they were to
be adopted. Zavalina and colleagues [32] proposed a framework to support changes
within metadata records. The simplified framework developed by Zavalina and
colleagues [32] is illustrated in Figure 5.1 and shows that additions, deletions, and
modifications to metadata need to be supported in a digital repository. An addition may
include adding a new field or subfield to the metadata schema, adding a qualifier or
attribute to an existing field (i.e., XML qualifier), or adding a data value or field instance.
A deletion may include deleting a field, deleting a qualifier or attribute of a field, or
deleting a data value or field instance. A modification may include populating an empty
field, replacing an existing data value or field qualifier/attribute, amending a data value or
field qualifier/attribute, or transposing of data values or field qualifiers/attributes. In
addition, the framework included recommendations for managing modifications
including logging modifications to identify what changes were made [32].

107
The HealthData.gov metadata schema does currently support the described
accommodations for changes (i.e., addition, deletion, modification). However,
HealthData.gov does not keep a historical record of the changes that were made within a
metadata instance, which has been noted as a significant limitation to the CKAN platform
[8]. Rather, in CKAN a historic metadata instance is overwritten by the current metadata
instance and a timestamp is provided that logs when the modification occurred. Any
subsequent modifications are logged with a new date and override the prior modified
metadata and the prior modification timestamp. Therefore, there isn’t a record to
determine what modifications were made or the latency between subsequent
modifications for a single metadata instance. As such, in order to create a data portal that
can accommodate modifications while also supporting the possibility to examine changes
to metadata quality, records of historical metadata need to be retained.

Additionally, to implement the system recommendations described in Table 5.1, quality
assessment needs to be inherent in a system. When a publisher adds new metadata
instances or modifies existing metadata instances, quality metrics must inform publishers
of concerns regarding completeness, accuracy, and consistency. If certain standards are
not met, the publisher would be required to revise the metadata entry before a submission
can be made. The adoption of such as system would require significant engineering to
construct the automated quality measures into the data entry process. Additionally, a
system needs to accommodate MeSH for indexing purposes. To facilitate more rapid
indexing, automated methods should be explored as an option for implementation.

108
Figure 5.1. Framework of metadata changes to support knowledge management.
Simplified from Zavalina and colleagues [32].
New
field/subfield
Addition

Qualifier/attrib
ute to existing
field
Data
value/field
instance
Field

Metadata
Changes

Deletion

Qualifier/attrib
ute
Data
value/field
instance
Population of
empty field
Replacement

Modification
Amendment

Transposition

109
5.4.2.2 Automated MeSH Indexing
Implementing a system that meets the functional and non-functional requirements
specified in Table 5.1 may help to ensure higher quality metadata is published. In
addition, the adoption and expansion of a controlled terminology to index the metadata
may help improve the search strategies and ultimately elevate the public participation
regarding the use of the health data. However, in order to implement a standardized
terminology such as MeSH, a process needs to be developed to proactively define MeSH
terms for each published metadata instance. There are two implementation processes that
can be adopted to implementing MeSH for data indexing: 1) human cataloguers
determine the appropriate MeSH terms for representing a dataset; 2) automatic methods
are used to determine the appropriate MeSH terms for representing a dataset.

With the adoption of MeSH as a controlled vocabulary for indexing publicly available
healthcare data from HealthData.gov, an automated indexing method may be preferred
given the laborious nature of manual indexing. Alternatively, a combination of both
automated and manual human indexing can be explored. Aronson [56] conducted a
survey with NLM indexers regarding their perceptions on the benefits and use of Medical
Text Indexer (MTI) for cataloguing biomedical literature. Less experienced indexers
reported using the MTI recommendations more often than the more experiences indexers
and found the MTI recommendations helpful. Several indexers expressed that they used
the recommendation for indexing articles that are in areas that they are less familiar with.
A significant number of indexers (75%) reported not being confident on the automatic

110
recommendations. However, 40% of the responders said (agree or strongly agree) that
the MTI recommendations help them to improve their productivity as indexers. Although
it is unknown whether MTI would offer accurate MeSH terms for indexing the
HealthData.gov metadata, further research can explore this topic.

5.4.2.3 Automated Assessment of Metadata Quality
The methods in this dissertation were used to measure current quality issues with the
HealthData.gov metadata. These methods could also be applied for prospective quality
assurance by continually evaluating the completeness, accuracy, and consistency of the
metadata. That is, in order to ensure an author supplies the highest quality metadata, an
automated audit using these methods can be carried out to determine if the supplied
metadata meets certain standards. The Qcomp, Qaccu, and Qcons can be measured after
an author attempts to submit a metadata entry. If the scores from any of these measures
were less than 1, the submission would fail the quality assurance process. A rule engine
can be created to offer feedback to the user on how to improve the quality assurance
scores. These rules would offer users feedback on duplicate entries for the notes, fields
that were left blank, misspellings, broken URL links, an inaccurate description of the data
source, a URL that does not include a domain and web page, and inconsistent formats
with the data standards. When calculating the number of datasets published in
HealthData.gov that have a Qcomp, Qaccu, or Qcons score less than 1, 65.5%
(1001/1529) failed the quality assessment.

111
5.5 DATA QUALITY IN OTHER DOMAINS
The results of the dissertation and aforementioned recommendations have broad
application to domains other than HealthData.gov. Data.gov, the parent website to
HealthData.gov, utilizes the CKAN data framework and the same metadata elements.
Additionally, in the U.S. CKAN has been adopted as the data framework for open data
portals by 8 cities, 2 states, 5 federal agencies, and 1 private organization. For instance,
the cities of Denver and Colorado adopted CKAN to publish GIS geographical datasets
[78]. One private organization, a software company called Accela, published free open
data around civic engagement on the website CivicData.io [79]. The quality of the
metadata on many of the websites that adopted the CKAN framework is yet unknown.
Reports have indicated that consumer participation is limited with Data.gov due to
challenges in understanding the data that is available and a lack of standards to support
data sharing and dissemination [1].

Many federal governments have adopted CKAN as a platform to support data public
sharing of data. A report by CKAN showed that the platform is the currently adopted to
support government data sharing in 24 countries including Argentina, Australia, Austria,
Brazil, Canada, Croatia, Estonia, Finland, Germany, Indonesia, Italy, Mexico,
Netherlands, Norway, Japan, Paraguay, Romania, Slovakia, Sweden, Switzerland, United
Kingdom, United States, Uganda, and Uruguay [80]. Interestingly, other countries have
reported similar challenges faced by the U.S. The United Kingdom launched
Data.gov.uk, and to date has seen limited usage of the available data [81]. The lack of

112
usage in the UK has been attributed to a lack of prioritization in the datasets that are
published. That is, datasets that are not considered important are being released through
Data.gov.uk, thereby causing difficulty for consumers in finding the most relevant and
impactful data for analytical purposes.

Many federal organizations publish data in data repositories other than CKAN. Evans and
Campos [1] reviewed consumer challenges of other federal websites including FOIA.gov
that provides data and reports about the Freedom of Information Act, ITDashboard.gov
that offers data regarding federal investments in information technology, Recovery.gov
that enables users to track use of American Recovery and Reinvestment Act funds,
USASpending.gov that offers a searchable database of federal awards and spending,
USA.gov that has a portal to government websites by topic and agencies, and
Regulations.gov that providers information about regulations issued by federal agencies.
Similar challenges were present in all these websites, even though they did not adopt
CKAN as the data portal. A similar observation for all these websites is the lack of
consumer participation, which may be due to inaccessible data resulting from the lack of
standardization and ineffective data storage practices [1].

ClinicalTrials.gov is another example of a website that doesn’t use the CKAN data portal
and publishes information on publicly and privately supported clinical studies in human
volunteers on a wide range of diseases and conditions [82]. The Food and Drug
Administration Modernization Act of 1997 (FDAMA) mandated the U.S. Department of
Health and Human Services, through NIH, establish a registry of clinical trial information

113
for both federally and privately funded trials conducted under investigational new
drug applications to test the effectiveness of experimental drugs for serious or lifethreatening diseases or conditions [82]. The result was the creation of ClinicalTrials.gov.
The website was developed jointly by the NIH and the Food and Drug Administration
(FDA) and the site was made available to the public in February 2000. In 2007, Congress
passed the FDA Amendments Act (FDAAA), which set requirements for registering
clinical trials at ClinicalTrials.gov. Section 801 of FDAAA (FDAAA 801) required more
types of trials to be registered and additional trial registration information to be
submitted. The law also required the submission of results for certain trials. The result is
a data repository with information on over 200,000 clinical trials [83].

In November 2014, the US Department of Health and Human Services issued a notice of
proposed rulemaking (NPRM) that described amendments to current requirements and
procedures, in accordance with FDAAA 801, for registering and submitting results of
clinical trials on ClinicalTrials.gov. The NPRM also suggested regulations intended to
provide more complete information on clinical trials to enhance patients' access and use
of the results of clinical trials. The proposal expanded the requirements for submitting
results for Applicable Clinical Trials of unapproved products (i.e., drugs, biological
products, or devices that have not been approved, licensed, or cleared by FDA), which
resulted in the development of a “Protocol data element definition” in December 2015.
The protocol data element definition included required data elements as mandated by
ClinicalTrials.gov and FDAAA 801 [84]. The name of each data element as well as a

114
definition and examples were provided in the protocol data element definition
document. A study that analyzed the metadata from ClinicalTrials.gov and other websites
that published data on clinical trials (e.g., International Clinical Trials Registry Platform,
European Clinical Trials database, Dec-Net register) noted that many efforts have been
made by different organizations and associations to standardize clinical trial registries,
yet there are still many differences in the descriptions of the protocol related to the name
of the metadata field and the definitions [85]. The study also found frequently incomplete
data including missing information for a contact person, email address, telephone
number, intervention arms that were completed, and timeframe requirements for a
clinical trial [85]. Additionally, the Clinical Trials Transformation Initiative [86]
evaluated the completeness and accuracy of the ClinicalTrials.gov metadata and revealed
that missing information is common. The metadata requirements for ClinicalTrials.gov
have changed over the years; therefore, older data may be less accurate and complete.
ClinicalTrials.gov staff have implemented measures to ensure greater data quality by
applying automated business rules regarding missing data and internal consistency
metrics. Additionally, staff manually review records and may request revisions from the
submitter [86]. Despite ClinicalTrials.gov efforts to improve the quality of the data,
research still noted that a manual review of individual records does not guarantee records
are compliant with ClinicalTrials.gov or legal requirements and quality issues still exist
with the website [76]. However, the required data elements are comprehensive and
support the opportunity for high quality data and rapid and easy-to-use search processes.
For example, searches are supported through the adoption of a controlled vocabulary. A

115
required data element when publishing information about clinical trials on
ClinicalTrials.gov is the use of MeSH terms for describing the focus of a clinical trial and
conditions being studied [84]. Those that submit information to ClinicalTrials.gov are
required to assign appropriate MeSH terms to describe the focus of the study and
conditions being studied. If the ClinicalTrials.gov staff found that a term assigned by the
data submitter was not a MeSH term or did not appropriately describe the clinical trial,
the staff would assign an appropriate MeSH term. ClinicalTrial.gov staff also use the
MTI to aid in the assignment of appropriate MeSH terms. Presently, a formal assessment
regarding the use of MeSH to find clinical trials has not been conducted. A study has yet
to examine whether the manual review of the MeSH terms leads to improved indexing of
clinical trials on ClinicalTrials.gov. However, the fact that ClinicalTrials.gov utilizes
MeSH terms to index clinical trials and developed a process to ensure high quality
indexing through manual review, there is need to examine if a similar process would be
effective with HealthData.gov. Given this mandate, ClinicalTrials.gov offers an excellent
example of a federally sponsored website for data sharing where a controlled terminology
was used for indexing, archiving, and retrieval.

Besides supporting government established data portals, there is also great opportunity to
adapt the recommendations and automated data quality assessment to research data.
Research data is valuable and ubiquitous; yet, having historical research data available
and indexed has been a challenge within the scientific community. Vierkant and
colleagues [87] explain re3data.org, a website funded by the German Research

116
Foundation and functions as a global registry of research data repositories that cover
varying academic disciplines. The web portal offers permanent storage of data and access
to these datasets to researchers, funding organizations, publishers, and scholarly
institutions. In creating this portal, a metadata schema was described to support the
description of research data repositories [88]. Currently, the schema is in its third revision
and contains properties describing the scope, content, infrastructure, and compliance with
technical, quality, and metadata standards. Several controlled vocabularies support the
schema depending on the metadata property, which is regulated by a team of researchers
at re3data.org. One such vocabulary is the DFG classification system that is used to
describe topic areas in the sciences [89]. To date, the re3data.org metadata schema does
not support MeSH as a standardized terminology; however, the schema does support
keywords. The fact that a controlled vocabulary has not been adopted for the keywords
poses a limitation of the schema.

The re3data.org schema also supports the collection of information regarding quality
management [88]. That is, a metadata property describes whether there is any form of
quality management concerning the data or metadata set forth by the organization
governing a research data repository. The quality management field can take on a value
of yes, no, or unknown. HealthData.gov is present in the re3data.org registry and
describes the quality management as “unknown”. Given the focus of this dissertation, the
adoption of the recommendations described in Table 5.2 and the use of an automated
quality assessment would satisfy this requirement for quality management.

117
5.6 LIMITATIONS OF THE RESEARCH
The limitations to the research are largely related to data availability and limits to the
MeSH terminology. The following information will explore these constraints in more
detail.

5.6.1 Limits to Data
A limitation of the present research is that new datasets are frequently published in
HealthData.gov. This study only examined datasets that were published over a 3-year
timeframe. Future research will need to be conducted to investigate the future quality of
the metadata and sustainability of coverage that MeSH provides for indexing the data in
HealthData.gov.

5.6.2 Limits to Methods
There were limitations to the methods used in the research. The human reviewers used in
the Qaccu validation were novices and reviewed a relatively low number of datasets.
Therefore, the low reliability between the human reviewers and the Qaccu may be
attributed to this limitation.

Additionally, the metrics that were used to assess metadata quality were difficult to
implement. There was considerable data preparation required prior to adopting the quality
metrics. There would need to be significant software engineering to adopt these metrics
for automated quality assessment.

118
Another limitation to the methods is that MeSH was the only controlled vocabulary
investigated. There may be other vocabularies or a combination of vocabularies that offer
greater coverage of the HealthData.gov concepts. Finally, the methods used in this study
are laborious and require considerable data preparation. Therefore, in order for the
automated methods for quality assessment to be implemented into a system, considerable
software engineering will need to be carried out to ensure that the methods can evaluate
the metadata in their native state.

5.7 FUTURE RESEARCH
There is ample opportunity to carry out additional research to explore the implications of
poor data quality and the adoption of the MeSH terminology for indexing. The following
section will suggest possible avenues of future research that can explore these aims.

5.7.1 Usability and Usefulness of MeSH
Future research needs to be conducted to determine if the indexing of MeSH does support
consumers in retrieving relevant datasets. If MeSH does prove to offer a more effective
search strategy for consumers, the feasibility and accuracy of indexing methods need to
be explored. That is, research can be conducted to determine if an automated indexing
method offers accurate cataloguing of the HealthData.gov metadata. Future research
should also examine if other terminologies can be used in combination with MeSH to
cover the current gaps in term coverage.

119
5.7.2 Ontological Development for MeSH Expansion
Further research can be conducted to expand the MeSH terminology to fulfill the present
gaps. This research would have to include the development of an ontology to identify the
data resources that are currently present and may be present in the future, that need
terminological characterization. The Institute of Medicine developed a vision for a
learning health system to integrate the nation’s electronic healthcare data to share and
learn from each other [90]. The Office of the National Coordinator for Health
Information Technology (ONC) embraced the learning health system in their strategic
plan and launched what is known as the Query Health initiative [91]. Query Health is a
public-private collaboration that has the sole goal of developing standards and services to
support the distribution of population health measures. To meet this goal, the ONC
developed a distributed query model to eliminate the centralization of data and required
organizations to process queries and disclose aggregated statistics around specific
measures. These aggregated measures are leading to an explosion of data released for
research and population health surveillance. The Query Health ontology was developed
to offer a terminology to maintain standard queries [92]. This ontology could be
leveraged to predict areas of future growth with data to develop a new ontology around
data characterization. Not only would the development of such an ontology help to guide
consumers in identifying the data that is available for analysis, but this development
could also help drive further expansion of the MeSH terminology for characterizing the
datasets.

120
Table 4.13 displays the current topics that were not covered by MeSH. Mainly, the
gaps identified were in terms characterizing documents (e.g., chartbooks, datasets,
downloads, PDF). However, there were gaps in MeSH for characterizing terms related to
humans, time, organizations, and equipment. As there is growth in population health data,
there may be a need to expand the MeSH terminology to fulfill the current gaps but also
predict future gaps.

5.7.3 Development of Data Selection and Quality Tools
The final area where future research could be conducted is the development of tools for
data selection to visualize data quality. A data selection tool could leverage the use of the
MeSH terminology to characterize datasets based on their hierarchical arrangement. For
instance, MeSH descriptors can be listed to allow users to select data relevant to a certain
topic. The datasets listed in a specific descriptor category can be selected by mapping that
descriptor to the MeSH entry terms. The MeSH entry terms are used for indexing the
datasets. Therefore, the selection of a descriptor would explode the query to show all
relevant datasets related to that descriptor. For example, if a user selected the descriptor
of “Health Care Economics and Organization” this would reveal all data that have an
entry term that falls under the concept categories, including: “Economics”, “Health
Planning”, “Organizations”, “Policy”, etc.

In addition to selecting data based on the descriptor, a user could also search for entry
terms using a text box. An entry term search will find exact matches as well as

121
synonymous terms. Also, users can opt to explode the search further by choosing a
concept that the entry term is mapped to. By choosing a concept, further terms will be
used to reveal data that are related to that concept.

The MeSH vocabulary can be downloaded as an XML file and used to develop the data
selection tool [47]. Descriptors can be populated on a webpage by only presenting those
descriptors that are mapped to a datasets. Also, the number of datasets for each descriptor
can be populated on the website next to the descriptor category name.

Another tool that can be developed is a visualization tool for data quality. Using an
automated data quality assessment, as described in section 5.4.2, a visualization of the
Qcomp, Qaccu, and Qcon can be measured offering users the ability to assess the
completeness, accuracy, and consistency of the metadata available for the dataset. This
visualization can appear as a stacked bar plot where each individual quality score is
revealed as well as the summative quality score. Therefore, users can use the qualityscoring tool for data selection purposes including sorting and filtering.

Overall, this dissertation demonstrated limitations to the quality of the HealthData.gov
metadata, which can be attributed to the lack of implementing a standardized metadata
framework. A framework that accommodates a controlled terminology, such as MeSH,
that is implemented using a standard, such as Dublin Core, may lead to higher quality
metadata. As a result, greater consumer participation in the use of publicly available

122
healthcare data may be achieved. The final chapter of this dissertation will offer a
more complete summary of the findings and the contributions this research as to the field
of health informatics.

123
CHAPTER 6
RESEARCH SUMMARY & CONTRIBUTIONS
As part of the Open Government Initiative, the United States Federal Government
published important datasets for use by the public. Health data is made available through
an online data repository, HealthData.gov. Although there has been increasing
participation by health agencies in disseminating data to the public, there has been very
low public participation in the use of the data. Literature suggested that a primary cause
for the lack of public participation is due to issues regarding the quality of the metadata
that represents the health data. This metadata is the key source for storing and searching
for health data. Using automated methods of assessing data quality, clear issues regarding
the completeness, accuracy, and consistency of the data were identified. To address the
quality concerns of metadata published on a publicly available health data repository,
recommendations around quality assessment, metadata standards, and terminology
adoption were explored. As a result, this research led to the following health informatics
advancements:
Development of automated metrics to assess metadata quality
Recommendations for a framework for publishing and sharing metadata
Recommendations to adopt and enhance a controlled terminology to index metadata
Overall, this research discovered that when the metadata underwent modifications, the
quality of the metadata improved. However, the proportion of the metadata instances that
were modified was lower from metadata created in recent years. In addition, quality was
measured based on the completeness, accuracy, and consistency of the metadata and was
found to be worse in the unmodified metadata. Regarding the completeness of the

124
metadata, tags were left blank most often followed by notes. Regarding the accuracy
of the metadata, over four percent of the URL links to the data sources resulted in an
error and two percent of the links were left blank. Given the fact that the URL is the link
to the source data, it is significant that the links did not work in about 5% of the metadata
instances. Essentially, the source data is not retrievable in these instances. Other
measures of accuracy were related to spelling and source data representation. Although
there were minimal spelling errors in the metadata, when examining how well the
metadata represented the source data there was lower accuracy in the metadata that was
unmodified versus the modified metadata. Similarly, the consistency of the metadata was
shown to be limited with regards to abiding by both CKAN and Dublin Core standards.
Consistency was found to be lower in the unmodified versus the modified metadata. That
is, the metadata format did not follow standard field naming schemes and the format did
not always conform to standards. The fact that there were proportionately fewer metadata
instances being modified in later years indicated an increasing risk to the quality of the
metadata. The limitations in the quality of the metadata may be a considerable factor
leading to challenges in searching and retrieving datasets.

When considering strategies to overcome the limitations in the quality of the metadata, a
standard indexing method may offer a method for improving the search and retrieval of
the metadata. In evaluating MeSH as a possible indexing terminology, a term coverage
study demonstrated adequate coverage of the HealthData.gov concepts. Currently,
HealthData.gov does not support effective and efficient search and retrieval methods to

125
find relevant datasets. This is largely due to limitations in the annotation principles
adopted by HealthData.gov and the CKAN data repository software, which may result in
suboptimal data searches thereby leading to issues around accessibility. Adopting a
standardized vocabulary such as MeSH for indexing the HealthData.gov metadata, the
search and retrieval methods may be better supported. Additionally, the Medical Text
Indexer (MTI) should be explored to investigate the application in automatically indexing
HealthData.gov metadata.

Based on this research, modifications of the existing data repository or a new data
repository can be adopted to support high quality metadata that is indexed and easily
retrievable. However, gaps exist in the coverage that MeSH offered for indexing the
metadata. Therefore, efforts to expand MeSH can be explored. Finally, the methods used
to assess the quality of the metadata were automated, and did not require extensive
human intervention. These methods could be adopted for ongoing quality assessment to
ensure authors are submitting the highest quality metadata to support consumer
participation.

126
REFERENCES
[1]

Evans AM, Campos A. Open government initiatives: Challenges of citizen
participation. Journal of Policy Analysis and Management 2013; 2(1):172-85.

[2]

Jaeger PT, Munson S. Engaging the Public in Open Government: Social Media
Technology and Policy for Government Transparency. Federal Register 2010.

[3]

Dawes SS, Helbig N. Information strategies for open government: Challenges and
prospects for deriving public value from government transparency. Electronic
Government: Lecture Notes in Computer Science, M.A. Wimmer et al. (Eds.):
EGOV 2010:50–60.

[4]

Dawes SS. Governance in the digital age: A research and action framework for an
uncertain future. Government Information Quarterly 2009;26(2):257-64.

[5]

Tapscott D, Williams AD, Herman D. Government 2.0: Transforming
government and governance for the twenty-first century. New Paradigm 2008.

[6]

Martin S, Foulonneau, M, Turki, S. 1-5 stars: Metadata on the openness level of
open data sets in Europe. In: Garoufallou E, Greenberg, J. (Eds.): Metadata and
Semantics Research: Springer International Publishing 2013: 234-45.

[7]

Shah NH, Jonquet C, Chiang AP, Butte AJ, Chen R, Musen MA. Ontology-driven
indexing of public datasets for translational bioinformatics. BMC Bioinformatics
2009;10:Suppl 2:S1.

[8]

Winn J. Open data and the academy: an evaluation of CKAN for research data
management. International Association for Social Science Information Services
and Technology 2013:28-31.

[9]

Coglianese C. The transparency president? The Obama administration and open
government. Governance 2009;22(4):15.

[10]

Obama B. The Obama-Biden plan. Change.gov 2007 [cited November 2013].
Available from: http://change.gov/agenda/technology_agenda/.

[11]

Obama B. Transparency and open government: White House 2009 [cited
November 2013]. Available from: http://www.whitehouse.gov/the_press_office/
TransparencyandOpenGovernment.

[12]

Data.gov. Data.gov 2014 [cited June 2014]. Available from: http://www.data.gov.

[13]

Orszag PR. Memorandum for the heads of executive departments and agencies.
Executive Office of the President: Office of Management and Budget: White
House 2009. Available from: https://www.whitehouse.gov/sites/default/files/omb/
assets/memoranda_2010/m10-06.pdf

[14]

OGP. The open government partnership: National action plan for the United
States of America. Whitehouse.gov 2011 [cited November 2013]. Available from:
https://www.whitehouse.gov/sites/default/files/
us_national_action_plan_final_2.pdf.

127
[15]

Sunstein CR. Memorandum for the heads of executive department and
agencies. Executive Office of the President: Office of Management and Budget:
White House 2011. Available from: https://www.whitehouse.gov/sites/default/
files/omb/memoranda/2011/m11-19.pdf.

[16]

HealthData.gov. HealthData.gov 2014 [cited June 2014]. Available from:
http://www.HealthData.gov.

[17]

Health Data Consortium. Health Data Initiative: Strategy and execution plan
2013. [cited November 2015]. Available from: https://drive.google.com/a/css.edu/
file/d/0B9hZC55hEGZUM19xczk2RTU2M2c/edit.

[18]

CKAN. About CKAN. CKAN 2014 [cited June 2014]. Available from:
http://ckan.org/developers/about-ckan/.

[19]

Napoli PM, Karaganis J. On making public policy with publicly available data:
The case of US communications policymaking. Government Information
Quarterly 2010;27(4):384-91.

[20]

Meijer A, Thaens M. Alignment 2.0: Strategic use of new internet technologies in
government. Government Information Quarterly 2010;27(2):113-21.

[21]

Robinson D, Yu H, Zeller WP, Felten EW. Government data and the invisible
hand. Yale Journal of Law & Technology 2008;11:159.

[22]

Chang A-M, Kannan P. Leveraging Web 2.0 in government. IBM Center for the
Business of Government 2008 [cited November 2015]. Available from:
http://www.businessofgovernment.org/sites/default/files/LeveragingWeb.pdf.

[23]

Beall J. Metadata and data quality problems in the digital library. Journal of
Digital Information 2006;6(3).

[24]

Bruce TR, Hillmann, D.I. The continuum of metadata quality: Defining,
expressing, exploiting. In: Hillmann DI, Westbrooks, EL., (Eds). Metadata in
Practice. Chicago, IL: American Library Association 2004.

[25]

Hillmann DI. Metadata quality: From evaluation to augmentation. Cataloguing &
Classification Quarterly 2008;46(1):15.

[26]

Ochoa X, Duval, E. Automatic evaluation of metadata quality in digital
repositories. International Journal on Digital Libraries 2009;10(2-3):24.

[27]

NISO. A framework of guidance for building good digital collections. National
Information Standards Organization 2007. [cited June 2014]. Available from:
http://www.niso.org/publications/rp/framework3.pdf.

[28]

Lei Y, Sabou, M., Lopez, V., Zhu, J., Uren, V., Motta, E. An infrastructure for
acquiring high quality semantic metadata. 3rd European Semantic Web
Conference: Budva, Montenegro 2006; 11-4.

[29]

Park J. Metadata quality in digital repositories: A survey of the current state of the
art. Cataloguing & Classification Quarterly 2009;47(3-4):15.

128
[30]

Tarver H, Zavalina, O, Phillips, ME, Alemneh, DG, Shakeri, S. How
descriptive metadata changes in the UNT Libraries' Collections: A case study.
Proceedings of the International Conference and Workshop on Dublin Core and
Metadata Applications: Austin, Texas 2014.

[31]

Zavalina OL, Kizhakkethil, P. Exploration of metadata change in a digital
repository. iConference Proceedings 2015.

[32]

Zavalina OL, Kizhakkethil, P, Alemneh, DG, Phillips, ME, Tarver, H. Building a
framework of metadata change to support knowledge management. Journal of
Information & Knowledge Management 2015;14(1).

[33]

Hughes B, Kamat, A. A metadata search engine for digital language archives. DLib Magazine 2005;11(2).

[34]

Bui Y, Park, JR. An assessment of metadata quality: A case study of the national
science digital library metadata repository.
Proceedings of the Annual
Conference of California Association of Independent Schools 2013.

[35]

Najjar J, Ternier, S, Duval, E. The actual use of metadata in ARIADNE: An
empirical analysis. Proceedings of the ARIADNE 3rd International Conference
2003.

[36]

Rühle SB, Baker, T, Johnston, P. User Guide. Dublin Core Metadata Initiative
2011 [cited November 2015]. Available from: http://wiki.dublincore.org/
index.php/User_Guide.

[37]

ISO. Information and documentation: The Dublin Core metadata element set. ISO
15836:2009.

[38]

Harrison AM, Yadav H, Pickering BW, Cartin-Ceba R, Herasevich V. Validation
of computerized automatic calculation of the sequential organ failure assessment
score. Critical Care Research and Practice 2013:975672.

[39]

Project Open Data. Project Open Data Metadata Schema v1.1 Project Open Data
2014 [cited November 2015]. Available from: https://project-opendata.cio.gov/v1.1/schema/.

[40]

Nelson SJ. Medical terminologies that work: The example of MeSH. Proceedings
of the 10th International Symposium on Pervasive Systems, Algorithms, and
Networks 2009:4.

[41]

Bodenreider O. The Unified Medical Language System (UMLS): Integrating
biomedical terminology. Nucleic Acids Research. 2004;32:D267-70.

[42]

Praz V, Bucher P. CleanEx: New data extraction and merging tools based on
MeSH term annotation. Nucleic Acids Research. 2009;37:D880-4.

[43]

Uwimana E, Ruiz ME. Integrating an automatic classification method into the
medical image retrieval process. Proceedings of the American Medical
Informatics Association Annual Fall Symposium 2008:747-51.

129
[44]

NLM. Collection development manual of the National Library of Medicine.
National Library of Medicine, National institutes of Health, U.S. Department of
Health and Human Services: Bethesda, MD 2004. [cited June 2015]. Available
from: http://www.nlm.nih.gov/tsd/acquisitions/cdm/.

[45]

Blake JB. From Surgeon General's bookshelf to National Library of Medicine: A
brief history. Bulletin of the Medical Library Association 1986;74(4):318-24.

[46]

Greenberg SJ, Gallagher PE. The great contribution: Index Medicus, IndexCatalogue, and IndexCat. Journal of the Medical Library Association
2009;97(2):108-13.

[47]

NLM. Medical Subject Headings: NLM 2014. [cited June 2015]. Available from:
https://www.nlm.nih.gov/cgi/request.meshdata.

[48]

NLM. History of MESH. U.S. National Library of Medicine 2003 [cited March
2015]. Available from: http://www.nlm.nih.gov/mesh/intro_hist.html.

[49]

Lipscomb CE. Medical Subject Headings (MeSH). Bulletin of the Medical
Library Association 2000;88(3):265-6.

[50]

NLM. The MEDLINE indexing process: Determining subject content. National
Library of Medicine 2015 [cited November 2015]. Available from:
https://www.nlm.nih.gov/bsd/disted/meshtutorial/principlesofmedlinesubjectindex
ing/theindexingprocess/.

[51]

NLM. MeSH Browser. National Library of Medicine 2015 [cited November
2015]. Available from: https://www.nlm.nih.gov/mesh/MBrowser.html.

[52]

Coletti MH, Bleich HL. Medical subject headings used to search the biomedical
literature. Journal of the American Medical Informatics Association
2001;8(4):317-23.

[53]

Aronson AR. MetaMap Evaluation. National Library of Medicine 2001. [cited
March 2015]. Available from: https://ii.nlm.nih.gov/Publications/Papers/
mm.evaluation.pdf.

[54]

Aronson AR. The effect of textual variation on concept based information
retrieval. Proceedings of the American Medical Informatics Association Annual
Fall Symposium 1996.

[55]

Aronson AR, Lang FM. An overview of MetaMap: Historical perspective and
recent advances. Journal of the American Medical Informatics Association
2010;17(3):229-36.

[56]

Aronson AR, Mork JG, Lang F-M, Rogers WJ, Neveol A. The NLM Medical
Text Indexer: A tool for automatic and assisted indexing. National Library of
Medicine 2008. [cited March 2015]. Available from: https://ii.nlm.nih.gov/
Publications/Papers/ii-bosc08.pdf.

[57]

HHS. HealthData.gov 2014
http://www.healthdata.gov.

[cited

June

2014].

Available

from

130
[58]

Wilmet S. GSpell. GNOME 2015 [cited June 2015]. Available from:
https://wiki.gnome.org/Projects/gspell.

[59]

Salton G, Wong, A., Yang, C. A vector space model for automatic indexing.
Communications of the ACM 1975;18(11):7.

[60]

Hallgren KA. Computing Inter-Rater Reliability for Observational Data: An
Overview and Tutorial. Tutorials in Quantitative Methods for Psychology
2012;8(1):23-34.

[61]

CKAN. Contributing guide. CKAN 2013 [cited March 2015]. Available from:
http://docs.ckan.org/en/latest/contributing/index.html.

[62]

McCray AT, Srinivasan S, Browne AC. Lexical methods for managing variation
in biomedical terminologies. Proceedings of the Annual Symposium on Computer
Application in Medical Care 1994.

[63]

Paik WL, Liddy ED, Yu E, McKenna, M. Categorizing and standardizing proper
nouns for efficient information retrieval. Corpos Processing for Lexical
Acquisition 1996;12,61-72.

[64]

U.S. Census. Population Estimates, Population Change, and Components of
Change. U.S. Census 2016 [cited January 2016]. Available from: http://
www.census.gov/popest/data/state/totals/2015/index.html.

[65]

Drees L. State open data policies and portals. Data Innovation 2014. [cited
December 2015]. Available from: https://www.datainnovation.org/2014/08/stateopen-data-policies-and-portals/.

[66]

Bonaguro J. Open data in San Francisco: Institutionalizing an initiative. City and
County of San Francisco Mayor Edwin M. Lee 2014. [cited December 2015].
Available from: http://sfmayor.org/Modules/ShowDocument.aspx?documentID=
425.

[67]

Rahm EI. Open data executive order. Mayor City of Chicago 2012. [cited
December 2015]. Available from: http://www.cityofchicago.org/city/en/narr/foia/
open_data_executiveorder.html.

[68]

Walsh MJ. An order relative to open data and protected data sharing. City of
Boston Mayor's Press Office 2014. [cited December 2015]. Available from:
http://www.cityofboston.gov/news/Default.aspx?id=6589.

[69]

Shah PK, Perez-Iratxeta C, Bork P, Andrade MA. Information extraction from
full text scientific articles: where are the keywords? BMC Bioinformatics
2003;4:20.

[70]

Ji L, Rui, P., & Hansheng, W. Selection of best keywords: A poisson regression
model. Journal of Interactive Advertising 2010;11(1):8.

[71]

Hopewell S, Ravaud P, Baron G, Boutron I. Effect of editors' implementation of
CONSORT guidelines on the reporting of abstracts in high impact medical
journals: Interrupted time series analysis. BMJ 2012;344:e4178.

131
[72]

W3C. Resource Description Framework (RDF) Model and Sytax
Specification. W3C 1999. [cited December 2015]. Available from:
http://www.w3.org/TR/1999/REC-rdf-syntax-19990222/.

[73]

Thibault JC, Roe DR, Facelli JC, Cheatham III TE. Data model, dictionaries, and
desiderata for biomolecular simulation data indexing and sharing. Journal of
Cheminformatics 2014;6(1):1-23.

[74]

CDC. About the CDC Unified Process. Centers for Disease Control and
Prevention 2016. [cited January 2016]. Available from: http://www2.cdc.gov/
cdcup/library/other/about_up.htm.

[75]

CDC. Non-functional requirements definition. Center for Disease Control 2015
[cited December 2015]. Available from: http://www2.cdc.gov/cdcup/
library/templates/cdc_up_non-functional_requirements_definition_template.doc.

[76]

Zarin DA, Tse T, Williams RJ, Califf RM, Ide NC. The ClinicalTrials.gov results
database: Update and key issues. The New England Journal of Medicine
2011;364(9):852-60.

[77]

DCMI. DCMI Type Vocabulary Dublin Core Metadata Initiative. Dublin Core
Metadata Initiative 2010. [cited January 2016]. Available from:
http://dublincore.org/documents/2010/10/11/dcmi-type-vocabulary/.

[78]

Denver, Colorado. Open data catalog Data. The City and County of Denver 2016
[cited January 2016]. Available from: http://data.denvergov.org.

[79]

Accela. Civic Data. Accela 2016 [cited January 2016]. Available from:
http://www.civicdata.io/about.

[80]

CKAN. CKAN instances around the world. CKAN 2015 [cited December 2015].
Available from: http://ckan.org/instances/.

[81]

Bright J, Margetts H, Wang N, Hale S. Explaining usage patterns in open
government data: The case of data.gov.uk. Proceedings from the International
Conference on Public Policy 2015. [cited January 2016] Available from:
http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2613853.

[82]

ClinicalTrials.gov. Background. ClinicalTrials.gov 2016. [cited January2016].
Available from: https://clinicaltrials.gov/ct2/about-site/background.

[83]

ClinicalTrials.gov. Trends, charts, and maps. ClinicalTrials.gov 2016. [cited
January 2016]. Available from: https://clinicaltrials.gov/ct2/resources/trends.

[84]

ClinicalTrials.gov. Protocol data element definitions draft. ClinicalTrial.gov 2015.
[cited January 2016]. Available from: https://prsinfo.clinicaltrials.gov/definitions.
html.

[85]

Viergever RF, Karam G, Reis A, Ghersi D. The quality of registration of clinical
trials: Still a problem. PLoS One 2014;9(1).

132
[86]

CTTI. Points to consider for statistical analysis using the database for
aggregate analysis of ClinicalTrials.gov. Clinical Trials Transformation Initiative
2012.
[cited
January
2016].
Available
from:
http://www.ctticlinicaltrials.org/files/ State_of_Clinical_Trials/
AACT2013_statistical_points_to_consider_12_27_13.pdf.

[87]

Vierkant P, Spier S, Rücknagel J, Gundlach J, Fichtmüller D, Pampel H, et. al.
Vocabulary for the registration and description of research data. re3data 2012.
[cited December 2015]. Available from: http://www.re3data.org.

[88]

Rücknagel J, Vierkant P, Ulrich R, Kloska G, Schnepf E, Fichtmüller D. et. al.
Metadata schema for the description of research data repositories. re3data, 2015.
[cited December 2015]. Available from: http://gfzpublic.gfz-potsdam.de/
pubman/faces/viewItemOverviewPage.jsp?itemId=escidoc:1397899.

[89]

DFG. Structure. DFG 2016 [cited January 2016]. Available from:
http://www.dfg.de/en/dfg_profile/statutory_bodies/review_boards/subject_areas/
index.jsp.

[90]

Friedman CP, Wong AK, Blumenthal D. Achieving a nationwide learning health
system. Science Translational Medicine 2010;2(57):57cm29.

[91]

Klann JG, Buck MD, Brown J, Hadley M, Elmore R, Weber GM, et al. Query
Health: Standards-based, cross-platform population health surveillance. Journal of
the American Medical Informatics Association 2014;21(4):650-6.

[92]

Klann JG. ONC S&I framework Query Health reference implementation. i2b2
Community Wiki 2013 [cited December 2015]. Available from:
https://community.i2b2.org/wiki/display/QueryHealth/Home.

