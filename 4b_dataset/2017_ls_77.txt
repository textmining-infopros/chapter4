© 2017
Stephanie Mikitish
ALL RIGHTS RESERVED

INFORMATION ENGAGEMENT: HOW SOCIAL SCIENCE DOCTORAL
STUDENTS SEEK, FILTER, ACCESS, AND ORGANIZE INFORMATION
By
STEPHANIE MIKITISH
A dissertation submitted to the
Graduate School – New Brunswick
Rutgers, The State University of New Jersey
In partial fulfillment of the requirements
For the degree of
Doctor of Philosophy
Graduate Program in Communication, Information and Library Studies
Written under the direction of
Marie L. Radford, Ph.D.
And approved by
______________________________________
______________________________________
______________________________________
______________________________________

New Brunswick, New Jersey

May 2017







ProQuest Number: 10753119






All rights reserved


INFORMATION TO ALL USERS
The quality of this reproduction is dependent upon the quality of the copy submitted.


In the unlikely event that the author did not send a complete manuscript
and there are missing pages, these will be noted. Also, if material had to be removed,
a note will indicate the deletion.









ProQuest 10753119


Published by ProQuest LLC (2017 ). Copyright of the Dissertation is held by the Author.




All rights reserved.
This work is protected against unauthorized copying under Title 17, United States Code
Microform Edition © ProQuest LLC.



ProQuest LLC.
789 East Eisenhower Parkway
P.O. Box 1346
Ann Arbor, MI 48106 - 1346

ABSTRACT OF THE DISSERTATION
Information Engagement: How Social Science Doctoral Students
Seek, Filter, Access, and Organize Information

By STEPHANIE MIKITISH

Dissertation Director:
Marie L. Radford, Ph.D.

The government and society are increasingly questioning the value of libraries
and higher education institutions (HEIs). While there is no one agreed upon standard of
value or a way to measure it, both Library and Information Science (LIS) and Education
research have suggested that library and educational resources and services should
demonstrate their impact on individual student outcomes. Engagement studies in both
areas suggest that by increasing student engagement, institutions can positively and
significantly affect student outcomes. Although little work has been done in the area of
information engagement (IE), engagement is a useful framework that can be defined and
measured on behavioral, emotional, motivational, and cognitive dimensions.
In order to explore, define, and measure IE, this dissertation study examines how
social science doctoral students find, filter, access, and organize information. Doctoral
students are an understudied population, despite their need for scholarly and often
difficult to obtain information. Because little is known about doctoral student IE, samples
from this population were drawn in a three part mixed methods study, which consisted of

ii

focus group interviews, individual interviews, and an online survey. Overall, 158 doctoral
students from the United States participated in all three phases of this research.
Based on the analysis of qualitative data from the focus group and individual
interviews, three factors emerged and were used to measure IE related behaviors in the
online survey. The first factor was personality, and according to the quantitative analysis,
participants who scored higher on an index based on this factor were more open to asking
for help/clarification; less unhappy if they retrieved a large quantity of information, even
if it was unexpected; and would be more likely to consider changing their research based
on what they found. The second factor was confidence, and participants who scored
higher on an index based on this factor believed that they had better searching abilities
and felt less challenged by commonly encountered obstacles to finding information. The
third factor was interest in library instruction, and participants who scored higher on an
index based on this factor were more likely to prefer an instruction session over face-toface help when they needed it, and think that library instruction would be beneficial to
others in their program.
The index scores for these factors had statistically significant relationships to each
other and information related behaviors, which included how much participants would
pay for a book that they needed for their research and where they would start a search on
an unfamiliar topic. The strength of these relationships increased for students in the
dissertation writing stage of their program and for students who lived more than an hour
away from campus. In addition to being the first study to identify these relationships, this
dissertation’s major contributions include identifying the critical factors that affect IE,

iii

doctoral student outcomes that information and libraries can support, and suggestions for
educating this population on information related topics.

iv

ACKNOWLEDGEMENTS
This dissertation would not have been possible without the help and support of
those in my personal, academic, and professional lives. First and foremost, I am
incredibly grateful for my parents’, family members’, and friends’ love, understanding,
and food.
From my academic life, I would like to thank my committee, the faculty and
students at SC&I, and the other scholars I have met in the past seven years. Of course, a
majority of the credit goes to my advisor, Marie, for guiding and supporting me through
this dissertation and other scholarly endeavors. I would also like to thank Dan for his
insights on statistics and life, Ross for his enthusiasm for my work coupled with his
suggestions to refine it, and Lynn for helping me grow as a researcher.
As for my fellow students, there are several who have encouraged me as they also
walked the path to the PhD. Over the years, many become friends as well colleagues. I
would like to thank the students from earlier cohorts for their excellent advice and
example, especially: Aaron T., Andrew S., Annie G., Emily K., Heewon K., Jessa L.,
Jonathan B., and Katie M. From my own cohort and later ones, I would also like to thank:
Alexa B., Barbara B., Camille R., Christine G., Connie P., Fannie R., Frank B., Ian D.,
Jacob S., Jessica C., Maggie B., Marie H., Miraida M., Si S., and (especially) Vanessa K.
Last, but certainly not least, I would like to thank the Rutgers University
Libraries, especially the Alexander Library Access Services department. Thank you for
teaching me, funding me, and perhaps most importantly for inspiring me to be the best
librarian I can be.

v

TABLE OF CONTENTS
ABSTRACT OF THE DISSERTATION……………………………………………....ii
ACKNOWLEDGEMENTS……………………………………………………………..v
TABLE OF CONTENTS……………………………………………………………….vi
LIST OF TABLES……………………………………………………………………..xiii
LIST OF FIGURES…………………………………………………………………...xvii
CHAPTER 1: INTRODUCTION……………………………………………………….1
Context: Assessment in higher education…………………………………………1
Statement of purpose: The need for deeper understanding and better metrics……3
Rationale and significance of study……………………………………………….5
CHAPTER 2: LITERATURE REVIEW………………………………………………8
Definitions of value……………………………………………………………….8
Organizational framework for classifying value definitions…………………….11
Institutional faculty and staff – Objective: The value of ownership……..11
Institutional faculty and staff – Subjective: The value of efficiency and
use……………………………………………………………......16
Students and those outside the institution – Objective & Subjective: The
value of contributing to outcomes……………………………..…18
Interactions with information…………………………………………………….20
Definitions and applications of engagement……………………………………..26
Engagement studies……………………………………………………...27
IE theories, models, and approaches……………………………………………..30
Astin’s theory of student involvement…………………………………...30

vi

The Digital Visitors and Residents (V&R) project………………………33
O’Brien and Toms’ user engagement framework………………………..34
From distant admirers to library lovers - and beyond: A typology of public
library engagement in America…………………………………..35
Michie, Van Stralen, and West’s Capabilities, Opportunities, Motivations
– Behavior (COM-B) model…………………………………….36
Doctoral students’ IE…………………………………………………………….37
Summary of IE literature…………………………………………………..…….43
CHAPTER 3: METHODS……………………………………………………………..46
Research questions……………………………………………………………….46
Problem statement………………………………………………………………..46
Research design………………………………………………………………….48
Summary of phases……………………………………………………....50
Phase 1: Exploring information engagement (IE) – Focus group interviews……51
Phase 1 data collection…………………………………………………...52
Phase 1 data analysis……………………………………………………..54
Phase 2: Creating IE typologies – Individual interviews………………………...56
Phase 2 data collection…………………………………………………...58
Phase 2 data analysis……………………………………………………..60
Phase 3: Testing IE factors – Online survey……………………………………..63
Phase 3 data collection…………………………………………………...65
Phase 3 data analysis……………………………………………………..66
Methods conclusion……………………………………………………………...71

vii

CHAPTER 4: RESULTS………………………………………………………………72
Description of sample…………………………………………………………....72
Gender……………………………………………………………………72
Age……………………………………………………………………….74
Ethnicity………………………………………………………………….75
Stage……………………………………………………………………...76
Discipline………………………………………………………………...77
Other Degrees……………………………………………………………78
Location………………………………………………………………….80
Summary of Description of sample...……………………………………82
Qualitative and Quantitative Results by Research Question…………………….82
Interpreting thematic coding tables………………………………………83
Results for Research Question 1 (RQ 1): What are the critical factors that
characterize information engagement (IE) for doctoral students in the
social sciences?..........................................................................................83
Facilitator and barrier themes coded as critically affecting IE…………..85
1.1 Tasks…………………………………………………………………85
1.2 Facilitators……………………………………………………………89
1.3 Barriers……………………………………………………………….98
Summary of RQ 1 results……………………………………………….104
Results for Research Question 2 (RQ 2): What information engagement (IE) types
exist for doctoral students?......................................................................105
Themes related to information engagement type……………………….106

viii

2.1 Factors and Change in Behavior……………………..……………106
Dependent variables (DVs) and other results for IE type and change in
IE……..…………………………………………………………113
Independent variables (IVs) and other results for IE type and change in
IE………………………………………………………………..116
Additive indices………………………………………………………...126
Cluster analyses………………………………………………………...129
Cluster analysis summary………………………………………………133
Summary of RQ 2 results……………………………………………….135
Research Question 3 (RQ 3) asked: How is information engagement (IE) related
to the value of academic libraries?...........................................................136
Themes related to the value of academic libraries……………………...138
3.1 Electronic Resources and 3.2 Print Resources……………………...139
3.3 Human Resources…………………………………………………..140
Simple and multiple linear regression…………………………………..145
Binary logistic regressions for behavior variables DVs………………..148
3.4 Social Networks and 3.5 Software………………………………….150
Summary of RQ 3 results……………………………………………….152
Research Question 4 (RQ 4) asked: How can academic libraries promote
increased information (IE) engagement of doctoral students?................153
Themes related to library instruction topics that can increase IE……....154
4.1 Outcomes/goals……………………………………………………..154
4.2 Tasks Facilitated by a Magic Wand………………………………...156

ix

Summary of RQ 4 results……………………………………………….160
Summary of Knowledge Claims………………………………………………..161
CHAPTER 5: DISCUSSION…………………………………………………………164
Discussion topics for Research Question 1 (RQ 1): What are the critical factors
that characterize information engagement for doctoral students in the
social sciences?........................................................................................164
Finding and accessing information……………………………………..165
Library instruction……………………………………………………...169
Flow experiences and multiple logins…………………………………..171
Library policies…………………………………………………………172
Other barriers………………………………………………………..….176
Summary of RQ 1 discussion…………………………………………..179
Discussion topics for Research Question 2 (RQ 2): What information engagement
types exist for doctoral students?.............................................................180
Resource preferences…………………………………………………...180
Other IE challenges……………………………………………………..182
Developing personality, confidence, and interest as independent
variables………………………………………………………...184
Indices and clusters……………………………………………………..186
Summary of RQ 2 discussion…………………………………………..188
Discussion topics for Research Question 3 (RQ 3), which asked: How is
information engagement related to the value of academic libraries?......189
Willingness to pay (WtP) and usage variables…………………………189

x

Library usage………………………………………………..………….192
Personality variables and resource preference……………...…………..192
Summary of RQ 3 discussion…………………………..………………193
Discussion topics for Research Question (RQ 4), which asked: How can academic
libraries promote increased information engagement of doctoral
students?...................................................................................................194
Outcomes/goals…………………………………………………………195
Social media networks………………………………….………………196
Tasks facilitated by magic wand ……………………………………….197
Summary of RQ 4 discussion…………………………………………..198
CHAPTER 6: LIMITATIONS, IMPLICATIONS, AND FUTURE STUDY….….200
Limitations……………………………………………………………...………200
Theoretical limitations……………………………………………….…200
Study design limitations…………………………………………….…..201
Implications………………………………………………………..……………206
Theoretical contributions and implications……………………………..206
Practical implications………………………………………………..….207
Implications for study design…………………………………….……..213
Directions for future study…………………………………………….………..214
Theoretical directions…………………………………………………..214
Study design directions……………………………………..…………..215
Conclusion………………………………………………….…………………..216
APPENDICES………………………………………………………………………....221

xi

Appendix A: Glossary…………………………………………………………..221
Appendix B: IE models………………………………………………………....222
Appendix C: Hierarchy of IE behaviors………………………………………..223
Appendix D: IRB approval notices……………………………………………..224
Appendix E: Interview and survey questions……………………….………….227
Appendix E1: Focus group interview questions…………..……………227
Appendix E2: Individual interview questions…………………………..228
Appendix E3: Survey questions………………………………...………229
Appendix F: Preliminary qualitative themes………………….…………..……239
Appendix G: Final codebook of qualitative themes…………………………....241
REFERENCES………………………………………………………………………...258

xii

LIST OF TABLES
Table 1: Dimensions of engagement……………………………………………………..26
Table 2: NSSE themes and engagement indicators……………………………………...28
Table 3: Overview of dissertation phases………………………………………………..50
Table 4: Participants’ responses to “My gender is”……………………………………...73
Table 6: Participants’ responses to “My age falls into the following range”……………74
Table 6: Participants’ responses to “My ethnicity is”……………………………………75
Table 7: Participants’ responses to “I am in the following stage of my program”………76
Table 8: Participants’ responses to “I am in the following department/discipline………77
Table 9: Participants’ bachelor’s discipline……………………………………………...78
Table 10: Participants’ master’s discipline………………………………………………78
Table 11: Participants’ responses to “The school where I am getting my degree is located
in the following region of the country”…………………………………………..80
Table 12: Region of participants’ bachelor’s institutions………………………………..80
Table 13: Region of participants’ master’s institutions………………………………….80
Table 14: RQ1 qualitative themes identified in Phase 1 and 2…………………………..85
Table 15: Participants’ responses to “I am an international student”……………………95
Table 16: Participants’ responses to “I am able to log onto databases through other
schools besides where I am getting my degree”…………………………………97
Table 17: Participants’ responses to “I live more than an hour away from where the
school I am getting my degree at is located”…………………………………...104
Table 18: RQ2 qualitative themes identified in Phase 1 and 2..………………………..106
Table 19: Stage 1 participants’ responses to “In the following stages of my doctoral

xiii

program, I would say that my habits searching for research-related information
changed”………………………………………………………………………..108
Table 20: Stage 2 participants’ responses to “In the following stages of my doctoral
program, I would say that my habits searching for research-related information
changed”………………………………………………………………………..109
Table 21: Stage 3 participants’ responses to “In the following stages of my doctoral
program, I would say that my habits searching for research-related information
changed”………………………………………………………………………..109
Table 22: Participants’ responses to “When I start a search on a research-related topic that
I am unfamiliar with, I likely will consult the following people or resources as
follows”…………………………………………………………………………114
Table 23: Participants’ responses to “When I start a search on a research-related topic that
I am unfamiliar with, I am most likely to consult”……………………………..115
Table 24: Personality variable responses ranked by mean response.…………………..120
Table 25: Confidence variable responses ranked by mean response…………………...123
Table 26: Interest variable responses ranked by mean response……………………….125
Table 27: Index summary………………………………………………………………127
Table 28: Linear regression prediction summary………………………………………127
Table 29: pClus center values…………………………………………………………..129
Table 30: Analysis of variance (ANOVA) for pClus differences………………………130
Table 31: cClus center values…………………………………………………………..131
Table 32: Analysis of variance (ANOVA) for cClus differences………………………132
Table 33: iClus center values…………………………………………………………...132

xiv

Table 34: Analysis of variance (ANOVA) for iClus differences………………………133
Table 35: Cluster prediction summary………………………………………………….135
Table 36: RQ3 qualitative themes identified in Phase 1 and 2…………………………138
Table 37: Participants’ responses to “I would rank the importance of these resources to
my research as follows”………………………………………………………...141
Table 38: Participants’ responses to “Last semester, I searched for research-related
information online”……………………………………………………………..142
Table 39: Participants’ responses to “Last semester, I asked someone at the library for
research-related help either online or in-person”……………………………….143
Table 40: Participants’ responses to “How often do you use library spaces to study during
the school year?,” re-coded into above categories……………………………...143
Table 41: Participants’ responses to “Last semester, I visited the physical library for
research-related purposes”……………………………………………………...144
Table 42: Participants’ responses to “If I need a _____ for research, I would be willing to
pay on average the following amount in US dollars”…………………………..145
Table 43: Linear regression prediction summary for how much participants would pay for
a book that they needed for their research……………………………………..146
Table 44: Descriptive statistics for MLR to predict Book$ using pInd, cInd, and iInd...147
Table 45: Classification table for personality cluster (pClus) membership prediction using
personality cluster membership………………………………………………...149
Table 46: Model summary of personality variables to predict whether a participant would
start a search with Google Scholar or a search engine or not…………………..149
Table 47: RQ4 qualitative themes identified in Phase 1 and Phase 2…………………..154

xv

Table 48: Participants’ responses to “I have used or would be willing to use the
information that I found in my searches to”……………………………………156
Table 49: Participants’ attendances in library information session………………….....158
Table 50: Participants’ responses to “I have used the following sites/networks to”…...159
Table 51: Participants’ responses to “I would be interested in learning about the
following”………………………………………………………………………160
Table 52: Summary of library instruction topics by doctoral student stage……………209

xvi

LIST OF FIGURES
Figure 1: Gender by Phase……………………………………………………………….73
Figure 2: Major participant age groups…………………………………………………..74
Figure 3: Major ethnic groups……………………………………………………………75
Figure 4: Participant stages……………………………………………………………...76
Figure 5: Participants by major disciplinary group………………………………………77
Figure 6: Breakdown of other degrees…………………………………………………...79
Figure 7: Moving between degrees………………………………………………………81
Figure 8: Doctoral student IE model (DSIEM)…………………………………………189
Figure 9: Astin’s I-E-O model………………………………………………………….222
Figure 10: Michie, Van Stralen, and West’s COM-B model…………………………...222
Figure 11: Proposed dissertation model………………………………………………...222
Figure 12: Davenport & Prusak’s IE hierarchy………………………………………...223

xvii

1
CHAPTER 1: INTRODUCTION
This chapter outlines the problem statement, context, statement of purpose, and
rationale and significance for this dissertation. The dissertation study consisted of three
phases of mixed methods data collection and analysis, which involved qualitative focus
group and individual interviews and a quantitative survey. This research took place in the
context of assessment in higher education. Assessment has grown in importance due to
governmental and societal demands for accountability. At the same time, assessment of
most colleges and universities is complicated by competition from online and other nontraditional colleges and universities. This dissertation’s purpose is to consider the
valuation of academic libraries with respect to assessment in higher education. The
valuation of academic libraries is specifically explored via the conceptualization of
information engagement (IE) and its role in promoting positive student-centered
outcomes for social science doctoral students, an understudied population that has a high
potential to use and value information from the academic library.

Context: Assessment in higher education
Research has demonstrated that earning a college degree benefits both college
graduates and society since it is reported that graduates are more likely to have “larger
earnings over a lifetime, lower unemployment rates, better health, higher marriage rates,
and greater civic involvement” (Rose, 2013) than non-graduates. Consequently, the
demand for a college education to earn this demonstrably valuable degree typically
remains high. However, the value of the individual higher education institutes (HEI) that
provide those degrees is less clear. Both society and the government have increased their

2
demands for HEIs to account for the rising cost of tuition and of tax dollars spent on
education. These demands are complicated due to competition among HEIs and the
introduction of new methods of instruction for which there are no standard ways of
assessment.
HEIs are primarily funded through student tuition and government assistance
(Heller, 2003). Of the two, tuition is the primary source of revenue into HEIs. In addition
to demands for accountability from those who pay student tuition, HEIs must also justify
their use of governmental funding. Public land grant institutions must be funded at a
certain level through their respective state governments, but although federal funding has
increased it is being distributed to more institutions than ever before, which has resulted
in fewer amounts per institution (Campos, 2015), and even less for underperforming
institutions (Nelson, 2003). Federal, state, and local governments also award money to
other types of institutions, and this has always been competitively allocated. Faced with
calls for accountability from various stakeholders in government and society, HEIs must
develop ways to conceptualize and measure their value, usually quantitatively. In
addition to demonstrating how they are improving their value to their students, they are
increasingly being pressured to develop ways to compare themselves with other
institutions.
In addition to the calls for accountability from those outside higher education,
many colleges and universities face competition from other HEIs, including the more
established traditional colleges and universities, and also from the relatively recently
established for profit educational institutions (Stevens & Kirst, 2015). Enabled in part by
technological advances, the latter do not require the instructors or students to meet in the

3
same building for class. In addition to reducing the overhead necessary to run physical
facilities, online colleges and universities also have the ability to attract and instruct large
numbers of students in a largely unregulated manner (Contreras, 2007; Stevens & Kirst,
2015). Although some believe that the accreditation system ensures that only institutions
providing quality educations are in operation, one study by Contreras (2007) found that
roughly 20 percent of legal HEIs are running unaccredited in the 34 states surveyed.

Statement of purpose: The need for deeper understanding and better metrics
Despite the critical need to demonstrate their value, many HEIs are having trouble
doing so. Astin’s (1993) two models of higher education are useful in describing why the
value of higher education is difficult to quantify. The industrial model view of higher
education values accountability and efficiency because it assumes that students receive
the same benefits from attending an institution, and the only variable is how much the
education costs, which ultimately lies in the effectiveness of the institution (Astin, 1993,
p. 17). The medical model compares the initial conditions of the student, which is
analogous to a patient’s initial prognosis, to their condition after attending college, which
is analogous to the treatment (Astin, 1993, p. 18). In other words, the opportunities
provided at an HEI will have different results, which depend on the student as well as the
institution. Prior to the 1980s, most assessments of HEI value were undertaken with the
industrial model in mind, but increasing evidence indicates that the medical model is
more appropriate. Astin (2004) has stated that a student’s chances of graduating can be
predicted 67% of the time based on incoming demographics alone.

4
Another complication with identifying the value of a college education is that the
benefits of earning a degree, such as higher earnings are not realized immediately upon
graduation (Rose, 2013). In other words, a degreed person’s success will rely on other
conditions encountered after graduation. Moreover, the differences in institution mission,
student population, and funding make it difficult to evaluate or compare HEIs. Changes
in accreditation standards over the last couple of decades reflect the ongoing struggle of
defining and measuring value. As HEIs look to conceptualize how their environments
and activities contribute to their value as an institution, they have exerted pressure on
their individual units to research and demonstrate their value or face budget reductions
and even elimination (Banta, Busby, Hahn, Black& Johnson, 2007). An important
indicator of value is the unit’s contribution to or facilitation of individual and institutional
outcomes. The different groups in HEI, such as faculty, staff, and students, have different
outcomes, which necessarily require different ways to measure them. Each individual
unit has different opportunities to affect these outcomes, even established units, such as
academic libraries.
Although one of the earliest support units to develop in HEIs (Hamlin, 1981) and
considered to be the heart of the university (Frade & Washburn, 2006) because of their
role as the major repository and gatekeeper of scholarly information, most academic
libraries can no longer assume that their value is self-evident (Poll & Payne, 2006).
Developing new ways to assess academic libraries also will benefit other types of
libraries and information centers. As Pritchard (1996) stated:
Few libraries exist in a vacuum, accountable only to themselves. There is always
a larger context for assessing library quality, that is, what and how well does the
library contribute to achieving the overall goals of the parent constituencies (p.
573).

5
In other words, research in assessing academic library value informs and is in turn
informed by research studying the value of other library and information centers that are
facing similar demands from their parent institutions to demonstrate their value. A major
barrier to assessing library value stems from disagreements over valid indicators of what
to value, particularly relating to student learning. One concept that has been gaining
acceptance has been engagement because of its role in increasing individually measured
student outcomes that are institutionally identified priorities, such as retention,
graduation, and learning.

Rationale and significance of study
A developing value metric in higher education is student engagement, which is
synonymous with student involvement. Based on work pioneered by Astin in the 1960s,
students that are more involved in certain aspects of college and university life are more
likely to have successful academic outcomes, which include retention, graduation, and
higher GPAs. Engagement has been defined as having several dimensions, including
cognitive, behavioral, conative, and affective (Corno & Mandinach, 1983; Fredericks,
Blumenfeld, & Paris, 2004). Although student engagement has been studied in a variety
of contexts, most engagement studies are monolithic, tending to only consider one
dimension of engagement.
One type of engagement that has not been well studied is IE. O’Brien and Toms
(2008) have created a theory of user engagement, but only as it applies to technology and
an online environment, so it was not used in this study, as explained in Chapter 2. In the
educational environment and with regard to information, the few scholars that do address

6
engagement only consider the behavioral dimension of IE, with the exception of an
article by Green and Macauly (2007) who measured student motivation, behavior, and
learning. Traditionally, library usage statistics are the most commonly used engagement
and value metric, but it can potentially be dangerous for academic libraries to equate
library use with value if the use of major library services, such as circulation of materials,
gate counts, and reference sessions, decreases (Behr & Hayward, 2008; Miller, 2014).
In order to best demonstrate academic library value, it is important to identify a
study population that would require scholarly information in order to reach their
individual and institutional goals. Ideally, this information would not be easily accessed
outside of academic library sources. Outcomes that require this sort of information
include theses, scholarly publications, grants, and patents. These types of resources
would be of interest to faculty and graduate students rather than undergraduates. Library
and Information Science (LIS) research suggests that doctoral students are a useful group
to study because as they transition from students to scholars (Fleming-May & Yuro,
2009), they are likely to change how they search for and evaluate information (Catalano,
2013; Spezi, 2016). The interdisciplinary nature of social science work puts these
doctoral students in a unique position to benefit from library instruction. However,
doctoral students are also an under-studied population (Fleming-May & Yuro, 2009; Du
& Evans, 2011; Switzer & Perdue, 2011). The few extant studies indicate that academic
library resources are losing visibility among this group (Nicholas, Watkinson, Abdullah,
Boukacem-Zeghmouri, Bravo, Świgoń, Xu, & Herman, 2016), which is alarming because
future HEI faculty and administrators will come from this population (Mikitish &
Radford, 2013).

7
This dissertation contributes to the study of academic library value by exploring
doctoral students’ engagement with information. It will seek to identify the most critical
individual factors that affect IE. By identifying these IE related factors and behaviors,
academic libraries can work toward more efficiently targeting their resources and
services. The next chapter reviews the higher education and LIS literature on academic
library value, student engagement, and doctoral student interactions with information.

8
CHAPTER 2: LITERATURE REVIEW
This chapter contains an overview of value and engagement studies in higher
education with a focus on the academic library context. Because an exhaustive review of
the literature found no information engagement theories (IE) currently being used in an
academic context, the chapter discusses four extant engagement theories, models, and
approaches, including one from higher education literature and three that deal with IE in
other fields. It concludes with studies of IE in library contexts and on the information
behavior of doctoral students, the population of interest for this study.

Definitions of value
The Oxford English Dictionary (OED, 2017) defines value as:
1. The material or monetary worth of something; the amount at which something
may be estimated in terms of a medium of exchange, as money or goods, or some
other similar standard.
2. Worth based on esteem; quality viewed in terms of importance, usefulness,
desirability, etc.
When exploring value in the educational and academic library context, one must consider
what is considered valuable and how that value should be measured. Arum and Roksa
(2012) describe these as the normative and technical aspects, respectively. Dunn (2010)
provides a useful definition of two types of value, the intrinsic and the extrinsic. She
defines the former as “the essence of something; the value of something in and of itself”
(2010, p. 13), while the latter has observable and measurable qualities that “implies a
hierarchy of choices” (2010, p. 10) that allow for comparison or ranking (2010, p. 12).
Because intrinsic value is innate, one may not always be able to evaluate it, because the
OED (2017) defines “evaluate” as: “to ‘reckon up’, ascertain the amount of; to express in

9
terms of something already known.” However, one can still judge a thing to have intrinsic
value, even if they cannot evaluate it. On the other hand, extrinsic value must be
evaluable.
The criteria for evaluating intrinsic and extrinsic value are identified through two
entirely different processes. Dunn (2010) describes the process of determining intrinsic
value as deciding that such value does exist “through reflection on the world, human
behavior and the human condition” (p. 15), and then producing “a list of values without
reference to source or methodology” (p. 14). Dunn (2010) continues to say that although
intrinsic values are often seen as an area mainly of interest for philosophers, scientists
have epistemic values, which are associated with “the learning, knowing and discovery of
science” (p. 2; Rooney, 1992). When discussing the value of graduate education,
Stimpson (2012) notes that German universities, one of the two foundations of the
American university, “lauded advanced, specialized learning as important in itself and as
a crucial key to modernity and progress” (p. 135). Although she did not frame it as such,
learning, modernity, and progress could arguably be considered intrinsic values in the
field of education.
In comparison, extrinsic value by definition must be observable and measurable.
Saracevic and Kantor (1997) identify two types of extrinsic value: value-in-exchange and
value-in-use. The former is measurable in more economic terms, such as the price or cost
of a good or service. By using a standard unit to evaluate the good or service, such as the
amount of money an item costs, one can rank items in terms of cost. Items with higher
costs are more valuable than ones with lower costs. It is also possible to compute return
on investment (ROI). For instance, a vendor can compare the cost of buying an item to

10
the amount of money he can sell it for before deciding whether it is worth procuring.
Different variables may also be combined in an evaluation, so the vendor can compare
the cost of buying an item and the amount of time it takes to receive the item to the
amount of money that he can sell it for. Lagerman and Lewis (2012) note that the value
of higher education tends to reflect economic logic, mainly “the collective effect of
individual returns” (p. 9) in which higher education leads to “a skilled population and the
inventions and ideas that make them productive” (p. 20). In the field of Library and
Information Science (LIS), Saracevic and Kantor (1997) note that the economic
evaluations of value-in-exchange may not be appropriate for information services,
especially because “there is no market involving prices and monetary exchanges” (p.
530). Instead, they argue that libraries are more fairly evaluated by value-in-use, which
can be measured explicitly using statements, or implicitly using behavior. These
evaluations can then be combined with more economic ones.
As explained in Chapter 1, it has become increasingly important for higher
education institutions (HEIs) to demonstrate their value. Individual HEIs have passed this
imperative down to their individual departments, including their libraries. Unfortunately,
the lack of a universally agreed upon definition of HEI value has led to many definitions
of value and even more ways of evaluating it. The numerous and sometimes conflicting
definitions of value suggest that no one definition can encompass all of the concept’s
related facets. In fact, one could say that there are certain questions that one must answer
before deciding how to conceptualize value. The answers to these questions, which must
be decided before one defines value and then decides how to evaluate it, suggests an
organizational framework for categorizing different definitions of value.

11
Organizational framework for classifying value definitions
The following questions must be answered before one can define and evaluate
value.
1. Who is the audience for the evaluation, and for what purpose is the evaluation
taking place?
2. How context dependent is the evaluation? In other words, will the variables being
evaluated be more objective, meaning that they depend less on context or other
variables, or are they more subjective, meaning that they depend more on context
or other variables?
3. What will be evaluated?
4. How will it be evaluated?
Although one may suggest other intermediate questions, the answers to these questions in
higher education and Library and Information Science (LIS) literature help explain why
the evaluation of HEI and academic library value has changed over time.

Institutional faculty and staff – Objective: The value of ownership
In the Value of Academic Libraries report, Oakleaf (2010) stated that the earliest
definitions of value were intended for institutional employees, which includes the faculty
and staff at the HEI. These definitions of value tended to be more objective in that
physical objects were used as variables, and there was little room for individual
interpretation of what was being measured. Value was assessed by counting the number
of resources, and institutions that owned a particular number or type of resource were
considered more valuable than those that had fewer resources. In the academic library
context, value usually referred to collection size, for which there were standards for the
amount of books a good or valuable library should have (Nelson, 2009).
From their inception during the colonial times before 1790 (Hamlin, 1981) until
the 1980s academic libraries demonstrated their value by adhering to professional and

12
accreditation standards for amount of resources, mainly collection size. Early academic
libraries were much smaller both in size and in scope than the libraries of today because
the institutions that they supported were smaller. Many libraries came into existence
through the donation of books, and many only were able to continue operation and book
purchasing thanks to nominal fees from the students rather than as a fixed percentage of
their parent institution’s annual budget (Hamlin, 1981, p. 19).
The Morrill Act of 1862 endowed institutions to teach the agriculture and
mechanical arts, and the donations of industry barons to higher education put pressure on
HEIs to add classes in the hard and social sciences. As scientific research expanded, the
information needs of those in higher education also increased. Hamlin (1981) claims that
this was when the academic library collections became essential to disseminating and
continuing this research, which strengthened the library’s status as the “heart of the
university” (p. 48) and consequently deserving of more budgetary support (p. 58). The
increased funds and interest in higher education lead to HEIs opening between the 1850s
and the 1900s at a faster rate than ever (Parsons, 2003). Because of societal and
governmental concerns of institutional quality, in 1885 the first of six regional
accrediting institutions formed in New England. Associations for the Middle States,
Southern, North Central, Northwest, and Western regions would form in 1887, 1895,
1895, 1917, and 1923, respectively (Nelson, 2009).
Although regional accreditation always has been voluntary, the impact of
accreditation on HEIs cannot be understated. As Coleman and Jarred (1994) explain,
“almost every relationship a college has with an external party…is contingent on
accreditation” (p. 274). Unlike other countries, the American government or a

13
government-sponsored entity does not take part in HEI “review, oversight, [or] quality
control” (Parsons, 2003, p. 31). This makes accreditation the standard indicator of quality
among American colleges and universities, and it is a major force behind any change in
the assessment conducted within these institutions (Oakleaf, 2010).
One such side effect of higher education’s expansion was increased public and
governmental scrutiny of HEIs. The increased enrollment and governmental spending
was financially beneficial to the growth of HEIs, but the increased public and
governmental interest towards these institutions continued even after the funding levels
decreased. Concerned that academic libraries would not have the resources to build
collections to serve their institutions, the Association of College and Research Libraries
(ACRL) issued their first set of standards for college libraries in 1959 as “a blueprint for
the decade of the 1960s” (Nelson, 2009, p. 3451). Though quality was emphasized,
especially the requirements for “a high-caliber staff and a rich and current collection of
materials” (Nelson, 2009, p. 3451), the standards also listed quantitative measurements
for areas such as collection size. Librarians could use these recommended benchmark
numbers to make a case for requesting funding from their institutions, and it is arguable
that this was the intended purpose of the standards.
Quantitative standards were useful for librarians if they led to increased budgets,
but the regional accrediting associations, who had begun to include libraries in their
evaluations in 1900, never adopted these standards as part of their criteria (Coleman &
Jarred, 1994). Certain academic libraries may also have lobbied against the adoption of
the standards for accreditation because their institutions could not or would not give them
the funds to reach the standards. It would be disastrous for the library staff if their

14
department caused the institution to not get accreditation. ACRL also noted the difference
between 2 year colleges, 4 year colleges, and universities. Consequently, until 2004
ACRL published different standards for each of these types of institutions. While a
review of the regional accreditation association standards can indicate broad reasons for
changes in library valuation and assessment, the changes in the ACRL standards are more
detailed in regard to libraries and better illustrate what changes affected academic library
evaluation models. The most significant change in the standards history occurred in 2004
when ACRL published its first standards for all college and university libraries. The 4
year college standards were the ones published in 1959, however they were not seen as
adequate for 2 year and junior colleges and universities for economic and cultural
reasons, respectively.
Using the standards for funding purposes was not feasible for 2 year colleges due
to their diversity, changing roles, increasing enrollment, and insufficient finances
(Nelson, 2009, p. 3453), which led to separate standards for them published in 1960. In
the 1960s enrollment in these types of HEIs increased five-fold from 200,000 in 1960 to
one million in 1971 (Rose, 13, p. 25). The dynamic changes in these types of institutions,
and possibly the political divisiveness among these ACRL constituents, are evident in
their being the only ones out of the three sets of standards to require guidelines for
applying the standards in addition to the most revisions. The standards themselves were
revised twice in 1972 and 1982, and the guidelines were first published in 1971 and
revised in 1979. The standards and guidelines were combined in 1990 and revised in
1994. In comparison, the college standards were revised four times in 1975, 1986, 1995,
and 2000, while the university standards were revised only one time in 1989.

15
In contrast to the dynamic and divided 2 year college ACRL members, those in
the university were relatively slow to act and change. Although they formed a committee
to determine criteria and norms for the standards in 1968 they did not publish them until
1979. In addition to the conservative nature of the universities, which continues to this
day due to their histories and size, there had been arguments about the quantitative nature
of the standards for years. In 1965, Clapp and Jordan argued that collection size should
depend on institutional characteristics, such as the number of programs, not an arbitrary
and un-researched number from a professional society.
Cognizant of the difficulties in setting and reaching the two and four year college
standards, the research university standards task force began with testing formulas for
valid quantitative standards, which led to peer grouping and regressions based on each
area of the standards, namely “resources, personnel, space, finances, public survey, and
administration” (Nelson, 2009, p. 3456). Such a method led to the inevitable result of
smaller institutions supporting the numbers, which were higher than their existing
resources, and larger institutions not supporting the numbers because they feared that
their institutions would cut their funding because they already met or exceeded the
standard (Nelson, 2009, p. 3456). Because they had difficulties compromising, university
libraries were the first group of the three to emphasize library performance rather than
standards (Nelson, 2009).
Diversity, whether due to institution history, mission, and size, was not the only
factor in the abandonment of standards and move to a performance model. Studies by
Carpenter (1981a, 1981b) and Crawford and White (1999) consistently found that most
libraries were not meeting the standards, so the standards were quantitatively failing as

16
tools to increase library funding. In the 1970s HEIs also began to adopt a more
“managerial orientation” (Oakleaf, 2010, p. 6) toward their units, including academic
libraries. The failure to increase funding, the demand for further justification for funding,
and a national educational movement towards identifying and evaluating the institution’s
contribution to student outcomes caused academic libraries to abandon the three
institution-specific standards and adopt a single set of standards for all ACRL institutions
that focused more on demonstrating the library’s impact on student learning outcomes
and institutional effectiveness (ACRL, 2004).
Compared to the ACRL standards, the regional standards set by the accrediting
associations have been increasingly vague as to how libraries and other learning
resources can demonstrate their quality (Nelson, 2009, p. 3450). However, librarians still
are interested in them according to Iannuzzi and Brown’s (2010) survey of academic
library directors’ on the 2004 ARCL Standards in which they found that a majority of the
respondents wanted the ACRL Standards to reflect the regional accreditation standards.

Institutional faculty and staff – Subjective: The value of efficiency and use
The next type of definitions of value were still intended for HEI faculty and staff,
but the definitions tended to allow for more subjective definitions and interpretations of
value. Rather than comparing resource amounts with other academic libraries, librarians
could focus measuring and evaluating the resources and services most important to their
institution. While owning a certain number or type of resources still could contribute to
the institution’s value, the expenditures to purchase and maintain those resources had to
be compared with their usage. Value came to be assessed by comparing counts of

17
resources, and more valuable institutions were the ones that were most effective, meaning
that the usage of their resources had higher relative values than the costs of those
resources.
Most of the studies that equate efficiency with value take a more financial view
by focusing on cost effectiveness (e.g., Aabø, 2009; King & Tenopir, 2013). They also
usually feature a return on investment (ROI) or cost-benefit calculation. Aabø (2009)
performed a meta-analysis of ROI studies in several, mainly public, libraries. In the 38
studies analyzed, Aabø (2009) found that the average return on investment was between
four to five dollars of resources obtained or services performed per one dollar allocated to
the library. Many ROI studies compare use of a resource or service to the cost in
providing the resource, but many do not have a stated goal so the ROI is just a number
that is difficult to interpret or use as a benchmark.
One time studies (e.g., Wong & Cmor, 2011) are not very useful because there is
no way to compare how a change in input measures affected output measures (see
Appendix A for definitions), but comparing one institution’s efficiency at one time to that
of similar institutions using benchmarks can serve as an indicator of relative value. A
study by Gatten (2004) compared user satisfaction survey results between libraries in
OhioLINK, an academic library consortium, and libraries that were not in OhioLINK.
The OhioLINK libraries had significantly higher satisfaction scores, and Gatten (2004)
further concluded that benchmarking libraries within the same consortium was useful
because these libraries were comparable in terms of mission, location, and access
policies.

18
Students and those outside the institution – Objective & Subjective: The value of
contributing to outcomes
The most recent definitions of value are framed by how the HEI benefits students
(Oakleaf, 2010). These definitions are increasingly subjective as academic library value
is evaluated by and presented to those outside of the library. This parallels the increasing
acceptance and adoption of outcomes-based education, in which institutions are
considered more valuable if they facilitate more positive outcomes for their students
(Spady, 2002).
The most recent studies of academic library value utilize the balanced scorecard
model or outcomes based models. The balanced scorecard model balances multiple
perspectives, while outcomes-based models measure how different factors affect studentcentered outcomes. Developed by Kaplan and Norton (1992), Matthews’ (2006) library
balanced scorecard often is cited, and it demonstrates how the financial perspective
affects the organizational readiness perspective, which in turn affects both the
information resources perspective and the internal process perspective, which both in turn
influence the customer perspective. Kyrillidou (2010) detailed the start of an Associatio
of Research Libraries (ARL) pilot project using the balanced scorecard, and Lewis,
Hiller, Mengel, and Tolson (2013) reported the results of the one year project.
Lewis et al. (2013) offer a useful history and analysis of the balanced scorecard in
their four institutions. The balanced scorecard is noted as one of the first tools used by
libraries to set goals. Lewis et al. (2013) also identified its nature as a change agent, its
origin in the business world, and the fact that it forced libraries out of their traditional
silos (p. 196) as being the three main problems with its implementation. Bielavitz (2010)

19
also noted that it is well set up to demonstrate and communicate accountability. Lewis et
al. (2013) considered the pilot to be a success and also hinted that benchmarks could be
made, which would give more external validity to a study done using this method.
An earlier study by Mengel and Lewis (2012) reported efforts to create a common
set of measures (p. 362) to allow for benchmarking. This was because their survey of the
measures at the four institutions found that out of a total of 94 measures there was only a
9.5 percent overlap with the numbers collected for the annual Association of Research
Libraries (ARL) statistics. More recently, Town and Kyrillidou (2013) have announced
the start of a values-based scorecard that can be used in conjunction with a balanced
scorecard with dimensions of relational capital, library capital, library virtue, and library
momentum.
In comparison to the balanced scorecard model, an outcomes-based evaluation
model is an evaluation plan that includes several planned assessments and how different
aspects of the library contribute to the outcomes. However no published studies were
found that have explicitly used this model. The lack of recent studies that use balanced
scorecard or outcomes based models reiterates the fact that there is no one size fits all
model for demonstrating value. The discussion so far has suggested that HEI units need
to provide valid indicators of their efforts to support the institution’s goals. For academic
libraries, this goal has been the provision of information through resources and services.
A difficulty in assessing the value of the information provided by academic
libraries is that the faculty, staff, and students at an HEI can get their information from
other sources. However, some groups need more specialized information that cannot be
gained cheaply or conveniently without the academic library. Faculty, graduate students,

20
and researchers at an HEI are groups whose outcomes, such as publications, grants,
patents, and careers, fulfill the HEI institutional goal of excellence in scholarship. These
groups are also likely to depend on information from the academic library, which
arguably makes them one of the best populations for LIS researchers to study. However,
before investigating how these groups interact with information, it is important to note
the different lenses through which LIS scholars view these interactions.

Interactions with information
A key topic of interest in LIS is “how people deal with information” (Savolainen,
2007, p. 126). The literature on this topic may be broken up into three groups:
information behavior, information practice, and information experience. Information
behavior is the most established of the three groups, meaning that it is the oldest and
encompasses the largest share of literature in the field (Fulton & Henefer, 2009;
Savolainen, 2007). Due to information behavior’s primacy in LIS (Fulton & Henefer,
2009), the other two groups have attempted to distinguish themselves from information
behavior. Unfortunately, there is some confusion as to what these terms mean, so there
can at times be some overlap between the groups. However, there is a general consensus
on the following areas, which will be described briefly below: founding discipline, metatheoretical and theoretical perspective, basic components, behaviors covered, and stance
on information-seeking.
Information behavior’s roots are in Psychology (Fulton & Henefer, 2009),
although certain researchers writing from the information experience group, such as
Heinström (2014), also base their work in Psychology. In contrast, information practice

21
researchers tend to claim roots in Sociology or Anthropology (Cox, 2012), which affects
the groups meta-theoretical and theoretical/methodological perspectives. There does not
seem to be a strict consensus on the founding discipline for information experience work,
so Psychology, Sociology, and Anthropology offer perspectives that all seem applicable
(Heinström, 2014; Partridge & Yates, 2014).
Virtually all researchers in the information behavior group have a constructivist
meta-theoretical perspective (Ford, 2015; Savolainen, 2007; Fulton & Henefer, 2009),
which means that they are interested in how individuals perceive and make sense of their
world. Consequently, their theoretical perspectives tend to be cognitive (Cox, 2012).
Information practice researchers tend to be social constructivist, which is also known as
collectivist, or social constructionists (Fulton & Henefer, 2009). However, their
theoretical perspective is strongly anti-cognitivist (Fulton & Henefer, 2009), turning them
away from “rational decision making and linear, purposeful behavior – and equally away
from explaining things through abstract social structures, such as class and gender” (Cox,
2012, p. 177). Information experience researchers tend to hold social constructionist
meta-theoretical perspectives, but they are not necessarily anti-cognitivist (Heinström,
2014).
Information behavior research starts with an information need, and although the
concept of an information need has evolved over the years, information behavior
researchers tend to clearly define a cognitive trigger that precedes interaction with
information (Ford, 2015). Once the participant begins to interact with information, there
are a number of internal and external factors, which are explained in the next section, and
which may affect the individual’s behaviors with information. Due to the cognitive

22
theoretical stance, the participant is often asked to articulate what they perceive these
factors to be and how they are affected by them (Savolainen, 2007; Ford, 2015).
Participants are generally also able to recall and explain their observable behaviors with
information. While certain factors may be unknown to the participant themselves,
researchers claim that they are able to measure and explain them (Bates, 2009).
Information practice researchers tend to claim that factors are decontextualized (Fulton &
Henefer, 2009), which means that factors are isolated or objectified so that the researcher
can measure how the factors affect the participant. While practice researchers may see
this as “too reductive or dehumanizing” (Cox, 2012, p. 183), information behavior
researchers need to identify and isolate these factors in order to measure their effects.
Information practice and information experience researchers are clear that there
need not be a specific need that initiates a participant’s interaction with information
(Savolainen, 2007; Cox, 2012). Information practice researchers are interested in how
participants interact with information in a social or dialogical manner (Savolainen, 2007).
In other words, the people that participants interact with and the language that they use to
describe information communicated are not mere factors as in an information behavior
perspective, but instead important carriers of meaning (Talja, Tuominen & Savolainen,
2005). People create “communities of justification” (Savolainen, 2007, p. 125), while
language explicitly or implicitly lays out the rules or criteria needed to participate in the
community (Cox, 2012). In other words, in order for an information practice researcher to
understand a participant’s interaction with information, they must understand the context
within which the participant acts. Only then can the researcher ask the participant to

23
describe their interactions or observe their interactions with information and fully
understand their responses or observations.
Information experience is differentiated from information behavior and
information practice mainly because, as the name implies, it investigates events or
episodes in which the participant interacts with information (Partridge & Yates, 2014).
However, unlike researchers in the other two groups, information experience researchers
are only interested in the factors that the participant relates to the experience, which may
or may not include information (Partridge & Yates, 2014). Based on the participant’s
response, the information experience researcher will use a combination of behavioral,
phenomenological, and sociocultural methods (Bruce, Davis, Hughes, Partridge &
Stoodley, 2014) to discover what information related factors mattered as part of the
experience. In comparison, an information behavior researcher will specifically ask
questions about an incident where a person interacted with information, even without a
deep understanding of the context surrounding the incident. An information practice
researcher would be interested in deeply understanding the context surrounding an
incident, or they might not even ask about a specific incident and just see how
participants in a group communicate information. When viewed as a research domain,
information experience researchers are more interested in the experiential data given by
participants, not participants’ opinions or explanations of their thoughts, feelings, or
behaviors (Partridge & Yates, 2014).
All researchers in each group are interested in participant behaviors, although they
have different ways of interpreting them. As the largest and most mature research
domain, information behavior researchers have identified and studied the largest range of

24
behaviors. Pettigrew, Fidel, and Bruce (2001) broadly group behaviors into ways that
participants “need, seek, give, and use information” (p. 44), while Ford (2015) groups
them into ways that participants “need, find, process, use, and organize information” (p.
7). From a practice perspective, these categories also apply, except for need (Savolainen,
2007). Information experience researchers would be on the lookout for any of the
behaviors studied by information behavior researchers, but they would not necessarily
ask participants about them.
Of the many behaviors researched, a majority of LIS research has been on how
participants seek information. In fact, up until the 1990s information behavior research
was actually called information seeking (Bates, 2009). Information behavior research is
concerned with how individual ideas or motives drive the information seeking
experience, which lends itself to a defined beginning and end state (Bates, 2009;
Savolainen, 2007; Ford, 2015). Critics of the approach, particularly those from the
information practice group claim that information behavior research implies a correct
manner of information seeking that is defined and evaluated by the researcher (Cox,
2012). Information practice research is more concerned with how information seeking
continues or habitualizes (Savolainen, 2007) the practices being researched. As a social
practice, information seeking would be part of an institutionalized activity, which implies
that there are (in)formal rules that regulate it (Savolainen, 2007), acceptable goals, and
even “appropriate emotional states” (Cox, 2012, p. 178), which means that information
seeking can be a more or less continuous experience as long as the participant remains in
the practice community. Information experience research does not necessarily look for

25
information seeking in and of itself, but once again ties it to the particular experience that
the participant is asked about or is observed inside (Partridge & Yates, 2014).
Of the three research areas that concern themselves with how participants interact
with information, IE mostly falls within the information behavior umbrella. Engagement
itself stems from psychology, and most engagement studies deal with specific incidents
of engagement. This is usually conceptualized as an individual participant’s engagement
during a specific incident. Although social factors, such as other people that the
participant interacts with or forms understandings with, matter their importance comes
from how the individual perceives and explains their influence on their own thoughts and
actions, which implies that the researcher has a more cognitivist stance.
Ford (2015) identifies information seeking as key components of information
behavior, and lists the following as significant factors influencing one’s information
behavior:
1.
2.
3.
4.
5.
6.
7.
8.

Gender
Age
Self-efficacy (one’s belief in their ability to succeed)
Cognitive style (most significant were verbal/visual and
wholistic/analytic)
Personality (e.g. Heinström’s (2014) deep/surface/strategic search styles)
Emotion (especially (un)reserved optimism or pessimism)
Work-related/organizational
Social/community

Based on the philosophical and practical literature on value, both for HEIs and academic
libraries, and the existing literature on information behavior, a major gap exists between
the two literatures. Both literatures provide suggestions for the normative question of
value, which is how academic libraries are valuable, and the information behavior
literature provides suggestions of what participant behaviors, feelings, and perceptions

26
may be measured, but there is no defined way to correlate these measurable factors to
academic library value. With roots in psychology, engagement may offer insight into the
academic library’s contributions to HEI value.

Definitions and applications of engagement
Like value, there are numerous definitions for engagement. The National Survey
of Student Engagement (2014) defines engagement most simply as being “amount of
time and energy devoted to educationally purposeful activities.” Fredericks et al. (2004)
and Corno and Mandinach (1983) view it as a meta-construct with several facets (see
Table 1, below).
Dimensions
Metric
Cognitive
Concentration, focus
Behavior
Time, persistence
Conative
Motivation
Affective
Enjoyment
Table 1: Dimensions of engagement (Fredericks et al., 2004; Corno & Mandinach, 1983)
Shernoff (2013) defines engagement as consisting of concentration, enjoyment, and
interest, but adds that it may be viewed beyond the individual level through interactive
ecological processes involving activities and relationships. In addition to these
definitions, engagement is influenced by ownership towards learning material (Sandeen,
2003); time, challenge, and interest (Light, 2001); skill, control, activity level, relevance,
and goal directedness (Furrer & Skinner, 2003). However, these more specific aspects of
engagement can fit into one of the four dimensions listed in Table 1.
Engagement may have an observed or substantive effect (Shernoff, 2013). It is
related to learning development, academic achievement, and other positive academic
outcomes such as retention and graduation. Finally, it can be viewed in the contexts of

27
the public versus the personal (Cambridge, 2006); the learning process or object of study
versus the practical context versus how it applies universally to the human condition
(Bowen, 2005); the social and the physical (Furrer & Skinner, 2003); and at the unit of
analysis of the individual, the group, or the institution.
Student engagement has been studied much more in the field of education than in
LIS. These studies can be grouped into broad categories. One area encompasses the
development and testing of engagement models for certain types of students, such as lowincome students (Snyder, 2008). Another area consists of engagement in certain
environments. Of these, those that deal with technology, such as motivational
engagement in online courses (Artino & Stephens, 2009) or engagement based on
technological affordances in computer-supported learning environments (Sinha, 2013)
are most closely related to IE. Like the academic library-related studies, past research has
demonstrated that it is possible and useful to measure engagement from different facets.

Engagement studies
A major trend in LIS studies of student engagement has been to align their data
and instruments with those from higher education. For instance, Mark and Boruff-Jones
(2003) published one of the earliest articles on how academic libraries could engage
students. They did this by describing the NSSE (Trustees of Indiana University, 2014)
and how its results could be fit into the Literacy Competency Standards for Higher
Education. The current NSSE measures ten engagement indicators on four themes as
shown in Table 2 below.

28
Theme

Engagement Indicator
Higher-order learning
Reflective & integrative learning
Academic challenge
Learning strategies
Quantitative reasoning
Collaborative learning
Learning with peers
Discussions with diverse others
Student-faculty interaction
Experiences with faculty
Effective teaching practices
Quality of interactions
Campus environment
Supportive environment
Table 2: NSSE themes and engagement indicators (Trustees of Indiana University, 2014)
Although ACRL currently is working to integrate more academic library related
questions on the NSSE, it is uncertain whether these questions will be strictly confined to
the academic library’s sources and activities or will incorporate more of the students’
information environment. More importantly, relying on higher education instruments and
standards can be counter-productive for academic libraries, because they are not usually a
priority in higher education, as evidenced by their diminishing representation or complete
absence in accreditation standards (Gratch-Lindauer, 2002). For instance, the newest
accreditations standards for the Middle States regional accreditation do not mention
libraries (Middle States Commission on Higher Education, 2015).
Green and Macauley (2007) studied doctoral students’ engagement with
information by conducting in-depth interviews with doctoral students in the United States
and Australia. They found their participants were intentional learners who not only used
information frequently, but also had high levels of information literacy. These findings
indicated that their participants had high levels of engagement in the conative, behavioral,
and cognitive dimensions, respectively. Later studies by other scholars tend to focus on
one or two dimensions of engagement.

29
Most library-related IE studies focus on the behavioral facet of engagement.
Webb, Lu, and Black (2008) primarily focused on the behavioral dimension of
engagement by investigating the time and intensity spent on a collaborative learning task
with multimedia technologies. In addition to identifying new ways that libraries could
engage students with these technologies, they also found that students were more likely to
be engaged if they felt that they had autonomy in their activities, which suggests an
affective dimension to engagement. Haddow (2013) also focused on behavioral
engagement by comparing withdrawn and retained students use of library resources as
measured by number of log-ins to authenticated library sources and items borrowed.
More recently, library studies of engagement have looked at other facets of
engagement. Shreeve and Chelin (2014) studied the impact and value of four information
literacy teaching interventions and conceptualized engagement as motivation, which is in
the conative dimension. Matteson (2014) considered the effect of several student
attributes on information literacy scores. These attributes included emotional intelligence
and motivation, which are part of the affective and conative dimensions, and of all the
attributes emotional intelligence had the greatest effect on information literacy. These
studies suggest that research into IE should look at theories and models of engagement
from outside the LIS discipline, especially those that consist of more than the behavioral
dimension. Rather than using a more generalized definition of engagement, this
dissertation defines IE as a the individual, situational, and institutional factors that affect
behaviors related to seeking, filtering, accessing, and organizing information (see
Appendix A). The next section offers engagement theories, models, and approaches from
the fields of higher education, health communication, and business.

30
IE theories, models, and approaches
Ideally, an IE model would allow the academic library to demonstrate its effects
on faculty, staff, and student outcomes that would demonstrate its value to the HEI.
Unfortunately, given the varying definitions of and ways to measure value and
engagement, no such LIS model exists. This section outlines IE models from higher
education, technology, and health communication. Related LIS and higher education
studies are discussed where appropriate.

Astin’s theory of student involvement
Astin (1993) was one of the first higher education researchers to correlate
increased engagement, which he referred to as involvement, with positive student
outcomes. He developed student involvement theory based on the results of a study that
he conducted from the late 1960s to the early 1970s that determined which student input
and college environmental factors had the greatest effects on student outcomes. Astin
(1993) was interested in what impact college had on students because the higher
education literature on college impact did not account for what the outcomes would have
been if a student had attended a different college or had not attended college at all, and no
studies examined the rate at which variables affected the change. To close this gap in the
research, his study considered the effects of college in terms of exposure time and
intensity, maturation, and social change on students. His outcomes were cognitive and
non-cognitive, and assessed via tests and surveys, respectively (Astin, 1993). He
developed the input-environment-outcome (I-E-O) model (Figure 9, Appendix B) to test
his variables. A major finding was Astin ‘s (1993) identification of the student’s peer

31
group as “the single most potent source of influence on growth and development during
the undergraduate years” (p. 398). He also found that students who were most involved in
college had the greatest positive outcomes.
This identification of involvement’s importance led him to create the theory of
student involvement, which has five basic postulates (Astin, 1999):
1. Involvement refers to the investment of physical and psychological energy in
various objects. The objects may be highly generalized (the student
experience) or highly specific (preparing for a chemistry examination).
2. Regardless of its object, involvement occurs along a continuum; that is,
different students manifest different degrees of involvement in a given object,
and the same student manifests different degrees of involvement in different
objects at different times.
3. Involvement has both quantitative and qualitative features. The extent of a
student’s involvement in academic work, for instance, can be measured
quantitatively (how many hours the student spends studying) and qualitatively
(whether the student reviews and comprehends reading assignments or simply
stares at the textbook and daydreams).
4. The amount of student learning and personal development associated with any
educational program is directly proportional to the quality and quantity of
student involvement in that program.
5. The effectiveness of any educational policy of practice is directly related to
the capacity of that policy or practice to increase student involvement, (p.
519).
In the higher education literature, the I-E-O model has been used in a variety of
methods and circumstances. While some studies have used it for the standard outcomes
of persistence, retention, graduation, and academic achievement (Kelly, 1996; Forman,
2009; Edmunds, 2010; DiRamio & Jarvis, 2011; Kjelgaard & Guarino, 2012), others
have used it to study other types of outcomes such as levels of involvement or activism
(Stein, 2007; Page, 2010; Sprow, 2011). The model also has worked in conjunction with
other theoretical constructs such as Pace’s Quality of Student Effort theory.
Unfortunately, Whitmire’s (2002) search of the higher-education literature only found a
few older studies that examined the relationship between academic library resources and

32
undergraduate student outcomes. These studies either found a weak or no relationship
between the two (Whitmire, 2002).
Although an increasing number of LIS studies are focusing on how libraries
impact student outcomes (for example Lindauer, 1998; Matthews, 2012; Soria, Fransen &
Nackerund, 2013), few have cited Astin’s work. Kuh and Gonyea (2003) cited student
involvement theory work by Astin (1993) and Pascarella and Terenzini (1991) in their
study of how the academic library promoted student engagement in learning. Based on
data from more than 300,000 students from 1984-2002 from the College Student
Experiences Questionnaire (CSEQ) Kuh and Gonyea (2003) were able to control for
demographic input variables of students and institutions. They also developed three
outcome measures for gains in information literacy, overall gains in college, and
satisfaction with the college experience. Although their study did not find that any library
experiences directly contributed to any of the outcomes they explained that this was
probably because the questions on the CSEQ are not meant to measure information
literacy.
In LIS literature, Whitmire (1998) was the only researcher found to utilize Astin’s
I-E-O model, which along with Pace’s Quality of Effort theory formed the theoretical
framework for her study. The study found that student background characteristics (such
as grades and class year); library experience (such as focused library activities);
experiences with faculty (such as informal interactions); course learning experiences
(such as active course learning); and writing experiences (such as conscientious writing)
were the most influential on student’s critical thinking (Whitmire, 1998). The
specification of focused library activities versus more routine library activities gave

33
insight into earlier research which seemed to be contradictory (Kuh &Gonyea, 2003, p.
258). One study by Ory and Braskamp (1988) found a positive relationship between
library activities and critical thinking, but another study by Terenzini, Springer,
Pascarella, and Nora (1995) had found a negative relationship. The former study involved
library activities that required a higher level of involvement, while the latter involved
library activities that required a lower level of involvement.
Although Astin’s theory and I-E-O model could be used as a theoretical
framework for the dissertation, it may not be appropriate for studying IE specifically.
Because such little work has been done on defining IE that it seems premature to study its
effects on student outcomes, especially if those effects are as indirect as previous research
seems to indicate. Another difficulty of studying IE in an academic library context is that
students do not have to access physical or digital resources or services very frequently.
Formal information sources, such as academic libraries, have reported a decrease in the
amount of information accessed, which was corroborated in the Digital Visitors &
Residents (V&R) project, a recent IE study by Connaway, Lanclos, and Hood (2013).

The Digital Visitors and Residents (V&R) project
One project that has investigated how individuals engage with and acquire
information, as well as the reason for decisions in these processes, is the Digital V&R
project, which is a collaboration between the Joint Information Systems Committee
(JISC) and the Online Computer Library Center, Inc. (OCLC). From 2011 through 2013
the research team conducted semi-structured interviews, collected diary entries from
participants about their IE, and conducted a survey. The study participants have been

34
students and faculty in the United Kingdom and the United States. The study broke
participants into four groups, based on their educational stage. The groups included late
stage secondary school and first year undergraduate students, second and third year
undergraduate students, postgraduate and Ph.D. students, and post doctorate scholars. By
monitoring how participants’ information practices change over time via the diary entries,
the study findings call for further research on the impact of IE rather than the impact of
academic library use on student success.
One of the project’s findings was that participants were least likely to contact a
librarian when looking for information (Connaway et al., 2013). This was true of each of
the four participant groups. Participants were more likely to ask their teachers or
professors for help finding information, then their peers, and other experts and
professionals (Connaway et al., 2013). While reference and other librarian consultations
are just one of the services that libraries provide, it is clear that although academic
libraries are providers of the monographs and serials necessary for research, they are not
necessarily where students and faculty go for information. By broadening the concept of
information to include that found outside of the academic library, it may be possible to
find new ways that the library can impact IE, although little work has been published on
this topic.

O’Brien and Toms’ user engagement framework
O’Brien and Toms (2008) conceptualize user engagement as a “quality of users’
experience with technology” (p. 950). Their study analyzed the applicability of flow,
aesthetic, play, and information interaction theories on users’ experience with technology

35
to what they called engagement attributes. These attributes included aesthetics, affective
appeal, attention, challenge, feedback, goal-directedness, meaningfulness, motivation,
perceived control, and sensory appeal. Engagement attributes were then woven into
sensual, emotional, and spatiotemporal threads of experience, that progressed through a
model of engagement that began with a point of engagement, proceeded to engagement,
and eventually ended in disengagement, with the possibility of re-engagement. Although
O’Brien and Toms’ (2008) framework and model drew on a variety of engagement
related theories, and included “physical, cognitive, and affective components of user
experiences” (p. 938), their explicit differentiation of their framework and model as a
quality of an experience, rather than a process or product makes it difficult to compare
with higher education models. Most importantly, the end goal for this conceptualization
of engagement was the experience itself, rather than outcomes that existed outside the
technological system such as graduation, which was why it was not found to be a suitable
model for this dissertation.

From distant admirers to library lovers - and beyond: A typology of public library
engagement in America
Although relating outcomes to engagement behaviors was outside the scope of
this dissertation, the next step in this line of research would ideally relate IE behaviors to
positive student outcomes. Outcomes of IE for this population could vary, and the
variance could perhaps be predicted by categorizing academic library users and nonusers. One way to categorize users and non-users is by creating IE typologies. The Pew
Research Institute (Zickuhr, Purcell, & Rainie, 2014) created a typology of public library

36
user engagement for library users and potential users based on a combination of user
centered factors. These factors included participants’ library usages, attitudes toward their
library, and modes of accessing library resources. In addition to explaining why users in
each group exhibited different types of behaviors, the study also indicated user-related
outcomes from their use of the library. By creating these groups, those working in public
libraries can better market their services to underserved users, according to Zickuhr et al.
(2014). Librarians also may bring non-users or users with low levels of engagement to
higher levels of engagement and library use. The Zickhur et al. (2014) study informs this
dissertation in that it identified user-centered concepts of engagement and ways to
differentiate users. However, it did not offer a suitable IE theory or model for this
dissertation.

Michie, Van Stralen, and West’s COM-B model
The theories, models, and frameworks described in this section did not provide a
suitable theory or model for IE. A general model that examines the engagement factors
that affect engagement behaviors was found in the field of health communication.
Michie, Van Stralen, and West (2011) suggested that a patient’s capabilities (C),
opportunities (O), and motivation (M) influence their IE behavior (B) with regard to
information on their treatment or other health care (see Figure 10, Appendix B). These
factors define a patient’s capabilities as their previous knowledge and informationseeking abilities. Opportunities are provided by the social and physical environments of
the patient. Motivation refers to the patient’s view of their agency and confidence in
finding the information.

37
The COM-B model forms the background for the initial approach to this study
(see Figure 11, Appendix B). The participant’s capabilities refer to their knowledge of
information sources, including what they have and have not used in the past. Their
opportunities for finding information will depend on what their institution offers in the
form of staff, which includes the services they provide, collections, space, and
documents. However, their information environment also includes non-institutional
sources of information, which the V&R study (Connaway et al., 2013) suggests may
make up the bulk of their information sources. Their motivation will depend on how they
feel about the assignment and their chances of finding information during a single critical
incident, as defined by Flanagan (1954). The behaviors suggested by the literature come
from Davenport and Prusak’s hierarchy of IE behaviors (Figure 12, Appendix C).

Doctoral students’ IE
Based on the limited work done on IE in an academic context, the ideal study
population would need to exhibit high levels of information use and work towards
outcomes that were closely related to information. In order to demonstrate academic
library value, the information required for this ideal study population would not easily be
accessed through other information sources. Earlier in the chapter, it was suggested that
faculty, graduate students, and researchers would be useful groups to study because their
outcomes, including theses, grants, patents, and publications required high amounts of
specialized information that would be expensive and/or inconvenient to access outside
the library’s resources and services. The V&R project is one of the few studies that has
compared the information behaviors of education levels from late high school (pre-

38
college) through faculty member (Connaway et al., 2013). One of the findings of this
project suggests that post-graduate and doctoral students are the group most likely to get
information from the academic library, while the faculty and life-long learners were the
least likely to mention the academic library (Connaway et al, 2013).
LIS research suggests that doctoral students, a sub-group of graduate students, are
a useful group to study because at the start of their program, they exhibit many of the
same behaviors as undergraduate or even other graduate student groups. As they
transition from students to scholars (Fleming-May & Yuro, 2009), they eventually begin
to resemble faculty members and more experienced researchers, especially in how they
search for and evaluate information (Catalano, 2013; Spezi, 2016), which puts them in a
unique position to benefit from library instruction. However, doctoral students are also an
under-studied population (Fleming-May & Yuro, 2009; Du & Evans, 2011; Switzer &
Perdue, 2011). IE among this population is even less studied. Aside from the work of
Green and Macauley (2007), no other studies have been found that have explored
doctoral students’ IE. This is also an important population to reach because recent studies
of junior scholars, which include doctoral students, post-docs, and junior faculty, have
reported that the library is losing visibility from this population in that many do not
realize that the resources that they use for their research are provided by the library
(Nicholas et al., 2016). While some studies suggest that graduate students, including
doctoral students, are less likely to start looking for information using the library
(Calhoun, Cantrell, Gallagher, & Hawk, 2009; Inger & Gardener, 2013), others suggest
that roughly the same amount will begin a search using library resources as an Internet
search engine (Niu, Hemminger, Lown, Adams, Brown, Level, McLure, Powers,

39
Tennant, & Cataldo, 2010). Blumer, Watulak, and Kenton (2012) found that library
resources are also more likely to make them confused and anxious, regardless of whether
or not they received instruction.
Most LIS studies of doctoral students focus on their information-seeking
behaviors. A meta-analysis by Spezi (2016) reports on doctoral student information
searching behavior from 2010 to 2015. While convenience was found to be a major factor
in finding information, as it is for most academic library users (Connaway et al., 2013),
Spezi (2016) confirms that most studies found information quality, such as whether it was
published in a peer-reviewed journal, was the most important factor in finding
information for this group. Spezi (2016) describes the difficulty in comparing findings
across studies as the literature tends to divide user groups by educational level, discipline,
and sub-discipline. The main objective of Spezi’s (2016) meta-analysis though was to see
if information search behavior had changed in the last five years. She concluded that the
most significant change was in the use of social media in finding information, but that if
anything, doctoral students across the disciplines were more likely to search for
information in similar ways than in differing ones.
An earlier meta-analysis by Catalano (2013) found that information search
patterns varied among graduate students. Catalano (2013) defined information-seeking as
behaviors pertaining to information selection and use, and noted that the research
questions could be grouped as the following:
1.
2.
3.
4.
5.

Who do graduate students go to for research help?
How do students use library resources and more informal sources?
What are the disciplinary differences in information behaviors?
What types of search strategies do students engage in?
How do students cope with information overload? (p. 253)

40
While there were only 11 out of 48 studies from 1997-2012 that only investigated
doctoral student information behavior, an additional 15 studies investigated doctoral and
Masters students and compared the differences between the two groups. A major issue
with the studies of doctoral and post-doctoral students was that “all of the studies were
qualitative in design and investigated a small sample within one discipline” (Catalano,
2013, p. 259). The studies that focused on doctoral and post-doctoral students were
mainly concerned with “researching and writing a literature review, or information source
preferences, in order to improve library services…[and the] role of research supervisors”
(p. 259). In terms of starting points for research, faculty members, namely the student’s
advisor, was the first person consulted and the Internet was the main starting point (p.
259-260). Other findings include the diverse ways in which students use libraries, and
two important factors that influenced information seeking were the student’s discipline
and whether or not they were an international student (Catalano, 2013).
The British Library and the Joint Information Systems Committee (BL/JISC)
recently published the most comprehensive study on doctoral student research behavior
to date (Carpenter, Wetheridge, & Tanner, 2012). This study consisted of 60 longitudinal
interviews and 13,593 total surveys submitted annually over a three year period in 72
HEIs in the United Kingdom. However, doctoral studies in the United Kingdom and the
United States differ in structure, which means that student needs and academic library
intervention opportunities differ.
Doctoral programs in the United States require students to have highly focused
and rigorous information needs as compared to other academic library user groups.
Programs usually begin with two years of coursework, which are followed by the

41
preparation of a reading list for qualifying or comprehensive exams, and culminate in
research for the literature review and other work on a dissertation. Consequently, doctoral
students have more demanding research requirements than undergraduates (Green &
Macauley, 2007; Fleming-May & Yuro, 2009) and even other graduate students (Harris,
2011), such as those in professional Masters programs. Doctoral students also may differ
from faculty members and other post-Ph.D. scholars in their information-seeking habits.
A study by Mikitish and Radford (2013) found that the information acquired during a
student’s doctoral program forms the foundation for their future scholarship, and some
students stated that they need less assistance in finding information as they advance in
their studies because they have collected what is most relevant to them and they are
aware of how to find more information in their area. Other researchers have suggested
that post-Ph.D. scholars go elsewhere besides the academic library for information, and
may not be open to learning different ways of engaging with the library information
services or resources (Ondrusek, 2008; Anthony, 2010; Searing & Greenlee, 2011).
Doctoral students also have a unique perspective attributed to the length and depth
of their studies compared to undergraduates. Many doctoral students have attended
different institutions for their undergraduate and other degrees (Monroe-Gulick & Petr,
2012). Experiences with different academic libraries can give them a perspective that
allows them to better use and assess their current one (Cook & Heath, 2001; Yu, Hong,
Gu & Wang, 2008). However, assessing that experience can be difficult. Monroe-Gulick
and Petr (2012) interviewed 15 incoming doctoral students in the social sciences to assess
how their information literacy skills measured up to the ACRL Information Literacy
Competency Standards for Higher Education (ACRL, 2010). However, the researchers

42
realized that their own bias and the difficulty of translating students’ descriptions of their
information literacy skills to match the standards caused them to initially assess the
students as not meeting many of the standards, when in actuality many of the students
were close to or did meet the standards (Monroe-Gulick & Petr, 2012).
Regardless of doctoral students’ information literacy skills at the beginning of
their program, most research highlights researching information for and writing the
doctoral dissertation as being a time when academic library interventions can have the
most impact (Libutti & Kopala, 1995; Fleming-May & Yuro, 2009; Du & Evans, 2011;
Harris, 2011). Suggested interventions have ranged from offering dissertation research
and writing seminars (Switzer & Perdue, 2011), to workshops on specific research and
writing topics (Fleming-May & Yuro, 2009), to explicitly pairing students with librarians
(Harris, 2011). However, offering the most effective type of intervention may depend on
the program of study (Fleming-May & Yuro, 2009).
Doctoral programs are generally separated into four areas of study: the
humanities, social sciences,the sciences, and professional schools. Research into
scholars’ information behavior, both pre- and post-Ph.D., tends to only focus on the first
three areas, and indicates that practices differ between areas (Ellis, 1993; Folster, 1995;
Line, 2000; Fleming-May & Yuro, 2009; Carpenter et al., 2012). Of these three areas,
social science work tends to be the most interdisciplinary in nature (Catalano, 2013),
social science students are more likely than students in the sciences to pick their topics of
study (Horlings & Gurney, 2013), and are more likely than students in the humanities to
favor journal articles than books or other texts. These factors arguably make social
science doctoral students more likely to benefit from library instruction on information

43
seeking, and most studies that focus specifically on doctoral students in one of the three
areas have recruited participants from the social sciences (Fleming-May & Yuro, 2009;
Monroe-Gulick & Petr, 2012; Mikitish & Radford, 2013). Due to the differences between
doctoral students and other academic library user groups, and the differences between
doctoral students in the social sciences compared to those in the humanities and the
sciences, the dissertation drew its participants from doctoral students in the social
sciences.

Summary of IE literature
The literature reviewed in this chapter identifies a set of four core claims
regarding IE. These core claims are:
1. IE is a framework that can, but does not have to, support behavioral, emotional,
motivational, and cognitive dimensions.
2. IE may be measured and analyzed using qualitative methods, quantitative methods, or
a combination.
3. IE can increase the likelihood of information users’, in this case doctoral students’,
attainment of short and/or long term outcomes.
4. The IE outcomes can be measured at the individual and/or the institutional level.
The four core claims listed above indicate that IE is a flexible framework, which is
important because information resources and use vary by individual and situation.
However, when considering the state of IE research in light of these four core claims, it is
clear that there are gaps in LIS research. One gap is that usually only one dimension of IE
is investigated at a time, and this is the behavioral dimension that outlines what students

44
and others in higher education do. Another gap is that much of the research seems to
imply that library resources and services are primary providers of information, which a
few studies, such as the V&R Project (Connaway et al., 2013), suggest narrows the
potential IE research area.
As explained in the literature review above, most of the IE and information
behavior literature focuses on seeking and accessing information, rather than organizing
or using it, even though information behavior is defined as encompassing these behaviors
(Pettigrew, Fidel, & Bruce, 2001; Ford, 2015). In practice, and based on studies by
academic librarians, most library instruction classes, especially one-shot classes, focus on
seeking and accessing information through the library. The very name of “library
instruction class” suggests that the session will include instruction about the library,
instead of instruction by librarians on information classes, as opposed to a designation
such as “information literacy.” However, even information literacy classes tend to focus
on finding and accessing information, despite academic library mission statements
commonly stating that the library would help discover and create knowledge (Kerr,
2010). While Kerr (2010) acknowledges that librarians might not be trying to actively
deceive those in their institution, her work suggests that in order to truly emphasize
information literacy rather than the library’s resources and services, librarians should
tailor their instruction to individuals’ particular situations and needs rather than
employing a one size fits all instruction.
The two major gaps in IE literature that this dissertation addresses are prevalence
of only considering one dimension of IE and focusing on the library rather than the
individuals who use, or could use, the library and other sources of information. To

45
address these gaps, the dissertation study data collection questions asked participants to
relate why they pursued information from a certain source or in a certain manner in order
to see whether emotional, motivational, or cognitive aspects could be used to discuss IE
for social studies doctoral students. Although most of the questions related to seeking and
accessing information, participants were also asked how they filtered and organized it.
Finally, the questions asked the participant to discuss information rather than the library
resources and services that provided the information. In this dissertation, a variety of
mixed methods are used to collect and analyze qualitative and quantitative data. The
research questions and methodology used follows in the next chapter.

46
CHAPTER 3: METHODS
Research questions
This chapter outlines the methodology used in the dissertation to answer the
research questions (RQs), which were based on the higher education and Library and
Information Science (LIS) literature reviewed in Chapter 2, and are as follows:
RQ1: What are the critical factors that characterize information engagement (IE) for
doctoral students in the social
sciences?
RQ2: What IE types exist for doctoral students?
RQ3: How is IE related to the value of academic libraries?
RQ4: How can academic libraries promote increased IE of doctoral students?

Problem statement
In the context of higher education assessment, the research in higher education
and LIS suggest that there is a need for deeper understanding of what to assess and better
metrics to assess the institutional departments and units that contribute to the institution’s
goals, especially with regard to student centered outcomes. The LIS studies in Chapter 2
illustrate the difficulty in assessment because information related behavior varies widely,
and library resources and services have so far been found to have an indirect effect at best
on the student success outcomes that are commonly studied, such as graduate, retention,
and recruitment (Whitmire, 2002; Kuh & Gonyea, 2003). However, studies that tie
library resources and services to information specific outcomes, such as critical thinking
(Ory & Braskamp, 1988) and focused library activities (Whitmire, 1998), found that the
academic library had a greater impact on student success outcomes. Engagement studies
offer academic libraries blueprints for how to appropriately define different types of

47
engagement factors, which include cognitive, motivational, and emotional factors that
can affect behavior in a variety of fields.
The few studies on IE and the relative dearth of studies on doctoral students
compared to other academic library users and potential users indicate that a study of IE
for doctoral students may provide new insight on academic library value. However, the
lack of research also suggests that such a study explore, conceptualize, and to some
extent test the effect of IE on doctoral student behavior. Qualitative methods allow the
researcher to collect rich data that explores the cognitive, motivational, and emotional
factors that could affect IE behavior. Quantitative methods allow the researcher to test the
effect of those factors on behavior. A mixed methods approach combines these two
methods.
An explanation of why and how mixed methods were incorporated in this
dissertation study follows in the Research design section that follows. A table that
summarizes the methods used in each phase prefaces the following section. A more indepth justification of the rationale behind the use of each method, the data collection
procedures, and the analysis procedures follow for each of the three phases of the
dissertation study. The data collection and analysis procedures for the three phases of the
study are described below. The data collection descriptions include details about
participant recruitment, and data collection instruments. The data analysis procedures
descriptions include the criteria used to add, modify, or remove items from the data
collection instruments or analyses used in subsequent phases.

48
Research design
The study utilized a mixed methods explanatory sequential design, as described
by Creswell and Clark (2011), and the Rutgers University Arts and Sciences Institutional
Review Board approved the methodology for each phase prior to data collection (see
Appendix D for approval notices). According to Creswell & Clark (2011), exploratory
sequential design consists of the following steps: a) qualitative data being collected and
analyzed, b) quantitative data being collected and analyzed, and c) the qualitative and
quantitative results interpreted (p. 88). Qualitative data collection and analysis methods
were used in Phase 1 and Phase 2 of this study. The qualitative data collection method for
Phase 1 was a focus group interview, and for Phase 2 the data was collected using
individual interviews. Both Phase 1 and 2 used the constant comparative method
(Charmaz, 2014) to analyze written verbal transcripts of the data. Phase 3 utilized a
survey to collect quantitative and qualitative data, which were analyzed using quantitative
and qualitative methods, respectively.
The use of qualitative and quantitative collection and analysis methods can be
difficult considering the differing epistemological paradigms that underlie the two
methodologies. Qualitative methodology utilizes an interpretivist paradigm, which is
based on the “individual’s cognitive viewpoint” (Bates, 2005, Part 2, para. 20), and
quantitative methodology utilizes a postpositivist paradigm, that embraces “ontological
reality” (Budd, Hill, & Shannon, 2010). The former requires an inductive approach, and
the latter, a deductive one. For this reason, Creswell and Clark (2011) outline six mixed
methods designs. A majority of the designs keep the qualitative and quantitative data
collection and analysis methods separate, and often one methodology is prioritized over

49
the other. Despite these guidelines, mixed methods research can be difficult, and few LIS
doctoral dissertations utilize it (e.g., Senteio, 2015; Das, 2013).
On the other hand, the exploratory sequential design can be useful when there are
no available measures or instruments to study a phenomenon and no single guiding
framework or theory exists to inform qualitative or quantitative data collection
instruments, such as interviews or surveys (Creswell & Clark, 2011). One affordance of
this study design is that the qualitative phase consists of a narrower selection of
participants, who can provide a broader range of rich and detailed information about the
phenomenon studied. After analyzing the qualitative data for key themes, a quantitative
phase can then recruit a broader selection of participants to give feedback on a narrower
range of topics. This study emphasizes theory development because the quantitative
survey aspect is then able to generalize the qualitative findings to a wider population
(Creswell & Clark, 2011). The different sampling, data collection, and analysis
techniques complemented each other when performed in sequence while also mitigating
the inherent weaknesses of each method, which is described in more detail in each phase
of the dissertation study below.
Due to these affordances and the lack of LIS research on IE and on doctoral
students, mixed methods allowed for a triangulation of the data that multiple qualitative
or multiple quantitative methods did not allow. Explanatory sequential design was
another mixed methods design that was also considered for this dissertation study, and is
very similar except that the quantitative aspect precedes the qualitative aspect.
Furthermore, in explanatory sequential design, the quantitative methodology is also

50
prioritized above the qualitative, but given the lack of scholarship in this area, an
inductive rather than a deductive approach seemed more appropriate.

Summary of phases
Purpose

Time
period
Spring
2016

Sample
size
N = 20

Data
Link to RQ
collection
Phase Identify IE
3 focus
RQ 1: identified IE factors
1
factors and
group
RQ 2: identified possible
behaviors in
interviews
IE types
order to refine
RQ 3: identified possible
Phase 2
IE behaviors
interview
RQ 4: suggested IE topics
questions
for library
instruction
Phase Explore IE
Summer N = 15
Individual RQ 1: identified critical IE
2
factors and
2016
(5 from
interviews
factors
behaviors
each
RQ 2: suggested use of IE
identified in
stage of
factors rather than
Phase 1 in
program)
IE types
order to refine
RQ 3: refined behaviors to
Phase 3
measure
survey
RQ 4: refined IE topics for
questions
library instruction
Phase Measured the Winter
123
Online
RQ 1: measured effect of
3
effect of
2017
doctoral survey
IE factors
factors on
students
RQ 2: categorized IE
behaviors
factors
explored in
RQ 3: tested IE factors on
Phase 2 in a
behavior
larger sample
RQ 4: measured interest in
IE topics for library
instruction
Table 3: Overview of dissertation phases
Table 3, above, outlines the dissertation study by phase. It includes the purpose,
time period, sample size, data collection method, and link to each RQ that the data
collection and analysis supported. This information is explained in greater detail below,
by phase.

51
Phase 1: Exploring information engagement (IE) – Focus group interviews
Because little research has been found on IE, as discussed in Chapter 2, this study
included two phases of qualitative data collection and analysis. Phase 1, the pilot study,
consisted of focus group interviews, that drew participants from a small population pool
and covered a wide range of topics. The main objective of Phase 1 was to collect data that
would inform the research questions, as well as refine the interview and survey questions,
for the subsequent phases. As noted by Connaway and Powell (2010), focus group
interviews are used frequently to identify the perceptions and attitudes of a target
population. As explained in Chapter 2, doctoral students were the target population of
this study due to their expected exhibition of high levels of information related
engagement. Connaway and Radford (2017) identify this data collection technique as
being useful, especially in the early stages of qualitative or quantitative research. This is
because by observing several interactions between participants in a relatively short
amount of time, the researchers are able to orient themselves quickly to the topic, develop
new ideas from what is present in the existing literature on the topic, and develop or
refine data collection instruments. The Fleming-May and Yuro (2009) and Carpenter et
al. (2012) studies also utilized focus groups, and this technique is preferable to
observational or experimental research designs because little is known about how to
frame the topic of IE.
Despite the many advantages of focus group interviews, there are several inherent
limitations to this approach. Participants are not necessarily representative of the
population that the researcher wishes to study, and the researcher only will be able to
cover four or five themes in each session (Connaway & Radford, 2017). Connaway and

52
Radford (2017) also caution that researcher or participant bias may affect participant
response, especially if individual participants feel intimidated or pushed to conform to the
group’s consensus. The data obtained from focus group interviews are also subjective,
which may make it difficult to analyze, summarize, interpret, and apply to the larger
population (Connaway & Radford, 2017).
The study sought to mitigate these Phase 1 limitations by using a mixed methods
approach that included individual interviews and a survey in Phases 2 and 3, respectively.
More specifically, participant responses in the focus group interviews of this phase
primarily informed RQ1 and RQ2, the individual interview questions asked in Phase 2,
and the survey questions in Phase 3. Analysis of the Phase 1 focus group data suggested
preliminary conceptualizations of IE, and identified short-term outcomes related to IE,
which will be discussed in Chapters 4 and 5.

Phase 1 data collection
Participants for Phase 1 of the dissertation were drawn from students in social
science doctoral programs at one large public research university in the northeastern
region of the United States. They were recruited using three methods. First, a recruitment
email was sent to the institution’s graduate student events listserv and posted on February
8, 2016. The listserv is opt-in and delivered weekly. It includes information for graduate
students on services, events, and invitations to participate in studies that other students
are running. Second, the departmental administrative assistants working in the selected
social science departments were requested to forward a recruitment e-mail to their
doctoral student listservs. This second recruitment method was suggested by Monroe-

53
Gulick and Petr (2012). Fourteen departments were contacted, including Anthropology;
Communication and Media Studies; Economics; Education; Geography; Industrial
Relations and Human Resources; Linguistics; Planning and Public Policy; Public Health;
Political Science; Psychology; Social Work; Sociology; and Women’s and Gender
Studies. Three administrators confirmed that they had posted the notice, and of these
three only students from two expressed interest in participating in the study. Third,
recruitment fliers were placed on campus in the Graduate Student Lounge and the
Graduate Student Reading Room in one of the university’s libraries.
Potential participants were screened to ensure that they were enrolled in social
science programs, excluding LIS. As described in the previous chapter, studies of social
science doctoral students have included participants from Anthropology, Political
Science, Psychology, and Communication (Fleming-May & Yuro, 2009; Monroe-Gulick
& Petr, 2012; Mikitish & Radford, 2013). However, the definition of “social science” is
quite broad as it includes any fields that "study human society and social relationships"
(OED, 2017). The Rutgers University Libraries (RUL, 2017) include the following areas
in the social sciences: African Studies; Anthropology; Childhood Studies;
Communication; Criminal Justice; East Asian Studies; Economics; Education;
Environmental Studies; General Social Science; Geography; Labor and Employment
Studies; Latin American, Caribbean, Spanish and Portuguese Studies; Latino Studies;
LIS; Linguistics; Planning and Public Policy; Political Science; Psychology/Behavioral
Sciences; Public Administration; Social Work; Sociology; South Asian Studies; and
Urban Studies (2017). LIS doctoral students were specifically not recruited or accepted to
participate in this study even though LIS is a social science. This is because LIS students

54
have information behaviors that are thought to be different from those in other disciplines
(Tracy & Searing, 2014), and their responses may skew the data.
Given these broad definitions of social science, any student, who was enrolled in
a program listed by the RUL (2017) list above, except for LIS, was invited to participate.
Any participant who was not enrolled in programs from these departments was asked if
they felt their work fell into the realm of social sciences, even if their program would
usually be classified in the arts and humanities or the sciences, and those who claimed
that theirs did were included in the study.
The focus groups occurred between February 19, 2016 and February 25, 2016.
Prior to each focus group interview, the participants were asked to sign an informed
consent form and complete a short demographic profile. Focus group interview questions
consisted of open questions regarding different aspects of IE and outcomes. The specific
focus group interview questions are listed in Appendix E1. During each of the three
sessions, one or two other graduate students enrolled in the university assisted the
researcher in moderating and note taking. An audio recording of each focus group was
made and transcribed verbatim by the researcher. The participants were each
compensated with a $20 gift card and light refreshments.

Phase 1 data analysis
The verbatim transcripts of the interviews were analyzed using the constant
comparative method (Charmaz, 2014) to elicit major themes relating to the research
questions. Transcripts were read in an iterative fashion to identify themes and subthemes, and a coding scheme was developed during this process. Each focus group

55
interview transcript first was read through in its entirety before noting themes and
patterns. Subsequent readings involved careful attention to patterns and emergent themes
and subthemes. These themes and subthemes were captured as codes and recorded in
NVivo (see NVivo, 2017), along with any “juicy quotes,” which are statements that best
encapsulate or illustrate the emerging themes/categories. Another graduate student
trained in the constant comparative method coded 20 percent of the responses to check
the validity of the codes. Any discrepancy in coding was discussed to resolve differences,
as needed. A list of preliminary themes is in Appendix F. A final codebook that lists the
final categories, their definitions, examples from the transcripts, and the number of times
that they appear is in Appendix G. The quotations in Appendix G give examples of the
code, while the examples given in Chapter 5 use participant responses to further illustrate
the discussion topics.
One contribution of the Phase 1 focus group interviews was the creation of a
preliminary codebook (Appendix F) that would be later refined in Phase 2. Therefore, the
criteria for code inclusion was less rigorous than in Phase 2. All of the focus group data
were collected before analysis began, so the researcher was unable to modify the
questions or follow-up questions based on the themes and sub-themes that emerged
interview by interview. However, the researcher did ask the group for consensus on IE
related outcomes and the effect of flow on IE, because these had not been reported in the
existing literature. The researcher also asked the group for feedback on the focus group
interview questions and the demographic survey. Based on the feedback, the question
order was modified in Phase 2. Although participants also offered feedback on the
demographic survey, the researcher did not incorporate this feedback into the next phase

56
of the study because the feedback mainly suggested removing questions that the literature
suggested might affect IE.
In summary, Phase 1:
x

Identified preliminary qualitative themes (see Appendix F)

x

Refined Phase 2 individual interview questions (see Appendix E1 for Phase 1
interview questions and Appendix E2 for Phase 2 interview questions)

Phase 2: Creating IE typologies – Individual interviews
The focus group interviews in Phase 1 recruited from one institution, and the
doctoral students participating were questioned about broad aspects of their IE. The
individual interviews in Phase 2 built upon these findings and broadened the pool of
potential respondents, while narrowing the area of inquiry based on the analysis of key
themes from the focus group interviews. The main objective of this phase was to narrow
the area of inquiry still further and inform the questions asked on the quantitative survey
instrument deployed to a wider range of potential participants in terms of disciplines and
institutions. Because the quantitative part of an exploratory design generalizes qualitative
findings for larger populations (Creswell & Clark, 2011), the participant’s disciplines and
institutions also helped determine which programs in which institutions were sent
recruitment notices in the following quantitative survey, Phase 3.
For Phase 2, individual interviews were used instead of other qualitative methods
because individual interviews provide more depth than focus groups and have been used
in a majority of the qualitative studies of social science scholars (Ellis, 1993; Green &
Macauley, 2007; Monroe-Gulick & Petr, 2012; Carpenter et al., 2012). A literature

57
review by Greifeneder (2014) found that individual interviews also are prevalent in LIS
literature, and were the most commonly used data collection technique in information
behavior research studies published in the Journal of the Association for Information
Science and Technology (JASIST), Information Research, the Journal of Documentation,
and iConference proceedings from 2012 to 2014. Connaway and Radford (2017) identify
the affordances of this method as having higher response rates than surveys, even if the
topic is complex or emotional for the participants. By interacting directly with the
participant, the researcher is able to be more flexible in pursuing more detailed
information and clarifying any potential misunderstandings (Connaway & Radford,
2017). Interviews can be used to study phenomena that are difficult to observe or that rely
on the participants’ interpretations of events. One-on-one interviews are an especially
useful data collection technique for this study because each participant’s motivations and
expected outcomes are hard to ascertain from their actions. Once these motivations,
expectations, and other unobservable facts are explicated, the researcher can validate and
verify findings in subsequent interviews (Connaway & Radford, 2017). Individual
interviews also have an advantage over focus group interviews in that they do not require
participants to be in the same physical or virtual space at the same time, which makes it
possible to recruit a more diverse set of participants.
The limitations of this technique are similar to those of the focus group interview.
Interviewer bias is still a major concern, and the one-on-one nature of the individual
interviews may make the participant more hesitant to give an honest opinion (Connaway
& Radford, 2017). The study used a semi-structured interview instrument in order to
lessen the possibility of participant intimidation. The other major limitation of interviews

58
is that it is more difficult to select participants who are a representative sample of the
applicable population compared to a randomized sampling method, which usually is used
in surveys. One way in which this study tried to recruit a diverse range of participants is
by recruiting students from different stages in their Ph.D. programs. Mikitish and
Radford (2013) suggested that program stage, which included a) coursework, b) prequalifying exam, and c) post-qualifying exam, may influence participant motivation,
knowledge, and behavior. Additionally, using mixed methods across all three phases, are
employed to mitigate these limitations (Creswell & Clark, 2011).

Phase 2 data collection
As mentioned previously, because Mikitish and Radford (2013) found that
students in the three program stages had different information needs and expectations, an
equal number of students from each of these stages were recruited using two different
methods. First, a recruitment email was posted on July 12, 2016 to the same graduate
student events listserv as in Phase 1. Two other opt-in listservs at the same institution also
were contacted. No participants mentioned explicitly seeing the recruitment notice on
those listservs, and it is unknown whether the notice was posted to them. The social
science departments that were contacted in the Phase 1 were not contacted for Phase 2 in
order to leave more interview slots available to students from other institutions. For the
second recruitment method, a recruitment email was sent to the Communication,
Research, and Theory Network (CRTNET) e-mail listserv managed by the National
Communication Association. The notice was posted on July 6, 2016. This was the only
national listserv used in recruitment for Phase 2 for three reasons. First, it was the only

59
national one suggested by focus group participants. Second, there were more focus group
participants in Phase 1 from the Communication and Media Studies Departments than
any other discipline. Third, it already had been used in a previous study of doctoral
students (Mikitish & Radford, 2013) on a similar topic.
Once potential participants were verified as being enrolled in the appropriate
programs identified in Phase 1 they were interviewed over Skype. As in the focus group
interviews, participants were asked to sign an informed consent form and demographic
profile, which also helped ensure that the desired minimum of three students from each
program stage were recruited. The individual questions, which are listed in Appendix E2,
utilized the critical incident technique (CIT) (Flanagan, 1954) and open questions
regarding different aspects of IE and success. The CIT requires participants to recall the
most critical factors influencing a specific incident, and they were modified with respect
to feedback from the participants in Phase 1.
The CIT was the most appropriate data collection methodology for Phase 2 of the
study because it allowed the researcher to ask more open questions about a topic of which
little is known, and it allowed the participant to decide what is relevant to the topic of
study (Flanagan, 1954; Fisher & Oulton, 1999; Radford, 2006). Compared to the fewer
open ended questions that elicited a wide range of feedback in the focus group
interviews, the CIT questions in this phase were more numerous, but had a narrower
focus. A few noted areas of concern with using the technique is whether the participants
can articulate their experience and opinion in a way conducive to answering the research
questions and whether the participants are valid judges of what is critical. The first
concern is addressed by selecting doctoral students, who are a group that the literature

60
suggests have the ability to articulate their thoughts on IE. The second concern is
addressed by the interview questions and how they frame a critical incident.
There are two definitions of what makes an incident critical from LIS studies that
use the CIT. The first interpretation, which is more in line with what Flanagan (1954)
described, asks the interviewee to recall a significant experience, usually a positive or
negative one (e.g. Gilstrap & Dupree, 2008; Connaway & Radford, 2010; Bickley &
Corrall, 2011; Johnson & Simonsen, 2015). The second interpretation of CIT asks the
interviewee to recall the last time they engaged in the topic or behavior of concern (e.g.,
Tenopir, King & Bush, 2004; Jamali & Asadi, 2010; Tenopir, 2013; King & Tenopir,
2013). The interview questions in the study reflect the first interpretation, and are listed in
Appendix E2. Participants who completed the interview received a $20 gift card as
compensation. The interviews were audio recorded and the researcher took notes in case
the audio file was damaged or lost. The interviews took place between July 11, 2016 and
July 22, 2016.

Phase 2 data analysis
As with the focus group interviews, verbatim transcripts of the interviews were
created and analyzed using the constant comparative method (Charmaz, 2006), building
on the code book that was developed in Phase 1, further eliciting and developing major
themes relating to the research questions. The final qualitative codebook from Phase 2 is
listed in Appendix G. Codes were tracked in NVivo, and the same graduate student who
analyzed the focus group interviews analyzed 20 percent of the participants’ responses to

61
check the validity of the codes. Any discrepancies in coding were discussed to resolve
differences, as needed.
Unlike the focus group interviews, the individual interviews were transcribed
using a transcription service, and then checked by the researcher against the audio
recording for accuracy. Just like the focus group interviews, the data were not analyzed
until all of it had been collected. However, because the interviews took place during an
eleven-day period, instead of a six-day period, the researcher was able to reflect on
previous individual interview participants’ responses and probe later participants for
more information on possible emerging themes or to clarify existing ones.
The CIT and format of the individual interviews made it possible to collect richer
information of greater depth than the focus group interviews, which were more focused
on identifying as wide a breadth of themes as possible. At the same time, the criteria for
adding, removing, and modifying themes and subthemes was much stricter. In other
words, if a single individual interview participant’s response suggested a possible theme
or subtheme, it usually had to be mentioned by at least a few other respondents before
being added to the final codebook, whereas in the focus group interviews it might have
been added as a theme or subtheme. Themes also were evaluated on whether they made
conceptual sense. For this reason, affect was removed as a theme, and its subthemes were
distributed to other theme or subtheme categories. A list of the themes and subthemes and
an example of each from this analysis appears in Chapter 4. A final codebook that lists
these categories, their definitions, examples from the transcripts, and the number of times
that they appear is listed in Appendix G.

62
The resultant themes from Phase 2 helped structure the Phase 3 quantitative
survey question collection instrument and analysis. The literature also suggested that
confidence, or its opposite, library anxiety, might also modify the behaviors that
participants enacted when seeking, filtering, or storing information. This was
corroborated by participant responses in both qualitative Phases (1 and 2), so it was
included. Overall, themes emerged that suggested five broad categories of variables:
personal, confidence, behavior, outcome, and demographic. The demographic questions
from the focus group and individual interviews were heavily modified for inclusion in the
Phase 3 survey. Questions that requested participant information which were confusing to
participants or did not seem to affect their IE were removed or changed. Specifically,
participant ethnicity, prior post-secondary education history, including discipline and
institution location, frequency of using the library’s physical space to study, and whether
the participant had attended a library information session at any point in their educational
career were removed from the demographic portion of the survey. How these decisions
affect how this study compares to previous research is covered in Chapter 5.
In addition to removing demographic questions, the responses to certain questions
were simplified and other questions were added. The possible responses to questions
about participant age and usage were grouped together based on participant feedback and
similar surveys. Instead, demographic questions about whether participants lived an hour
away from their institution’s campus and whether they had access to resources at another
institution were added. Questions about the other four categories besides demographic
information were created based on the qualitative findings from this phase, and described
in the section below.

63
In summary, Phase 2:
x

Refined the preliminary codebook developed in Phase 1

x

Indicated which concepts identified in the qualitative Phases (1 and 2) would be most
effectively operationalized in the Phase 3 survey

Phase 3: Testing IE factors – Online survey
The final phase of data collection and analysis consisted of a quantitative survey
administered online. The findings from the previous two phases gradually narrowed the
topic of inquiry, and the potential applicant pool increased over the two phases. Phase 3
continued to narrow the topic of inquiry and increase the potential applicant pool in order
to generalize the qualitative findings to a larger population.
In addition to being the most appropriate method for the overall study design,
surveys are also one of the most commonly used data collection instruments for
qualitative and quantitative studies in LIS research. A literature review by Julien,
Pecoskie, and Reed (2011) found that from 1999-2008 surveys were the most commonly
used method in studies indexed in the Library Literature and Information Science Fulltext database. The two largest studies of doctoral students (Carpenter et al., 2012) and
library engagement (Zickuhr et al., 2014) utilized surveys to corroborate their qualitative
findings and to classify different types of engagement, respectively. The affordances of
the survey include easier and more efficient administration compared to focus group
interviews and individual interviews, which means that a larger, more diverse, and
representative sample of the population being studied can participate. Because the survey
questions are the same and many responses are limited (e.g., multiple choice or Likert

64
scale questions) it is possible to quantitatively compare survey responses (Connaway &
Radford, 2017).
Despite the relative ease with which surveys can be disseminated and analyzed,
this method also does have limitations. Researcher bias is the most significant potential
area of concern with this method because it can lead to confusing questions or responses
that do not encapsulate what a participant would want to respond to the question. Topics
that have not been well studied, such as IE, are especially susceptible to researcher bias.
The study mitigates researcher bias by using the focus group interview and individual
interview data to inform the survey questions. Another area of potential concern is
participant response rate, because surveys tend to have lower response rates than
interviews (Connaway & Radford, 2017).
Bearing in mind the affordances and limitations of this method, the survey
consisted mainly of Likert scale questions, a few open questions, and demographic
questions. The questions are found in Appendix E3. The demographic and Likert scale
questions were used to differentiate participants by their IE. The open questions allowed
participants to give more detailed feedback on their IE. The survey instrument was hosted
in Qualtrics (2017). Before deploying the survey, the questions were pre-tested by eight
doctoral students in the researcher’s interdisciplinary doctoral program, but not in the LIS
area of concentration.
There is a debate as to the number of points a Likert scale should contain (Leung,
2011; Carifio & Perla, 2007). On one hand, “reliability measures, test-restest and internal
consistency, were found to be independent of the number of scale points,” according to
Matell and Jacoby, (1971, p. 666), but they also acknowledged that this result agreed with

65
some findings of similar studies and contradicted others. Carifio and Perla (2007)
reported that when responses to 457 Likert responses were correlated between scales of
5- and 7-points, the correlation was very close at .92 (p. 109). On the other hand, Leung
(2011) argues that 11-point Likert scales seem to minimize skewness and kurtosis (how
many responses are outliers), which makes the data collected “closer to internal level of
scaling and normality” (p. 412). However, other dissertations that have used this model
have often used 5-point scales (Senteio, 2015; Larson, 2010; Lin, 2009) or 7-point scales
(Larson, 2010) depending on the content. In the pre-test, an 11-point scale was used, but
the students previewing the instrument found the scale was too long and did not
differentiate meaningfully between response levels. As a result of the survey instrument
pre-test, in addition to modifying the questions based on their feedback and that of the
researcher’s dissertation committee members, the original 11- point Likert scale was
changed to a 7-point Likert scale.

Phase 3 data collection
As in the previous two phases, the potential pool of applicants was chosen
strategically based on the participants and findings from the earlier phases. In this case,
the potential participant pool was based primarily on the disciplines and institutions of
students who had already participated. The participants were recruited using three
strategies. Once again, a recruitment notice was also sent to CRTNET and posted on
January 17, 2017. Second, a recruitment email was posted on the graduate student events
listserv used in the previous two phases. It was posted on January 23, 2017.

66
When these two methods failed to attract the minimum desired number of
participants, the chairs and administrators of social science programs in the 10
institutions were emailed and asked to forward the recruitment email to their doctoral
students. Participants in the earlier study phases had come from six of these institutions,
and an additional four that were not attended by previous participants was added. These
were selected randomly from a list of Doctoral Universities according to the Carnegie
Classification of Institutions of Higher EducationTM (2017), with one from each of the
four Census regions of the United States (2017). These emails were sent on January 26,
2017. An additional four institutions selected randomly from the list of Doctoral
Universities used before, with one from each of the four Census regions, also were
solicited on January 29, 2017. The programs contacted in both e-mail requests included
Anthropology; Communication; Education; Linguistics; Psychology/Behavioral
Sciences; Social Work; Sociology; and related fields. Political Science programs also
were also contacted, even though there had been no participants from this discipline in
the earlier phases. The survey ran from January 26, 2017 through January 31, 2017, and
participants were compensated with a $10 Amazon gift card.

Phase 3 data analysis
Two types of data analysis procedures were used in Phase 3. First, a descriptive
analysis of the data was created. In this analysis, the mean value for each 7-point Likert
scale question was computed. Second, an inferential model tested if participant responses
to the five broad categories (personal, confidence, behavior, outcome, and demographic),
suggested by the qualitative phases of the study, could significantly predict their

67
responses to how much they would pay for an article needed for research, how much they
would pay for a book needed for research, and whether the first place they would look for
information was a particular person or a resource. These last three variables, the amount
that the participant was willing to pay for a book and an article, and where they would go
first to look for information, became the dependent variables (DVs).
There were many possible methods considered for the inferential analysis. The
first was deciding what behavior to predict. As discussed in Chapter 2, library value can
and should not be solely based on monetary value. On the other hand, monetary value
arguably has the most face validity to those outside the academic library, and perhaps
even outside of academia (Melo & Pires, 2011; Kingma & McClure, 2015; Ko, Shim &
Pyo, 2016). Monetary value also can be related to the amount of effort a student or other
library user will expend to obtain a resource, either through the library or not. A multiple
regression model could show which variables have the most influence on how much a
student would pay for the article or book. Statistics about the goodness of fit,
approximate variance explained by the model, significance of the independent variables
(IVs), and effect size were computed (see results in Chapter 4).
In addition to predicting how much a participant would pay for a research related
article or book, the qualitative data suggested that some participants might be more
inclined to seek help from a person or a non-human resource. Knowing whether students
in this study population might have a predisposition to seeking information from a person
or a particular resource can help academic librarians tailor their services accordingly.
Although the survey asked students to choose one of seven human or non-human
resource, the analysis was simplified into either a human or non-human resource. Based

68
on the data, participants were also grouped by whether they would start a search on a
topic that they were unfamiliar with using Google Scholar or a search engine, or another
resource. This allowed the researcher to run a binary logistic regression, which tests the
ability of the IVs to predict what option out of two a participant will choose. Compared to
other possible models, such as analysis of variance (ANOVA), multiple ANOVA
(MANOVA), or (linear) regression models, binary logistic regression is a robust test that
does not place many limits on the IVs used in the analysis. In this analysis, the goodness
of fit omnibus statistic, approximate variance explained by the model via the Nagelkerke
statistic, significance of the IV(s), and effect size were computed (see results in Chapter
4).
Generating the IVs was a more involved process. There were two main types of
methods utilized to create the IVs: statistical grouping and creating additive indexes. The
first method involved grouping participants based on their responses and checking to see
how well those groupings could predict the DV values or groups. The second involved
adding together the Likert scale response values based on the question categories and
comparing those with the DVs.
There were three statistical grouping methods considered, and only one was
tested. Garson (2014) identifies three similar methods for grouping data, which include
factor analysis, cluster analysis, and multi-dimensional scaling. The third one, multidimensional scaling, was not considered because group memberships are not saved
automatically in SPSS and group labeling would need to be confirmed in a subsequent
cluster analysis (Garson, 2014, p. 201). Cluster analysis was preferable to factor analysis
because the latter is described by Garson (2014) as a type of fuzzy clustering where

69
"objects may be assigned membership in disjoint, hierarchical, or overlapping clusters on
a probabilistic basis" (p. 16).When comparing factor analysis and cluster analysis, the
latter is simpler to use as the number of factors is not influenced by the number of
variables as in factor analysis, and SPSS automatically suggests which variables were
most important in assigning participants to groups (Garson, 2014, p. 201). Finally, cluster
analysis is more appropriate because it groups participants as individuals rather than
factors that might underlie their responses.
The cluster analyses used k-means clustering algorithms. This algorithm assigns
participants to clusters based on how close the participant’s responses were to other
participant’s responses in the cluster. K-means clustering “is very sensitive to outliers”
(Norusis, 2012, p. 390), so only 7-point Likert scales were included in the analyses. In
order to test the validity of these clusters, they were then compared to other types of
clusters using binary logistic regression. This tests whether one type of variable, in this
case a group identified via cluster analysis, can predict a dichotomous outcome. This
particular method was selected because it does not assume that the original responses
follow a normal distribution, which was less likely since a 7-point Likert scale was used
instead of an 11-point one. It also is more reliable than similar methods, such as
regression, when there only are two possible outcomes (Garson, 2012, p. 126).
Participant responses to personality related questions, confidence related
questions, and the behavior related questions were separately clustered. Outcome related
data had too many missing responses to be included in this analysis. Demographic
information was not collected on a 7-point Likert scale, and since many of the questions,
such as gender, could not be fit onto a 7-point scale, they were not included in the cluster

70
analysis. Once an initial cluster was generated based on the related questions, they were
analyzed to see if questions should be removed from the clustering process or if more or
fewer clusters should be generated. There is no prescribed way to do this, but the most
common approach aims to have as many cases fall into as few clusters as possible and to
remove variables that do not strongly predict which cluster a case falls into (Norusis,
2012).
In this study, two clusters were generated for personality, confidence, and
behavior questions. The cluster centers, which indicated the response that a participant in
the cluster was most likely to put down for a question, was noted, as was the number of
cases in each cluster. The F tests that appear in the ANOVA table, as noted by SPSS
(2017), should not be interpreted as usual because the clusters were created by giving
more weight to questions where the responses between those in each cluster varied the
most. In other words, a variable that is non-significant in the table should not necessarily
be automatically removed from the model, but it does give some indication as to which
questions had the most difference between responses among the people in each cluster.
The questions used in each cluster were modified according to these guidelines, and the
final questions used to determine each cluster appear in Tables 23-25 in Chapter 4. The
results of this analysis are reported in Chapter 4, but since the overall analyses were weak
in predicting the DVs, the researcher also generated IVs by creating an additive index.
As mentioned above, an additive index is generated by adding up the Likert scale
responses to questions in each group. So the personality variable additive index was
created by adding up the responses to the questions categorized as personality. Additive
indexes are similar to scales, but are less rigorous in their construction. For this reason,

71
statistics used to estimate the strength of a scale, such as Cronbach’s alpha, were not
computed for these indices. The cluster analyses also suggested questions to not include
in certain scales, so an initial index was created using all of the questions and then an
alternate index was created using the questions suggested by the cluster analyses. The
scale means for each IV on the DV are included in Chapter 4. The results of the multiple
regression and binary logistic regression of these IVs on the DVs are also included in
Chapter 4.

Methods conclusion
The dissertation study took place over three phases of data collection and
analysis. The phases ran from February 19, 2016 marking the start of data collection in
Phase 1, and January 31, 2017 marking the end of data collection in Phase 3. The results
of each phase follow in the next chapter.

72
CHAPTER 4: RESULTS
This chapter presents the qualitative and quantitative results of the dissertation. It
begins with a summary of the demographics of the participants in each phase. As
described in the Methods Chapter, Phase 1 consisted of 3 focus group interviews with 20
participants, Phase 2 consisted of 15 individual interviews, and Phase 3 consisted of a
survey completed by 123 participants. Results from the qualitative and quantitative data
collected and analyzed are organized by research question (RQ) (see page 46 for a list of
the RQs), in the three phases. Study participants include all participants in this
dissertational study, unless they are specifically limited to a certain phase of the study.

Description of sample
The gender, age, doctoral program stage, discipline, and institutional location for
participants from all stages of the dissertation are presented below. The ethnicity and
educational history of participants from Phase 1 and Phase 2 of the study are also
presented.

Gender
The demographic survey was based on the characteristics mentioned in the literature as
possibly influencing information engagement, and built upon the demographic survey
questions that were developed for another study of this population (Mikitish & Radford,
2013).

73
Gender
Female
Male
Phase 1
14 (70%)
6 (30%)
Phase 2
10 (67%)
5 (33%)
Phase 3
86 (70%)
36 (29%)
Total
110 (70%)
47 (30%)
Table 4: Participants’ responses to “My gender is”

Other
0
0
1 (<1%)
1 (<1%)

Total per phase
20
15
123
158

Gender by Phase
Phase 1

Phase 2

Phase 3

0

20

40

60
Female

80
Male

100

120

140

Other

Figure 1: Gender by Phase
Table 4, above, lists the numbers of participants in each phase reporting as each
gender, and Figure 1, above, illustrates the gender composition of each phase. According
to the Council of Graduate Schools (CGS) and Graduate Records Examination (GRE)
Program’s Survey of Graduate Enrollment and Degrees, the percentage of women
enrolled in United States doctoral programs and studying Education; Public
Administration and Services; or Social and Behavioral Sciences in the Fall 2015 semester
ranged from 60-68 percent (Okahana, Feaster, & Allum, 2016, p.37).

74
Age
Age
Group
Phase
1
Phase
2
Phase
3
Total

18-24

25-34

35-44

45-54

55+
1 (5%)

No
Response
0

Total per
phase
20

1 (5%)

16 (80%)

1 (5%)

1 (5%)

0

11 (73%)

3 (20%)

0

0

1 (7%)

15

13
(11%)
14 (9%)

98 (80%)

11 (9%)

1
(<1%)
2 (1%)

0

0

123

125
15 (9%)
1
1 (<1%)
(79%)
(<1%)
Table 4: Participants’ responses to “My age falls into the following range”

Major Participant Age Groups
Phase 1

Phase 2

Phase 3

0

20

40

60
18-24

80
25-34

100

120

140

35+

Figure 2: Major participant age groups
Table 5, above, groups the study participants by age, and Figure 2, above,
simplifies the data into three main groups. Participants in Phase 1 and 2 were asked to
share their exact age. The youngest participant in these phases was 24, and the oldest was
59. 78 percent of the participants in the study (across all phases, combined N = 158) fell
into the 25-34 year old group. The data from the first two phases of the study suggested
that age might affect information engagement in participants 35 and above, so the groups
indicated on the Phase 3 survey used this age as the lower limit for the middle selection.

75
Ethnicity
Ethnicity
Phase 1
Phase 2
*Phase 3
Total

African American
4 (20%)
0
n/a
4 (11%)

Caucasian
11 (55%)
11 (73%)
n/a
22 (63%)

Ethnicity

Middle
South Asian
Mixed
Eastern
Phase 1
0
1 (5%)
1 (5%)
Phase 2
1 (7%)
0
2 (13%)
*Phase 3
n/a
n/a
n/a
Total
1 (3%)
1 (3%)
3 (9%)
Table 6: Participants’ responses to “My ethnicity is”
*Data not collected for this phase of the study

East Asian
0
1 (7%)
n/a
1 (3%)

Other
1 (5%)
0
n/a
1 (3%)

Latinx
2 (10%)
0
n/a
2 (6%)

Total per
phase
20
15
n/a

Major Ethnic Groups

Phase 1

Phase 2

0

5

10
Caucasian

15

20

25

Other

Figure 3: Major ethnic groups
Table 6, above, lists the ethnicity reported by the participants in the first two
phases. The Okahana et al. survey (2016) found that 13 percent of all graduate students
enrolled in social and behavioral science programs were Black/African American, 60
percent were White, 5 percent were Asian, 12 percent were Hispanic/Latino, and 4
percent were Two or More Races. A further breakdown of doctoral students versus other

76
graduate students was not available, so it is not possible to say that this study’s
participants are representative of the ethnic breakdown of social science doctoral
students. Participants in Phase 3 were not asked to report their ethnicity because analysis
of the participant responses collected in Phases 1 and 2 did not indicate ethnicity to be a
critical factor in participant IE.
Stage
Stage

Before
exams

Before
After
No
proposal/prospectus proposal/prospectus response
defense
defense
Phase 1
8 (40%)
1 (5%)
11 (55%)
0
Phase 2
5 (33%)
5 (33%)
5 (33%)
0
Phase 3 60 (49%)
35 (28%)
27 (22%)
1 (<1%)
Total
73 (46%)
41 (26%)
43 (27%)
1 (<1%)
Table 7: Participants’ responses to “I am in the following stage of my program”

Total
per
stage
15
20
123

Participant Stages
Phase 1

Phase 2

Phase 3

0

20
Before exams

40

60

Before proposal/prospectus defense

80

100

120

140

After proposal/prospectus defense

Figure 4: Participant stages
Table 7, above, lists the number of study participants in each doctoral program
stage. Study participants in the earliest stages of their programs, who had not passed their
qualifying/ comprehensive exams, accounted for 46 percent of the participants overall
(total N = 158). Participants who had passed their qualifying/ comprehensive exams but

77
not defended their dissertation proposal/prospectus, accounted for 26 percent of the
participants overall (N = 158). Participants who had defended their dissertation
proposal/prospectus, accounted for 27 percent of the participants overall (total N = 158).
Discipline
Discipline
Phase 1
Phase 2
Phase 3
Total

Communication
7 (35%)
13 (87%)
56 (46%)
76 (48%)

Psychology
1 (5%)
1 (7%)
29 (25%)
31 (20%)

Sociology
6 (30%)
0
15 (12%)
21 (13%)

Linguistics
1 (5%)
0
9 (7%)
10 (6%)

Social Work
1 (5%)
0
8 (7%)
9 (6%)

Discipline Political Ecology & Anthropology Education
Global
No
Science Evolution
Affairs
response
Phase 1
0
1 (5%)
1 (5%)
1 (5%)
1 (5%)
0
Phase 2
0
1 (7%)
0
0
0
0
Phase 3
3 (2%)
0
0
0
0
3 (2%)
Total
3 (2%)
2 (1%)
1 (<1%)
1 (<1%)
1 (<1%)
3 (2%)
Table 8: Participants’ responses to “I am in the following department/discipline

Participants by Major Disciplinary Group
Phase 3

Phase 2

Phase 1

0

20

40
Communication

60

80

Psychology

Sociology

100

120

140

Other

Figure 5: Participants by major disciplinary group
The table above (Table 8) reports number of study participants in each discipline.
Some study participants did not respond to the question, and the (n) values for each
resource are in the last column in the table. Participants in Communication and related

78
disciplines, such as Journalism and Media Studies, accounted for 48% (total N = 158) of
the study participants across all three phases. The next two disciplines, Psychology and
Sociology, accounted for 26% and 13% (total N = 158) of participants, respectively.
Although graduate program chairs and/or administrators were asked to forward
participation notices to students in Anthropology and Education doctoral programs, only
one participant per department was included in this study, and only in Phase 1.
Other Degrees
Bachelor’s
Social
Sciences Humanities
Discipline
Sciences
Phase 1
13 (59%) 4 (18%)
5 (23%)
Phase 2
13 (72%)
1 (6%)
4 (22%)
*Phase 3
n/a
n/a
n/a
Total
26 (65%) 5 (13%)
9 (23%)
Table 9: Participant’s bachelor’s discipline
*Data not collected for this phase of the study

Other

Master’s
Social
Sciences Humanities
Discipline Sciences
Phase 1
17 (94%)
0
1 (6%)
Phase 2
12 (75%)
4 (25%)
0
*Phase 3
n/a
n/a
n/a
Total
29 (78%)
4 (11%)
1 (3%)
Table 10: Participant’s master’s discipline
*Data not collected for this phase of the study

Other

0
0
n/a
0

0
0
n/a
0

Not
applicable
0
0
n/a
0

Total
degrees
22
18
n/a
40

Not
applicable
3
0
n/a
3 (8%)

Total
degrees
21
16
n/a
37

79

Other degrees
Same social science,
61%
From sciences, 6%

Social science, 76%

From humanities, 18%

Different Social science,
15%
From humanities

From sciences

Same social science

Different Social science

Figure 6: Breakdown of other degrees
Participants Phases 1 and 2 of the study were asked to list the disciplines and
levels of any other postsecondary degrees that they earned (Tables 8and 9). There are
more degrees listed than participants because some participants received their degrees in
multiple areas. As depicted in Figure 6, above, most participants (25 out of 35 total, 71%)
received all previous degrees in the social sciences. For those who started in the sciences
or humanities, there was a gradual narrowing of disciplines as the participants progressed
in their studies. Participants in Phases 1 and 2 combined reported receiving bachelor’s
degrees in the social sciences (total N = 26), sciences (total N = 5), and humanities (total
N = 9). Participants in Phases 1 and 2 combined reported receiving master’s degrees in
the social sciences (total N = 29), sciences (total N = 4), and humanities (total N = 1).

80
Location
Institutio
Midwest
Northeast
South
West
No
Total per
n
response
phase
Phase 1
0
20 (100%)
0
0
0
20
Phase 2
4 (27%)
5 (33%)
4 (27%)
2 (13%)
0
15
Phase 3
46 (37%)
16 (13%) 33 (27%) 27 (22%)
1 (<1%)
123
Total
50 (32%)
41 (26%) 37 (23%) 29 (18%)
1 (<1%)
158
Table 11: Participants’ responses to “The school where I am getting my degree is located
in the following region of the country”
The table above (Table 11) reports the number of participants from each census
region of the United States. Despite recruiting solely in the Northeast for Phase 1 of the
study, more students from the Midwest participated in this study than from any other
region.
Bachelor’s
Institution
Phase 1

Midwest

Northeast

South

West

Other

2 (10%)

7 (35%)

8 (45%)

0

Phase 2
*Phase 3
Total

7 (47%)
n/a
9 (26%)

4 (27%)
n/a
11 (31%)

1
(5%)
0
n/a
1
(3%)

4 (27%)
0
n/a
n/a
12
0
(34%)
Table 12: Region of participants’ bachelor’s institutions
*Data not collected for this phase of the study

Master’s
Midwest Northeast
South
West
Institution
Phase 1
2 (11%) 10 (56%)
5 (28%)
0
Phase 2
4 (25%)
6 (38%)
5 (31%)
1 (6%)
*Phase 3
n/a
n/a
n/a
n/a
Total
6 (18%) 16 (47%) 10 (29%)
1 (3%)
Table 13: Region of participants’ master’s institutions
*Data not collected for this phase of the study

No
response
2 (10%)

Total
institutions
20

0
n/a
2 (6%)

15
n/a
35

Other
1 (6%)
0
n/a
1 (3%)

Total
institutions
18
16
n/a
34

Participants in Phases 1 and 2 reported the other postsecondary institutions that
they received degrees from in the past, which is listed in the two tables above (Table 12
and Table 13). The last column reports the number of institutions because some students
received degrees of the same level at multiple institutions. In Tables 11-13, some

81
participants did not respond to the question, and the (n) values for each resource are in
the second to last column in the table. Students earned the largest number of bachelor’s
and master’s degrees in the Northeast (total N = 15), followed by the Midwest (total N =
15), South (total N = 22), and West (total N = 1).

Moving between degrees

Remained in region

Moved for Master's

Moved for Doctorate

Figure 7: Moving between degrees
As Figure 7, above, depicts, most students received previous degrees in the same
census region of the United States. Of those who moved to an institution in another
region, slightly more were likely to move in order to pursue a Doctoral degree. Of these,
three moved back to the same region where they had received their Bachelor’s degree.
Based on the demographic information reported above, the results of this study
best apply to Communication, Psychology, and Sociology students who are under 35
years old and Caucasian. Students in the final survey phase were not asked to report
educational history because there were no strong relevant findings in those areas. The
demographic results of the study are difficult to contextualize because there is no
comprehensive survey of all doctoral students in the United States. The Okahana et al.
study (2016) offers the comprehensive data on graduate students, but offer limited data

82
on only doctoral students. However, the findings outlined above indicate that although
the convenience sampling methods differed by each phase, a similar proportion of
participants fell into each of the demographic areas.
Summary of sample
x
x

x
x
x

The gender breakdown of each phase was similar to what is reported nationally for
social science doctoral students in the United States.
Participants’ ages were mostly below 35 years old, the most commonly reported
ethnicity was Caucasian, and a majority of the participants were in the prequalifying/comprehensive exam stage. It is not clear whether this is representative of
social science doctoral students in the United States.
Study findings are most applicable to students in Communication; Psychology; and
Sociology because a majority of the participants were in these disciplines.
Participants in the first two phases tended to have received all of their degrees in
social science fields, and the majority in the Phase 2 all received their degrees in the
same discipline.
Participants in the first two phases tended to stay in the same region of the United
States when pursuing higher education degrees.

Qualitative and Quantitative Results by RQ
The qualitative and quantitative results are reported under their corresponding
RQ. The first and fourth RQs only include qualitative results. The findings related to the
first RQ identified factors that were tested quantitatively in order to answer other research
questions. The findings related to the fourth RQ are based on the findings of the previous
three RQs and qualitative data from Phases 1 and 2. The qualitative results are presented
as counts in charts and in thematic coding tables. The quantitative results are described
using charts and tables.

83
Interpreting thematic coding tables
Relevant qualitative themes for RQs are listed in tables, beginning with Table 14,
below. The tables in this chapter may combine themes or sub-themes. For instance, in
Table 14, below, the library barrier theme adds the counts for four sub-themes that are
types of library policy barriers. A full listing of the themes, definitions of themes,
citations for themes related to other work, participant responses that are examples of
themes, and counts for each theme are in Appendix G. While the participant responses in
Appendix G provide exemplars for the particular theme, the participant responses in this
chapter and Chapter 5 will further illustrate the relevant findings as part of the discussion.
Each thematic coding table also includes two types of counts each for Phase 1 and
Phase 2. For the focus group interviews in Phase 1, there is a count for how many groups
out of the three total focus groups included at least one participant response that was
coded into the theme and a count of the total number of times a participant response was
coded for that theme across all groups. For the individual interviews, there is a number
for how many individual interviews out of the 15 total interviews included at least one
participant response coded into a theme and a total number of times a participant
response was coded for that theme across all individual interviews.

Results for Research Question 1 (RQ 1): What are the critical factors that
characterize information engagement (IE) for doctoral students in the social
sciences?
The major results for RQ 1 follow in the bulleted list below, and Table 14, which
follows afterwards, outlines the major qualitative themes identified from Phases 1 and 2.

84
x

x

x

x

x

x

When considering the Task1 (1.1)2 that participants were working on when looking
for information, the Stage Based (1.1.1) ones for where they were in their program
were not found to be a critical factor of IE because participants were more likely to
relate incidents where they found information that helped them Situate [their] Work
(1.1.2) in a more general sense rather than on a search for a specific task. Teaching
(1.1.3) was also not found to be a task that significantly affected IE.
Based on participant responses, this study identified the following critical Facilitators
(1.2) of IE:
o a wide variety of Searching (1.2.1) and Organizational (1.2.2) strategies,
which suggests that these critical factors are best measured at an individual
rather than institutional level.
o the Convenience (1.2.3) afforded by institutionally provided resources or
services.
The study also identified the following facilitators as not being critical to their IE:
o Flow (1.2.4) experiences that occurred during participants’ information
seeking, writing, or some combination of the two.
o Library Instruction (1.2.5), even if the student was an international student.
o having access to resources at other institutions via Multiple Institutional
Logins (1.2.6).
Based on participant responses, this study identified the following as Barriers (1.3) to
IE:
o Not Finding (1.3.1) information, which was mostly attributed to not knowing
the keywords used for their Topic (1.3.1.1) or what was used in the topic’s
Field (1.3.1.2).
o frustration over Not Knowing (1.3.2) how to access materials through their
institution’s library or due to specific Library Policy (1.3.2.1), which was
especially prevalent in the focus group interviews.
The study also identified an item’s Cost (1.3.3), a perception that there was a
Distance (1.3.4) from the library or its resources, or the delivery Time (1.3.5) taking
too long as barriers to IE, but not significant ones.
The critical factors resulted from qualitative analysis of the interviews in Phase 1 and
Phase 2. They were divided into facilitators, which helped improve participant IE,
and barriers, which hindered participant IE as summarized below.
The demographic factors described in the Description of the sample above did not
seem to affect the Task, Facilitator, and Barrier qualitative themes applied to
participant responses, except where noted.

Qualitative themes are capitalized
2 The first number indicates the research question, the second number indicates the subsection that the theme appears in, and the third number indicates the number of the theme
in the sub-section.
1

85
Facilitator and barrier themes coded as critically affecting IE
RQ1 Themes

Focus groups
Group
Times
count
coded

Individual interviews
Participant
Times
count
coded

x

Tasks
o Stage based
 Coursework
3
11
 QualComp
3
11
 Proposal
2
2
 Dissertation
2
5
 Other
1
2
o Situate work
3
27
o Teaching
3
11
x Facilitators
o Flow
3
44
o Searching strategies
3
41
o Organizational strategies
3
32
o Library instruction
2
3
o Convenience
3
19
o Multiple institutional
1
1
logins
x Barriers
o Not finding
3
26
o Not knowing
2
38
o Specificity
 Topic
2
12
 Field
1
8
o Library policy (incl.
3
37
embargo, request limits,
ILL fee, recall)
o Cost
3
11
o Far distance
2
4
o Timeliness (incl. item
2
7
delivery time and
deadline)
Table 14: RQ1 qualitative themes identified in Phase 1 and 2

8
5
4
7
6
11
6

20
21
10
20
13
27
8

15
13
12
11
6
2

39
104
37
21
14
6

7
11

18
36

11
8
6

21
19
15

5
4
4

8
7
7

1.1 Tasks
Tasks were defined as when the participant mentioned a specific assignment when
describing interactions with information. The focus group interview and individual

86
interview questions (see Appendix E1 and E2, respectively) asked participants to recall a
time when they had to search for information related to their doctoral studies. The subthemes for Task included Stage Based (1.1.2) activities, which were defined as when the
participant described work in any of the following stages of doctoral study included:
Coursework, defined as the time before qualifying/comprehensive exams” (example,
“Well last semester I had to do a…project in one of my classes”(P73)); QualComp
Exams, defined as the time spent researching, writing, or defending
qualifying/comprehensive exams (example, “when I was doing my comps” (P4));
Proposal, defined as the time spent researching, writing, or defending dissertation
proposal/prospectus (example, “I was looking for writing a proposal” (P10)); and
Dissertation, defined as the time spent researching or writing their dissertation (example,
“My experiences were different when I was in course work then they are now post
coursework and dissertation torture” (P15)).
Some participants described major changes in their IE in different Stages of their
doctoral program. As one participant who was finishing writing his dissertation
explained:
My experiences were different when I was in coursework than they are now post
coursework and dissertation torture. In course work, when I was being given
projects and questions…[and] some modes of inquiry that kind of thing… I found
I would go to the library website and type in my topics...Before the primary
hurdle was figuring out what's there, now it's getting it…And so often the hurdle I
run into is I'll be able to track down the article that I need, go into the library
system, find it, and…I can’t [access] it in there…They don't have that journal or
it's in an embargo period or what have you. Which you know isn’t the library’s

Participants were assigned numbers in order to protect participant privacy and ensure
anonymity. They will be referred to as Participant 6 or P6. Participant quotations are
verbatim from transcripts of audio recordings, with only minor grammatical errors
corrected.
3

87
[fault], that's not what you guys do. You're navigating a broader economic
marketplace… (P15).
This statement suggests that other students in this Stage might face similar IE challenges,
but the qualitative results indicated students faced similar IE challenges regardless of
Stage and any of the demographic factors listed in the Description of the sample in the
above section.
One explanation for the similar challenges at each PhD program Stage was that
participants instead linked changes to specific tasks, which happened to occur at different
stages of their program. For instance, Participant 4, who was writing her dissertation, said
that the major change in her information searching occurred when she was preparing for
her comprehensive exams because of the nature of the assignment. This participant
described the change by saying, “Mostly in my daily life I do that sort of forward
searching, ‘This is what I want, how can I get it?’ Where as [searching for my exams]
was totally backwards” (P4). Forward searching refers to Known Item searching in LIS,
and often occurred for participants in the earlier stages of their programs. Other doctoral
students acknowledged that this was the easiest type of search for them to accomplish. As
one participant recounted:
In terms of specific research…I’m still in coursework and so I tend to, as I’m
reading an article that I find interesting, [I make] notes of…the articles that they
reference and then I’ll just look for those. So it tends to be pretty straightforward
because I know what I’m looking for and I know it exists. It’s not like I’m
searching for…a general keyword. I know the exact article, the author, all that
information, so it, it doesn’t tend to be too difficult right now for me to find the
things that I’m looking for (P3).
In other words, the nature of the assignments in certain stages of the doctoral program
might necessitate different searching skills. Although doctoral students might be more
likely to have done the “backwards” (P4) searching by their qualifying exams, that does

88
not mean that students in their Coursework stage would not need that skill for a class
assignment.
While Stage Based tasks were a major sub-theme in Tasks, Situate Work (1.1.2)
and Teaching (1.1.3) were two others. The sub-theme Situate Work was applied to times
when participants looked for information about the field of study without a specific task
(e.g., a paper) in mind. While they may have applied the information to a stage related
task or their research interests, this association came after they found the information.
Participant 3 explains the process as “a lot of it is just me trying to work my way
backwards so that I know I have a good understanding.” The Situate Work subtheme was
present in all of the focus groups and 73% (N = 11) of the individual interviews,
respectively. In the individual interviews, the Situate Work theme was least coded in
responses from participants in their Proposal stage (not coded in N = 3, 66% of N = 5
participants).
Engaging with information when doing Teaching (1.1.3) related activities, which
were defined as preparing for class or teaching, such as “information that [a participant
looked] for in order to help [their] students” (P2), was the third most common task found
to be present in the data (all focus group and N = 6, 40% of the individual interviews,
respectively). In the individual interviews, this theme was more likely to be found in
interviews with female rather than male participants (N = 4, 27% versus N = 2, 13%), and
among students who were writing their Dissertation (N = 4, 80% of the 5 total students in
this stage).

89
1.2 Facilitators
Information engagement was assisted and hindered by various factors, which
were coded as Facilitators and Barriers, respectively, as defined by Radford (1993). Data
analysis found that participants’ responses were coded as Facilitators more often than
Barriers, and Facilitators were most often individual strategies or systems for Searching
(1.2.1) or Organizing (1.2.2) information. Searching Strategies were defined as methods
described by participants for finding information, and consisted of: Keyword, Pearl
Growing, Known Item, and Exploratory. Keyword searches described instances when
participants looked for information using specific terms (example: “within the journal,
different keywords” (P1)). Keyword was the most commonly coded of the Searching
Strategies (all focus group interviews and N = 12, 80% of individual interviews). Because
so many participants used this strategy, there was no significant difference between
participants with different demographic factors as identified in the Description of the
sample.
Pearl Growing (Markey & Atherton, 1978), was a strategy in which participants
used the list of references in documents to find other related documents. One participant
described using this strategy by saying, “I was sort of like searching quick search for
references to find, you know, authors that I'd heard of or topics that I found interesting,
and I kind of used that to jump from article to article” (P9). It was the second most
frequently coded Searching Strategy (all focus group interviews and N = 11, 73% of
individual interviews). As with Keyword searching, there was little difference between
the responses coded with this theme among participants with different demographic
factors.

90
Pearl Growing could be considered a type of Known Item search. Known Item
searches referred to instances where participants looked for a specific item, or when they
could say, “This is what I want, how can I get it” (P4). However, Pearl Growing and
Known Item searches differed because in the former, a document provided several
possible related documents to find, while in Known Item searches, the participant was
only looking for one document. Known Item searches were coded less frequently than
Pearl Growing searches (all focus group interviews and N = 7, 47% of individual
interviews versus all focus group interviews and N = 11, 73% of individual interviews,
respectively), and were the least commonly coded Searching Strategy. The individual
interview participants were more likely to use this Searching Strategy as they progressed
in their programs as none of the students in Coursework or QualComp Exams mentioned
using Pearl Growing, however there did not seem to be any differences based on
demographic factors.
Finally, Exploratory searches were ones in which participants researched a
specific topic, and described as “just exploring what's important to me” (P18). They were
coded frequently (N = 2 focus group interviews and N = 10, 66% of individual
interviews). Unlike the Known Item searches, participants in Coursework or QualComp
Exams were more likely to have this code applied to their responses. Out of the five
(33%) individual interview participants who did not have responses with this code, four
were in their Proposal stage and one was writing her Dissertation. Demographic factors
did not appear to influence the use of Exploratory Searching.
Organizational Strategies (1.2.2) were defined as methods by which participants
saved information because as Participant 3 stated, “It's so helpful to have an organization

91
system for all the things that you have.” The most commonly used Organizational
Strategy was for Information outside of documents (N = 2 for focus group interviews and
N = 13, 87% of individual interviews). One participant described using “big cardboards
in different colors and Post-its” (P13) for this purpose. Participants across all
demographic factors organized their Information, with no appreciable differences.
The next most commonly utilized Organizational Strategy were Folders, which
were physical or electronic folders that organized documents. Some participants had
elaborate organizational schemes for their folders. For example, one participant explained
that “It's one list per folder. For different topics I just have a folder, or if I'm looking for
this particular search on this particular day, I’ll have a folder for that one day even”
(P20). Folders were coded in participant responses less frequently than Information (N =
2 focus group interviews and N = 11, 73% of individual interviews). As with
Information, participant demographics did not seem to affect their use of Folders.
Lists and Schedules were rarely mentioned in the focus group and the individual
interviews. Lists consisted of items to be found or added to their collection (example:
“I've definitely gone to the library, gotten a book I realized that I need to add it to my
list” (P11), and were mentioned by most participants (N = 2 focus group interviews and
N = 1, 7% of individual interviews). Schedules were plans with dates/times to read/work
with information (N = 1 focus group interview and N = 2, 13% of individual interviews).
One participant explained that, “I'll put together an action plan. I’ll read this book and
this article on this day” (P20). The two different individual interview participants who
used the List and the Schedule were female, but other than that there were no differences
among different demographic factors in participant use of Lists and Schedules.

92
Participants were more likely to identify searching strategies than organizational
strategies. The diversity of strategies ties into a key finding for this research question
because all of the participants in the focus group interviews, except for one (5%),
attended the same institution, but the strategies that they used to find and organize
information differed from person to person. Program stage rather than institution was part
of the criteria for selecting participants in the Phase 2 individual interviews, but in that
phase there were eight (53%) individual interview participants who attended the same
institution, meaning that they had access to the same resources, as another participant.
This finding suggested that institutional factors were less important than individual ones
when identifying critical IE factors.
Analysis of participant responses from all focus group and individual interviews
in Phase 1 and Phase 2, respectively, included instances of the Flow (1.2.4) theme
because there was a specific question about it in each interview protocol (see Appendix
E1 and E2). Flow is a state in which one experiences high levels of focus, attention, and
enjoyment in an activity (Csikszentmihalyi, 1990). One participant described it as: “It's
like it was there and I was in the moment” (P7). Participants’ descriptions of Flow
differed, with participants in all focus group interviews and three (20%) of individual
interview participants describing it as a combination of Writing and Searching. One
participant stated, “I guess well, how do you define the difference between searching and
then actually starting to consume the information, because I generally don't associate
being in the zone with just searching for stuff.” (P2), and three (20%) individual
participants describing it as occurring when Only Writing. A small number of
participants did not experience flow while searching or writing (N = 7 codings of No

93
Flow over N = 2 focus group interviews and N = 1, 7% of individual interviews). One
participant explained that “I'm not sure I've ever been in the zone, I'm not sure it's ever
happened” (P5). Aside from only female participants associating Flow with Only
Writing, reports of Flow or lack thereof did not differ significantly among participants
based on demographic information.
When searching for information, participants’ institutions provided systems or
services that facilitated the process. Convenience (1.2.3) was a related concept to Flow,
and was defined as applying to situations when finding and/or accessing info was fast and
easy (Connaway, Dickey & Radford, 2011). One response coded as Convenience was:
“Yeah I [search] electronically cuz I'm really lazy so I’ll sit in front of my computer, and
I'll go to special issues of journals in my field” (P15). The participants’ institutional
systems or resources facilitated Convenience and Flow, but this was not always the case.
One student, who had nearly finished writing her dissertation, admitted that, “honestly,
the library’s the last place that [she would] go for articles” (P9) because it was less
convenient than using Google Scholar. Analysis of the focus group participants’
responses yielded more instances of the Convenience code than among the individual
interview participants (coded in all focus group versus N = 6, 40% of individual
interviews). There was no demographic difference in the responses coded with the
Convenience theme.
Library Instruction (1.2.5) was defined as when an academic librarian provides
formal (e.g., class based) or informal (e.g., one on one) instruction in finding/accessing
info (example: “Yeah we definitely have an information session my first year with the

94
liaison” (P17)). Some participants had received library instruction multiple times, but the
results of it were mixed. As Participant 21 recounted:
It seemed every other class I took my freshman [and] sophomore years, we had a
day devoted to going to the library…A librarian…or someone who worked in the
library who specialized in this and research would give us a presentation on,
‘Here are the different resources, here’s how you access those resources, [and]
here’s how you do research.’ Like kind of a basic research class and I found that
very helpful…but that was a while ago (P21).
On the other hand, Participant 28 explained:
I feel like I've had library introduction classes my whole life…In high
school...pretty much every year we would go to the library for our English class
and they would show us the databases, and I remember always…[thinking,]
‘Alright you already showed this to us, I've seen it a million times.’ (P28)
Of course, some students had not received Library Instruction, but that did not prevent
them from knowing how to use several higher level search features, such as searching in
multiple fields and using truncation, which one participant called “the asterisk feature”
(P28), in keywords. Another participant who knew about the advanced search feature said
she might have seen it at some point in her education, but she had “to figure it out on
[her] own” (P33). Other advanced skills included narrowing the search results to a subject
(P32) and using the controlled vocabulary subject terms that appeared in databases or the
library catalog (P22), but it was not always clear where participants learned these skills.
Demographic factors did not seem to have an effect on whether a participant described
higher level search features in their responses.
Participants in the final stages of their program were more likely to agree that
library instruction should occur earlier in the program rather than later, as encapsulated in
the response below:
I mean really they should have…scheduled one-on-one or something with
librarians to force us to get into the habit of understanding how the library works

95
and what resources are available to us because at [institution]. It’s an R1...and
they are able to do so much with research… You don't even know what’s
possible. …Unless they make the time for it that first year, I think we’re all kind
of missing out actually on resources that are available to us and would make our
lives so much easier and better, especially as researchers, but we don't have time
by the time it comes down to it to really want to and be able to use it. It’s kind of
too late, we’ve already established less effective and efficient habits (P29).
Library Instruction was an institutionally provided service that was coded more
frequently in the individual interview participants’ responses (N = 21 times in N = 11,
73% of individual interviews) than the focus group participants’ (N = 3 times in N = 2
focus group interviews). In order to learn more about factors that might influence how
Library Instruction could be more often perceived as a facilitator to engagement,
questions about Library Instruction were added to the quantitative survey in Phase 3. The
Likert scale statements asked students the extent to which they agreed that certain topics
or types of instruction would be helpful if provided by librarians. Results related to these
questions appear in the findings for RQ 2 and 4, below.
Only three (9% of N = 35) interview participants in Phases 1 and 2 disclosed that
they were international students, and their experience or lack of experience with library
instruction suggested that their international student status could be a factor in their IE, so
this was added as a demographic question in the survey and reported in Table 15, below.
International student?
No
Yes
Prefer not to answer
(N = 123)
*Phase 1
n/a
n/a
n/a
*Phase 2
n/a
n/a
n/a
Phase 3
103 (84%)
18 (15%)
2 (1%)
Table 15: Participants’ responses to “I am an international student”
*Data not collected for this phase of the study.
As reported in Table 15, above, 18 (15%) of the participants in Phase 3 were
international students. Some participants chose the “Prefer not to answer” option, and the

96
n values for each resource are in the last column in the table. The Okahana et al. survey
(2016) found that a similar proportion of all graduate students in the social and behavioral
sciences were enrolled in the Fall 2015 semester, but did not offer a further breakdown of
doctoral versus other graduate students. Because fewer than 10 percent of the participants
in the entire study were international students, and it is not possible to compare this
number to the entire population of social science doctoral students in the United States,
whether a student is international was not found to be a critical factor of IE.
One infrequently coded but significant facilitator to IE was that a small number of
participants (N = 1, 5% of focus group participants and N = 2, 13% of individual
interview participants) were able to legally access other institution’s collections. Legal
access was coded under Multiple Institutional Logins (1.2.6), and described situations
where the participant could personally access resources at other academic institutions
(example: “I also have access to [another institution's] databases)” (P5)). Illicit access to
collections described situations where participants acknowledged receiving information
through explicitly “highly illegal” 4 (P4) means, either by using someone else’s
credentials or asking a person at that institution to send them a resource. Participants
were likely to preface these requests by stating that they understood that this was
something that “no one should ever do” (P24). Incidents with this Illicit access included
some “illicit sites that [one participant would] use…[when] looking for something [they

While the participants explicitly agreed that certain behaviors, such as torrenting or
copying an entire book, were illegal, there was an implicit morally gray area around other
workarounds, which are described in the “Library policies” sub-section that follows. This
reflects the findings of a meta-analysis by Williams et al. (2010), which suggests that
ownership, sharing, and copying in the digital world is changing, and individuals are
using social and situational factors rather than ethical and legal factors to guide their
behavior.

4

97
couldn’t] find” (P5). Multiple Institutional Logins appeared in the data 22 times in three
(100%) of focus group interviews compared to 15 times in four (27%) of individual
interviews. Illicit access to resources is a sensitive topic, and is less important in this
study than seeing how access to multiple institutions’ resources played a factor in
facilitating IE.
At the same time, sometimes it was unclear whether such access was Illicit, even
when participants claimed to be “a hundred percent about PDF sharing” (P9). For
instance, one student explained:
I'm taking a class at [another institution besides theirs], so I actually have access
to their library and [second other institution], which includes [third other
institution] and a couple other schools that are around there, and then [current
institution] and [fourth other institution]…[It’s] like I am the Pirate Bay. I'll figure
out some way to just access all of them at once (P5).
Demographic factors did not seem to affect whether or not participants had Multiple
Institutional Logins or different perceptions of Illicit access to materials. In order to
investigate how many participants in a larger sample had this type of access, the
demographic question used in the Phase 3 survey did not specify if the access was legal
or illicit, and the results are reported in Table 16, below.
Additional logon?
Yes
No
Prefer not to answer or no
(N = 123)
response
*Phase 1
n/a
n/a
n/a
*Phase 2
n/a
n/a
n/a
Phase 3
50 (41%)
68 (55%)
5 (4%)
Table 16: Participants’ responses to “I am able to log onto databases through other
schools besides where I am getting my degree”
*Data not collected for this phase of the study
As reported in Table 16, above, 50 (41%) of the participants were able to log onto
databases through other schools. Some participants chose the “Prefer not to answer”
option or did not respond to the question, and the n values for each phase are in the last

98
column in the table. A majority of those who could log onto other schools’ databases (N
= 28 out of 50, 56%) were in the first stage of their program, which was before their
Qualifying/comprehensive exams. This was an important finding because nearly half of
the survey participants are able to log into other institution’s databases, but the literature
does not discuss its effects on doctoral students’ perceptions and expectations of the
services and resources provided by their home institution. There is also a dearth of
literature on library instruction for students taking classes at other institutions. A further
discussion and the implications of this topic are in Chapter 5 and 6, respectively.

1.3 Barriers
Barriers (Radford, 1993) hindered participant IE. One major barrier to IE was Not
Finding (1.3.1), which described situations where a participant was frustrated by not
finding information that they believed should be easy to find. As one student in
coursework explained:
I think for me a lot of the frustration comes from, or it varies depending on the
stakes I have…in whatever it is I need to find. So if…I need to write one of my
random weekly things and I need the articles here on this reading list…if I don’t
find something I’m looking for I’ll just look for anything else that fits that. But if
there’s something that really matters…[and] it weighs a lot more in terms of what
I’m going to be doing in the future, I spend a lot more time making sure that I’m
finding the things that I need to find, and if I don’t find these things and I get
stuck then…now I’m running out of time…That's where a lot of my frustration
comes from because if I can’t find it when I need to find it I'm extra pissed. I'm
not even mad anymore I'm seething and…I'll just try something else (P3).
Another major barrier was Not Knowing (1.3.2), which described situations where a
participant acknowledged that their lack of knowledge prevented them from finding
information. Not Finding was more often coded than Not Knowing (all focus group
interviews and N = 7, 47% of individual interviews versus N = 2 focus group interviews

99
and N = 11, 73% of individual interviews). Both Not Finding and Not Knowing were
coded with similar frequencies over different demographic factors.
Participants described Not Knowing as being due to the Specificity (1.3.1.1) of
the information that they were seeking. The Specificity code was applied to situations
where the participant acknowledged that the information that they sought was esoteric,
rare, old, or otherwise difficult to seek/access. These qualities were illustrated by the
following quotation:
Well it was old, so that’s an important part of why it was hard to find. It’s not a
particularly popular article, nor is it a particularly popular journal, so it was very
quickly reaching the adjective esoteric…I think the fact that it was old was the
most important thing, particularly because it looked…when I got the article that
somebody had physically gone to a hard copy and scanned in this article so it may
not have ever been digitized before. I may be the first person in the history of the
Internet to ask for this article to be digitized (P24).
The participants framed Specificity as either surrounding the Topic they were searching
for or trying to access, or due to the way that their Field structured information. The
Topic sub-theme was applied to situations where the participant was frustrated by a
specific research topic. In one case, a participant explained that “for certain topics…if
they’re not in the research literature then I’ll start on a regular search engine first and then
go from there to try to find the terms that might be related” (P1). The Field sub-theme
was applied to situations where the participant expressed frustration about the research in
their discipline, often because of the terms that scholars used in their work. Participant 7
stated that:
Part of my frustration sometimes just in Communication as itself is that
everybody makes up their own names...So like, "We're all talking about the same
thing, but you decided that you was going to coin this phrase that nobody else
knows but you, so now all those articles are under this coined phrase and it took
me eight days to find it (P7).

100
Field Specificity could also apply to the types of resources used in a field. For instance,
Participant 4 explained that “there are people in my discipline who read books, but that
doesn’t serve me particularly well, partially because I work in a really small field and
there aren’t that many books about what I do.” Participants were more likely to identify
Topic related Barriers (N = 2 focus group interviews and N = 12, 80% of individual
interviews) than Field related ones (N = 1 focus group interview and N = 8, 53% of
individual interviews). There did not seem to be any differences between participant
responses based on demographic factors.
Library Policies (1.3.2.1), which were defined as rules or practices created,
followed, and enforced by an academic library, also presented significant Barriers to
participant IE. Some policies were more general, such not being allowed to write in
library books (P14), but there were four sub-themes of Library Policy that were
especially significant barriers to IE. One was the Embargo, which referred to instances
when the library did not have access to recent content within a certain timeframe.
Participant 16 described this as “a moratorium, like when articles just come out and the
library doesn’t have access to them.” Participants also stated that even when their
frustration was more with the academic publishing procedure than the library, this
frustration could become “a projection of the library” (P1). This was the most commonly
cited sub-theme of Library Policy in the focus groups (coded N = 10 in N = 2 focus group
interviews and N = 10), but it was not coded in any of the individual interviews.
Demographic factors did not seem to affect whether a participant response was coded
with the Embargo Barrier.

101
Participants were also hindered by Request Limits, which referred to times when
the library could only request a certain number of items through interlibrary loan (ILL).
One participant recounted a time when she “tried to request it via the interlibrary loan and
then [she] got an email saying that…[the library] had already exceeded [their] limit for
requesting articles” (P2) so they not fill her request. This was the least often coded
Library Policy sub-theme (N = 2 focus group interviews, N = 1, 7% of individual
interviews). Only participants who were female said that the Request Limit was a Barrier.
While Embargoes and Request Limits hindered participant IE, Library Policies
that could cause them to have to pay money or relinquish access to information made
them more emotionally distraught. Another ILL related Barrier was the ILL Fee, which
were library charges to users to cover the shipping or licensing of ILL materials. One
participant explained:
One time I really needed this book chapter, [and] the library didn't have the book.
I requested it through ILL, and then they got back to me…and I never had this
happen before. It's not even a very big book, but they came back to me and said
that they [could] get this from another library, but [I’d] have to pay $20 or
something, and they’ll pay a certain amount of money…I just said, ‘Forget it, I'm
not doing that,’ and it was at [a nearby institution] so I got somebody to check it
out from there and give it to me (P19).
In response to Participant 19’s statement above, Participant 16 explained that “it’s not
that it’s just twenty bucks, it’s the principle. Why should you have to pay for it?” The
ILL Fee was coded more often in the focus group interview data than the individual
interview data (N = 6, 40%).
In a similar fashion to the ILL Fee, the library Recall policy, which required the
participant to return an item earlier if another user requested it, also upset participants.
When describing his frustration with the Recall policy, one participant stated, “I was

102
“was very upset about the recall process. Because when you're collecting stuff you want
that stuff definitely, because I am selfish, I’m doing my research I want it, I need it, ain't
none of your business how long I need it” (P14). The recall policy was such a Barrier to
continuing access to information that some participants were even willing to pay money
to the library in order to hold onto useful resources. One participant admitted that she was
willing to pay the library a lost item fee in order to ensure permanent access to materials
that she needed for her dissertation. As she recounted:
There was a book written by [name] and she wrote about widow narratives and
she is a key in my dissertation process. And [every]…3 months, I can renew [the
book], but if somebody’s requesting it, I always feel a little weird about giving up
the gem that’s gonna make my process easier and the book isn’t available any
longer for me to find online to buy (P32).
Participants also calculated the benefit of buying versus returning the item. Participant 23
stated that “it’s cheaper to pay the library sometimes…for overdraft charges or for their
copy of the book than for me to pay for a book from 1960 where there’s only three copies
and they're going for 170 dollars” (P32). Another participant, after explaining his anger at
the Recall policy said, “if it's a book that I need to be on my bookshelf because it’s key,
or if it's only a few bucks, I'll just buy it” (P14). The individual interview data had the
Recall code more often than the focus group interview data (N = 8, 53% and N = 1
respectively). Demographic factors did not seem to impact whether a participant response
was coded with ILL Fee or Recall as a Barrier.
There were three participant identified barriers that did not seem to critically
hinder IE. The Cost (1.3.3) of items was a barrier where information was expensive to
find and/or access (example: “They’re very expensive” (P14). While Cost was a barrier
(N = 11 times identified in all focus group interviews and N = 5, 33% of individual

103
interviews), individual students seemed to differ on how it affected them, which was
often explained in terms of what they would be willing to pay for an item. Participants in
their Coursework, QualComp Exam, and Dissertation stages were less likely to mention
Cost as a Barrier. Other demographic factors did not seem to affect the presence of this
code in participant responses. The Timeliness (1.3.5) barrier applied to all time related
barriers, such as one instance where one participant said that, “Because of time, I’ve
decided this is not important anymore” (P17). While Timeliness could be perceived as a
critical barrier, it was coded infrequently (N = 2 focus group interviews and N = 1, 7% of
individual interviews). The low number of responses coded with Timeliness of a Barrier
indicated that this was not a Barrier whose presence, or lack thereof, significantly
affected participants.
One Barrier that was not coded frequently, but for some participants could
critically hinder their IE, was Distance (1.3.4) (N = 4 times identified in N = 2 focus
group interviews and N = 4, 27% of individual interviews). As one student explained,
“I'm always off campus, so I have to login through the firewall, and that process is too
much for me” (P19). Distance seemed to affect students that were in the dissertation
writing phases of their program, even if they had used the physical library and its
resources in the past. One participant explained that:
I don't spend a lot of time in the library website or what-have-you because I live
one hour away now and I’m doing all sorts of family-partner stuff and
researching. Walking in isn’t so…easy anymore [due to the distance], that’s a
hurdle for me. So I go to the website when I have a gap or I have a specific paper
I'm looking for, and I always tick that box, the ‘If you have the PDF,’ because if
you don't have the PDF my thinking is, ‘Well that doesn't really help me,’ right?
(P15).

104
Distance could mean that the participant was physically far from campus, or they
perceived that they were too far to access library resources. Aside from their stage in the
program, other demographic factors did not seem to affect their perceptions of Distance.
Because Distance could hinder student IE, a question about how far a participant lived
from their institution’s campus was added to the Phase 3 survey.
Distance student? (N = 123)
Yes
No
*Phase 1
n/a
n/a
*Phase 2
n/a
n/a
Phase 3
13 (11%)
110 (89%)
Table 17: Participants’ responses to “I live more than an hour away from where the
school I am getting my degree at is located”
*Data not collected for Phase 3.
As reported in Table 17, above, a majority (89%, N = 110) of the participants in
Phase 3 lived within an hour from the campus where they were getting their doctoral
degree. Based on the results from each phase, few students lived either an hour away
from campus, or stated that they lived far enough from campus for it to hinder their IE.
Consequently, distance was identified as a barrier in this study, but not a critical one.

Summary of RQ 1 results
A common theme among the qualitative findings for RQ1 was that the critical
factors of IE were tied to individual rather than task or institution factors. Tasks were
factors in IE because participants did look for information for that fit into certain doctoral
program related assignments, such as looking for articles for their qualifying exams.
However, participants were more likely to relate critical incidents where they found
information for these tasks where they often sought to situate their work more generally.

105
Then these results were applied to a program related task rather than starting with a
specific task in mind.
In regards to institutional factors that affect IE, participants at the same institution,
with the same resources and services available, had different ways of perceiving and
using them. The ability to legally or illicitly access other institution’s resources and
services also supported measuring individual rather than institutional factors of
engagement. Barriers were difficult to describe in terms of task or institution related
factors. Not finding information and not knowing how to access it hindered participants,
but the specifics varied from individual to individual even if they were working on the
same task type or at the same institution. Even the library policies that were critical
factors for some participants were not necessarily critical barriers for students at the same
institution. The next RQ findings also look at the individual factors that differentiate
different IE types of social science doctoral students in the United States.

Results for Research Question 2 (RQ 2): What IE types exist for doctoral students?
The major results for RQ 2 follow in the bulleted list below, and Table 18, which
follows afterwards, outlines the major qualitative themes identified from Phases 1 and 2.
x

x

The qualitative results of the Phase 1 focus group interviews and Phase 2 individual
interviews identified:
o knowledge and personality related Factors (2.1) that could possibly
differentiate between participant IE. Participants were highly likely to start
searches on topics that they were unfamiliar with on Google Scholar or the
references of an item with which they were familiar.
o program stages and information searches where participants reported a
Change in Behavior (2.2). The coursework stage and pre-qualifying exam
stage were the program stages where participants reported the most change in
their information seeking.
Participant perceptions of information, IE, and institutional resources were further
defined by the survey participants in Phase 3, with the following quantitative results:

106
o Participants responses to personality, confidence, and interest in library
instruction could also be added together into an indices. However, index
scores were not predictive of other index scores, except:
 Confidence index scores could predict interest index scores for
participants in the post-qualifying exam stage of their program.
 Personality index and confidence index scores could separately predict
interest index scores for participants who lived more than an hour from
the institution where they were getting their degree.
o Participants could be clustered into groups based on personality (N = 2
clusters), confidence (N = 2 clusters), and interest in library instruction (N = 3
clusters). However, membership in these clusters could not strongly predict
membership in other clusters, even when controlling for demographic
variables or creating stronger clusters with participants whose responses were
more similar than average.
The findings from RQ 1 identified critical factors that could affect participant IE.
The findings from this RQ group Phase 3 participants based on the qualitative findings of
Phases 1 and 2. These groups are based on participants’ levels of agreement to IE related
statements about information, searching for information, and their institutional resources.

Themes related to information engagement type
*RQ2 Themes

Focus groups
Group
Times
count
coded

Individual interviews
Participant
Times
count
coded

x

Factors
o Prior knowledge
3
9
15
55
o Tenacity
3
16
11
24
o Certainty
1
12
8
13
o Satisficing
3
8
5
8
o Hoarding
2
6
4
8
3
13
12
24
x Change in behavior
Table 18: RQ2 qualitative themes identified in Phase 1 and 2
* Please see Interpreting thematic coding tables on page 83 for how to interpret Table 18.
2.1 Factors and 2.2 Change in Behavior
The qualitative themes, identified in Table 18, above, investigate times when
participants deviated from their normal IE behavior when engaging with information and

107
what Factors (2.1) could potentially differentiate participants. IE behavior includes any
actions that participants took when finding, filtering, accessing, or storing information.
Change in Behavior (2.2) was defined as anything that necessitated a temporary or
permanent change in information related behavior, such as Participant 4’s comment that
“I would say that when I did my comps my searching strategy was vastly different.”
Change in Behavior was identified as an important theme because participants tended to
engage with familiar information sources in familiar ways, unless they hit a barrier,
which was a set of themes included in the results of RQ 1. Change in Behavior was coded
13 times in all of the focus group interviews and 24 times in 12 (80% of N = 15) of the
individual interviews. Because so many participants described a change in their behavior,
demographic factors did not seem to affect this theme.
Despite the lack of qualitative difference among participants with different
demographic factors, the responses coded Change in Behavior indicated that participants
IE might change during certain stages of their doctoral program. To test this finding,
Phase 3 survey included a question about how they felt their information searching habits
changed at different parts of their program. The following three tables (Table 19, Table
20, and Table 21) report the participant perceptions of change in their information
searching habits in different doctoral program phases on a 7-point Likert scale ranging
from 7 being Very much and 1 being Very little. Responses from participants in the same
stage appear in the same table because it was unlikely for participants in an earlier stage
to report changes due to a later stage, but this was not always the case. Some participants
did not respond to the question, and the n values in the last column in the tables indicate

108
the number of participants who answered for this stage and the percentage of the total
number of participants in the stage that this number represents.
Searching habit change – Stage 1 (N = 60)

Mean

Standard
deviation
1.98
1.85
1.70

n of
responses
57 (95%)
58 (97%)
26 (43%)

Before starting
2.91
During coursework
4.16
While preparing for my qualifying/comprehensive
4.46
exams
While preparing my proposal/prospectus
4.94
1.91
16 (27%)
While writing my dissertation
5.11
2.09
9 (15%)
Table 19: Stage 1 participants’ responses to “In the following stages of my doctoral
program, I would say that my habits searching for research-related information changed”
Participants in Stage 1, according to Table 19, above, had not yet passed their
qualifying/comprehensive exams, which is why a majority of them could not comment on
how much their IE related to searching changed from the qualifying/comprehensive exam
stage and beyond. However, on average they did report that their habits changed more
during coursework and while preparing for their exams than before starting their
program. The standard deviation for each mean was fairly large, which indicated that
participant responses varied quite a bit. An unexpected finding was that participants (N =
27, 45% of Stage 1 participants) reported how their searching habits changed in later
stages of their program. This finding possibly reinforces the RQ 1 finding that
participants situate their work without necessarily having a certain task in mind, but they
do apply it to certain program related tasks when they find it.

109
Searching habit change – Stage 2 (N = 35)

Mean

Standard
deviation
2.27
1.67
2.00

n of
responses
32 (91%)
35 (100%)
35 (100%)

Before starting
3.22
During coursework
3.91
While preparing for my qualifying/comprehensive
3.80
exams
While preparing my proposal/prospectus
3.64
2.23
28 (80%)
While writing my dissertation
3.53
2.27
17
Table 20: Stage 2 participants’ responses to “In the following stages of my doctoral
program, I would say that my habits searching for research-related information changed”
Participants in Stage 2, whose results are reported in Table 20, above, had passed
their qualifying/comprehensive exams. The greatest amount of change was reported
during coursework and while preparing for their exams. They reported a greater change
in their searching habits than the previous group. Similar to the participants in Stage 1,
the standard deviations were nearly two, which means that participant responses varied
greatly. Nearly half (N = 17, 49% of Stage 2 participants) reported how much their
information searching habits changed in later stages of their doctoral program.
Searching habit change – Stage 3 (N = 27)

Mean

Standard
deviation
2.27
1.92
1.69

n of
responses
25 (93%)
27 (100%)
26 (96%)

Before starting
3.16
During coursework
4.70
While preparing for my qualifying/comprehensive
5.15
exams
While preparing my proposal/prospectus
4.52
1.87
25 (93%)
While writing my dissertation
4.28
1.99
25 (93%)
Table 21: Stage 3 participants’ responses to “In the following stages of my doctoral
program, I would say that my habits searching for research-related information changed”
Participants in Stage 3, whose results are reported in Table 21, above, had passed
their proposal/prospectus and were writing their dissertations. Like the previous group,
they felt that their searching habits had changed the most during coursework and while
preparing for their qualifying/comprehensive exams. The standard deviation for each
mean was still approximately two points, which means that even in this final stage of

110
their studies, the participant responses varied greatly. Taken together, the responses from
Tables 18, 19, and 20 suggest that participants did feel that their searching habits changed
a little, but as a whole not very much. A more detailed analysis of this finding and the
implications for how this might affect RQ 4, which asks how libraries can increase IE,
are discussed in the next chapter.
In addition to providing insight as to when and to what extent participants felt that
their information searching changed, the participant responses coded as changes in
behavior also identified why participants changed their behavior. The factors that caused
participant searching behavior to change included a variety of challenges that the
participant perceived. Participants in the Phase 3 survey were asked to indicate how
strongly they felt challenged by these factors, and their responses were included in the
confidence variables discussed in the next section.
The findings from the Change in Behavior theme suggested that participants did
not change searching, and potentially other IE behaviors as well. Participant responses in
the focus group and individual interviews suggested five factors that caused differences
in how participants searched for, filtered, and stored information. Other LIS scholars had
previously identified two of these themes. Khosrowjerdi and Iranshahi (2011) defined the
participant’s prior knowledge as covering how often one used a resource, when one knew
of a resource, and when one had used a particular resource in the past. In this study, the
prior knowledge code was applied to statements where participants explicitly discussed
knowing of or having used a particular resource more than once. Participant 22 described
her Prior Knowledge as follows:
I’ll go into specific databases EBSCOhost, Comm Mass Media Complete, you
know, MUSE, if I’m looking for something rhetorical sometimes I’ll go to

111
specific journals, but I'm a 4th year doctoral student and I didn’t always do that
and I learned the hard way that it’s really best to go to specific journals (P22).
Prior Knowledge was the most frequently coded theme in this category in the individual
interviews (55 times coded in 15, 100% of individual interviews), and it was coded in all
focus group interviews. Demographic factors did not seem to affect the presence or
frequency of this code in participant responses, which is odd because the participants in
the last stage of their programs should have accumulated more Prior Knowledge than
participants in earlier stages of their program. However, this finding once again
emphasizes that the participants were constantly looking for different types of
information on different topics, so despite amassing Prior Knowledge on certain topics,
they still ran into the same IE Barriers.
Satisficing, the other previously identified code, refers to the degree to which a
person will accept alternatives to a known resource that they cannot find and/or access
(Simon, 1955). As Participant 9 succinctly put it, “Yeah, I’ll substitute.” In this study, the
degree to which a student satisficed was a factor in their IE. Satisficing was coded in all
focus groups, but only five (33%) of the individual interviews. Only one male participant
in the individual interviews Satisficed, and participants in the Dissertation stage were
more likely than the other stages to Satisfice. Other than these differences, demographic
factors did not seem to affect whether this code was applied to participant responses.
This study identified three factors that differentiated IE types among the
participants. The first, was the participant’s Tenacity, which was the degree to which a
participant would pursue a known item. Participant 6 had a high degree of Tenacity
because as he put it, “nothing stops me per se, I mean, if it’s out there I'll find it.”
Tenacity was not limited to searching, as participant explained that she “would really

112
want to do most of the stuff [herself], but sometimes [she would] spend an hour or two
hours trying to do something” (P16), even though she realized that this was often not the
most efficient approach. Tenacity was the most often coded factor (N = 16 times) in all of
the focus group interviews and the second most often coded factor in the individual
interviews (24 times in N = 11, 73%). Responses coded with the Tenacity theme seemed
to be in opposition to statements coded with the Satisficing theme. However, a
participant’s level of Tenacity was often dependent on the situation at hand. One
participant, who was talking about a critical incident when her search for information was
unsuccessful, concluded by saying:
Maybe I didn’t have the stubbornness to continue trying to find the article
elsewhere… [but sometimes] you’ll put in the request and…you don't
immediately hear back. So by then I’m already working on other things, and so
when I get that email I’m like, ‘Aw man,’ but then…unless…everything I needed
to know depended on that article, I’m not gonna continue pursuing it. So I’ll
just…be like, ‘OK, I need to find this information somewhere else,’ and…I’ll just
find other articles. It’s really hard to depend on one article…So…it wasn’t the end
of the world (P33).
The variability in the searching incidents described made it difficult to discern if Tenacity
actually affected IE. Participant descriptions of their Tenacity did not seem to be affected
by different demographic factors.
The second factor identified in this study, which was also related to finding
information, was the participant’s Certainty, which was defined as the degree to which a
student felt comfortable in their subject area, was more often than not a desired state that
participants aimed to achieve via their information seeking. For instance, Participant 8
explained that “I feel that I should first try to know everything that’s out there.” Certainty
was only coded in eight (N = 53%) of the individual interviews, but in only one focus

113
group. The Certainty theme is conceptually related to Prior Knowledge, and likewise did
not seem to be affected by demographic factors.
With regard to storing information, some participants also reported high degrees
of Hoarding, which was the degree to which a student would collect items, even if they
admitted that they did not always read what they had collected. One participant admitted
that “I pick up an awful lot of books from [home institution] library that I don't end up
opening, and I don't know, I guess it's good that I at least have them” (P17). The
Hoarding theme was the most rarely coded (N = 6 in 2 focus group interviews and N = 8
in 4, 27% of individual interviews), and participants in the Coursework or QualComp
Exam stage were less likely to have this code applied to their responses. No other
demographic factors seemed to affect Hoarding, and because it did not relate to finding
information there were no questions about it in the Phase 3 survey.

Dependent variables (DVs) and other results for IE type and change in IE
The quantitative results of the Phase 3 survey operationalized the qualitative
themes identified in the study’s earlier phases. Participants were asked what they
normally do when engaging with information, what they had done in certain situations,
what they would do in hypothetical situations, and what their preferences for certain
information sources were. Participant responses to IE questions came from statements to
which participants indicated their level of agreement on a 7-point Likert scale, multiple
responses for various behaviors, and open ended questions about value. The 7-point
Likert scale responses were used to differentiate between different IE types via cluster

114
analyses or additive indices. These clusters and indices were independent variables (IVs)
that were then used to predict various behaviors as dependent variables (DVs).
Descriptive statistics were generated in cases in which participants were most
likely to search for information when they were unfamiliar with a topic, and the results
are reported in Table 22, below. Some participants chose the “Prefer not to answer”
option or did not respond to the question, and the n values for each resource are in the last
column in the table.
Resource preferences (N = 123)

Mean

Standard
deviation
1.29
1.03

Prefer not to answer or
no response
0
1

Google Scholar
6.21
The references from an item that I am familiar
6.12
with
A specific database/journal for that area
5.98
1.36
0
Google or another search engine
5.12
1.81
0
Someone who specializes in that area, even if I
4.02
1.71
0
do not know them personally
Someone that I know, even if it is not their
3.72
1.97
0
specialty
The library catalog to find a book in that area
3.67
2.14
0
Table 22: Participants’ responses to “When I start a search on a research-related topic that
I am unfamiliar with, I likely will consult the following people or resources as follows”
(with 7 being highly likely and 1 being highly unlikely)
When looking for information on a research-related topic that they were not
familiar with, participants were most likely to start with Google Scholar, as indicated by
the high mean value for the respondents’ rankings of this resource. They also preferred
starting with a familiar resource and looking at the references cited within. They were
also more willing than not (based on a score of 4 or more) to look at a specific
database/journal for that area, using a search engine, and speaking with someone who
specializes in the topic area, even if they do not know them. Participants were less likely
to ask someone they knew, and ranked the library catalog as the resource they were least

115
likely to consult. The standard deviations for Google or another search engine, another
person (known or specializing in the area), and the library catalog had standard deviations
of around 2, which means that although the averages for these resources were lower, they
varied more than the averages for the other resources. Overall these finding were not
surprising for this population, and similar findings from other studies are discussed in
Chapter 5.
In addition to asking how much participants preferred each resource, this survey
also asked participants to indicate the resource that they were most likely to consult when
starting a search on a topic that they were unfamiliar with, and the results are listed in
Table 23, below. Some participants chose the “Prefer not to answer” option or did not
respond to the question, and the n values for each resource are in the last column in the
table.
Resource most likely to consult (N = 123)

N = Participants
who selected
resource
55
28
23

% of total

Google Scholar
44.72
A specific database/journal for that area
22.76
The references from an item that I am familiar
18.70
with
Google or another search engine
8
6.50
Someone who specializes in that area, even if I do
5
4.07
not know them personally
Someone that I know, even if it is not their
2
1.63
specialty
The library catalog to find a book in that area
1
0.81
No response
1
0.81
Table 23: Participants’ responses to “When I start a search on a research-related topic that
I am unfamiliar with, I am most likely to consult”
When asked to choose which source they were most likely to start with for an
unfamiliar topic, nearly half of the participants chose Google Scholar. Although
participants rated the references from an item that they were familiar with higher overall

116
than in the previous question, in this question the next most likely resource was a specific
database/journal for that area. As in the previous question, a person that the participant
knew and the library catalog to find a book in the topic area were the least likely
resources that participants chose.
The results from this question were split into a dichotomous variable. This
variable, GSE, labeled any participant who preferred Google Scholar or a search engine
as a 0, and any participant who preferred another source as a 1. Splitting the results into
another set of dichotomous variables was also considered. In this second variable, PS,
any participant who was most likely to ask a person was labeled as a 0, and any
participant who preferred to consult a non-person resource as a 1. However, because only
seven (6% of N = 123) participants responded that they were most likely to ask a person,
this second variable was not used as a DV.

Independent variables (IVs) and other results for IE type and change in IE
The IVs came from Likert scale responses to three question variable types:
personality, confidence, and interest. These variables were aggregated via cluster analysis
and into an additive index, which summed the responses into a single scale. These IVs
were then used to predict the DVs described in the section above, with the interest IV
also being used as a DV in some analyses. The results of these analyses follow in the next
sub-sections.
The first variable was personality, and it came from data that suggested that some
participants were more open to asking people for help then others. One participant stated
that:

117
I’m actually not on site at the university where I am getting my PhD, so I’m living
in a different state and working at a different institution than my PhD
institution…[I] do feel kind of stuck between two worlds because on one side…I
just started as an assistant professor at this community college and…I don't know
how long I’ll be there…But also, I’m not on site at the [institution] to walk in and
see a librarian and maybe this just speaks to my age…I don't wanna ask random
people to do stuff for me if I can’t look at them in the eye (P29).
This participant was not the only one to say that due to her distance, but perhaps more
importantly, her preference to not ask people for help if she could not see them,
prevented her from reaching out to a librarian. Another participant stated that:
I haven’t talked to a librarian about that particular topic, you know, about those
method topics…Part of the reason I suppose might be that I’m writing my
dissertation from Chicago while my PhD university is in Detroit, 5 hours away, so
that may be one reason. But also when I was there I really didn’t talk to librarians
directly very often (P31).
Although the students had moved away from where they were getting their degree, it
seems that Participant 29’s preference for asking for help might have been affected by
distance, but not Participant 31’s. This suggests that openness to asking questions could
be one way to differentiate between participants.
Openness could also apply to being open to altering searching behavior. Almost
all 15 participants followed a “normal routine” (P31) similar to the one described below:
What I’ll typically do is…I’ll [brainstorm]…all the ways of studying [the topic]
and that’ll kind of give me an idea of where I need to narrow or focus or some
keyword that I need to use. And then I’ll go into Comm & Mass Media Complete,
or even Comm Abstracts, and I’ll…go through multiple databases to see what
does each of them return. And then if that still isn’t getting me very much that,
well at that point I can usually snowball a little bit…Like I’ll find one or two
articles, I’ll read those, I’ll check the references and then…that will tell me which
journals people have been talking about this in. But if I've done that…and I really
haven’t found anything that I think is the thing I'm looking for, then I would
contact my professor…or someone in the field cuz I'm out of coursework now.
But I would contact someone who specializes in that and say, you know, ‘I'm
looking for such and such I can’t find it, which journal should I look in?’ …But
usually I get by just, you know, looking through the databases and snowballing
with articles (P22).

118

However, sometimes participants ran into certain challenges. One challenging situation
was when a search returned too many results. As one participant related:
The advantage of Summon is that you can...put in a term and then you get
hundreds of thousands of results and then from there you can narrow it by date, by
topic. So if I wanted Communication related articles or I only wanted articles in
journals or newspapers, I can narrow it there. So it’s a good way to get to sort of
cast a very wide net. The difficulty with that is…if you don't know what you're
looking for then casting that wide net gives you so many results that sometimes it
can be overwhelming (P23).
Some participants were fine looking through several pages of results, like the participant
who was willing to sift through “five hundred to a thousand search hits to see, to find
things that…were pretty close to the area that [he] was trying to discover” (P25). On the
other hand, many participants were overwhelmed by the deluge of information that they
encountered in a search, or just the knowledge that there was so much information
available. One participant who was writing her dissertation described:
Billions of articles…We swim in articles, there's so many articles, none of them
seems important…there are just so many and so few that actually do something
big or important…I know when I'm searching that there's stuff I'm missing, but I
don't care because I can’t possibly cover it all, right? I mean for my master’s
thesis there was so little that I actually did cover it all and that’s the kind of thing
that I like to work on but in course work that’s usually not the kind of thing I
could work on and for my dissertation it’s not either. My dissertation is, it’s
narrow but it covers a ton of fields that all have their own really in depth, many,
you know, hundreds of years of research on what I'm studying so, so I guess I just
feel…if I miss something, big deal. I mean I can’t possibly hit it all you know and
how would I know if it’s the best thing, what is the best thing? It depends on the
angle you're taking (P22).
Whether participants felt overwhelmed by the number of resources returned seemed to
identify a useful facets of the participants’ personalities. This factor was combined with
the whether the participant was willing to ask another person for help. Together, these
formed the personality factor, which hypothesized that the degree to which participants

119
were open to asking for help/clarification; not unhappy if they retrieved a lot of
information, even if it was unexpected; and would consider changing their research based
on what they found would affect their behavior. Participant personality was also
hypothesized to be related to participant confidence and interest in library instruction.
Phase 3 survey questions related to the personality factor are listed in Table 24, below.
Responses marked with an * in Table 24 indicate responses that were reversed scaled
because agreement with them indicated that the person was less open to asking for
help/clarification; unhappy when they retrieved too much information; and would not
consider changing their research based on what they found.

120
Personality variable statements (N =
123)

Code

Mean Standard
deviation

Prefer not to
answer or no
response
0

*I tend to stick to a routine when
**Rout
5.79
1.29
searching for research-related
information
If I were unable to access a particular
askCopy
5.78
1.54
0
article/study, I would feel
comfortable asking someone for a
copy
If I had a question about a particular
askClar
5.69
1.48
0
article/study, I would feel
comfortable asking someone for
clarification
I use Google Scholar because it casts
GSNet
5.11
1.82
2
the widest net
*I would only ask someone for help
askExh
5.11
1.91
0
finding or accessing information if I
had exhausted all other options
I wouldn’t mind changing a topic of
chgTop
4.98
1.50
2
research for a certain project based on
what I found in a search
*In the past, I have not asked for
pstTime
4.94
1.77
7
research help because there wasn’t
enough time
*I only search until I find what I need onlyNeed 4.19
1.54
2
*In the past, I have not asked for
pstNotBot 4.08
2.05
11
research help because I didn’t want to
bother anyone
*In the past, I have not asked for
pstIncomp 4.07
2.07
8
research help because I didn’t want to
seem incompetent
*If the library can’t get it, I won’t
notPurs
3.71
2.05
1
pursue it further, even if it seems
incredibly useful
Table 24: Personality variable responses ranked by mean response where 7 means
completely agree and 1 means highly disagree
*Reverse scaled in clusters and additive indices, but actual responses are reported in this
table
**Variable code names are used in body of text below
Table 24, above, ranks participant responses to the personality variable statements
by mean value descending. The three responses with the lowest mean values had the
largest standard deviations, which indicates that although on the average participants did

121
not agree with them the majority of the responses were actually spread out about two
points above and below the mean. On a 7-point scale, this is a large amount of variance.
Some participants chose the “Prefer not to answer” option or did not respond to the
question, and the n values for each question are in the last column in the table. These
questions operationalized the qualitative findings on the tenacity, satisficing, and
certainty themes (see Appendix G for definitions and examples).
The participant responses in Table 24 indicate that participants were most likely
to report sticking to a routine when searching (Rout, see Table 24 for corresponding
question and for the data labels that follow) than any other statement. When searching for
information, most participants strongly agreed that they used Google Scholar because it
cast the widest net (GSNet), and on average disagreed that they only searched until they
had found what they needed (onlyNeed). Based on what they found, more agreed that
they would not mind changing their topic (chgTop) than disagreed.
With regard to accessing resources, participants were more willing than not to
tenaciously pursue them, either by asking for a copy (askCopy) or for clarification
(askClar) about them, than not. In fact, participants were most likely to disagree that they
would not pursue a resource, even if the library could not get it, if it seemed incredibly
useful (notPurs). However, they were more likely to agree that they would try and
exhaust all of their options before asking for help (askExh) than not. When thinking of
their past actions, they reported not being deterred from asking due to time constraints
(pstTime), not wanting to bother anyone (pstNotBot), or seeming incompetent
(pstIncomp). These variables related to finding and accessing information were clustered
or added together into an index which were used as IVs in later analyses. The next set of

122
variables focused more on how participants felt when finding information rather than
what they would do when finding or accessing it.
In a similar process to the personality question responses, the responses to
questions that indicated participant confidence were grouped together and reported in
Table 25, below.

123
Confidence variable statements (N =
123)

Code

Mean

Standard
deviation

Prefer not to
answer or no
response
0

I am confident in my ability to find
confGood
6.28
0.97
something GOOD ENOUGH for
what I need
*It can be a challenge looking for
chlEso
5.71
1.14
4
information on an esoteric topic or
other topic where there is not much
information available
*It can be a challenge having to find
chlEvery
5.66
1.48
1
everything I need to know about a
topic
*It can be a challenge not being to
chlAccess
5.30
1.75
1
access what I need to when I need it
*It can be a challenge looking for
chlOld
5.25
1.46
3
information in older texts/research
*It can be a challenge deciding what chlDecImp 5.24
1.45
0
is important or will be important in
my field in the near future
*It can be a challenge looking for
chlNonsch 5.00
1.64
10
information that is not in scholarly
resources
I am confident in my ability to find
confExact
4.93
1.26
1
EXACTLY what I need
*I frequently feel overwhelmed
Overwh
4.86
1.72
0
about the amount of information that
is available
*It can be a challenge knowing the
chlTerm
4.66
1.72
0
correct words used by scholars in
my field to define my ideas
*It can be a challenge having to
chlNewOld 4.56
1.68
3
switch between searching for newer
research (to stay relevant in my
field) and older research (to see
where an area of research began), or
vice versa
*I feel that I am expected to know
exptKnow 4.39
1.74
1
more than I actually do about
searching for information
Table 25: Confidence variable responses ranked by mean response where 7 means
completely agree and 1 means highly disagree
*Reverse scaled in clusters and additive indices, but actual responses are reported in this
table

124
Table 25, above, ranks participant responses to the confidence variable statements
by mean value descending. In their responses to this question, the participant responses
had a small standard deviation for the question that asked if they felt confident in their
ability to find information that was good enough. The high mean value and small
standard deviation indicates that overall the participants had high confidence in their
ability to find information that was good enough. Some participants chose the “Prefer not
to answer” option or did not respond to the question, and the n values for each question
are in the last column in the table. These questions operationalized the qualitative
findings on the Certainty and Change in Behavior themes (see Appendix G for definitions
and examples). These responses identified certain facets of the participants’ confidence
levels. They hypothesized that the degree to which participants were confident in finding
information and felt less challenged by the obstacles identified in the Change in Behavior
theme would cause participants to behave differently from participants who were less
confident and felt more challenged. Participant confidence was also hypothesized to be
affected by participant personality and to affect participant interest in library instruction.
Responses marked with an asterisk (*) in Table 25 indicate responses that were reversed
scaled because agreement with them indicated that the person felt less confident and
more challenged by various obstacles to finding information.
Overall, participants were very confident in their ability to find something good
enough for what they needed (confGood), but less confident in finding exactly what they
needed (confExact). On a similar note, they were on average unlikely to feel
overwhelmed about the amount of information available (Overwh) or that they were

125
expected to know more than they actually did about searching for information
(exptKnow).
The average response values for the statements that dealt with various challenges
in finding information were all greater than 4, which is the mid-point on the 7-point
Likert scale. This indicates that participants were more likely feel challenged when
finding information, due to the reasons above, than not. These variables were clustered
and added into indices, which were then analyzed as DVs or used as IVs.
Interest variable statements (N = 123)

Code

Mean

Standard
deviation

Prefer not
to answer
or no
response
4

*I would prefer to have face-to-face
prefF2f
4.73
2.03
research help when I need it rather than a
mandatory session on searching for
research-related information
I think it would be helpful to require
helpIncl 4.08
1.84
0
other students in my program to attend a
research related workshop that includes
library resources and services
I think it would be helpful to require
helpOnly 3.14
1.63
2
other students in my program to attend a
research related workshop that ONLY
includes library resources and services
Table 26: Interest variable responses ranked by mean response where 7 means completely
agree and 1 means highly disagree
*Reverse scaled in clusters and additive indices, but actual responses are reported in this
table

Table 26, above, ranks participant responses to the interest in library instruction
variable statements by mean value descending. The relatively lower mean values for the
interest variables and high standard deviations, compared to those of the personality and
confidence variables, suggest that there was a larger degree of variance in the majority of
participant responses. Some participants chose the “Prefer not to answer” option or did

126
not respond to the question, and the n values for each question are in the last column in
the table. These questions operationalized the qualitative findings on the instruction
theme (see Appendix G for definitions and examples). These responses identified certain
facets of participants’ preference for library instruction over face-to-face research help
when needed and whether they thought it would be helpful for other students in their
program. They hypothesized that the degree to which participants were interested in
library instruction would affect behavior. It was also hypothesized that participant interest
in library instruction was affected by participant personality, confidence, or both. The
responses marked with an asterisk (*) in Table 26 indicate responses that were reversed
scaled because agreement with them indicated that the person was less interested in
library instruction.
As reported in Table 26, above, students were slightly more likely to prefer faceto-face (prefF2F) research help when they needed it rather than a mandatory session.
Participants were more likely to agree that other students in their program would find a
research related workshop that included library resources and service as more helpful
(helpIncl) than one that only included library resources and services (helpOnly).
However, given that the mean response to the former was barely above the 4.00 midpoint
on the scale, overall it did not seem that participants thought such a workshop would be
very helpful. These variables were added into separate indices and clustered separately.

Additive indices
The three types of variables identified above, which included personality,
confidence, and interest variables, were summed into additive indices. This was done by

127
adding up participants’ Likert scale response values for each variable type. The results
are summarized in the table below (Table 27).
Index
Minimum
(participant N =
Value
123)
Personality
26/77
Index (pInd)
Confidence
20/84
Index (cInd)
Interest Index
2/21
(iInd)
Table 27: Index summary

Maximum
Value

Mean Value

Standard
Deviation

71/77

44.63

8.51

69/84

40.02

10.08

21/21

10.33

3.90

The results in Table 27, above, indicate that the mean values for each variable were
approximately half of the total possible values for each index. The standard deviations
were somewhat low, which means that responses did not vary much from participant to
participant, unlike what might be suggested by the minimum and maximum values for
each index.
The additive indices were used to predict interest index values (iInd). The
combinations tested are listed in Table 28 below, and as reported there, no models were
significant.
Independent variable(s)

Dependent
variable
pInd
iInd
cInd
iInd
pInd & cInd
iInd
Table 28: Linear regression prediction summary

Method
SLR
SLR
MLR

Was model
significant?
No
No
No

Although participant scores on one index were not found to be related to their
scores on another index, further analysis indicated that they could be predictive when
controlling for certain demographic variables, specifically participants’ stage in program
and distance from campus.

128
While there was no linear relationship between a participant’s confidence index
score or their interest index score when looking at all 123 Phase 3 participants, there were
a few significant relationships between the index scores for certain sub-groups of
participants. Simple linear regression reported that confidence index score (M = 40.81,
SD = 10.03) significantly predicted interest index score (M = 11.78, SD = 3.46), F(1, 25)
= 9.67, p < .01, adjusted R2 = .25. According to Cohen (1988), this is a large effect size.
When the confidence index score increases by one point, the interest index score
decreases by 0.18 points. In other words, students who are more confident in their ability
to find information are less likely to think that a library instruction course would be
helpful for other students, but this was only for students in the final stage of their doctoral
program.
A small subset (N = 13, 11%) of the total Phase 3 study population responded that
they lived an hour or more away from the institution where they were getting their
degree. For these participants their scores on the personality and confidences indices
could separately and significantly predict their interest index score. Confidence index
score (M = 41.38, SD = 10.71) significantly predicted interest index score (M = 10.69, SD
= 3.38), F(1, 11) = 10.26, p < .01, adjusted R2 = .435. According to Cohen (1988), this is
a large effect size. When the confidence index score increases by one point, the interest
index score decreases by 0.22 points. Personality index score (M = 48.00, SD = 7.85)
significantly predicted confidence index score (M = 41.38, SD = 10.71), F(1, 11) = 4.94,
p < .05, adjusted R2 = .25. According to Cohen (1988), this is a large effect size. When
the personality index score increases by one point, the confidence index score increases
by 0.76 points. These findings indicate that living more than an hour away impacted

129
participant personality, confidence, and interest variables as defined in this study more
than any other demographic variable, such as gender, age, stage, or international student
status. In addition to predicting scores on other indices, participant scores on certain
additive indices were able to predict some participant behavior, which is reported in the
findings for RQ 3.

Cluster analyses
The cluster analyses for the personality, confidence, and interest variables (pvar,
cvar, and ivar, respectively) were clustered via k-means clustering into groups labeled
pClus, cClus, and iClus, respectively. The relevant results of the clustering analyses
follow.
Personality cluster
pClus A center value
pClus B center value
(pClus) results
(participant N = 57, 46% of
(participant N = 66, 54% of
(Participant N = 123)
N)
N)
*Rout
2
2
askCopy
5
6
askClar
5
6
GSNet
6
5
*askExh
2
3
chgTop
5
5
*pstTime
3
4
*onlyNeed
4
4
*pstNotBot
2
5
*pstIncomp
2
5
*notPurs
3
5
Table 29: pClus center values
* Reverse scaled in clusters and additive indices, and reversed responses are reported in
this table
Participants in this cluster agreed more with this statement, but due to being reverse
scaled the value for the cluster in this table is smaller than in the other cluster
Table 29, above, lists the average response values for the personality variables in
pClus A and pClus B. pClus A members were less likely to ask others for help for a

130
variety of reasons, and they were slightly more likely to agree that they used Google
Scholar. Those in pClus A were much more likely (3 pt difference) to not have asked for
help because they did not want to bother anyone (pstNotBot) and because they thought it
would make them look incompetent (pstIncomp). They were slightly more likely (2 pt
difference) to agree that they would not pursue something if the library could not get it,
even if it seemed very useful (notPurs). Participants in pClus A were only slightly more
likely (1 pt difference) to use Google Scholar because it cast the widest net (GSNet), not
ask for help until they had exhausted all other options (askExh), and not ask for help in
the past due to time constraints (pstTime). The higher values in Cluster B indicate that
participants in this cluster were slightly more likely (1 pt difference) to feel comfortable
asking for copies of items (askCopy) and asking for clarification (askClar). The strength
of these differences is compared in Table 30, below.

ANOVA
Cluster
Error
Mean
df
Mean Square
df
Square
Rout
6.453
1
1.620
121
askCopy
59.022
1
1.918
121
askClar
50.730
1
1.781
121
GSNet
31.911
1
3.073
119
askExh
22.061
1
3.501
121
chgTop
8.430
1
2.198
119
pstTime
24.113
1
2.934
114
onlyNeed
8.892
1
2.317
119
pstNotBot
231.974
1
2.130
110
pstIncomp
276.348
1
1.868
113
notPurs
108.312
1
3.339
120
Table 30: Analysis of variance (ANOVA) for pClus differences

F

Sig.

3.983
30.776
28.480
10.384
6.302
3.836
8.219
3.838
108.906
147.929
32.441

.048
.000
.000
.002
.013
.053
.005
.052
.000
.000
.000

As SPSS (2017) notes, the significance (sig.) of the results should not be
interpreted in the usual way where any value below 0.05 is significant. Instead, the higher

131
the value in the Sig. column, the less likely the variable indicates a meaningful difference
between the participants in the two groups. Because all values to 0.50, which is the
general benchmark of significance, all variables were retained in this analysis.
Confidence cluster
cClus A center value
cClus B center value
(cClus) results
(participant N = 46, 37% of (participant N = 77, 63% of
(Participant N = 123)
N)
N)
*chlEso
3
2
*chlEvery
4
2
*chlAccess
3
2
*chlOld
3
2
*chlDecImp
4
2
*chlNonSch
4
3
confExact
5
5
Overwh
4
2
*chlTerm
5
3
*chlNewOld
5
3
*exptKnow
5
3
Table 31: cClus center values
* Reverse scaled in clusters and additive indices, and reversed responses are reported in
this table
Participants in this cluster agreed more with this statement, but due to being reverse
scaled the value for the cluster in this table is smaller than in the other cluster
The table above (Table 31) lists the average response values for the confidence
variables in cClus A and cClus B. Although the values in cClus B are less than those in
cCluster A, which makes it seem like the participants in that group disagree with the
statements saying that they feel more challenged when searching for different types of
information, due to reverse scaling those in cClus B actually agree that they felt more
challenged when looking for the various types of information. This is generally a two
point difference on average.

132
ANOVA
Cluster
Mean Square

Error
Mean
df
Square
chlEso
22.591
1
1.109
117
chlEvery
116.444
1
1.242
120
chlAccess
44.162
1
2.710
120
chlOld
26.160
1
1.918
118
chlDecImp
72.988
1
1.530
121
chlNonSch
34.451
1
2.410
111
confExact
20.128
1
1.428
120
Overwh
92.612
1
2.215
121
chlTerm
106.168
1
2.095
121
chlNewOld
104.161
1
1.961
118
exptKnow
132.941
1
1.933
120
Table 32: Analysis of variance (ANOVA) for cClus differences

F

Sig.

20.380
93.719
16.295
13.638
47.693
14.293
14.096
41.808
50.677
53.108
68.777

.000
.000
.000
.000
.000
.000
.000
.000
.000
.000
.000

df

As SPSS (2017) notes, the significance (sig.) of the results should not be
interpreted in the usual way where any value below 0.05 is significant. Instead, the low
values in the Sig. column indicate a meaningful difference between the participants in the
two groups. In this final cluster solution, the confGood variable was removed because its
value in this table when included was too high, which indicated that it was not useful in
meaningfully differentiating between the two groups.
Interest cluster (iClus)
results (N = 123)

iClus A center
value (participant
N = 40, 33% of
N)
2
2
2

iClus B center
value (participant
N = 36, 29% of
N)
6
4
3

iClus C center
value (participant
N = 47, 38% of
N)
2
6
4

*prefF2F
helpIncl
helpOnly
Table 33: iClus center values
* Reverse scaled in clusters and additive indices, and reversed responses are reported in
this table
Participants in this cluster agreed more with this statement, but due to being reverse
scaled the value for the clusters in this table are smaller than in the other cluster
The table above (Table 33) lists the average response values for the personality

variables in pClus A and pClus B. The reverse scaling in this analysis makes the results a

133
little more difficult to interpret, but participants in iClus A and iClus C were actually
more likely to disagree that they would prefer F2F help when needed rather than a
mandatory session on how to search for information (prefF2F). However, the participants
in these two clusters differed on how much they agreed that a session that included
library resources and services (helpIncl) or would only include this information
(helpOnly) would be. Those in iClus C preferred to thought that the sessions would be
much more helpful, as indicated by a 4 point and 2 point difference, respectively.
ANOVA
Cluster
Error
Mean Square
df
Mean Square
df
PrefF2F
169.218
2
1.267
116
HelpIncl
105.897
2
1.678
120
HelpOnly
64.218
2
1.612
118
Table 34: Analysis of variance (ANOVA) for iClus differences

F

Sig.

133.570
63.099
39.846

.000
.000
.000

As SPSS (2017) notes, the significance (sig.) of the results should not be
interpreted in the usual way where any value below 0.05 is significant. Instead, the low
values in the Sig. column indicates a meaningful difference between the participants in
the three groups.

Cluster analysis summary
The results of these cluster analyses indicate that difference in student personality,
confidence, and interest in library instruction can group doctoral students in the social
sciences. The major difference between the personality clusters was that those in the first
cluster, pClus A, were less likely to ask for help because they did not want to bother
anyone and they did not want to appear incompetent. There were slightly fewer
participants in pClus A (N = 57, 46%). In terms of confidence, those in the second
confidence cluster, cClus B, were more likely to feel challenged by obstacles to finding

134
information and therefore less confident overall. A majority of the participants were in
cClus B (N = 77, 63%).
Although the interest clusters were only based on three statement, three clusters
emerged from the analysis. The second group, iClus B, was the smallest of the three
groups (N = 36, 29%), and were most likely to prefer face-to-face help rather than a
mandatory library instruction session. Participants in the other two groups, iClus A and
iClus C agreed that they would prefer a mandatory library instruction session on
searching for information, but disagreed on how helpful a session would be for other
students in the program. Participants in iClus A felt that the sessions would not be helpful
to other students, regardless of whether they included or only consisted of library
resources in the instruction. Those in iClus C were more likely to indicate that they
thought a session that included library resources would be helpful than one that only
included library resources. iClus C also had the most participants (N = 47, 38%).
Although the cluster analyses uncovered difference between participants, the null
hypotheses for whether membership in one cluster predicted membership in another
cluster was not rejected. This meant that cluster membership for a particular factor did
not predict cluster membership for another factor. Stronger clusters were also generated
in which only the participants with a shorter than average distance from the cluster center
were included in the cluster. This meant that the respondents in each cluster had higher
than average levels of agreement with other clusters members to statement responses.
However, the prediction power for these smaller clusters was weaker than in the larger
ones. The specific binary and multinomial logistic regressions used to these hypotheses
are summarized in Table 35, below.

135
Independent
variable(s)
pCluster membership

Dependent variable

Method

cCluster membership

pCluster membership

iCluster membership

cCluster membership

iCluster membership

Binary logistic
regression
Multinomial
logistic regression
Multinomial
logistic regression
Multinomial
logistic regression

pCluster membership iCluster membership
& cCluster
membership
Table 35: Cluster prediction summary
*Result not reported due to small effect size

Was model
significant?
*Yes
No
No
No

Summary of RQ 2 results
The qualitative results from the focus group and individual interviews in Phase 1
and Phase 2, respectively, suggested that there might be certain factors that could
differentiate between the IE in doctoral students in the social sciences. The qualitative
data also included times when participants changed their IE behavior, especially when
searching for information. These changes were operationalized into three concepts that
were hypothesized to be related to each other and to participant IE behavior. The first,
personality, hypothesized that the degree to which participants were open to asking for
help/clarification; not unhappy if they retrieved a lot of information, even if it was
unexpected; and would consider changing their research based on what they found would
affect their behavior. The second, confidence, hypothesized that the degree to which
participants were confident in finding information and felt less challenged by the
obstacles identified in the change in behavior theme would cause participants to behave
differently from participants who were less confidence and felt more challenged. The
third, interest, hypothesized that the degree to which participants were interested in
library instruction would affect behavior.

136
Personality, confidence, and interest were grouped together in additive indices
and clusters. When the results of all participants were analyzed, no index scores were
able to significantly predict other index scores and no cluster memberships were able to
predict membership in other clusters. However, it was possible to predict interest index
scores using confidence index scores for participants in the last stage of their program. It
was also possible to predict interest index scores using personality and confidence scores,
separately, for the participants who lived more than an hour away from their school. In
RQ 3, the ability of the index scores and cluster memberships to predict Phase 3
participant behavior was quantitatively tested.

Research Question 3 (RQ 3) asked: How is information engagement (IE) related to
the value of academic libraries?
The major results for RQ 3 follow in the bulleted list below, and Table 36, which
follows afterwards, outlines the major qualitative themes identified from Phases 1 and 2.
x

x
x

Participants valued Electronic Resources (3.1), especially Google products (3.1.1),
more than any other, but focus group and individual interview participants also
valued Print Resources (3.2).
o Participants in Phase 3 reported the highest usage of online resources, with
114 (91%) searching online at least once a week.
o Participants in Phase 2 reported the highest usage of library resources, with
four (27%) asking a librarian for help at least once a month and seven (47%)
visiting a library at least once a month in the previous semester.
o Participants in the Phase 3 survey phase were willing to pay an average of $30
for a book and $10 for an article that they needed for their research. However,
more than half (N = 75, 61%) of survey respondents would not pay anything
for an article.
Participants used a variety of Human Resources (3.3) and Social Networks (3.4) to
find and share information. They also valued Software (3.5) that could do more than
keep track of bibliographic information.
Demographic factors, as described in the Description of the sample at the beginning
of Chapter 4, did not seem to significantly affect the qualitative codes applied to
participant responses in the focus group and individual interviews

137
x

x

Participant willingness to pay for a book could be significantly predicted using
participant personality, confidence, and interest index scores.
o An increase of 1 point on a participant’s personality index score increased the
amount that they would be willing to pay for a book $0.50.
o An increase of 1 point on a participant’s confidence index score increased the
amount that they would be willing to pay for a book $0.34.
o When a participant’s personality, confidence, and interest index scores were
combined, an increase of 1 point on a participant’s personality index score
significantly increased the amount that they would be willing to pay for a
book $0.44.
o Students who lived more than one hour away from their campus were willing
to pay $0.95 more for a book for an increase of one point on their confidence
scale. This was more than any demographic sub-group of the survey
population.
Personality variables could significantly predict whether a participant started a search
on a topic that they were unfamiliar with using Google Scholar or a search engine
versus any other resource for 90 (73.4%) of the participants.
The qualitative findings from Phase 1 and Phase 2 and presented in RQ 2 suggested

different IE types that could exist in social science doctoral students. The quantitative
findings in RQ 2 divided the Phase 3 participants into multiple clusters and scored them
on indices based on personality related, confidence, and interest in library instruction
variables. The qualitative and quantitative results for this research question test whether
the clusters and index scores could predict the IE behaviors for the participants in Phase
3.

138
Themes related to the value of academic libraries
*RQ3 Themes

Focus groups
Group
Times
count
coded

Individual interviews
Participant
Times
count
coded

x

Electronic resources
o Article
3
63
15
63
o Specific database or journal
3
36
13
62
o Websites
 Google search or
3
72
15
83
scholar
 Scholarly
3
30
15
63
x Library
 Illicit
3
22
4
15
x Human resources
o Academics
 Librarian
3
38
15
62
 Advisor or
3
18
7
22
Committee
 Author
3
12
5
11
o Students (incl. PhD)
3
23
7
16
o Virtual communities
 Academia.edu or
3
17
6
19
ResearchGate
 Facebook or Twitter
2
9
2
4
3
52
15
56
x Print or physical resources
x Software
o Citation
3
11
6
12
o Other
3
24
11
28
x Institutional
o Current
3
61
15
75
o Other
3
23
8
21
Table 36: RQ3 qualitative themes identified in Phase 1 and 2
* Please see Interpreting thematic coding tables on page 83 for how to interpret Table 36.
Table 36, above, reports the resources that participants utilized to find and
organize information. These resources are usually self-explanatory, so definitions and
examples are given in Appendix G rather than in this section. These resources relate to
the topic of library value because data analysis resulting in the qualitative themes
identified in the first two phases of the study found that participants accessed and utilized

139
a variety of resources, some of which were provided by their academic library, and others
that were not.

3.1 Electronic Resources and 3.2 and Print Resources
Although participants in all focus group and individual (N = 15) interviews
reported that they used Print (3.2) or other Physical Information resources, participants
also reported that they accessed Electronic Resources (3.1), specifically journal Articles,
most frequently. Participant 35 stated that “it's almost always gonna be articles that I
would want to find first,” and Participant 4 took this preference further by stating that
“unless there’s…an article that's contained within a book…I’m always searching at… the
article level for basically everything I do.” Participants were also knowledgeable about
specific databases, such as “Articles+” (P2), and participants in all focus group and 13
(87%) individual interviews were able to name at least one that they had used.
Participants used Google (3.1.1) products, because Google cast the “widest net” (P14,
P19). As one participant explained:
I would say initially in my research I was using the library…Somebody early on
had directed me that way [by saying that] if you want to do a lit review for this
field you should go to the library site, and there was…I haven't been to it for a
long time, but there's something called LLBA? It's the Language, Linguistics, and
Behavior Abstracts area of the library site. So initially I was using that a lot to do
lit review. Honestly, I kind of gave up on it and just resorted to Google in the last,
almost exclusively in the last few years just because, yeah, it has everything.
Maybe it's just a field to field difference. In my field, I’m not too concerned as to
whether it's a peer-reviewed journal article or just a conference proceedings or
even an unpublished manuscript…I like to cast my net pretty wide and get
everything that's out there. So…for me the Google Scholar method is kind of
what I mostly use (P19).
While Participant 19’s switch from library resources to Google Scholar was due to the
fact that she was in her Dissertation stage and that others in that stage might be more

140
likely to have similar stories, a participant’s stage and other demographic factors did not
seem to affect their use of Google Scholar because it was used so often. In addition to
Google Scholar, participants also used the Google search engine for a “regular Google
search” (P10) more frequently than library resources. Despite the prevalence of Google
Scholar and Search, focus group and individual interview participants had used a library
resource at least once. Library resources included a participant’s home institution’s
website and catalog, but could also include research guides, which one participant called
“research starters” (P3). Demographic factors did not seem to affect library use.

3.3 Human Resources
In the focus group and individual interviews, participants’ responses were more
likely to include academic Librarians than other Human Resources (3.3). Sometimes the
participants directly stated that they “went to the reference librarian” (P10), but
sometimes the researcher had to infer that a librarian was contacted, as in the following
quotation: “Sometimes I’ll do that chat, but after 10 they’re not there” (P16). These
findings suggested that the library’s electronic, physical, and human resources were
valuable. In order to test these findings in a larger sample, questions of resource
importance and library usage, in visits or asking a librarian for help, were operationalized
into Likert scale responses and reported in Tables 36, 37, 38, 39, and 40, below. Some
participants chose the “Prefer not to answer” option or did not respond to the question,
and the n values for each resource are in the last column in the table.

141
Importance of resources (N = 123)

Mean

Standard
deviation
0.32

Prefer not to answer
or no response
2

Journals (includes peer-reviewed and
6.91
professional/trade)
Books (includes edited volumes)
5.41
1.59
0
Conference presentations and
4.30
1.60
0
proceedings
News articles
3.74
1.78
2
Government and non-government
3.71
1.82
8
reports and whitepapers
Non-text documents
3.03
1.73
26
*Other
5.60
1.84
113
Table 37: Participants’ responses to “I would rank the importance of these resources to
my research as follows (with 7 being very important and 1 being very unimportant)”
*Other items included dissertations/theses (N = 2), self-collected data (N = 1), recordings
(N = 1), social media discourse/podcasts/texts/databases (N = 3), and media, such as
broadcasts, videos, and Youtube (N = 3).
Table 37, above, reports the average ranking of each resource’s importance to
participant’s research. Journals were ranked by almost all 123 survey respondents (N =
121, 98%) as being very important to their research. The small standard deviation
indicates that the range of rankings for this resource’s importance were high for a
majority of the participants. Books and conference presentations and proceedings were on
average ranked as important as their mean rankings were above 4.0. However, their
standard deviations indicate that a majority of the responses ranked the items as being of
average importance. Participants ranked news articles, reports, and non-text documents a
bit below 4.0 on average. Participants also had the option to write in other resources, and
although only ten participants did so, they ranked those sources as being important to
their research (mean = 5.60). However, the relatively large standard deviations indicate
that the ranking for these resources varied more than for the more highly ranked sources.
This means that journals tended to be ranked as important to the participants, but the

142
importance of other sources varied, which would make predicting their value for each
respondent more difficult.
Online
usage

Never

Less
At least
often
once a
than once month
a month
0
2 (10%)

A few
times a
month

At least
once a
week

Prefer not
to answer
or no
response
1 (5%)

Total
for
phase

Phase
1
0
16 (80%)
20
1
(5%)
Phase
0
0
0
2
13
0
15
2
Phase
0
0
2 (13%) 7 (47%) 114 (93%)
0
123
3
Total
1
0
4
9
143
1
158
Table 38: Participants’ responses to “Last semester, I searched for research-related
information online”
The table above (Table 38) reports how often participants looked for information
online, which included library and non-library provided resources, in the previous
semester. Most (N = 114, 91%) participants looked for information online at least once
weekly. Although one participant in the Phase 1 claimed to never look for information
online during that time frame, they had looked for information there in the past.
Participants in the first two phases of the study were asked to list the number of times
they looked for information online and then select a time period, such as week, month, or
semester. While this yielded more detailed information, it also made it difficult to
compare with the other usage data. The time periods above were taken from the Public
Library Typology study (Pew, 2016) and used in the Phase 3 survey. Responses from the
first two phases were then coded into the appropriate category.

143
Help
usage

Never

Less often
than once a
month

At least
once a
month

A few
times a
month

At
least
once a
week
1 (5%)

Prefer not
to answer or
no response

Total
for
phase

Phase
9 (45%)
9 (45%)
0
1 (5%)
0
20
1
Phase
3 (20%)
8 (53%)
1 (7%) 3 (20%)
0
0
15
2
Phase
52 (42%) 52 (42%)
13
2 (2%) 4 (3%)
0
123
3
(11%)
Total
64
69
14
6
5
0
158
Table 39: Participants’ responses to “Last semester, I asked someone at the library for
research-related help either online or in-person”
The table above (Table 39) reports how often participants asked a librarian for
help online, in person (FtF), by phone, and by email in the previous semester. Participants
in the Phase 2 asked librarians for help the most often, with at least four (27%) asking at
least once a month, compared to two (10%) and 19 (15%) in Phases 1 and 3, respectively.
The results reported above were turned into a dichotomous variable of high and low use.
Participants who never asked the librarian for help were coded as 0 and those who asked
for help were coded as 1s. These variables were labeled AskHL.
Study
space
usage
Phase 1

Never

10
(50%)
7 (47%)
n/a

Less
often
than once
a month
2 (10%)

At
least
once a
month
1 (5%)

A few
times a
month

At least
once a
week

Prefer not to
answer or no
response

Total
for
phase

3 (15%)

3 (15%)

1 (5%)

20

Phase 2
3 (20%)
0
1 (7%) 4 (27%)
0
15
*Phase
n/a
n/a
n/a
n/a
n/a
n/a
3
Total
17
5
1
4
7
1
35
Table 40: Participants’ responses to “How often do you use library spaces to study during
the school year?” re-coded into above categories
*Data not collected for this phase of the study

144
The table above (Table 40) reports how often participants used library space to
study in the previous semester. Participants in the first two phases expressed more
confusion over this question than any other, and it was not asked in Phase 3 of the study.
Never

Phase 1

Less
often
than once
a month
4 (20%)

At least
once a
month

A few
times a
month

At least
once a
week

Prefer not
to answer
or no
response
1 (5%)

Total
for
phase

7
5 (25%) 1 (5%)
2
20
(35%)
(10%)
Phase 2
3
5 (33%) 2 (13%) 2 (13%)
3
0
15
(20%)
(20%)
Phase 3
33
44 (36%)
31
10 (8%) 5 (4%)
0
123
(27%)
(25%)
Total
43
53
38
13
10
1
158
Table 41: Participants’ responses to “Last semester, I visited the physical library for
research-related purposes”
The table above (Table 41) reports how often participants used the library’s
physical space to work on research in the previous semester. Eight (40%), seven (47%),
and 46 (37%) of the participants in each phase reported using these spaces at least once a
month. The results reported above were turned into a dichotomous variable of high and
low use. Participants who visited the library less than once a month were coded as 0 and
those who visited it at least once a month were coded as 1’s. These variables were labeled
VisitHL. As reported in the results for RQ 1, 110 (89%) of the survey participants lived
within an hour of the campus, so living that far away does not seem to account for the
low number of library visits.
In addition to how often participants accessed information online, asked a
librarian for help, and used the library’s space, the survey also asked two value related
questions about their willingness to pay for the two most commonly coded resources,
books and articles.

145
Willingness to pay
Mean
Standard
n who would pay $0 Maximum in
(WtP) (N = 123)
deviation
(in USD)
USD ($)
Book
28.73
17.23
13
80
Article
10.16
10.31
75
60
Table 42: Participants’ responses to “If I need a _____ for research, I would be willing to
pay on average the following amount in US dollars”
The table above (Table 42) summarizes how much the average participant would
be willing to pay for a book or an article that they needed for their research. Overall,
participants were willing to pay more for books than articles. However, a significant
proportion of participants (N = 75, 61%) were not willing to pay at all for articles. The
responses for how much they would be willing to pay for a book were used as the
dependent variable in a series of simple and multiple regressions, which are explained
later on in this section. The article amounts reported above were turned into a
dichotomous variable of willingness to not pay and willingness to pay. Participants who
were not willing to pay were coded as 0 and those who were willing to pay were coded as
1’s. These variables were labeled ArtHL.

Simple and multiple linear regression
The additive indices were used to predict how much participants would be willing
to spend on a book for research (Book$). The combinations tested are listed in Table 43
below, and details on significant models follow.

146
Independent variable(s)
Personality index (pInd)

Dependent
variable
Book$

Method

Was model
significant?
Yes

Simple linear regression
(SLR)
Confidence index (cInd)
Book$
SLR
Yes
Interest index (iInd)
Book$
SLR
No
pInd & cInd & iInd
Book$
Multiple linear regression
Yes
(MLR)
Personality (p) variables
Book$
MLR
No
Confidence (c) variables
Book$
MLR
No
Interest (i) variables
Book$
MLR
No
Table 43: Linear regression prediction summary for how much participants would pay for
a book that they needed for their research
Two SLR models were significant. The first significant model used the
personality index (pInd) to predict how much the participant would be willing to spend
on a book for research (Book$). pInd (M = 44.63, SD = 8.51) significantly predicted
Book$ (M = 25.93, SD = 18.47), F(1, 121) = 6.672, p = .011, adjusted R2 = .044.
According to Cohen (1988) this is a small effect size. When the pInd value increased by
1, the amount the participant was willing to pay for a book that they needed for research
increased $0.50.
The second significant model used the confidence index (cInd) to predict how
much the participant would be willing to spend on a book for research (Book$). cInd (M
= 40.02, SD = 10.08) significantly predicted Book$ (M = 25.93, SD = 18.47), F(1, 121) =
4.250, p = .041, adjusted R2 = .026. According to Cohen (1988) this is a small effect size.
When the cInd value increases by 1 the amount the participant was willing to pay for a
book that they needed for research increased $0.34.
Only one MLR model was significant. This model used the personality index
(pInd), confidence index (cInd), and interest index (iInd) to predict how much the

147
participant would be willing to spend on a book for research (Book$). The means,
standard deviations, and intercorrelations are reported in Table 44 below.
Mean
Std. Deviation
1
2
3
Book$
25.93
18.467
.229*
.184
.081
1. pInd
44.63
8.508
.383*
-.168
2. cInd
40.02
10.076
-.168
3. iInd
10.33
3.900
Table 44: Descriptive statistics for MLR to predict Book$ using pInd, cInd, and iInd
*p < .05
These variables significantly predicted Book$, F(3, 199) = 3.50, p = .018, but
only pInd contributed significantly to the prediction. The adjusted R squared value was
.058, which according to Cohen (1988) is a small effect size. When pInd increases by 1
point, the amount that participants were willing to pay for a book increases by $0.44.
Overall, this model is weaker than when pInd was used by itself to predict how much a
participant was willing to spend on a book that they needed for research.
As in RQ 2, participants were also broken into sub-groups based on demographic
characteristics to see if the IV for a particular sub-group could predict the DV. Once
again, the small subset (N = 13, 11%) of the total 123 participants who lived more than
an hour from their school had confidence index scores that could significantly predict the
DV, which was how much participants were willing to pay for a book that they needed
for their research. Confidence index score (M = 41.38, SD = 10.71) significantly
predicted the amount that participants were willing to pay (M = 24.62, SD = 16.39), F(1,
11) = 6.967, p < .05, adjusted R2 = .332. According to Cohen (1988), this is a large effect
size. When the confidence index score increases by one point, the amount the participant
is willing to pay increases by $0.95. This means that students who are more confident in

148
their ability to find information and feel less challenged by obstacles to finding
information are willing to pay more money for a book that they need for their research.

Binary logistic regressions for behavior variables DVs
Binary logistic regressions (BLR) were performed for the five dependent
dichotomous variables (DVs) defined below:
1. ArtHL: whether participants were not willing to pay to access an article or
they were willing to pay
2. AskHL: whether participants did not ask a librarian for help in the Fall 2016
semester or they did
3. VisitHL: whether participants visited the library less than once a month or at
least once a month
4. GSE: whether a participant chose Google Scholar or a search engine as the
resource that they would consult first when looking for information on a topic
that they were unfamiliar with or not
5. PS: whether a participant would consult a person or use a non-person resource
first when looking for information on a topic that they were unfamiliar with.
The independent variables (IVs) included the personality, confidence, and interest
variables, indices, and clusters developed in the results listed under RQ2. Overall, this
resulted in 45 BLRs. Of these, only one of the models was significant.
When BLR was performed to test the hypothesis that personality variables could
predict whether a participant would start looking for information on a topic that they did
not know about using Google Scholar or search engine, or another resource. Personality
variables investigated whether participants were open to asking for help/clarification;
were not unhappy if they retrieved a lot of information, even if it was unexpected; and
would consider changing their research based on what they found would affect their
behavior. These variables were able to significantly predict this DV, x2 = 31.584, df = 11,
N = 109, p = .001. The Cox and Snell R2 of .252 and Nagelkerke R2 of .336 indicate a

149
strong association between the personality variables and most preferred resource. This is
further demonstrated in the table below (Table 45), which indicates that personality
variables correctly predicted that participants would start with Google Scholar or a search
engine versus another resource for 90 (73.4%) of the participants.
Classification Tablea
Observed

Predicted
GSE

Percentage
Correct
68.0
78.0
73.4

0
1
0
34
16
1
13
46
Overall Percentage
a. The cut value is .500
Table 45: Classification table for personality cluster (pClus) membership prediction
using personality cluster membership (N = 123)
Step 1

GSE

The table below (Table 46) shows that the GSNet variable, which was the
participant’s 7-point Likert scale response to how much they agreed that they used
Google Scholar because it cast the widest net, is a significant predictor in the model.
Variables in the Equation
B
S.E.
Wald
df
Sig.
Exp(B)
a
Step 1 GSNet
.654
.154
18.156
1
.000
1.924
ChgTop
-.249
.176
1.993
1
.158
.780
PstIncomp
.193
.164
1.396
1
.237
1.213
NotPurs
-.152
.137
1.233
1
.267
.859
Rout
.186
.192
.934
1
.334
1.204
PstNotBot
-.089
.178
.247
1
.619
.915
OnlyNeed
-.079
.169
.220
1
.639
.924
AskCopy
.083
.196
.180
1
.671
1.087
AskExh
-.046
.123
.142
1
.707
.955
AskClar
-.075
.210
.126
1
.723
.928
PstTime
.049
.149
.110
1
.740
1.051
Constant
-1.942
1.809
1.152
1
.283
.143
a. Variable(s) entered on step 1: AskClar, AskCopy, AskExh, NotPurs, Rout, GSNet,
ChgTop, OnlyNeed, PstNotBot, PstTime, PstIncomp.
Table 46: Model summary of personality variables to predict whether a participant would
start a search with Google Scholar or a search engine or not (N = 109)

150
The Exp(B) indicates how much one unit of change in GSE affects their grouping
in the ArtHL variable (Leech, Barrett, & Morgan, 2011). Because the Exp(B) is more
than 1 for GSE, those who agreed more with the statement that they used Google Scholar
because it cast the widest net were also more likely to not start their search with Google
Scholar or a search engine. This result is discussed more in Chapter 5, but one reason for
this relationship might be that participants preferred a different resource because they felt
that Google Scholar’s ability to cast the widest net was not helpful when they were
unfamiliar with a topic.
The qualitative findings in Table 36 also suggest that in addition to their Current
Academic institution’s resources, many individual interview participants (N = 8; 53%)
had access to Other resources at another institution, either through having their own login
credentials or asking a friend at the other institution for access to a resource. Participants
in the focus groups (coded in N = 22 times) were much more likely to use Illicit websites,
such as Sci-Hub to electronically access items compared to individual interview
participants (N = 4, 27%). As discussed in the results for RQ 2, further investigation into
illegal usage of resources was not further explored in the Phase 3 survey, but is discussed
in Chapter 5 and 6.

3.4 Social Networks and 3.5 Software
The two remaining qualitative themes in Table 36 focus on participant usage of
Software (3.5) and Virtual Communities, usually in the form of Social Networks (3.4).
Once information resources were obtained, participants in all focus group interviews and
eleven (73%) in the individual interviews participants used Software that did something

151
more than or Other than manage citations to organize and store them, such as Mendeley
(P4). Overall, twice as many participant responses in the focus group interviews (24
versus 11) and individual interviews (28 versus 12) were coded as using these programs
than programs that only managed Citations, such as Zotero (P16). Demographic factors
did not seem to affect the use of either types of Software.
Social Networks were utilized more by focus group participants (N = 26 times)
versus the individual interviews (N = 23 times coded over N = 8, 53% of interviews).
Academia.edu and ResearchGate were the virtual communities that participants used
most often to find information, post their research, and connect with scholars. When
discussing the benefits of Academia.edu, one participant explained that:
I look for resources on [Academia.edu] because there's a lot of unpublished
research… People are putting…works in progress up there and then also there's a
lot of articles that get published that you have access to even if you don't have
access to that databases. So it’s…kind of open source on Academia.edu which I
appreciate immensely. So my own university doesn’t carry…some of the
databases I need…[so] I love Academia.edu, I think it’s one of the greatest things
out there, I use it all the time (P31).
ResearchGate and Academia.edu also allowed participants to network by including
mechanisms that allowed them to request information from scholars. Some participants
agreed that they would sometimes ask certain researchers for articles “just for the act of
asking…[to show they were] interested and engaged in the scholarship” (P14) or “direct
questions to authors [in order to create] networks [so]…people start recognizing your
name” (P9). Participants who were not in Communication or related programs seemed to
use ResearchGate more often than students who were, but no other demographic
differences seemed to affect participant use of or interest in these sites. These qualitative
findings were operationalized as frequency and Likert scale questions in the Phase 3

152
quantitative survey, and the results of these questions are reported in RQ 4 because
library instruction does not often cover these topics, but they can do so in order to
promote increased IE in doctoral students.

Summary of RQ 3 results
The findings for this RQ suggest that there are a number of factors that affect
what social science doctoral students use and value when engaging with information.
Online resources and access are the most highly valued resources when participants
related critical incidents when they looked for information and ranked them in the survey.
The participants in the Phase 2 individual interviews reported the highest usage of
librarian services and library space.
Predicting participant behavior was difficult. Participant responses and the indices
and clusters formed in RQ 2 were used as independent variables to predict six total
dependent usage and willingness to pay variables. Of these, the indices were most likely
to predict significantly positive relationships between participant scores and their
willingness to pay for books that they needed for their research. The largest any subgroup was willing to pay was $0.95, and these were the students that lived more than an
hour away from their institution. Personality variables significantly predicted whether a
participant started a search in an area they were unfamiliar with for 90 (73.4%) of the
survey participants. The implications of the findings from RQs 1, 2, and 3 combine with
qualitative data from the dissertation study to suggest ways that academic libraries can
increase IE through instruction in RQ 4.

153
Research Question 4 (RQ 4) asked: How can academic libraries promote increased
information (IE) engagement of doctoral students?
The major results for RQ 4 follow in the bulleted list below, and Table 47, which
follows afterwards, outlines the major qualitative themes identified from Phases 1 and 2.
x

x
x

Phase 1 and Phase 2 interview participants connected the information that they found
in their searches as contributing to scholarly Outcomes/goals (4.1).
o Participants in the focus group interviews (N = 20) were likely to apply
information to the Outcomes/goals suggested by other participants. On the
other hand, participants in the individual interviews (N = 15) were less likely
to apply information to other Outcomes/goals because they could not think of
many. This finding suggests that doctoral students do not normally think of
information as applying to other Outcomes/goals, but agree that they are
related if Outcomes/goals are suggested.
o Phase 3 survey participants were likely to agree that they had used or would
use the information that they found while searching for all suggested IE
Outcomes/goals.
When asked to think of Tasks Facilitated by a Magic Wand (4.2), participants gave a
variety of responses. Most of these related to finding information rather than
organizing or filtering it.
Phase 3 survey participant responses showed that they used several academic social
networking sites and were interested in learning more about information topics. These
networks and information topics are currently not covered in most library instruction
as reported in LIS literature, but including them could increase IE in this population.
o Participants used Academia.edu and ResearchGate.net to find information and
post their research more than any other social network.
o Participants were more likely to connect with other scholars after meeting
them in person.
o Participants were equally likely to connect with other scholars prior to
meeting them in person, if ever, on Academia.edu, Facebook, LinkedIn,
ResearchGate.net, and Twitter.
o Participants were most likely to respond that Twitter did not apply to their
finding or posting information, or meeting other scholars.
The results of the first three research questions described IE and looked for

relationships between IE and behavior for the social science doctoral students who
participated in this dissertation study. The final research question results report how
participants related IE to their own goals, and what information topics they were most

154
interested in learning about. Academic librarians can use this information to reframe
library instruction sessions for this group.
Themes related to library instruction topics that can increase IE
*RQ4 Themes

Focus groups
Group
Times
count
coded

Individual interviews
Participant
Times
count
coded

x

Outcomes/goals
o Future work
3
21
7
11
o Networking
2
19
4
8
o Career
3
6
4
5
x Tasks facilitated by magic
wand
o Other
n/a
n/a
13
32
o Seeking
n/a
n/a
7
12
o Organizing
n/a
n/a
4
10
o Filtering
n/a
n/a
4
5
Table 47: RQ4 qualitative themes identified in Phase 1 and Phase 2
* Please see Interpreting thematic coding tables on page 83 for how to interpret Table 47.
4.1 Outcomes/goals
As indicated in Table 47, above, the qualitative themes for this RQ fell into two
broad categories, Appendix G defines these themes and gives illustrative examples.
Outcomes/goals (4.1) were activities that participants felt were related to the information
with which they engaged. These were broken up into three sub-themes, and of these,
Future Work, which included identifying and applying for grants, and deciding where and
what to publish next, were coded in all focus group and 11 (47%) of the individual
interviews. Demographic factors did not seem to affect the application of this code to
participant responses. One participant described future work by saying that “It's made me
think what other studies and what other potential research I could do...also it gives me
pilots or more goals in the future” (P20).

155
Networking described incidents when participants connected or planned to
connect with other scholars. Sometimes the participants did not explicitly say that they
were networking, such as Participant 9 who would “exchange ResearchGate information
with people that I’m interested in getting to know.” The Career code was applied to
information that would aid participants in their career related aspirations. One participant
described looking up information about other scholars in her area so that she could
“follow them and try to be like them” (P16). Participants in the focus groups also
identified Networking (N = 19 coding instances) and Career Planning (N = 6 coding
instances) more often than individual interview participants (8 coding instances in N = 4,
27% and 5 coding instances in N = 4, 27%, respectively). Demographic factors did not
seem to affect participant experience or interest in Networking or applying information to
their future Careers.
The Outcomes/goals themes were applied less frequently in the individual
interview data because participants in the individual interviews did not think of many IE
related Outcomes/goals on their own. This finding suggests that doctoral students do not
normally think of information as applying to other Outcomes/goals, but agree that they
are related if Outcomes/goals are suggested to them. This finding was also a result of the
Phase 3 quantitative survey, as indicated in Table 48, below. Some participants chose the
“Prefer not to answer” option or did not respond to the question, and the n values for each
topic are in the last column in the table.

156
IE related outcomes (N = 123)

Mean

Standard
deviation
1.41
1.46

Prefer not to answer or
no response
0
0

Suggest topics for research
5.65
Suggest place to
5.56
present/publish
Identify and apply for grants
5.50
1.52
1
Manage my scholarly identity
5.48
1.46
1
Network
5.31
1.55
2
*Other
6.33
1.15
120
Table 48: Participants’ responses to “I have used or would be willing to use the
information that I found in my searches to” (with 7 being definitely will use and 1 being
definitely will not use)
*Other results included “identify community partners for collaborative research
opportunities” (N = 1).

The table above (Table 48) reports how likely participants would be willing to use
the information that they found for the activities suggested by the qualitative results of
Phases 1 and 2. In general, all participants were more likely than not to use the
information that they found for each of the activities as indicated by the mean response
value to being over the four-point middle of the 7-point Likert scale. The standard
deviations are relatively the same, and that combined with the relatively high mean
rankings for each topic suggest that a majority of the participants were willing to apply
information to those outcomes.

4.2 Tasks facilitated by a Magic Wand
In addition to investigating what Outcomes/goals doctoral students in the social
sciences related to IE, the study also identified which areas of IE they were most
challenged by. The magic wand question, which asked, “If you had a magic wand that
could help you in future searches, what would you have it do?” was only used in the
individual interviews. The most common sub-theme for Tasks Facilitated by a Magic
Wand (4.2) was coded as “Other” in Table 47 (32 coding instances in 13, 87% of

157
individual interviews). This theme was assigned to the responses in which participants
wanted a broad range of technological and other interventions that would help them do
more than find, filter, and organize information. Out of all of the focus group and
individual interview questions, the magic wand question had the widest range of
responses. Some responses would require magic to exist, such as the participant who
wanted a “robot wand” that she could “point…at [her] desk and just tell it to get all the
books [she would] need...[which]…it would already know” (P32).
Others participant responses were more actionable. For instance, some
participants suggested that the wand essentially remove certain library policies, as
described by the following participant:
Man, I just want my things…like I just want them immediately. I guess it would
be a library that has everything…So if a thing just got published it should come
out and it should be available to me…[Also,] I can have them for as long as I need
them (P28).
Other responses described existing library resources and services for finding, filtering,
accessing and organizing information. For instance, most databases offer a thesaurus that
would define “potential synonyms” (P33) or suggest the correct term “[translated] into
academ-ese” (P26), but none of the interview participants had heard of it, although
Participant 22 did indicate that she was aware of subject terms. Another participant
wished to be able to tell the system to “exclude these specific journals” (P30), which can
be accomplished using the NOT command or similar when constructing searches. Most
library catalogs, databases, and Google Scholar allow users to be able to search for
keywords, concepts, and an author’s last name (requested by P27) at the same time, using
the advanced search.

158
Finally, some responses to this question suggested tools for finding information
that could be possible in the near future. One participant thought it would be useful to see
recommendations to related articles, which she described as, “‘people who read this
article also looked at these’ just to see if it's something that I wouldn't have thought to
look up” (P35). An increasing number of databases are providing this sort of information,
such as Science Direct, Scopus, and Mendeley. Another participant responded that:
It’d be really kinda cool to have like a magic wand that for any book or article
would magically show a brain map or…some sort of brainstorm of…the history
of every idea that brought that piece into birth so that you could kind of see where
it was contextualized in the broader history of ideas and where it might lead, I
guess because of that history (P29).
For the IE topics that are commonly covered in library instruction according to the LIS
literature, the area that participants identified most often as being in need of a magic
wand was finding information (N = 7, 47% of individual interviews).
Participants in the Phase 1 and 2 interviews were asked if they had received
library instruction.
Instruction (N = 123)
Yes (n)
Phase 1
11 (55%)
Phase 2
12 (80%)
*Phase 3
n/a
Table 49: Participants’ attendances in library information session
*Data not collected for this phase of the study

No (n)
9 (45%)
3 (20%)
n/a

Table 49, above, reports the number of participants in the first two phases who
had attended a library information session at any point in their post-secondary education.
It was difficult to draw any meaningful conclusions from this information because many
of the participants, even those in Phase 2 who were more likely to have received this
instruction than in the first, claimed that any information sessions before their doctoral
program were not very helpful to them once they started their current program.

159
Participants reported that library instruction was not helpful because they did not
remember what was covered, they attended a session at a different university, or the
material covered in their undergraduate and master’s programs was not detailed enough
to help them in their doctoral programs. Therefore, this question was not included in the
survey in the Phase 3. Instead, questions asking about what they would like to learn were
asked in order to answer RQ 4. The next two tables report on IE related topics that
participants would be interested in learning about and that academic librarians could
consider offering. Some participants chose the “Prefer not to answer” option or did not
respond to the question, and the n values for each network are in the last column in the
table.
Social network
uses (N = 123)

Find
Post my
information research

Connect with
other scholars
prior to meeting
them in person,
if ever

Connect
with other
scholars
after
meeting
them in
person
20
56
37
22
9

Prefer
not to
answer
or no
response

Academia.edu
49
31
25
49
Facebook
21
15
25
39
LinkedIn
22
10
28
50
ResearchGate.net
55
34
28
43
Scholarly
41
5
14
61
liststerv
Twitter
24
12
25
23
69
Table 50: Participants’ responses to “I have used the following sites/networks to…”
As indicated in Table 50, above, participants were most likely to indicate an
activity on ResearchGate.net (total activity N = 139), followed by Academia.edu (total
activity N = 125), Facebook (total activity N = 117), LinkedIn (total activity N = 97),
Twitter (total activity N = 84), and a scholarly listserv (total activity N = 69). Three

160
participants also included Google Scholar to either find information or post their research
as open responses to this question.
Topics of interest

Mean

Standard
deviation
1.30
1.53
1.97
1.78

No
response
0
1
0
0

Managing my scholarly identity
5.80
Research methodologies/methods
5.42
Turning my dissertation into a book
5.24
Reusing data from other studies or making mine
5.11
available for reuse
Open access publishing
5.05
1.83
1
The library resources available to me
4.73
1.70
0
*Other
6
1.73
120
Table 51: Participants’ responses to “I would be interested in learning about the
following (with 7 being very interested and 1 being very disinterested)
* Other results included interdisciplinary works, publishing non-conclusive research to
prevent someone else from wasting their time doing the same thing, and how to manage
academic-related social media accounts, like ResearchGate (N = 1 for each).
When asked which topics they would be most interested in learning about,
participants were most interested in learning to manage their scholarly identity. Some
participants chose the “Prefer not to answer” option or did not respond to the question,
and the n values for each topic are in the last column in the table. In general, they were
more interested than not to learn about all of the topics as all the mean rankings for each
topic were higher than 4.0, the mid-point of the 7-point scale. The relatively high mean
and low standard deviation for the “Managing my scholarly identity” response indicates
that participants were overall very interested in learning about this topic. Participants had
the highest standard deviation around their mean ranking for the “Turning my dissertation
into a book,” which suggests that the interest in this topic varies more than interest in
other topics.

161
Summary of RQ 4 results
The findings for RQ 4 suggest ways that academic libraries can increase
participant IE. Some findings, such as participants not finding library instruction helpful
support findings in other studies. Other findings suggest topics that library instruction can
cover that have not been reported in the LIS literature. These topics came from the
goals/outcomes that the interview participants connected to information and tasks that
they would want a magic wand to help them accomplish. The survey participants
confirmed their interest and usage of social network usage and information topics that
librarians can cover in instruction sessions, which could increase IE. The results of all
four RQs and how they connect with other findings in LIS literature are further discussed
in Chapter 5.

Summary of Knowledge Claims:
The Knowledge Claims identified by this dissertation regarding social science
doctoral student IE follow below, by research question.
RQ 1.1 Based on the qualitative interviews in Phases 1 and 2 of the dissertation study,
individual factors rather than situational or institutional variables led to the creation of
factors that predicted IE behavior.
RQ 1.2 Significant facilitators, which helped IE, included searching and organizing
strategies, as well as awareness of the academic library resources and services that made
IE more convenient.

162
RQ 1.3 Significant barriers, which hindered IE, included not finding and not knowing
how to access information. The former were based on topic or field specificity, and the
latter was usually related to library policy.

RQ 2.1 The qualitative interview data analysis from Phases 1 and 2 suggested that an
individual’s knowledge and personality affected their IE behavior.
RQ 2.2 As a group, the doctoral students who participated in this study were likely to
start with familiar resources, until they encountered IE challenges, which usually
occurred in their coursework and qualifying exam stages.
RQ 2.3 The individual factors that affected IE behavior were conceptualized as
personality, confidence, and interest.
RQ 2.4 Participants in the Phase 3 survey were grouped via cluster analysis and additive
indices based on their responses to questions that were related to these variables.
RQ 2.5 There was a significant relationship among index scores indicated that Phase 3
participants who had passed their dissertation proposal stage were less likely to think that
library instruction would be helpful to other students in their program if they had higher
confidence in their ability to find information.
RQ 2.6 A similar relationship was found for students who lived more than an hour from
their institution. Also, distance students who had higher personality scores, meaning they
were more open to asking for help/clarification; less unhappy if they retrieved a lot of
information, even if it was unexpected; and would be more likely to consider changing
their research based on what they found, were also more likely to be more confident in
their information searching abilities.

163

RQ 3.1 The qualitative and quantitative data analysis indicated that participants valued
both electronic and print resources. Information was valuable for this group, and Phase 3
participants were willing to pay an average of $30 and $10 for a book and article that
they needed for their research, respectively. At the same time, 75 (61%) of the Phase 3
participants were not willing to pay anything for an article.
RQ 3.2 The personality and confidence scales could predict how much a participant
would be willing to pay for a book that they needed for their research, with an increase of
$0.50 and $0.34 per point for the personality and confidence scales, respectively. For the
distance students, this amount was $0.95 per point increase on the confidence scale.
RQ 3.3 Participant responses to the personality variable questions could also predict
whether a Phase 3 participant would start a search on a topic that they were unfamiliar
with 73 percent of the time.

RQ 4.1 The qualitative and quantitative data analysis suggests that the participants in the
study did find information important, but found it difficult to apply to outcomes beyond
searching for, finding, and accessing a document. However, once other goals, such as
career goals were suggested, many expressed interest.
RQ 4.2 The information related goal that the participants in Phase 3 were most interested
was how to manage their scholarly identity. This topic intersects with other IE tasks, such
as finding and organizing information, which academic librarians can cover during
library instruction.

164
CHAPTER 5: DISCUSSION
This chapter discusses the findings presented in Chapter 4 by research question
(RQ). Each RQ contains multiple points of discussion broken up into different
subsections. For each subsection, the discussion compares and contrasts the relevant
qualitative and quantitative data with the findings from other Library and Information
Science (LIS) studies. Each RQ section ends with a summary of the major points and
how they connect to the next RQ.

Discussion topics for Research Question 1 (RQ 1): What are the critical factors that
characterize information engagement (IE) for doctoral students in the social
sciences?
This dissertation has conceptualized IE to include behaviors related to finding,
filtering, accessing, and organizing information. The existing LIS literature suggested
that different strategies arose due to situational, institutional, and individual factors. The
social science doctoral students in this dissertation faced many of the same situational
and institutional challenges to completing IE activities, but developed different strategies
for overcoming them. Rather than viewing these challenges as applying similarly to
students based on their stage or demographic factors, this dissertation found that similar
types of challenges could occur in any stage. The results of this RQ identified Facilitators
and Barriers that helped and hindered IE for most participants.

165
Finding and accessing information
While expert searchers and doctoral students often have similar levels of
education and experience, several studies have shown that doctoral students differ from
expert searchers in how they formulate search strategies and use search tools (Korobili,
Malliari, & Zapounidou, 2011; Bøyum & Aabø, 2015; Catalano, 2013; Spezi, 2016).
Some studies emphasize that these differences are less prominent as doctoral students
progress in their studies (Barrett, 2005; Spezi, 2016), but even those who complete their
doctorate and end up supervising their own doctoral students will not necessarily have
acquired expert searching skills (Barry, 1997; Research Information Network, 2011;
Catalano, 2013; Nicholas et al., 2016). These varying levels of searching ability suggest
that individual experiences and knowledge gained, which Khosrowjerdi and Iranshahi
(2011) conceptually linked in their term prior knowledge, account for differences in how
searchers use search tools and techniques. The results of RQ 1 suggest that knowledge
and experiences are important factors in the strategies that doctoral students use to find
information. The similarity of the major tasks in social science doctoral education
programs helps ensure that by the time doctoral students have completed their
dissertation proposal/prospectus, which will be referred to as the proposal in the rest of
this chapter, they will have encountered the major challenges to finding information. At
this stage, strategies for overcoming similar challenges will change very little, if at all.
These challenges usually coincided with a doctoral program task that required different
searching strategies or resource types.
Previous literature has identified the stage after students have completed their
coursework and their qualifying or comprehensive exams as being the stage where the

166
largest proportion of students will drop out of their doctoral program (RockinsonSzapkiw & Spaulding, 2014). Poor information searching skills might account for some
of this attrition, and several LIS studies have investigated the benefits of dissertation
research and writing seminars, workshops on research and writing topics, and pairing
students with librarians (Libutti & Kopala, 1995; Fleming-May & Yuro, 2009; Du &
Evans, 2011; Harris, 2011). However, the results of this RQ suggest that the dissertation
proposal or dissertation writing stages may be too late to reach certain students. For
instance, Participant 6 explained that in coursework and through his qualifying exams he
primarily used books and journal articles. When he began doing research for his
dissertation proposal, he had to learn how to find information on government websites,
government reports, and terrorist websites. An instructional or other intervention that
taught him how to efficiently search for information in these resources could have been
helpful.
However, other participants described major changes when preparing for their
qualifying or comprehensive exams. Rather than separating these differences by
demographic factors, such as the participant’s stage in their program, it is can be more
useful to describe the change in terms of varying searching behavior for tasks that could
occur in multiple stages. For instance, P4 discussed forward and backward searching,
which were non-LIS terms for Known Item and Pearl Growing, respectively. The latter
has been identified as a popular information search strategy among doctoral students
(Earp, 2008). Barry (1997) refers to Keyword Searching as a novice searcher strategy,
even though the faculty who supervise doctoral students are likely to use it. Participant 3
seemed to find keyword searches more difficult because she did not know what, if

167
anything, would result. This reflects a finding from a study by Chu and Law (2007),
which was that while many graduate students used keywords to find information,
complex keyword searches were often ineffective. The responses from Participants 6, 4,
and 3, above, suggest that defining an area of research is a conceptual challenge for
doctoral students. This challenge is at the heart of what Gardner (2009) defined as
progressing from a knowledge consumer to creator, and by Fleming-May and Yuro
(2009) as the development from student to scholar.
A complicating factor in this process for academic librarians who would want to
assist doctoral students in their development is that this change can occur at any time.
Students in Coursework can face similar challenges if they need to find information on
unfamiliar topics for which they do not have specified readings. Sometimes the syllabus
or the instructor were able to help them identify the proper keywords. As one student
recounted, “I would’ve never have been, ‘Yeah, feminist episiotomy, let me put those
two things together.’ I know what epistemology is but…feminists have their own?”
(P26). While their course instructor could occasionally help point them towards useful
resources or offer helpful keywords, sometimes the instructor themselves was unfamiliar
with the area. The same participant recounted another instance where:
I was writing a paper on family construction...about how animals are constructed
like family, and I... was so lost trying to write this paper. I didn’t even know
where to start. I was typing in family, I was typing in family construction...I don't
do family scholarship. I wouldn’t have even known the journals for family
communication...I was just walking in blind…I submitted two drafts to this
professor and she was like, ‘Just keep looking, just keep looking,’ and I was like,
‘Give me more information, tell me what to search, just tell me what to find, tell
me the words,’ and she…was just [repeated], ‘Keep searching, keep searching’
(P26)

168
Relying solely on Keyword Searching to find documents, or the belief that one
could find relevant documents if only one could guess the correct keywords, caused
participants in all stages of their doctoral program high levels of frustration. Library
Instruction sometimes covers alternative ways to browse for materials and how to
systematically develop synonyms for and different combinations of keywords.
Unfortunately, many participants did not think that librarians could help them in
this area. As one student explained:
I think one of the main things is knowing how [searching] works and what words
to use… and I don't know how that could be taught cuz it’s not necessarily a
library skill, right? It’s a critical thinking skill or something, but knowing what
terms to use to find what you're looking for…I think that, in general, that’s what
people have the most trouble with cuz they’ll, you know, they're used to Google
(P28).
This response also illustrates that some students who have reached the highest levels of
education are unsure as to what their academic libraries and librarians can offer them.
Participants were also confused by how to access information that they had found. For
instance, Participant 15 described his main IE related challenge as finding information,
but that while writing his dissertation the main challenge was accessing what he had
found. Unlike some of the other study participants, Participant 15 knew about the
interlibrary loan (ILL) article delivery service, but like many students in the dissertation
writing phase, he was more willing than not to satisfice (Simon, 1955) with other, more
conveniently accessible materials. This finding suggests that information literacy or other
Library Instruction is a facilitator, but one with mixed effectiveness for doctoral students
facing IE challenges.

169
Library instruction
More often than not, the Phase 1 and Phase 2 participants had received some form
of Library Instruction, usually on information literacy topics, at some point in their
educational careers. The timing and effectiveness of this instruction varied, but many
were one-shot instruction classes. For instance, Participant 21 did not remember
everything that had been covered in classes from his freshman and sophomore years, but
he seemed to have found the frequent instruction helpful, although it was “a while ago”
(P21). This raises an interesting distinction between participant perceptions of
helpfulness versus measurable impact of its effectiveness over time. Few studies have
measured the long term benefits of one-shot classes, and those that have tend to rely on
self-reported data, such as how helpful attendees found the instruction (Wong, Chan, and
Chu, 2006) rather than objective indicators of success. In contrast, Participant 28 seemed
to find multiple instruction sessions to be less helpful because they repeated the same
instruction, specifically about databases. This may be why she did not associate library
instruction with the critical thinking skills needed to identify more useful keywords for
searching.
While it is likely that most one-shot classes cover databases and how to use them,
these and other forms of Library Instruction are not standardized within a discipline or
even within a single institution. In spite of this uneven coverage of content, other studies
have found that faculty members advising doctoral students assume that students have
learned relevant searching skills in a class at some point of their education (Spezi, 2016).
Even librarians may make this assumption, as recounted by one participant who said that
during her program’s orientation to the library, “there was kinda the assumption like, you

170
should know how to do it already. So they just showed us literally where on the webpage
[the search box] was so we could get to it ourselves” (P35). This response appears to be
the opposite of the assumption that graduate students were information illiterate, which
Green (2010) and Monroe-Gulick and Petr (2012) argued was inaccurate.
For doctoral students, an assumption that they know how to use services or
resources, like databases, at one institution, or even for an area of research, is doubly
dangerous. The results of this dissertation found that most participants moved to a
different institution at least once between degrees, and some will move from a different
disciplinary area (Monroe-Gulick & Petr, 2012). One participant, stated that a one-shot
introduction to “different selective databases…for communication” (P25) at the
beginning of his master’s program was very helpful because he had done his
undergraduate degree in political science. However, the Phase 1 and Phase 2 participants
seemed to learn many of the advanced search features outside of the library, such as P33
who used multiple fields and truncation. This result was similar to Rempel (2010), who
found that doctoral students were likely to teach themselves searching skills that other
populations would need to be taught in a library instruction class. According to Catalano
(2010), most graduate students have not heard of advanced search features at all.
These differences in perceptions of library instruction’s helpfulness and
effectiveness, and whether that perception was the result of other individual
characteristics and/or if it affected participant behavior, was one of the reasons why
interest in library instruction became both an independent and a dependent variable in the
dissertation study, rather than a demographic question about whether or not a participant
had received it. At its best, Library Instruction was reported to be an effective Facilitator

171
that introduced doctoral students to information resources and services. Data analysis
enabled a comparison of how participants felt about having a mandatory instruction
session versus face-to-to face help, which other studies have found to be preferable to
doctoral students (Fleming-May & Yuro, 2009; Mikitish & Radford, 2013). Participant’s
responses indicated that they were fairly ambivalent about whether a research workshop
that included library resources and services would be helpful to other students in their
program, and they typically disagreed that such a workshop that only included library
resources and services would be helpful. However, participants in each stage of their
program had a slight increase in the group’s average interest in library instruction scale,
and some, like P29, advocated for mandatory one-on-one instruction early in students’
doctoral careers. This suggests that more experienced students found Library Instruction
to be more helpful over their years in the program, but this was not a statistically
significant different, nor was a rationale readily apparent. One unambiguous finding from
the qualitative data in this dissertation and that was also found in other studies of doctoral
students (Catalano, 2013; Sadler & Given, 2007) was that if a student received helpful
instruction from a librarian once, they were more likely to ask another librarian for help
in the future.

Flow experiences and multiple logins
This study is among the first to discuss doctoral student perceptions of Flow (also
in O’Brien & Toms, 2008) and their use of Multiple Logins for accessing the resources at
other institutions. The former is important because it further connects LIS studies to
engagement studies from other fields. The latter is important because it is a reminder that

172
doctoral students often navigate diverse sources of information. This could be
accomplished using legal methods, such as Participant 5 who was taking classes at
another university, and illegal means of access, such as torrenting entire books (P5). Both
Flow and Multiple Logins act as Facilitators related to convenience in finding and
accessing information. The expectations that form when comparing various sources of
information shapes the perceptions of what information should cost and how easy it
should be to access (Williams, Nicholas, & Rowlands, 2010; Hoy, 2017). While it is has
been found in previous research that all library users and information seekers prefer
Convenience (Connaway et al., 2011), the doctoral student participants were willing to go
extreme, even what they admitted were illegal lengths, to obtain information, so while
Flow and Multiple Logins facilitated their IE, a lack of either did not deter participants
from pursuing information in most cases. Instead, other Barriers, including Library
Policies, were likely to hinder participant IE.

Library policies
While not being able to find or access information were Barriers that all
participants faced at multiple times in their doctoral careers, some Barriers were more
significant to students at some institutions. In addition to participants expressing
frustration at Not Finding information, several participants also expressed more severe
and lasting frustration with library policies if they were uncertain why they were in place.
The former were coded under the Not Finding theme, and the latter under the Not
Knowing theme. More participants in the focus group interviews expressed frustration
and anger at Not Knowing rather than Not Finding. Their frustrations ranged from

173
confusion over how library systems and services worked, to disappointment with the
library’s collections, to outright disagreement with certain policies.
The rationale and system through which library systems and services worked was
a main source of confusion for all study participants. Many participants did not know the
extent of the resources available to them, which meta-analyses of graduate and doctoral
student studies have also found (Catalano, 2013; Spezi, 2016). Other scholars have
suggested that this confusion may be due to commercial resources such as Google,
especially since undergraduate and graduate students have reported using Google and
Google Scholar prior to and more often than library databases (Wu & Chen, 2014).
Participants reported that Google Scholar was used as a resource for finding and
accessing articles. Google Scholar’s ability to access resources from a variety of websites
dovetailed with participants’ general belief that they should be able to quickly access any
resource for free. The perception of convenience and belief that Google cast the “widest
net” (P14, P19) likely contributed to student disappointment with academic library
collections, which has been reported in other studies (Rowlands, Nicholas, Williams,
Huntington, Fieldhouse, Gunter, Withey, Jamali, Dobrowolski, & Tenopir, 2008; Spezi,
2016).
Focus group interview participants were particularly upset with having to pay a
Fee for interlibrary loan items and with the Recall policy, which required them to return
books earlier than the original due date if another library user requested them. Reports on
the use of SciHub (Hoy, 2017), suggest that a large number of academics believe that
information, especially articles that are in PDF format, should be shared (P9), and for the
doctoral students who participated in the Phase 1 and Phase 2 interviews, this could apply

174
to digital or physical items. However, this was usually not the participants’ first option.
Like Participant 19, who described asking a friend to check out a book from their
institution to give to her rather than paying an ILL fee, focus group and individual
interview participants indicated that illegally downloading or sharing articles was more
often than not the “last step” (P24) the student would pursue if getting it themselves,
through a resource like Academia.edu and ResearchGate, or the library, had failed.
The candor of the interview participants in discussing behavior and the general
agreement in the focus groups to these comments resonated with Williams et al.’s (2010)
suggestion that social and situational factors rather than ethical and legal considerations
shape views on sharing information. While some participants described what academic
publishers might consider a clear case of digital piracy, such as torrenting an entire book
to read a chapter for a class assignment (P5), participants seemed uncertain where social,
ethical, and legal boundaries lay. For instance, in the first focus group interview,
participants did not begin talking about asking their friends at other institutions for
articles until around the middle of the interview. After other participants agreed that they
had also done this, or would consider doing so in the future, participants then talked
about requesting PDFs of articles from people they did not know on Twitter using the
icanhazpdf hastag.
The icanhazpdf hashtag is an example of the complexities of educating users on
how to ethically copy, share, and access information (Williams et al., 2010). While this
hashtag is increasingly being reported in LIS studies (Greenhill & Wiebrand, 2012;
Gardner & Gardner, 2015; Swab & Romme, 2016), it was first reported in another field
(Chandra & Chatterjee, 2011). The participants in the first focus group were curious to

175
hear about the practical aspects how to use the hashtag, but less concerned with the legal
and moral implications. At the same time, they also discussed when they would consider
using it, and seemed to come to a consensus that it was not the first place they would go
to access an article because it did not seem as unproblematic as asking the library’s interlibrary loan service for a copy.
In a similar fashion, individual interview participants were not likely to mention
asking friends at other institutions for articles until later on in the interview.
Unfortunately the small sample sizes in Phase 1 and Phase 2 makes it impossible to say
which factors influenced why more participants in the focus groups discussed asking
people to send them articles or downloading them from places such as torrent sites or
SciHub. This is an area for further research, and it is discussed in Chapter 6.
One reason for requesting electronic copies of articles was so that participants
could keep a copy for themselves. Participants also wanted to hold onto physical
materials, which explains their frustration with the Recall policies at their institution,
especially in the focus group interviews. Some responses, like Participant 14’s who
directly said that his wanting to hold onto materials was selfish, seemed to reflect
findings that Millennials have lower empathy and less concern for others (Twenge,
Campbell, & Freeman, 2012). However, Participant 14 was a member of Generation X,
which suggests that the need to access necessary information crosses generational
distinctions. While the individual interview participants were less visceral in their
rejection of the recall policies, their calculations about the value of the resources that they
obtained did take them into consideration, such as Participant 32 who admitted that she
would rather pay a fee for losing a book than returning it. These reactions to the Recall

176
policy suggest the propensity of some students to Hoard information, which is a finding
discussed a bit more in RQ 2, and suggests that some students feel secure having more
information, even if they do not intend to use it. This practice, which has also been
referred to as squirreling information away, has been noted in studies of researchers and
students (Centre for Information Behaviour and the Evaluation of Research, 2008;
Connaway & Dickey, 2010; Carpenter et al., 2012). The recall policy takes that security
away.
Despite Library Policy seemingly being a significant Barrier to IE from the focus
group interview data, the interview participants and findings from other studies of
doctoral students did not report Library Policies as being significant Barriers. The amount
and frequency of Library Policies mentioned as Barriers likely reflect a sampling issue
because all of the focus group participants (except for one) attended the same institution,
so the fees and policies that vexed the focus group participants might simply be nonexistent at other institutions. For this reason, participants in the Phase 3 survey were not
solicited for their perceptions of library policies as IE barriers.

Other barriers
Other LIS studies for the doctoral student population suggest that Cost (Tenopir,
2014), Distance from campus (Brahme & Walters, 2010), and delivery Time (Carpenter
et al., 2012; Catalano, 2013) could present Barriers to IE. As described above, the focus
group participants seemed to value free access to information very highly, and they
specifically tended to agree that they should be able to access any resource that they
needed for free. Some participants were especially tenacious in convincing the library to

177
purchase DVDs (P14, P18) or books (P18, P16, P21) so that they could borrow them.
They were also willing to reach out to other scholars, including colleagues at other
institutions or the author(s) of a document for a copy, when all else failed. This practice
fills a similar function as the “invisible college,” a term first coined in the 1600s by
Robert Boyle (OED, 2017), and later applied to the circulation of prepublications and
other scholarly communication among researchers and academics (Crane, 1972).
Certain discipline specific archives, such as arXiv.org for physics mathematics,
and computer science, are modern day equivalents of the invisible college through their
provision of open access to publications, especially pre-publications. However, these are
primarily for the sciences, and only one participant (P19) in Phases 1 and 2 mentioned a
social science archive similar to arXiv.org, and that was lingbuzz, a linguistics archive. In
the social sciences, the use of academic social networks, such as ResearchGate and
Academia.edu seem to provide an updated take on invisible colleges. Rather than
anonymous sites, such as SciHub, academic social networks fall more in line with the
personal aspect of sharing information. Participants, such as Participant 9, felt that asking
for information on these sites could also facilitate Networking with other scholars.
As mentioned before, participants were also willing to spend money on certain
resources, whether it was paying a late fee (P32) or buying it from a vendor, especially if
it was not too expensive (P14). The varying circumstances that caused some students to
pay for materials that they could access through the libraries prompted the inclusion of
willingness to pay (WtP) questions for books and articles, and the responses to how much
Phase 3 survey participants were willing to pay were dependent variables for behavior
predicted by their personality, confidence, and interest indices and clusters.

178
Phase 1 and Phase 2 participants were less willing to compromise on Time,
especially the time needed to travel if they had to come to campus to pick up items, such
as Participant 15. Although no students reported being in online doctoral programs,
which meant that at one point they had attended at least some of their classes on their
institution’s campus, some participants had moved away from campus after they
completed their coursework. Participants who had defended their proposal were more
likely to have moved away, and some were working at other institutions. Distance was
not a significant factor in doctoral student attrition reported by the CGS Ph.D. Attrition
Project (Sowell, Zhang, Bell, & Redd, 2008; Sowell, Bell, & Kirby, 2010), but Distance
can affect access to information. Participants were not willing to come back to campus to
access materials (P15) or to ask for help (P29). With regard to asking for help, some
participants noted that they also did not ask for help when they were closer to campus
(P31), which suggests that being willing to ask for help was an individual factor that was
less liable to change over time.
These findings differed from a study of distance doctoral students in education,
which found that these students actually preferred to use library resources and were likely
to contact librarians for help (Brahme & Walters, 2010). Although mentioned
infrequently as a Barrier in Phases 1 and 2, the participants in the Phase 3 survey were
asked if they lived over an hour from their program’s institution, and this variable had
significant impacts on their views of library instruction and willingness to pay for a book,
which are further discussed under RQ 3.
Large scale studies (Carpenter et al., 2012) and meta-analysis of doctoral students
(Catalano, 2013; Spezi, 2016) have suggested that delivery or the Time needed to access

179
materials is a major, if not the biggest barrier, that these students face. This barrier,
however, was not mentioned very often in the Phase 1 and Phase 2 interviews. Students
in coursework were more likely to state that a lack of Time before an assignment was due
prevented them from contacting a librarian (P26), or increased their annoyance at not
being able to find a resource (also reported in Radford, 1993). However, it is also
possible that students in the social sciences, or at least the ones in this study, were simply
more willing to Satisfice (Simon, 1955), or find something good enough for their
assignment (P3). Catalano’s (2013) meta-analysis of graduate student, including doctoral,
information seeking behavior concluded that Satisficing due to time constraints was
found in most studies for this population. Taken together with Participant 3’s statement
above, it seemed that time was a barrier, but that its effect was situational and contextual.
A willingness to Satisfice was likely a personality trait, and therefore of greater
importance to the study, so Phase 3 participants were asked about that rather than how
they felt about delivery or access time.

Summary of RQ 1 discussion
This dissertation’s findings highlight the importance of doctoral stage on student
IE behavior, and it is important to also keep in mind their ever constant need to seek and
incorporate new information as they situate their work (Barrett, 2005) and become
experts of their field (Fleming-May & Yuro, 2009). Library resources, services, and
Policies can help or hinder student IE. Library Instruction can be effective in teaching
students about databases and how to find information, but student experiences with it
vary in unpredictable ways. Library Policies can hinder student access to information, but

180
this can vary by institution. While Task and institutional variables acted as Facilitators
and Barriers to the participants, as discussed in this section, individual student factors,
especially how they perceived these Facilitators and Barriers, appeared to have the most
influence on their perceptions of the library and their IE behavior. The section to follow
on RQ 2 continues this discussion.

Discussion topics for Research Question 2 (RQ 2): What IE types exist for doctoral
students?
The discussion about RQ 1’s results posited that situational and institutional
variables posed what were generally considered Facilitators and Barriers to IE for the
doctoral students in this study. However, the range of participant responses, as measured
by their responses and behaviors, to these Facilitators and Barriers varied, and other LIS
studies report similar variations. Consequently, RQ 2 describes individual factors that
might influence a particular student’s response to IE challenges, Facilitators, and
Barriers.

Resource preferences
LIS studies of doctoral students often begin with an acknowledgment that the
complex needs of doctoral students within and between academic disciplines, coupled
with their journey to establish themselves as scholars, make them a group that has a high
potential to need information that their academic libraries can provide (Fleming-May &
Yuro, 2009; Harris, 2011; Mikitish & Radford, 2013). A major complication with
offering instruction to this group of students is that doctoral work tends to be

181
interdisciplinary (Earp, 2008), and disciplinary differences are often cited as significantly
altering IE behaviors, such as resource preference and searching behavior (Catalano,
2013; Spezi, 2016). However, later studies have suggested that doctoral students in
certain areas have more similarities than differences (Spezi, 2016), and this finding was
reflected in the dissertation data.
The interview participants in Phases 1 and 2 primarily discussed finding and using
journal articles and books. However, journal articles were what participants tended to
look for first (P35), and sometimes they would not even consider looking at a book unless
it had a specific article in it (P4). Phase 3 participants were asked to rank the importance
of several types of resources, and results of this question indicate that journals are by far
the most important resource for these social science students. This finding is consistent
with other studies that focused on or included social science doctoral students (Research
Information Network, 2010; Fleming-May & Yuro, 2009; Carpenter et al., 2012; Niu &
Hemminger, 2012; Spezi, 2016), and more senior researchers also find journals,
especially peer-reviewed ones, as the most important resource for their work (Nicholas et
al., 2010; Tenopir, King, Christian, & Volentine, 2015).
Resource preference was one area where the participants in this study were
similar to social science doctoral students who had participated in other studies. Other
scholars have stated that doctoral students have similar requirements, levels of
information need, and academic ability (e.g., Harris, 2011), so it is possible that they vary
less in the range of behaviors than other student groups. This was very evident when the
individual interview participants in Phase 2 were asked to recount critical incidents where
they searched for information as most described identifying keywords, going to specific

182
databases, and then finding related items through “snowballing” (P22) before contacting
someone else. This process was similar to the responses for critical incident questions
where information was successfully found.
The similarity of responses resonates with Monroe-Gulick and Petr’s (2012) study
findings of first semester social science graduate students who were pursuing a master’s
degree. Snowballing, which has been referred to as pearl growing (Markey & Atherton,
1978), is often used by doctoral students when seeking information on an unfamiliar topic
(Earp, 2008; Barrett, 2005; Vezzossi, 2009). By starting with this strategy, rather than
keyword searches, many researchers, including Macauley and Green (2007), concluded
that graduate students are not as information illiterate as some LIS scholars and
practitioners might assume. It also suggests that doctoral students, like more senior
researchers, understand that quality and trustworthiness are important indicators of
research quality (Tenopir et al., 2015). However, the nature of doctoral work can lead to
other challenges besides finding information.

Other IE challenges
In addition to Not Finding information, discussed above in RQ 1, initial Keyword
Searches could also yield too many results, which was overwhelming (P23). Few
participants were willing to search through thousands of results like Participant 25, and
many were not concerned about potentially missing useful information (P22). Participant
25 was in his coursework stage, and Participant 22 was finishing her dissertation, which
suggests that the range and variance of doctoral student perceptions of information

183
overload (Barrett, 2005; Carpenter et al., 2012) are individual characteristics that may not
necessarily change as the student progresses through their program.
The interdisciplinary nature of social science work (Earp, 2008) also requires
social science students to find information in unfamiliar disciplines that could include
having to learn new tools, such as different databases, or new types of information, such
as primary sources rather than secondary sources, like Articles. These old and esoteric
articles (P24) were sometimes provided by library ILL services, but articles received
through this service were missing some context. For instance, Participant 24 was an
individual interview participant in Phase 2, and his comment that he might have been the
only person to ever ILL a certain article highlights a Barrier to Situating Work. He
believed that he might have been the only person to ever ask for that article to be
digitized, but that may be because ILL services do not make these numbers publically
available, if they are tracked at all. In contrast, many participants found Google Scholar
to be helpful because it allowed them to see how many times other scholars had cited a
particular article. Google Scholar was also useful for participants looking for more than
articles, and students in all stages of their program were likely to use it as a tool, if not
their primary tool for finding information because it returned so many different types of
results. If the participants, such as Participant 19, were interested in documents beyond
peer-reviewed articles, then Google Scholar was more helpful.
The diversity of responses to common information related tasks and IE challenges
suggested that different individual characteristics might be stronger predictors of
behavior than demographic ones, such as the participant’s stage in their program, their
gender, or their age. A discussion as to why interest in library instruction was tested as an

184
independent and a dependent variable in this study appears in the discussion of RQ 1
findings, above. The interview data formed the basis for the personality and confidence
variables, respectively, and these are explained in the next section.

Developing personality, confidence, and interest as independent variables
The individual interview findings in Phase 2 built upon the initial focus group
interview findings in Phase 1, and then these qualitative findings were tested
quantitatively in Phase 3. The qualitative thematic codebook based on the Phase 1 and
Phase 2 data lists and defines the themes and factors that differentiated between
participants’ information seeking and include: Prior Knowledge, Tenacity, Certainty, and
Hoarding (see Appendix G). However, these concepts were not tested to be underlying
factors in participant behavior in Phase 3 because in the prior two phases they were found
to be inter-related and participants varied from situation to situation.
For instance, the final codebook based on data from Phase 1 and Phase 2
juxtaposes Tenacity and likeliness to Satisfice at two opposites on a continuum of how
strongly participants would pursue an item or search for an item. Some participants
appeared to be very tenacious when relating how they performed IE related or other
tasks, such as Participant 16 who would spend over an hour looking for information
before asking another person for help, even though she realized help was available.
Tenacity, as defined in the final codebook in Appendix G, relates to a desire for
independence, which other studies of doctoral student preferences for information
seeking and other information behaviors have reported (Fleming-May & Yuro, 2009;
Carpenter et al., 2012; Mikitish & Radford, 2013; Catalano, 2013; Spezi, 2016).

185
However, Tenacity could also vary depending on how important the information was to
the participant at different moments in time.
In analyzing the interview data, it becomes apparent that the open-ended focus
group interview questions and the critical incident interview questions were useful in
identifying the range of doctoral student IE behaviors in general or in critical incidents of
their choice. Analysis of these responses confirmed that the 35 total interview
participants had similar levels of knowledge and similar IE behaviors to what other LIS
studies of this population have reported. Analysis of results for RQ 2 found that in
general and in successful searches for information, participants had similar preferences or
routines.
Analysis of the qualitative data found participant response to information
searching challenges to be more useful in grouping participants. Analysis of Phase 3
survey data attempted to test the effect of three different concepts on participant behavior.
The first, interest in library instruction was discussed in RQ 1. The second was
personality because when searching for information, some participants were more open to
explore a lot of information, even if it meant sifting through large amounts of information
or using unfamiliar searching resources or tools. These students were less likely to
express information overload (Barrett, 2005). When faced with a finding or access
challenge, some participants were more hesitant than others to ask for help. Other studies
have found that students were hesitant to ask librarians for help (Catalano, 2013), as well
as faculty members (Carpenter et al., 2012; Spezi, 2016). These two tendencies combined
to form the personality variables. Participants with high levels of agreement to these
questions were more open to receiving information and seeking it from others.

186
The final independent variable that came from analysis of the Phase 1 and Phase 2
data was participant confidence, which is the opposite of feeling anxious about finding
the correct information, a hindrance reported by Blummer et al. (2012) and Spezi (2016).
Participants with high levels of confidence believed that they could find information and
did not claim to feel as challenged by particular obstacles, such as the participant who
asserted that, “Nothing stops me per se. I mean, if it’s out there I'll find it” (P6). Other
studies of doctoral students have also reported that their participants had varying levels of
confidence, but these were mainly small scale qualitative studies (Barrett, 2005; Earp,
2008; Fleming-May & Yuro, 2009; Mikitish & Radford, 2013).

Indices and clusters
No other study for this population created an additive index or clusters to predict
IE behavior. The Zickuhr et al. study (2014) was the only one that grouped participants
based on their responses to Likert-scale response questions using factor and cluster
analysis. This inductive style of grouping participants has important advantages over
deductive ones like the one used in this dissertation. Most importantly, researcher bias
has less of an effect on the creation of the factors that separate participants. On the other
hand, it is unclear what the effect of situation variables had on these groupings, and the
analyses used in the Zickuhr et al. (2014) study required more participants than this
dissertation could recruit.
When using index scores to predict other index scores and cluster membership to
predict other cluster memberships, the results were largely unsuccessful. For instance, the
cluster analyses also found personality cluster to be able to predict confidence cluster, but

187
due to the small effect size the results of that analysis were not reported. The implications
and future work outlined in Chapter 6 discusses how future studies can mitigate the
limitations of this study. However, the ability of some index scores for participants in
certain sub-groups to predict their score on other indices suggests that the independent
variables are related.
The first sub-group that had significant results were those in the final stage of
their doctoral program. The confidence index scores of participants in the Dissertation
stage were able to significantly predict their interest index score with a large effect size.
This was a negative relationship, which meant that as participants increased in their
confidence to find information, they were less likely to be interested in library instruction
classes. At face value, this finding seems to contradict the wisdom behind offering library
instruction for students in this stage, but follow-up interviews or collecting more data on
which part of the Dissertation stage the students were in might play a factor. In other
words, if the students who were more interested despite their high confidence scores, it
could be because they had not yet finished their dissertation literature review, and
actually could benefit from additional instruction, such as the dissertation research and
writing workshops suggested by other studies (Libutti & Kopala, 1995; Fleming-May &
Yuro, 2009; Du & Evans, 2011). The other possible issue could be the wording of the
question. One question asked about a personal preference for face-to-face instruction for
the participant, but the other two asked how useful they thought library instruction would
be for other students in their program. As discussed in Chapter 6, further work needs to
clarify the questions used as variables in the interest and other variable questions.

188
The other sub-group that had higher predictability were the distance students.
Perhaps because there were only 13 of these in the survey portion of the study, their
views were more homogenous. Specifically, confidence score could predict interest
score, and personality score could predict confidence score. The confidence index score’s
ability to predict the interest index score had one of the largest effect sizes in this study.
Based on the findings for this sub-group of distance students and the students in the postproposal stage, it is likely that confidence does in fact have a negative relationship with
interest in library instruction. For the distance sub-group, distance students who scored
higher on the personality index were more likely to score higher on the confidence index.

Summary of RQ 2 discussion
In conclusion, the qualitative study findings in Phase 1 and Phase 2 suggested
several possible ways to categorize participants. However, these categorizations did not
seem to take into account the variance of behaviors that participants described in different
situations, especially ones that challenged their IE. Rather than using factor analysis,
which was used in the Zickuhr et al. (2014) study, this study used additive indices and
clustering to group participants by their responses to a variety of challenging situations.
The results of the analyses for this RQ indicate that certain index scores could predict
others, but this was only statistically significant and applicable to a large proportion of
the participants of certain sub-groups. Moreover, the clusters formed in this study could
not predict membership in other clusters, unlike the clusters generated and tested in the
Zickuhr et al. study (2014). One major contribution of this study is that it then uses the

190
Rather than using capabilities, opportunities, and motivation to predict behavior, the
qualitative findings of this dissertation’s Phase 1 and Phase 2 interviews suggested other
individual factors, which were tested as independent variables (IVs) that could influence
their behavior. This data also suggested different IE behaviors that could be tested as
dependent variables (DVs). Library value literature has suggested that economic and
usage variables have strong face validity among those in LIS and others (Saracevic &
Kantor, 1997; Oakleaf, 2010).
The Phase 1 and Phase 2 interview data identified the Cost of materials and
Library Policies, such as ILL Fees, as Barriers that prompted some participants to buy
materials. As discussed under RQ 1, participants were willing to purchase resources, in
spite of a strong and widespread belief that they should be able to access materials,
especially articles, for free (Maughan, 1999). LIS studies have investigated academic
library users’ willingness to pay (WtP) for library services and resources (Melo & Pires,
2010; Kingma & McClure, 2015; Ko et al., 2016), although none have focused
specifically on doctoral students.
As reported in Chapter 4, participants in this dissertation study were willing to
pay an average of $28.73 with a maximum of $80 and an average of $10.16 with a
maximum of $60 for a book and article that they needed for their research, respectively.
These are much lower than the $66.67 and $30.39 that faculty were willing to pay for inperson and remote access of resources and services, but much more than the $5.31 and
$13.14 that students were willing to pay for the same in a study by Kingma and McClure
(2015). Ko et al. (2016) reported that university faculty and students in South Korea were
even less willing to pay for books and journal articles. Faculty were willing to pay $6.31

191
for a book and $2.81 for a domestic journal article, while students were willing to pay
$0.57 and $0.67 for a book and a journal article, respectively. These results are not
comparable to this dissertation’s findings because a further breakdown of what each level
of student would pay in the Ko et al. (2016) is not available. However, they do reinforce
the idea that these values can vary greatly, which was reflected in the few significant
results for tests involving this DV.
Although personality index and confidence index scores could separately predict
how much the participant would pay for a book, the effect size for these were very small.
By combining all three indices, the model as a whole was significant, however only the
personality index variable significantly contributed to the model. The effect size though
was slightly larger than when the personality index model was used by itself to predict
the amount. This suggests that the indices as a whole are not very strong predictors for
the willingness to pay DV, and suggestions to improve future work are discussed further
in Chapter 6.
The distance sub-group of 13 (11%) survey respondents’ confidence index scores
were able to predict the amount that the participant was willing to pay for a book with a
large effect size. The amount increase was almost one dollar more for each dollar
increase on the confidence scale, as reported in Chapter 4. This was the largest value per
point increase on the willingness to pay variable, and this finding reinforces the idea that
certain demographic factors, especially distance from campus (Catalano, 2013), can
affect IE behaviors.

192
Library usage
Comparing library usage numbers for use of reference services and visits to the
physical library with other studies was also difficult. The average participant in Phase 3
of this study asked a librarian for help and visited the library less often than once a month
in the previous semester. These usage levels are lower than those reported by participants
in other studies of social science doctoral students (Fleming-May & Yuro, 2009; Green &
Macauley, 2007; Mikitish & Radford, 2013), but similar to those in larger studies
(Carpenter et al., 2012). This suggests that students who are willing to be individually
interviewed may be more likely to use the academic library than those who are willing to
take an online survey, which is discussed further in Chapter 6. Library usage could not be
statistically predicted based on the IVs used in this study, and the implications of this are
also discussed in Chapter 6.

Personality variables and resource preference
The last type of DV was resource preference prediction for a specific situation,
which was when the participant was looking for information on a topic that they were not
familiar with. As reported in Chapter 4, 63 (51%) participants in Phase 3 selected Google
Scholar or a search engine as the source that they were most likely to consult first, which
is similar to other studies that found half of the doctoral students in the study were likely
to start looking for information using an Internet search engine (Niu et al., 2010;
Blummer, 2012). As reported in Chapter 4, this preference was significantly predicted by
the variables used to create the personality index and the personality cluster. It was the
only significant finding out of the 45 binary logistic regressions run for all variables

193
related to this RQ. The variables predicted what the participant would start with correctly
in 73.4 percent of the cases. However, only one of the individual personality variables
was significant. This variable asked the participant to rate on a 7-point Likert scale if they
used Google Scholar because it cast the widest net. This means that out of the 11
questions grouped into this variable, only the participant’s response to this question had a
significant effect on their resource preference. Specifically, it suggests that the Phase 3
participants were more concerned with information overload, similar to the participants in
Barrett’s (2005) study, which is why those that believed that Google Scholar cast the
widest net were less likely to use it or a search engine when starting a search on an
unfamiliar topic. The fact that it took 11 variables to predict this suggests that greater
variability in the IV is more likely to yield a significant prediction of the DV, but more
work is needed on this topic.

Summary of RQ 3 discussion
The few successful DV predictions indicate that more work is warranted on the
topic of social science doctoral student IE. Stronger models were achieved by increasing
the variability of the IVs and limiting the response pool to sub-groups of the study
population. The significant findings suggest that the model should be changed a bit to
include the interest index as a DV of the personality and confidence constructs separately.
The index prediction findings suggest that as participants increase in their confidence
index score, their interest in library instruction index score decreases. Greater confidence
scores made it more likely for participants to pay more for books, which was an
unexpected finding considering the interview participants were generally against paying

194
for anything that they could get from their library. However, this finding makes more
sense given that it is most pronounced in distance students, who live farther away from
the library. This underscores the importance of situational and individual variables on
seeking information. Only by looking at a sub-group of doctoral students that cannot
easily get to the library, who are confident in their searching abilities, and who feel less
challenged by searching obstacles, was it possible to predict that they would be willing to
pay more for books that they needed for their research. While more research is needed to
explain contradictory findings within this dissertation study and within LIS literature, the
topics that libraries can include in instruction sessions to help students increase the IE is
less ambiguous. Suggestions for library instruction are covered in the next RQ.

Discussion topics for Research Question (RQ 4), which asked: How can academic
libraries promote increased IE of doctoral students?
The first RQ identified the critical factors that affected the Phase 1 and Phase 2
social science doctoral student participants’ IE, and identified them to be mainly
situational and institutional factors. The second RQ identified individual factors in Phase
3 participants that scored them on additive indices and grouped them based on
personality, confidence, and interest variables. The third RQ attempted to use these
individual variables to predict Phase 3 behaviors, which included willingness to pay for a
book and an article, library usage, and resource preference. The fourth RQ relates the
findings of these questions to suggestions for how academic librarians can promote
increased IE in this population via library instruction. The sub-sections for this question

195
relate to the qualitative themes developed in Phases 1 and 2, and which are listed in
Appendix G.

Outcomes/goals
As discussed in Chapter 3, engagement studies tend to relate engagement
activities to defined outcomes and goals, but few studies of doctoral students have looked
at goals beyond finding information. participants in the phase 1 focus groups were able to
generate several outcomes/goals that they connected with their searches. These
outcomes/goals were related to the participants’ continual situating of their work. As
discussed in RQ 1, participants are willing to take information that they have found and
apply it to future goals, which included grants, jobs, or other future related career goals,
participants were excited to realize that their searches could lead to successful outcomes
in those areas. At the end of one focus group, a participant enthused: “I feel like I've
learned more. I didn't know what to expect coming here, but I'm actually leaving with
information, so that's great” (P16).
In contrast, the individual interview participants were less likely to apply the
information that they found to Outcomes/goals. Occasionally some even asked the
researcher what other participants had suggested. Despite not being able to think of many
Outcomes/goals, once Outcomes/goals were suggested to them, they were usually willing
to agree that they would consider them to be related to IE, and if they had not done
something similar, they would consider doing so in the future. The survey participants’
responses to questions asking if they would consider using information that they found to
various goals/outcomes were also overwhelmingly positive. Out of the options, using

196
information to Network had the lowest mean score, which once again suggests that
certain participants are hesitant to reach out to other scholars. Findings suggest that the
use of social media is one topic that seems to be gaining interest among this group of
students.

Social media networks
The qualitative data collected in this study seemed to indicate that certain social
media platforms were very useful to some participants when used for certain tasks, such
as finding information or connecting with scholars, but it did not seem to be widespread.
The Carpenter et al. (2012) study suggested that doctoral students are cautious to adopt
new technology. Since then, more research has been published on whether these are
social or academic networks (Thelwall & Kousha, 2014), disciplinary differences
between usage (Ortega, 2015), and their limitations and possibilities for growth
(Williams & Woodacre, 2016). While scholars in LIS seem to accept that social
networking sites, such as Twitter, Facebook, and LinkedIn, can be used to maintain ones’
scholarly identity, more recent articles have differentiated between those sites and
academic social networking sites, such as Academia.edu, ResearchGate, Zotero, Google
Scholar, and Mendeley (Thelwall & Kousha, 2015; Ovadia, 2014; Williams & Woodacre,
2016). The participant responses resonated with the findings of the literature summarized
by Williams & Woodacre (2016).
The quantitative survey findings suggests broad patterns of use for certain
platforms, and participants in this study seemed to use social networking and academic
social networking sites differently. For instance, survey participants were more likely to

197
use Academia.edu and ResearchGate to find information and post their own research.
Participants were more likely to use Facebook and LinkedIn to connect with scholars
after meeting them in person, and to a lesser extent to connect with scholars prior to
meeting them in person. Participants were more likely to use scholarly Listservs only to
find information, while Twitter had low usage rates overall.
Although academic social media sites use was not widespread according to the
Phase 3 survey, when survey participants were asked to rate their interest on 7-point
Likert scales on information topics, the “Managing my scholarly identity” option had the
highest mean score. On average, each participants was more likely to be more interested
in this topic than not, which similar studies have found (Carpenter et al., 2012; Gessner,
Jaggars, Rutner, & Tancheva, 2011). In addition to identifying that social science
doctoral students are likely to be interested in managing their scholarly identity, this
dissertation’s findings, specifically the participants’ responses to the magic wand
question, also suggest that reframing instruction on how to find information could be
useful in increasing IE.

Tasks facilitated by magic wand
Only the individual interview protocol included the magic wand question, which
asked, “If you had a magic wand that could help you in future searches, what would you
have it do?” This means that only 15 participants responded to it. Out of all of the focus
group and individual interview questions, the magic wand question had the widest range
of responses. Some responses would require magic to exist, such as the “robot wand”
(P32) that would know what the user wanted and would deliver it immediately. Others

198
were more manageable, especially the removal of library policies that prevented
immediate and indefinite access (P28). In fact, some responses described existing library
resources and services for seeking, filtering, accessing, and organizing information. For
instance, controlled vocabulary such as subject terms or thesauri could identify the
synonyms (P33) or scholarly terms (P26). Advanced searching allows for the exclusion
of specific journals (P30) and looking for keywords in multiple fields (P27).
Finally, some responses to this question suggested tools for finding information
that could be possible in the near future. These include pop-ups for recommended articles
based on what a user has accessed, which is a new feature in Science Direct, Scopus, and
Mendeley. Although researchers today might have to hope to find an encyclopedia article
or meta-analysis similar to the “brain map” (P29) that contains the history of thought
behind an idea, in the next 10 or 20 years, this brain map could indeed be automatically
generated by linking together information that scholars have accessed and cited. While
the results of the magic wand and other questions indicated that the doctoral student
participants in Phase 1 and Phase 2 are not aware of the existing library resources and
services that can benefit them, they were eager to learn once they realized that they were
not struggling alone, a common feeling among doctoral students (Carpenter et al., 2012).

Summary of RQ 4 discussion
In conclusion, the dissertation’s qualitative data suggests that academic librarians
should offer instruction the use of academic social networks to maintain a scholarly
identity and how to find information for this population. The former is an emerging topic,
and currently may only apply to certain scholars in certain fields. The latter is a common

199
topic in library instruction, but the results of this and other studies suggest that it has not
been very effective. When asked to rate their interest in learning about information
topics, participants in the Phase 3 survey were least interested in learning about the
library resources available to them. This suggests that library instruction classes need to
reframe themselves to get appropriate information to the correct users and entice the nonusers to take advantage of the library’s resources and services. A discussion of the
limitations of this study, implications of these results for practice and theory, plus
directions for future work follow in Chapter 6.

200
CHAPTER 6: LIMITATIONS, IMPLICATIONS, AND FUTURE STUDY

This chapter outlines the limitations and implications of the dissertation. Based on
the qualitative and quantitative findings, it then recommends directions for future study.
Each section is further split into theoretical and study design recommendations.

Limitations
This dissertation explored an emerging topic in an understudied population. It
drew on engagement studies in Education and user studies in Library and Information
Science (LIS), but did not draw from an established theory. Due to its exploratory nature,
its results are not generalizable to its target population, which is social science doctoral
students, beyond the participant group itself. The theoretical and study design limitations
are identified below.

Theoretical limitations
In this dissertation, the model proposed and later revised was based in the
literature review including the limited empirical results available to date. No previous
model of information engagement (IE) within LIS could be found for this population. The
engagement models from the field of Education were more applicable to this area of
research than the LIS model of O’Brien and Toms (2008) because the latter deals solely
with information in an online environment. However, many of the variables and
outcomes that are measured using models from Education are not applicable to
information or academic library contexts. A very basic model of engagement from the

201
field of Health Informatics by Michie, Van Stralen, and West (2011) formed the basis for
this dissertation’s model.
In addition to being limited to a single model, the study only drew from LIS and
Education research. Although data analysis ultimately concluded that individual
perceptions are important in shaping the engagement of this population, theories and
studies from the field of Psychology were not consulted in the study design. In a similar
manner, although the topic of academic library deals with value, it did not draw from the
theories or studies in Economics.

Design limitations
The restricted range of the doctoral student population was a major limitation in
this study. The chosen study population was social science doctoral students in programs
in the United States, except for LIS doctoral students. This population makes up a small
percentage of students in higher education, and the design excludes doctoral students in
the sciences, humanities, and professional schools; other graduate students; and
undergraduate students. Out of the remaining students, a relatively small sample of 158
doctoral students participated in the study. In Phase 1, only 20 students from one solicited
university participated in the focus group interviews which were designed to be
exploratory, informing Phase 2 and 3. In Phase 2, only 15 students from one national
listserv and a listservs in one institution participated in the individual interviews, which
were also exploratory, building on Phase 1 results to inform the survey development. In
Phase 3, 123 students from 15 solicited institutions filled out the online survey. The
population size limited the analysis methods, such as exploratory factor analysis, that

202
could have been used to analyze the quantitative data. In addition to having a small
sample size, the participants were not randomly sampled from the study population. This
means that the study results are not generalizable to the study population beyond those
surveyed. It is possible that the formats of the in-person focus group and phone individual
interviews could have made it more likely for library users rather than potential users to
respond to the recruitment fliers, but this was not reflected in the library usage data
collected in all three phases.
Several measures were taken to mitigate the effect of these limitations. First, the
methodology was designed to intentionally narrow the range of responses by starting with
open questions in Phase 1, moving to critical incident questions in Phase 2, and ending
with quantitative survey questions in Phase 3. At the same time, the pool of potential
study participants was widened. In Phase 1, the researcher emailed the social science
doctoral program chairs at one university with a request to email their students a study
recruitment flier, while in Phase 3, the researcher emailed the program directors at over
100 departments in the social sciences at 14 institutions with this request. In terms of
inclusion in the study, potential participants in Phase 2 had to pass the strictest criteria for
inclusion in the study, because only five students from each of the three phases was
included. This limitation was removed in the other two phases. Nearly all of the potential
participants who expressed interest in the study passed the screening for each phases, but
a few self-selected to not participate afterwards by either not scheduling an interview,
failing to show up for an interview at the appointed time, or not filling out the survey.
The mixed methods design of the study facilitated the measures taken in each phase of
the dissertation study. By varying the data collection and analysis methods in each phase,

203
the strengths of each helped triangulate participant responses across the study. The
qualitative data gathered in Phases 1 and 2 provided rich examples of doctoral student IE,
which identified a variety of behaviors and factors that could potentially explain those
behaviors. The quantitative survey used in Phase 3 helped test the extent to which the
factors uncovered in Phases 1 and 2 predicted behaviors in a larger sample of
participants.
The data collection design also was a study limitation. Participants were asked to
share increasingly less demographic information at each phase of the study because that
information did not seem to affect the results, and the small sample sizes made those
potential effects difficult to measure. The range of responses were also limited. For
example, usage was split into five ordinal levels, which narrowed the variability of the
data. Participants in each phase also self-reported their past or intended actions, so it is
possible that they reported what they wished they had done or would do instead of what
they actually had or would do in certain situations. In addition to doing this purposely, it
is also possible that they did so unintentionally due to the Hawthorne Effect
(Roethlisberger & Dickson, 1939). The interview data collection was limited in that it
only consisted of audio recordings and notes taken by the researcher and assistants, so
there were no visual cues in the data. The transcription process also included mainly
verbal data, although laughter was recorded. However, other information such as the
length of pauses and participant tone of voice were lost.
Finally, the entire dissertation was susceptible to researcher bias. Miles,
Huberman, and Saldaña (2015) state that researcher bias includes the “effects of the
researcher on the case” (p. 296) and vice versa. This bias potentially went beyond the

204
subjectiveness that Corbin and Strauss (2015) claim is inherent to qualitative research, as
in the data collection and analysis in Phases 1 and 2. An example of avoidable potential
bias was that the researcher interviewed and paid each of the participants in Phase 1 and
Phase 2. In Phase 3, the researcher contacted and paid each of the participants. This high
level of involvement could have had an effect on the participants’ joining the study and
their responses to the interview and survey questions. Radford and Connaway (2017)
suggest that assistant moderators should compensate participants in focus group
interviews rather than the primary researcher. Shoaf (2003) also suggests using a
professional moderator for focus groups in order to reduce researcher bias, and a similar
logic would suggest using another person besides the researcher to conduct the individual
interviews.
Another example of the researcher’s effect on the study was that some
participants may have been affected by the fact that the researcher was a doctoral student,
like them, and eight of the Phase 1 focus group participants were in the researcher’s own
doctoral program, albeit in different departments and area of concentration of study.
These affiliations could have made the participants more candid, but they may also have
been more reticent so as not to hurt the researcher’s feelings or damage their reputation in
the researcher or their peers’ eyes. So while Young (1993) suggests that participants in
focus groups may be less inhibited in their responses, it is more likely that participants
felt more pressure to conform to the group’s consensus (Connaway & Radford, 2017). At
the same time, the study could potentially effect the researcher because she was a
doctoral student. Miles et al. (2015) also caution that the researcher may go native, which

205
involves being “co-opted into the [participants’] “perceptions and explanations” (p. 294).
As a doctoral student herself, the researcher was already a native.
In order to mitigate potential researcher bias, measures were taken to obtain
additional input and points of view in the data collection and analysis process.
Specifically, in the focus group interviews, there were one or two graduate students
taking notes as the researcher moderated the discussion. These assistant moderators, who
were also in the LIS Area of Concentration in the PhD program, were encouraged to ask
clarification or follow-up questions, which helped give more insight into areas that the
researcher may have overlooked. These notes were incorporated into the data analyzed in
Phase 1. Participant responses were written on white boards during the focus groups so
that they could suggest additions or modifications, which Connaway and Radford (2017)
suggest is a useful way to ensure that the data collected accurately reflects participant
responses. Finally, the researcher developed the qualitative coding schemes, so these
were highly subjective. The application of the codes was subjective, so another researcher
trained in this method coded 20 percent of the qualitative data. There was a high
percentage of agreement in inter-coder reliability, which suggests that the coding scheme
and its application were appropriate and adequately reflect the data. . These study design
and theoretical limitations have important implications for future studies, which are
described below.

206
Implications
Despite the limitations outline above, the qualitative and quantitative analysis
identified useful implications for future studies. Major theoretical and study design
implications are identified next.

Theoretical contributions and implications
Bates (2005) described the three phases of theory building as being description,
prediction, and explanation. Theories often accomplish those phases using models. The
revised dissertation model uses the data collected over the three dissertation study phases
to build upon four existing theories and/or models of engagement, described in Chapter 2.
The model incorporates the factors of personality, confidence, and interest as being
related to IE. It tests the effect of these factors on IE behavior. These tests indicate that
personality variables had a significant effect on a participant’s resource choice. Another
series of tests also found that participant personality and confidence had a significant
effect on how much they would pay for a book needed for research, and that the largest
effect size for this relationship was for students who lived more than an hour away from
their institution. The personality and confidence variables were based on information
seeking, which is a relatively theory rich area of LIS (Kim & Jeong, 2006). Resource
preference relates to library usage, and the amount one is willing to pay relates to
economic value. Usage and value are two ways of measuring academic library value
(Saracevic & Kantor, 1997; Oakleaf, 2010). Therefore, these findings link the areas of
information seeking and academic library value in a novel way.

207
At the same time, the model is simple and it has limited predictive power for IE
behaviors. This implies that future work should focus on model and theory building, and
suggestions for how to expand this work follow in the Directions for future study section.
The findings related to the model imply that variables for this population’s engagement
should be measured at the individual rather than the institutional or situational level. This
is not to say that institutional or situational variables do not affect doctoral student
behavior, but rather that they should be the same for each study participant until enough
about the individual factors is known to partial out the effect of other variables in the
model.
The limited number of quantitative significant findings also implies that the value
of academic libraries to participants and the IE of this population might benefit from
being studied separately until more is known about both. The dissertation’s findings
about doctoral students’ constant situating of their work implies that the cognitive
information behavior approach might have limitations that a different meta-theoretical
approach, such as the social constructionist information practice approach, would avoid.
This meta-theoretical approach is rarely, if ever, taken for value studies, and could also
be used to extend future work in the study of academic library and other types of value.

Practical implications
The theoretical implications outlined above imply that doctoral student
personality and confidence factors affect behaviors such as library usage and willingness
to pay for research books, which were used as proxies for academic library value. If these
information seeking related factors affect those behaviors, then in order to increase their

208
value, academic libraries need to affect information seeking. One way to affect
information seeking is through library instruction, so the findings suggest takeaways for
library instruction, especially for social science doctoral students.
The qualitative data anlaysis suggests that the effectiveness of library instruction
that a student may encounter up to and including their doctoral education can vary widely
in effectiveness and perceived helpfulness. The focus group interviews in Phase 1
produced a much wider range of responses than the individual interviews in Phase 2. This
finding was partially due to differences in the methodologies (Connaway & Radford,
2017), as discussed in Chapter 3. The focus group participants also enjoyed sharing
information, learning about new resources and strategies for overcoming obstacles, and
commiserating over shared challenges, such as not picking the correct keyword when
searching for information. Other studies of doctoral students that used a focus group
method for collecting data have made similar observations (Fleming-May & Yuro, 2009;
Carpenter et al., 2012). While this finding suggests that group instruction might be a
useful and enjoyable format for this population, the results from dissertation analysis on
ambivalence to library instruction discussed in RQ 1 and limited interest in instruction as
measured by the interest variable, index, and clusters suggest otherwise. This is not to say
that this population will always prefer face-to-face instruction, which this study and other
studies (Fleming-May & Yuro, 2009; Mikitish & Radford, 2013; Carpenter et al., 2012)
have suggested is more desirable.
Qualitative and quantitative data analysis suggests that doctoral students in the
social sciences may benefit from different types of library instruction at different times in

209
their doctoral career. These times relate to the challenges that they are likely to report
having in each stage of their program, and are listed below in Table 52.
Seek
Keywords;
Google
Scholar and
similar;
Disciplinary
databases*
Disciplinary
databases

Filter
Subject
headings

Access
Organize
Article and
Folders
book delivery

Other
Liaison’s role;
Scholarly info

Subject
headings

Before
exams

Related
databases;
Liaison
consultations

Thesaurus

Norms of
scholarly ID in
field
Networking;
Academic Social
Networks

Before
proposal

Liaison
consultations

Thesaurus

Article and
book
delivery
Interlibrary
loan;
Challenges if
moving off
campus
Challenges if
moving off
campus

1st
semester

2nd year

Citation
managers
Other
organization
apps

Data backup
and security

Findability;
Metrics to
determine
scholarly impact
After
Challenges
Challenges
Challenges
Challenges
Data use/reuse
proposal encountered♦ encountered♦ encountered♦ encountered♦ Copyright
Table 52: Summary of library instruction topics by doctoral student stage
*Topics in italics indicate that students should be made aware of these topics, but not
necessarily in detail unless the student requests it
♦Indicates that students should be encouraged to lead the conversation/lesson by sharing
what challenges they have encountered in order to fill any gaps in their knowledge
Each row in Table 52, above, suggests different IE topics across multiple IE behaviors
that can benefit students in different stages of their doctoral programs. Each column
suggests a sequence for providing instruction on different IE behaviors and topics so that
skills and knowledge scaffold, or build upon what was previously covered. It should also
be emphasized that Table 52 only suggests a rough guide for academic librarians, which
they can modify based on the skills, knowledge, and needs of the class or student that
they are instructing. Librarians can also ask faculty and students for their input on the
topics, timing, and sequence of topics outlined above, which could allow for possible

210
collaborations with faculty and students. After defending their proposal, doctoral students
can also help run sessions for students in later cohorts, and the challenges that they have
encountered can also provide valuable insight for future instruction for students in their
program.
In addition to the topics outlined above, the interactions of the students in the
focus groups suggest that group instruction may be more effective if reframed in three
ways. First, doctoral students should attend sessions with other students in their discipline
or who are studying similar research topics that require the same types of resources if
students from multiple disciplines are being included. Currently, doctoral student
instruction, with the possible exception of some dissertation writing workshops, is very
similar to the course based instruction offered to other levels of students. However,
students that are studying the same topic, but from different fields can provide valuable
feedback to each other for how to identify keywords or introduce field specific resources
to others. This leads to the second point, which is, instead of focusing on how to do
searches in general, the participants should be asked what challenges they have faced and
how they have overcome them. Third, the doctoral students should be encouraged to
share the tips and tricks that they have learned for finding information.
Research on graduate student information seeking has already found that this
population, which includes doctoral students, are likely to consult their peers (George,
Bright, Hurlbert, Linke, St. Clair, & Stein, 2006), although the degree to which they do so
can vary by discipline (Kerins, Madden, & Fulton, 2004). However, the studies of
graduate and doctoral student information seeking indicate that doctoral students will
have serious gaps in their information seeking knowledge and skills if they only refer to

211
other students for help. Therefore, having a librarian in who can facilitate these
discussions and answer any questions that the group cannot answer with their collective
knowledge could be a collaborative and effective way to teach information seeking and
other IE skills.
In some ways this approach is similar to the flipped classroom approach. This
approach originated in the field of Education, and is unique because it moves the
theoretical component, which is usually covered by a lecture, outside of the classroom,
and it moves the practical application, which is usually an assignment, into the classroom
(Educause, 2012). This approach has been found to increase meaningful student
engagement (Schullery, Reck, & Schullert, 2011), and Arnold-Garza (2014) argues that
many characteristics of the flipped classroom approach are similar to what the ACRL
recommends for information literacy instruction (ACRL, 2012). In particular, ArnoldGarza (2014) states:
The most fertile ground…is Category 7: Pedagogy, which almost perfectly
describes a flipped classroom, with emphases on diversity in learners and
teachers, use of interactive and progressive activities, use of appropriate
technology, connecting skills to real-world needs, and seeing the learner
holistically, not just in one learning context (p. 12).
Reports of flipped classroom information literacy instruction primarily come from use in
undergraduate classes (Maddison, Beneteau, & Sikoloski, 2014; Maddison, 2015; Cohen,
Poggiali, Lehner-Quam, Wright, & West, 2016), where it was found that assigning
students to view materials prior to the class was an effective use of time and helped
ensure that the students were starting with a similar baseline of knowledge (Steffy, 2013).
In fact, Miller (2013) suggests that putting in the students in small groups could facilitate
peer learning, even if some of the students had not watched the lectures ahead of time.

212
The one reported study of flipped instruction in graduate information literacy instruction
suggests that the sample of master’s level students benefited similarly from this
instruction style. From a library perspective, flipped instruction has the potential to be
more effective for doctoral students because their relatively higher baseline of knowledge
mitigates one of the primary challenges of incorporating this method in information
literacy instruction, which is the challenge of assigning “pre-work to a class you haven’t
visited yet” (Arnold-Garza, 2014, p. 16). From a doctoral student perspective, if the
library offered a few short electronically available videos or handouts, they could review
them as needed, and research suggests that they might be more inclined to ask a librarian
for help or attend a library instruction session if these materials are helpful (Sadler &
Given, 2007).
Another way to help doctoral students, and other academic library users, would be
to provide preemptive reference (Matthews, 2008) in which library staff or a service
could automatically appear in situations where users may need help. This point of use
help could include pop-ups in library catalogs or databases. Unfortunately, research on
strategic pop-ups indicates that users may find them intrusive, and in some cases
invasions of privacy. Mu, Dimitroff, Jordan, and Burclaff (2011) conducted a usability
study for a virtual reference service that had three different methods of alerting users to
the service plus one control that did not mention it. The 22 participants, including 10
graduate students, found the notice for the service that included pictures and text or a
notice with just text as being more helpful than the pop-up notice that displayed when
they did not find any results to a search or found too many results. Furthermore, the
participants found the pop-up notice to be a violation of their privacy and intrusive. A

213
study of academic librarian perceptions of virtual reference services found that while 50
librarians out of 102 thought that the user would follow a pop-up prompt to ask for help if
they received too many or too few hits, 43 felt that the user would ignore it (You,
DesArmo, Mu, & Dimitroff, 2014). More importantly, 89 responded that the user would
feel negatively about such a pop-up in those situations, and more specifically, 27
responded that the user would feel it was intrusive and 18 responded that the user would
feel it was an invasion of their privacy (You et al., 2014). However, both studies did find
that factors such as awareness of reference services, pop-up timing, and transparency as
to why the pop-up was displaying could decrease the perceived intrusiveness and
violations of privacy.

Implications for study design
The significant factors behind this dissertation’s strongest qualitative and
quantitative findings imply that similar factors in future studies can help focus the
population and variables in future work. The use of mixed methods to triangulate data
helped to focus disparate data on under-studied topics into significant quantitative
findings. The gradual narrowing of topic and broadening of potential population pool
described in the Limitations section, above, provided logical links between the findings
from each phase.
The need to have more variability in the dependent variables is another study
design implication of this dissertation. The amount of money that a participant would pay
for a book that they needed for their research was the variable that had the most
significant findings. The other dependent variables were mainly bivariate, and only one,

214
which was whether a participant would start with Google Scholar or a search engine
instead of another resource when looking for information that they were unfamiliar with,
had a significant finding. Suggestions for how to address the limitations and implications
of this study in future work follow.

Directions for future study
The limitations and implications of this dissertation provide several directions for
future study. The theoretical and study design suggestions are reported separately, below.

Theoretical diretions
Based on the limitations and implications of this dissertation, future studies on the
topic of doctoral student IE and academic library value may benefit by incorporating
theories from other fields, particularly Psychology and Economics. Theories from
Psychology could extend research by incorporating theories of individual behavior and
personality. There are also several psychological instruments that could be incorporated
into the study design, which are identified in the next section. Researchers have tested
these instruments over large samples, and their use can help make findings more
generalizable to a larger population. Economic theories have created different methods of
studying how people make rational value based decision. One such method is the
Contingent Valuation method, which has been used in LIS studies of academic library
value (Kingma & McClure, 2015; Ko, Shim, &Pyo, 2016), and provides more context for
willingness to pay variables. Theories from the field of Education can also extend future
work. Individual learning theories, such as Bandura’s Social Cognitive Learning theory

215
(2001), consider at many of the same factors as engagement, such as attention, agency,
and motivation. Possible study designs that incorporate some of these theories and
methods are identified in the next section.

Study design directions
Different meta-theoretical perspective and theories suggest different methods for
studying engagement and library value. Qualitative methodologies can expand
knowledge about the different types of engagement factors, and more importantly to
identify the other variables used to measure engagement behavior. Methodologies that
take into account theories from Psychology and Economics can provide a different
perspective on the personality, confidence, and interest variables developed in the
dissertation. Quantitative methodologies can incorporate quasi-experimental designs in
order to measure differences in IE based on different types of library instruction methods.
For instance, a few randomly selected mandatory instruction classes could utilize the
flipped instruction method suggested in the section above. The surveys used in future
studies can also incorporate a larger range of responses in the variables. For instance, the
dependent variable for usage could ask how many days it has been since the participant
has visited a library rather than asking them how often they would do so in a typical
semester.
Future work in both qualitative and quantitative studies could benefit from using
more methods to triangulate interview and survey data. For instance, a future study could
include an observation of an information seeking session where the user experiences
challenges or is tasked to find information on a topic that is unusual. This unusual topic

216
could be in an unfamiliar discipline, or it could provoke a strong emotional reaction,
which could be due to its importance or potential to change their field.
Depending on the study design, future work should utilize different sampling
procedures. By recruiting random participants from clearly defined population pools,
such as all of the social science doctoral students in one university, the study might be
more likely to include more non-users of library resources and services. The results of
this study would likely be more applicable to the population than the results of this
dissertation’s purposive sampling procedure. The sampling can also be used to interview
and survey a more diverse set of participants. Possible participants could include
academic librarians who instruct doctoral students, doctoral students in areas other than
the social sciences, and experienced researchers who may be researchers or faculty
members. These suggestions for future work, which are based on the limitations and
implications of this dissertation’s findings, have the potential to increase the LIS field’s
understanding of how doctoral student IE affects their valuation of the academic library.

Conclusion
This dissertation’s limitations, implications, and suggestions for future work
highlight the difficulties, but also the possibilities for defining and measuring academic
library value. The values of education and the institutions that provide it are increasingly
measured by individual outcomes that individual institutions identify and prioritize. What
gives academic libraries an advantage over other units in higher education institutions
(HEIs) is that LIS research has been shifting from the perspective of the user in the life of
the library to the library in the life of the user for years (Connaway & Hood, 2015).

217
An important intersection between institutional and individual outcomes in higher
education and academic libraries is the information needed to produce the scholarly
publications, grants, and patents that can contribute to successful careers in academia.
Doctoral students, and especially social science doctoral students, due to the
interdisciplinary nature of the work in those disciplines, are an understudied population
that need to engage with information and could benefit from library instruction to get that
information. By framing this study in terms of engagement, this dissertation connects the
fields of Education and LIS research. A brief summary of the dissertation’s research
questions (RQs) and significant findings follows.

RQ 1: What are the critical factors that characterize information engagement for doctoral
students in the social sciences?
Based on the qualitative interviews in Phases 1 and 2 of the dissertation study, individual
factors rather than situational or institutional variables led to the creation of factors that
predicted IE behavior. Significant facilitators, which helped IE, included searching and
organizing strategies, as well as awareness of the academic library resources and services
that made IE more convenient. Significant barriers, which hindered IE, included not
finding and not knowing how to access information. The former were based on topic or
field specificity, and the latter was usually related to library policy.

RQ2: What information engagement types exist for doctoral students?
The qualitative interview data analysis from Phases 1 and 2 suggested that an individual’s
knowledge and personality affected their IE behavior. As a group, the doctoral students

218
who participated in these phases were also likely to start with familiar resources, until
they encountered IE challenges, which usually occurred in their coursework and
qualifying exam stages. The individual factors that affected IE behavior were
conceptualized as personality, confidence, and interest. Participants in the Phase 3 survey
were grouped via cluster analysis and additive indices based on their responses to
questions that were related to these variables. A significant relationship among index
scores indicated that Phase 3 participants who had passed their dissertation proposal stage
were less likely to think that library instruction would be helpful to other students in their
program if they had higher confidence in their ability to find information. A similar
relationship was found for students who lived more than an hour from their institution.
Also, distance students who had higher personality scores, meaning they were more open
to asking for help/clarification; less unhappy if they retrieved a lot of information, even if
it was unexpected; and would be more likely to consider changing their research based on
what they found, were also more likely to be more confident in their information
searching abilities.

RQ3: How is information engagement related to the value of academic libraries?
The qualitative and quantitative data analysis indicated that participants valued both
electronic and print resources. Information was valuable for this group, and Phase 3
participants were willing to pay an average of $30 and $10 for a book and article that
they needed for their research, respectively. At the same time, 75 (61%) of the Phase 3
participants were not willing to pay anything for an article. The personality and
confidence scales could predict how much a participant would be willing to pay for a

219
book that they needed for their research, with an increase of $0.50 and $0.34 per point for
the personality and confidence scales, respectively. For the distance students, this amount
was $0.95 per point increase on the confidence scale. Participant responses to the
personality variable questions could also predict whether a Phase 3 participant would
start a search on a topic that they were unfamiliar with 73 percent of the time.

RQ4: How can academic libraries promote increased information engagement of doctoral
students?
The qualitative and quantitative data analysis suggests that the participants in the study
did find information important, but found it difficult to apply to outcomes beyond
searching for, finding, and accessing a document. However, once other goals, such as
career goals were suggested, many expressed interest. In fact, the information related
goal that the participants in Phase 3 were most interested was how to manage their
scholarly identity. This topic intersects with other IE tasks, such as finding and
organizing information, which academic librarians can cover during library instruction.

The analysis of qualitative and quantitative data indicates that information plays
an important role in the lives of doctoral students, but it can still be difficult to tease out
what their perceptions or actions will be. The findings of this dissertation show that it is
possible to identify patterns of behavior, especially for certain sub-groups of the
participants. By identifying the information needs, behaviors, and outcomes of this understudied group, librarian practitioners and LIS scholars can increase IE in effective and

220
significant ways that benefit academic libraries, HEIs, and the students, faculty, and staff
that those institutions serve.

221
APPENDICES
Appendix A: Glossary
Academic libraries: "libraries that belong to institutions of higher education including
publicly funded, federal, state, provincial, or national universities or colleges, privately
funded universities or colleges, two-year community or junior colleges which can be
publicly or privately funded, tribal colleges, professional schools, and special focus
institutions that offer a single or small set of programs" (Curzon & Quiñónez-Skinner,
2009, p. 11)
Engagement: "the amount of time and energy devoted to educationally purposeful
activities" (Trustees of Indiana University, 2014)
Information: a "physical surrogate of knowledge" (Farradane, 1979, p. 17)
Information engagement: the individual, situational, and institutional factors that affect
behaviors related to finding, filtering, accessing, and organizing information
Input: "that which is put in or taken in, or which is operated on or utilized by any process
or system (either material or abstract)" (OED, 2015)
Input measures: quantifiable variables used to measure input
Output: "that which is produced in an industry or process" (OED, 2015)
Output measures: quantifiable variables used to measure output
Standard: "a definite level of excellence...or a definite degree of any quality, viewed as a
prescribed object of endeavour or as the measure of what is adequate for some purpose"
(OED, 2015)
Student learning outcomes: results or competencies that students gain from their college
education (Spady, 2002)
Value of academic libraries: a measure of the positive impact that library resources and
services makes on the institution's faculty, staff, and students (Oakleaf, 2010)

224
Appendix D: IRB approval notices

225

226

227
Appendix E: Interview and survey questions
Appendix E1: Focus group interview questions
1. Think about a time that you looked for information related to your coursework,
qualifying exams, proposal, or dissertation. Tell me about the kind of information you
were trying to fine. [Probe if needed on looking for citations, conducting a literature
review, and researching methods]
2. What other goals does your information seeking help you achieve?
3. How do you get started looking for information?
4. What are some of the things that have stopped your searches in the past? What do you
do when you are stuck?
5. The concept of flow often comes up in studies of engagement. Flow is a state in
which one experiences high levels of focus, attention, and enjoyment in an activity. It
is also known as being in the zone. Can you tell me about times that you have felt this
way while searching for information? How did these feelings improve your
searching? What other things would you like to tell me about your experiences
searching for information? What else helps you achieve your goals? (Probe, ask about
library use or librarians, if not mentioned).

228
Appendix E2: Individual interview questions
1. Tell me about how you get started looking for information related to your classes
or research.
2. Remember a time when you looked for information regarding [task suggested by
their stage] and were successful. Describe how you initially regarded the task,
what you did while looking for the information, and why you pursued the
strategies that you did.
a. What did you expect to find when looking for the information based on
what you knew and had done before?
b. What opportunities did you perceive?
c.

What facilitated your efforts to find the information?

d. In what ways did this information-seeking session change what you did on
future searches?
3. Remember a time when you looked for information regarding [task] and were
unsuccessful. Describe how you initially regarded the task, what you did while
looking for the information, and why you pursued the strategies that you did.
a. What did you expect to find when looking for the information based on
what you knew and had done before?
b. What opportunities did you perceive?
c. What hampered your efforts to find the information?
d. In what ways did this information seeking session change what you did on
future searches?
4. What other goals does your information seeking help you achieve?

238
Thank you for your interest in this study! Please contact mikitish@scarletmail.rutgers.edu
with any questions.

239
Appendix F: Preliminary qualitative themes
I. Tasks
A. Stage based
1. Coursework
2. Qualifying/comprehensive
exams
3. Proposal
4. Dissertation
5. Other
B. Conference submission
C. Article
D. Teaching
E. Situate work
II. Resources
A. People
1. Professional academics
a. Advisor
b. Committee member(s)
i. Outside member
c. Librarian
2. Students
a. PhD student
3. Relationship
a. Pre-existing
4. Social network
a. Academia.edu
b. Researchgate.net
c. Twitter
d. Facebook
e. LinkedIn
f. listserv
B. Collections
1. Databases
a. EBSCO
b. Google Scholar
c. Journals
d. Articles+
e. arXiv
f. LingBuzz
g. Academia.edu
h. Researchgate.net
i. NJVid
C. Item type
1. Book
a. Edited volume

b. E-book
2. Article
a. Review article
3. Media
a. DVD
b. Stream
4. Other
D. Websites
1. Google
a. Search
b. Scholar
c. News
d. Video
e. Images
f. Books
g. Alerts
2. Amazon
3. RSS alerts
4. YouTube
a. TED talks
5. Vimeo
6. Government
7. Wikipedia
8. Microsoft research
9. Library
a. Research guides
b. Catalog
10. Academic web pages
11. “Popular” (non-scholarly)
12. Illicit
E. Software
1. Citation managers
a. Refworks
b. Mendeley
2. Mentalmodeler.org
3. Papers
4. Other
F. Library
1. Type
a. Academic
b. Public
2. Affiliation
a. Affiliated
b. Non-affiliated

240
III. Barriers
A. Library policy
1. Embargo
2. Request limit
3. ILL fee
4. Recall
B. Timeliness
1. Item delivery
2. Deadline
C. Specificity
1. Topic
2. Field
D. Book chapters
E. Not finding
F. Cost
G. Illegal to obtain
H. Off-campus
IV. Affect
A. Positive
1. Happy
B. Negative
1. Disappointment
2. Frustration
3. Uncertainty
a. Item related
b. Access related
4. Distraction
5. Worried about losing face
V. Outcomes/goals
A. Networking
B. Future work
1. Grants
2. Publishing
a. Where to publish
b. What to publish
C. Career
1. CV’s
2. Jobs
D. Teaching
VI. Facilitators
A. Multiple institutional logins
B. PDFs
C. Friends at other institutions
D. Google book snippets
E. Organizational strategies
1. Schedule

F.

G.

H.
I.
J.
K.
VII.
A.

VIII.
A.
B.
C.

2. Folder
3. Information
4. List
Searching strategies
1. Known item
2. Citation tracking
3. Exploratory
Flow
1. Writing and searching
combined
2. Only writing
3. No flow
4. Keyword
Contacting author
“Pushes” to related articles
Library instruction
Convenience
Change in behavior
Event
1. Qualifying exams
2. Type of paper
Factors
Tenacity
Hoarding
Multitasking

Appendix G: Final qualitative thematic codebook
Name

Definition

I. Tasks

Participant mentions the following
tasks when describing interactions
with info
Participant describes work in any
of the following stages of doctoral
study
Time before
qualifying/comprehensive exams

A. Stage based

1. Coursework

2. QualComp
exams
3. Proposal

4. Dissertation

5. Other

Time spent researching, writing, or
defending
qualifying/comprehensive exams
Time spent researching, writing, or
defending dissertation
proposal/prospectus
Time spent researching or writing
dissertation

Time before doctoral studies began

Examples

Focus Group
Individual Interviews
Interviews
N of
N of
N of
N of
instances groups instances participants

11

3

20

8

11

3

21

5

P10: Okay so I was looking
for writing a proposal…

2

2

10

4

P4: My experiences were
different when I was in
course work then they are
now post coursework and
dissertation torture.
P5: Yeah. Well, yeah, I
mean, recently I was looking
for um…so I’m doing a
practicum this semester…

5

2

20

7

2

1

13

6

241

P7: Well last semester I had
to do a…a project in one of
my classes…
P4: …but it…Well, so, when
I was doing my comps…

B. Conference

Participant describes researching,
writing, or presenting at a
conference

C. Article

Participant researching, writing, or
presenting non-conference
publication
Participant describes preparing for
class or teaching

D. Teaching

E. Situate work

Participant deals with information
about field of study without a
specific task (e.g., paper) in mind

II. Resources
A. Human
resources
1. Academics

Resources accessed for info
People that participant consults for
finding/accessing info
Scholars and those employed by an
academic institution

a. Advisor

Committee chair (Carpenter et al.,
2012; Libutti & Kopala, 1995)

P2: Yeah, workshops at
conferences, you know,
when they have like pre
conferences.

3

1

0

0

0

0

1

1

P2: Would you consider also
information that we look for
in order to help our students?
P3: ...a lot of it is just me
trying to work my way my
way backwards so that I
know I have like a like a
good understanding or
something so…; P10: ...I try
to look for really old ones to
see where it started…

11

3

8

6

27

3

27

11

P1: ...for one of my friends
or colleagues, so I’ll like
save that PDF and I’ll send it
to them.
P13: I do, too. I go to my
advisor, first, my first
advisor, they always said it
was…

43

3

40

12

12

3

22

7

242

b. Committee
member
c. Author

Any committee member besides
chair
Author of a specific work

d. Librarian

Academic librarian

2. Students
a. PhD student

Non-doctoral students
Doctoral students at their current
and other institutions
Online social networks for
scholars. Participants do not have
to be connected to specific
individuals or have profiles of their
own.

3. Virtual
communities

a. Academia.edu
b.
Researchgate.net
c. Twitter

d. Facebook

P9: Can I add something? I
often use my committee…
P2: …I ended up emailing
the person who wrote the
article…
P10: I went to the reference
librarian…;
P16: ...Sometimes I'll do that
chat, but after 10 they're not
there.
P2: Or a student who is older
than you, like a mentor.
P16: Is that like Orchid?

2

11

6

12

3

11

5

38

3

62

15

0
23

0
3

1
16

1
7

1

1

1

1

7

2

11

6

10

2

5

2

2

1

4

2

7

2

12

5
243

P6: ...the scholars I do follow
on Academia.edu.
P4: So ResearchGate has
actually been really
awesome.
P1: But now I know on, on
Twitter I’m connected
around the world and have
used the hashtag icanhazpdf.
P13: And sometimes you just
write on Facebook with
somebody who you don't

6

even really know as a person
just like you find what they
want and okay.
P4: It’s linked to all your
publications, and you’re
searchable and you can make
like you make, you know,
you have like a network like
LinkedIn or something?
P2: And also I’m on a really
cool listserv…

e. LinkedIn

f. listserv
4. Other

B. Electronic
resources
1. Specific
database

a. Journals

2. Articles
3. E-books
4. Websites

Includes people not covered by
other categories (e.g. mothers)
(Carpenter et al., 2012)
Sources of digital info where
participant finds/accesses info
Participant names specific
databases (e.g., JSTOR) or
database aggregators (e.g.,
Articles+)
Journal collection accessed directly
or through library webpage
(Carpenter et al., 2012)
Journal articles found/accessed
online (Carpenter et al., 2012)

1

2

2

5

2

3

3

2

2

6

4

5

1

0

0

12

3

35

11

P1: Yeah, within the journal,
different keywords…

24

3

27

6

P4: I like almost exclusively
search for scientific articles.
P17: I've gotten kicked off of
an ebook before…
P1: ...I’ll start on a regular
search engine first and then
go from there to try to find

63

3

63

15

5

3

3

3

1

1

0

0

P11: Yes I’d definitely rather
have an electronic form...
P2: ...there’s like the
Articles+ thing…

244

1

a. Google

i. Search
ii. Scholar

iii.Other

Other Google services besides
Search & Scholar

b. Amazon
c. Videos

d. Nonacademic

Online video respositories or
streaming sites (e.g., YouTube,
Vimeo)
Includes popular and news sites
that do not have scholarly articles

e. Wikipedia
f. Scholarly

Online content provided by an
academic institution

i. Library

Includes participant's home
institution's website and catalog

the terms that might be
related to…
P6: …so yeah…there you go.
But most of it’s going on my
own on the computer and
Googling for stuff, so.
P10: Just a regular Google
search.
P14: ...there’s not going to be
anything in Google Scholar
about it.
P14: ...the other one is
Google Image search.
P14: ...I'm so angry I go to
Amazon…
P6: Or vimeo is something
that I just learned about.

2

8

5

9

2

18

11

41

3

48

13

13

3

9

6

5

1

5

3

9

2

3

3

13

3

6

3

3

1

2

2

9

2

3

3

30

3

63

15
245

P7: ...I found myself actually
looking at more popular
websites…
P2: I like Wikipedia as my
starting point.
P1: ...but it's pretty much this
website that has pretty much
a genealogy tree for PhD
academics.
P3: I think the library at
[current institution] also has
the really good, I forget what

9

ii. Not HE
affiliated

g. Illicit

C. Print or
physical
resources

1. Books
a. Monograph

b. Edited
volume

7

1

6

4

22

3

15

4

3

2

2

1

40

3

56

15

1

1

0

0

3

1

0

0

246

they're called research
starters?
Website provided through an
P5: Also there’s some
institution/organization besides a
like…so usually anything
college/university, e.g.
that’s put out by Microsoft
government, Microsoft research
Research I’ve found to be
good for media studies.
Participant acknowledges receiving P5: Well there’s also some, I
information through illegal means
mean, there’s a
couple…illicit sites that I use
if I’m looking for something
that I can’t find.
Print/physical resources for
P18: ...there's a lot of really
finding/accessing info
great documentaries that cost
hundreds and hundreds of
dollars that there's no way
that I could afford. One of
them called the Black
Oppressed, they just have on
VHS, so I asked if they could
purchase the DVD.
P10: Then I go to the section
and I look through books…
P1: …also books,
monographs, readers, and
yeah. I guess that’s it.
P6: Yeah, so well actually,
there’s been really good
books like bin Laden, like the
bin Laden reader.

c. Textbook
D. Software

1. Citation
managers

2. Other

E. Institutional

1. Current
academic
2. Other

III. Barriers

P16: And the same was for
other textbooks…
Includes programs and apps that
help participant organize
information
Software whose primary purpose is P16: So, our advisors tell us,
to create bibliographies (e.g.,
“Oh, you know, you need to
Zotero, Refworks)
get a citation, manager,” so
I'm trying to use Zotero.
Software that does more than or
P4: I just recently started
other things besides create
using Mendeley.
bibliographies (e.g. Mendeley)
Physical or electronic resources
provided by a higher educational
institution
Institution where participant is
P5: ...I was having a difficult
currently enrolled
time on [current institution's]
library website…
Includes all other institutions
P5: ...I actually have access
except for where participant is
to [other institution's] library
currently enrolled
access because I did a
consortium class there last
semester and they haven’t
revoked my…
Prevent participants from
P18: I’m usually not coming
finding/accessing info
to campus for a couple of
days anyway...I only come to
campus for my books, but…

5

2

0

0

11

3

12

6

24

3

28

11

61

3

75

15

23

3

21

8

1

1

0

0

247

A. Library
policy

Rules or practices created,
followed, and enforced by an
academic library

1. Embargo

Library does not have access to
recent content within a certain
timeframe

2. Request limit

Library can only request a certain
number of items through interlibrary loan for free or at all

3. ILL fee

Library charges participant to
cover the shipping or licensing of
interlibrary loan materials

4. Recall

Library requires participant to
return item earlier because another
user has requested it
Time related barriers

B. Timeliness

1. Item delivery

Participant receives item after it is
needed

13

2

12

6

10

2

0

0

3

2

1

1

6

2

0

0

8

1

2

1

P17: Like because of time,
I’ve decide this is not
important anymore.
P20: ...I don't really I don't
follow up because I know
that by the time I get it, I’m
not going to want to read it
or anything like that.

2

2

3

1

1

1

3

2

248

P14: ...with key books I
know I’m going to need to
take notes, and you're not
supposed to do that in library
books.
P16: …there’s something
called a moratorium, like
when articles just come out
and the library doesn't have
access to them.
P2: ...I tried to request it via
the interlibrary loan and then
I got an email saying that,
“Oh we already exceeded our
limit for requesting articles…
P16: It's not that it's just
twenty bucks, it's the
principle. Why should you
have to pay for it?
P14: Oh no, I was very upset
about the recall process.

2. Deadline

C. Specificity

1. Topic

2. Field

D. Book
chapters

E. Not finding

Participant realizes they will
receive item after it is needed, so
does not request it
Participant acknowledges that the
info is esoteric, rare, old, or
otherwise difficult to seek/access
Participant is frustrated by specific
research topic

P18: Or I have a deadline
tomorrow, whatever that is.

2

1

1

12

2

21

11

8

1

19

8

12

2

4

2

26

3

18

7

249

P1: …for certain topics that
again, like if they’re not in
the research literature then
I’ll start on a regular search
engine first and then go from
there to try to find the terms
that might be related to…
Participant is frustrated by research P1: Especially because it’s
field in general
psychology, a lot of things
that are like, not
trademarked.
P18: Book chapters, I would
definitely go to Google
Scholar because you can
definitely if it's in an
important article that's in a
book I'm surprised that I can
find pretty much free PDFs
on the web…
Participant is frustrated by not
P3: Yeah, it's a combination
finding information that they
of either not knowing or just
believe should be easy to find
not finding.; P13: I have a
problem with not finding out
the journal that I want to

4

search in, not the journal
article but the journal itself…

F. Cost
G. Distance

Info is expensive to find/access
Participant feels that they are far
from campus

11
4

3
2

8
7

5
4

H. Not knowing

Participant acknowledges that they
do not know something

P14: They’re very expensive.
P19: I'm always off campus,
so I have to login through the
firewall, and that process is
too much for me.
P13: ...there are physical
books that I want to read that
are newly published but it's
not at the library, and is there
like, is there a is there a list?

38

2

36

11

IV.
OutcomesGoals

Participant identified outcomes and
goals related to, but beyond
finding/accessing info
Participants have connected or plan P9: Yeah, like any time I go
to connect with other scholars
to a conference we exchange
ResearchGate information
with people that I'm
interested in getting to
know…
Information found did not directly P20: It's made me think what
link with what was needed in
other studies and what other
described incident, but is saved for potential research I could
later use
do...also it gives me pilots or
more goals in the future,
future different goals.

0

0

6

5

19

2

8

4

5

3

2

2

A. Networking

B. Future work

250

1. Grants

2. Publishing
a. Where

b. What

C. Career
1. CV's

Participants plan to use
information to apply for future
grants
Participants use info to decide
where to publish (e.g., identifying
journals with high impact factors)

Participants use info to decide
what to publish (e.g., seeing what
has been published in a specific
journal)
Info will aid participants in their
career related aspirations
Participants plan to use or have
used information to design their
curriculum vitae

2. Jobs

Participants plan to use or have
used information to find or apply
to positions

V. Facilitators

Assist participants in
finding/accessing info
Participant can personally access
resources at other academic
institutions

A. Multiple
institutional
logins

P13: I also look up which
grants they get if I really like
those scholars.
P18: And that's really good
too because you can do a
keyword search in the
journal, and that can be really
useful especially if that's
where you want to publish it.
P15: It wasn't so much the
where to publish, like that's
part of it, but it's more like
what's getting out…
P16: So you can follow them,
and try to be like them.
P11: So like actually looking
at people’s cv’s I really like
to model my cv off of their
cv, right?
P4: ...I’ve done a lot of
information searching to
tailor my cover letters to
specific employers…

P5: Privileges. And I also
have access to [another
institution's] because [J] at

2

2

2

2

0
8

0
3

1
1

1
1

6

3

5

4

2

1

1

1

2

1

0

0

2

1

4

3

1

1

6

2

251

some point logged into her
account on my computer…
B.
Organizational
strategies
1. Schedule

2. Folder

3. Information

4. List

Methods by which participants
saved information

3

1

4

3

2

1

2

1

4

2

23

11

13

2

7

4

10

2

1

1
252

P3: It's so helpful to have an
organization system for all
the things that you have.
Participant created a plan with
P20: ...I'll put together an
dates/times to read/work with info action plan. Like I’ll read this
book and this article on this
in this day, so that’s kind of
what my flow’s in, with a
plan of what I'm going to do
next or how I'm going to do
with all these materials that
I’ve found.
Participant organized documents in P20: It's one list per folder.
folders
Like for different topics I just
have a folder, for if I'm
looking for this particular
search on this particular day,
I’ll have a folder for that one
day even.
Participant organized information
P13: I do the mental
other than documents
modeling with big
cardboards in different colors
and Post-its. I do that all the
time, otherwise I don’t, and
it’s half of my room.
Participant created a list of items to P11: I've definitely gone to
find
the library, gotten a book I

realized that I need to add it
to my list too…
C. Searching
strategies
1. Known item

Methods described by participants
for finding info
Participant looked for specific item

2. Pearl growing

participants used list of references
in documents to find other related
documents (Markey & Atherton,
1978)

3. Exploratory

Participant researched specific
topic

4. Keyword

Participant looked for information
using specific terms
A state in which one experiences
high levels of focus, attention, and
enjoyment in an activity
(Csikszentmihalyi, 1990)

D. Flow

P4: So yeah, I would do, I
would say mostly in my daily
life I do that sort of like
forward searching, like this is
what I want, how can I get
it…
P9: ...I was sort of like
searching quick search for
references to find, you know,
authors that I'd heard of or
topics that I found
interesting, and I kind of
used that to jump from article
to article.
P18: Yeah no pressure, I
have time, I'm just exploring
that's important to me like
being...
P1: Yeah, within the journal,
different keywords…
P7: It's like it was there and I
was in the moment. Yeah.

0

0

5

2

7

3

14

6

14

3

25

11

6

2

22

10

14

3

38

12

21

3

24

11

253

1. Writing and
searching
combined

Flow was specifically a
combination of writing and
searching

2. Only writing

Flow specifically applied to
writing only
Participants has not encountered
flow related to info

3. No flow

E. Pushes to
related articles

Database suggests related articles

F. Library
instruction

Academic librarian provides
formal (e.g., class based) or
informal (e.g., one on one)
instruction in finding/accessing
info

G. Convenience

Finding and/or accessing info was
fast and easy (Connaway, Dickey
& Radford, 2011)

P2: I guess well, how do you
like define the difference
between searching and then
actually starting to consume
the information, because I
generally don't associate
being in the zone with just
searching for stuff.

3

7

3

0

0

7

3

7

2

1

1

8

1

0

0

3

2

21

11

19

3

14

6

254

P5: I'm not sure I've ever
been in the zone, I'm not sure
it's ever happened.
P4: It's true, I’ve definitely
had the articles that are
similar to the one you're
searching for.
P17: Yeah we definitely have
like an information session
my first year with the liaison,
but I just didn't know just
enough about like sociology
or like graduate school
research to know why that
was meaningful.
P15: Yeah I do it
electronically cuz I'm really
lazy so I’ll sit in front of my
computer, and I'll go to
special issues of journals in
my field.

16

VI. Change in
behavior

Anything that necessitates a
temporary or permanent change in
info related behavior

VII. Factors

Factors that might impact
information engagement
The degree to which participant
will pursue a known item

A. Tenacity

B. Hoarding

Degree to which student will
collect items

C. Prior
knowledge

Participant often uses
resource/service, knows of it, or
has experience using it
(Khosrowjerdi & Iranshahi, 2011)

P4: I would say that when I
did my comps my searching
strategy was vastly different
from what I typically, not
vastly different…

P6: Well nothing stops me
per se, I mean, if it’s out
there I'll find it.
P17: ...But I pick up an awful
lot of books from [home
institution] library that I don't
end up opening, and I don't
know, I guess it's good that I
at least have them…
P22: I’ll go into specific
databases um EBSCOhost,
Comm Mass Media
Complete, you know, MUSE,
if I’m looking for something
rhetorical uh sometimes I’ll
go to specific journals, but,
you know, I'm a 4th year
doctoral student and I didn’t
always do that and I learned
the hard way that it’s, it’s
really best to go to specific
journals

13

3

24

12

0

0

0

2

16

3

24

11

6

2

8

4

9

3

55

15

255

D. Certainty

Degree to which student will rely
on keywords as their only
searching strategy.

E. Satisficing

Degree to which student will
accept alternatives to a known
resource if they cannot find/access
it (Simon, 1955)

IX. Tasks
Participant responses to, “If you
facilitated by had a magic wand that could help
magic wand you in future searches, what would
you have it do?”
A. Filtering
Participant wanted magic wand to
identify for relevant results if too
many were returned

B. Seeking

Participant wanted magic wand to
find relevant information

C. Organizing

Participant wanted magic wand to
store information or create a
system for doing so

P15: To see who's writing
right now, and what they
seem to be taking swings at,
and from there if I'm feeling
lost and I need to find a path
I go there, which seems like
it's kind of similar to you.
P9: Yeah, I’ll substitute.

P21: A magic wand would be
able to know exactly...what
I'm looking for...[and] would
pull up those exact articles so
kind of like better filtering
mechanisms.
P26: Tell me what the correct
term is to translate it into
academ-ese.
P24: You could...write up a
PDF, but not have to print it
out...or be able to search your
own notes.

12

1

13

8

8

3

8

5

5

4

12

7

10

4

256

D. Other

Participant wanted magic wand to
do something besides filter, seek ,
or organize information

P28: Man, I just want my
things…like I just want them
immediately. I guess it would
be a library that has
everything…So if a thing just
got published it should come
out and it should be available
to me…[Also,] I can have
them for as long as I need
them.”

32

13

257

258
REFERENCES
Aabø, S. (2009). Libraries and return on investment (ROI): A meta-analysis. New Library
World, 110(7/8), 311-324.
Anthony, K. (2010). Reconnecting the disconnects: Library outreach to faculty as
addressed in the literature. College & Undergraduate Libraries, 17(1), 79-92.
Arnold-Garza, S. (2014). The flipped classroom: Assessing an innovative teaching model
for effective and engaging library instruction. College & Research Libraries
News, 75(1), 10-13.
Artino Jr, A. R., & Stephens, J. M. (2009). Academic motivation and self-regulation: A
comparative analysis of undergraduate and graduate students learning online. The
Internet and Higher Education, 12(3), 146-151.
Arum, R., & Roksa, J. (2011). Academically adrift: Limited learning on college
campuses. Chicago, IL: University of Chicago Press.
Association of College and Research Libraries. (2010). Information Literacy Competency
Standards for Higher Education. Chicago: Association of College and Research
Libraries.
Association of College and Research Libraries. (2011). Standards for Libraries in Higher
Education. Chicago: Association of College & Research Libraries.
Astin, A. W. (1993). What matters in college? Four critical years revisited. San
Francisco, CA: Jossey-Bass.
Astin, A. W. (2004). To use graduation rates to measure excellence, you have to do your
homework. Chronicle of Higher Education, 51(9), B20.
Bandura, A. (2001). Social cognitive theory: An agentic perspective. Annual Review of
Psychology, 52(1), 1-26.
Banta, T. W., Busby, A. K., Kahn, S., Black, K. E., & Johnson, J. N. (2007). Responding
to a fiscal crisis: A dataǦdriven approach. Assessment & Evaluation in Higher
Education, 32(2), 183-194.
Barrett, A. (2005). The information-seeking habits of graduate student researchers in the
humanities. The Journal of Academic Librarianship, 31(4), 324-331.
Barry, C. A. (1997). Information skills for an electronic world: Training doctoral research
students. Journal of Information Science, 23(3), 225-238.
Bates, M.J. (2005a). An introduction to metatheories, theories, and models. In Fisher,

259
K.E., Erdelez, S., & McKechnie, L.E.F. (Eds.), Theories of information behavior
(1-24). Medford, NJ: Information Today, Inc.
Bates, M. (2005b). Information and knowledge: An evolutionary framework for
information science. Information Research, 10(4), paper 239. Retrieved from
http://InformationR.net/ir/10-4/paper239.html
Bates, M.J. (2009). Information behavior. In M.J. Bates & M.N. Maack (Eds.),
Encyclopedia of Library and Information Sciences (3rd ed.) (2381-2391).
http://dx.doi.org/10.1081/E-ELIS3-120043263
Behr, M. D., & Hayward, J. L. (2008). Do off-campus students still use document
delivery? Current trends. Journal of Library Administration, 48(3-4), 277-293.
Bickley, R., & Corrall, S. (2011). Student perceptions of staff in the Information
Commons: A survey at the University of Sheffield. Reference Services Review,
39(2), 223-243.
Bielavitz, T. (2010). The Balanced Scorecard: A systemic model for evaluation and
assessment of learning outcomes? Evidence Based Library and Information
Practice, 5(2), 35-46.
Blummer, B., Lohnes Watulak, S., & Kenton, J. (2012). The research experience for
education graduate students: A phenomenographic study. Internet Reference
Services Quarterly, 17(3–4), 117–146.
Bowen, S. (2005). Engaged learning: Are we all on the same page. Peer Review, 7(2), 47.
Bøyum, I., & Aabø, S. (2015). The information practices of business PhD students. New
Library World, 116(3/4), 187-200.
Brahme, M., & Walters, L. (2010). While technology poses as the great equalizer,
distance still rules the experience. Journal of Library Administration, 50(5-6),
484-514.
Brophy, P. (2006). Measuring library performance: Principles and techniques. London:
Facet.
Bruce, C., Partridge, H., Davis, K., Hughes, H., & Stoodley, I. (2014). Information
experience: approaches to theory and practice. Bingley, UK: Emerald Group
Publishing.
Budd, J. M., Hill, H., & Shannon, B. (2010). Inquiring into the real: A realist
phenomenological approach. The Library Quarterly, 80(3), 267-284.

260
Calhoun, K., Cantrell, J., Gallagher, P., & Hawk, J. (2009). Online catalogs: What users
and librarians want. Retrieved from
https://www.oclc.org/content/dam/oclc/reports/onlinecatalogs/fullreport.pdf
Cambridge, D. (2006). Personally engaged information literacy in general education
through information ecology and fieldwork. In Gibson, C. (Ed.), Student
engagement and information literacy (143-168). Chicago, IL: Association of
College and Research Libraries.
Campos, P.F. (2015, April 5). The real reason college costs so much. The New York
Times, p. SR4
Carifio, J., & Perla, R. J. (2007). Ten common misunderstandings, misconceptions,
persistent myths and urban legends about Likert scales and Likert response
formats and their antidotes. Journal of Social Sciences, 3(3), 106-116.
Carpenter, J., Wetheridge, L., & Tanner, S. (2012). Researchers of tomorrow: The
research behaviour of Generation Y doctoral students. Retrieved from
http://www.jisc.ac.uk/media/documents/publications/reports/2012/Researchersof-Tomorrow.pdf
Carpenter, R. L. (1981a). College libraries: A comparative analysis in terms of the ACRL
standards. College and Research Libraries, 42(1), 7-18.
Carpenter, R. L. (1981b). Two-year college libraries: A comparative analysis in terms of
ACRL standards. College and Research Libraries, 42(5), 407-15.
Charmaz, K. (2014). Constructing grounded theory. London, UK: Sage.
Catalano, A. (2013). Patterns of graduate students' information seeking behavior: A metasynthesis of the literature. Journal of Documentation, 69(2), 243-274.
Centre for Information Behaviour and the Evaluation of Research. (2008). Information
behaviour of the researcher of the future: A CIBER briefing paper. London, UK:
CIBER. Retrieved from
http://www.jisc.ac.uk/media/documents/programmemes/reppres/gg_final_keynote
_11012008.pdf
Chandra, S., & Chatterjee, P. (2011). Digital indiscretions: New horizons in medical
ethics. The Australasian Medical Journal, 4(8), 453.
Chu, S. K. W., & Law, N. (2007). Development of information search expertise:
Research students' knowledge of source types. Journal of Librarianship and
Information Science, 39(1), 27-40.
Clapp, V. W., & Jordan, R. T. (1965). Quantitative criteria for adequacy of academic

261
library collections. College and Research Libraries, 26(5), 371-380.
Cohen, J. (1988). Statistical power analysis for the behavioral sciences (2nd ed.).
Hillsdale, NJ: Lawrence Erlbaum Associates.
Cohen, M., Poggiali, J., Lehner-Quam, A., Wright, R., & West, R. (2016). Flipping the
classroom in business and education oneshot sessions: A research study. Journal
of Information Literacy, 10(2), 40-63.
Coleman, P., & Jarred, A. D. (1994). Regional association criteria and the Standards for
College Libraries: The informal role of quantitative input measures for libraries in
accreditation. The Journal of Academic Librarianship, 20(5), 273-284.
Connaway, L.S., and Dickey, T.J. (2010). The digital information seeker: Report of
findings from selected OCLC, RIN, and JISC user behavior projects. Retrieved
from http://www.jisc.ac.uk/media/documents/publications/reports/2010/digitalinf
ormationseekerreport.pdf
Connaway, L. S., Dickey, T. J., & Radford, M. L. (2011). “If it is too inconvenient I'm
not going after it:” Convenience as a critical factor in information-seeking
behaviors. Library & Information Science Research, 33(3), 179-190.
Connaway, L.S., Lanclos, D., & Hood, E. (2013). "I find Google a lot easier than
going to the library website." Imagine ways to innovate and inspire students to
use the academic library. ACRL 2013: Imagine, Innovate, Inspire, 10-13 April
2013, Indianapolis, Indiana (USA). Retrieved from
http://www.ala.org/acrl/sites/ala.org.acrl/files/content/conferences/confsandpreco
nfs/2013/papers/Connaway_Google.pdf
Connaway, L.S., and Powell, R.R. (2010). Basic research methods for librarians. Santa
Barbara, CA: Libraries Unlimited.
Connaway, L. S., & Radford, M. L. (2010). Virtual reference service quality: Critical
components for adults and the net-generation. Libri, 60(2), 165-180.
Connaway, L.S., & Radford, M.L. (2017). Research methods in Library and Information
Science (6th ed.). Santa Barbara, CA: Libraries Unlimited.
Contreras, A. (2007). How not to fix accreditation. Chronicle of Higher Education,
53(49).
Cook, C., & Heath, F. M. (2001). Users' perceptions of library service quality: A
LibQUAL+ qualitative study. Library Trends, 49(4), 548-584.
Corbin, J., & Strauss, A. (2015). Basics of qualitative research: Techniques and
procedures for developing grounded theory. Thousand Oaks, CA: Sage.

262

Corno, L., & Mandinach, E. B. (1983). The role of cognitive engagement in classroom
learning and motivation. Educational Psychologist, 18(2), 88-108.
Cox, A. M. (2012). An exploration of the practice approach and its place in information
science. Journal of Information Science, 38(2), 176-188.
Crane, D. (1972). Invisible colleges: Diffusion of knowledge in scientific communities.
Chicago, IL: University of Chicago Press.
Crawford, G. A., & White, G. W. (1999). Liberal arts colleges and standards for college
libraries: A quantitative analysis. The Journal of Academic Librarianship, 25(6),
439-444.
Creswell, J.W. & Clark, V.L.P. (2011). Designing and conducting mixed methods
research (2nd ed.). Thousand Oaks, CA: SAGE Publications, Inc.
Csikszentmihalyi, M. (1990). Flow: The psychology of optimal performance. NY:
Cambridge University Press.
Curzon , S. C., & Quiñónez-Skinner, J. (2009). Academic libraries. In M.J. Bates & M.N.
Maack (Eds.), Encyclopedia of Library and Information Sciences (3rd ed.) (1122). http://dx.doi.org/10.1081/E-ELIS3-120043263
Das, A. (2013). Information-seeking among pregnant women: A mixed method
approach (Doctoral dissertation). Retrieved from ProQuest Dissertations &
Theses Global. (Order No. 3596481).
Davenport, T. H., & Prusak, L. (1997). Information ecology: Mastering the information
and knowledge environment. Oxford, UK: Oxford University Press.
Dickenson, D. (2006). How students and faculty use academic libraries differently. Fast
facts—Recent statistics from the Library Research Service, No. 242. Retrieved
from
http://www2.cde.state.co.us/artemis/edserials/ed311010internet/ed311010242inter
net.pdf
DiRamio, D., & Jarvis, K. (2011). Special issue: Veterans in Higher Education--When
Johnny and Jane come marching to campus. ASHE Higher Education Report,
37(3), 1-144.
Du, J. T., & Evans, N. (2011). Academic library services support for research information
seeking. Australian Academic & Research Libraries, 42(2), 103-120.
Dunn, K. (2010). Toward an understanding of the epistemic values of biological
scientists as expressed in scholarly publication. (Doctoral dissertation). Retrieved

263
from Dissertations & Theses @ Rutgers University. (Order No. 3418758).
Earp, V.J. (2008). Information source preferences of education graduate students.
Behavioral & Social Sciences Librarian, 27(2), 73-91.
Edmunds, A. O. (2010). An examination of the likelihood of persistence of students with
discrepant high school grades and standardized test scores. (Doctoral
dissertation). Retrieved from ProQuest Dissertations & Theses Global. (Order No.
3423016).
Educause. (2012). 7 things you should know about flipped classrooms. Retrieved from
https://library.educause.edu/resources/2012/2/7-things-you-should-know-aboutflipped-classrooms
Ellis, D. (1993). Modeling the information-seeking patterns of academic researchers: A
grounded theory approach. The Library Quarterly, 63(4), 469-486.
Evaluate, v [Def. b]. (2017). OED Online. Oxford University Press. Retrieved from
http://www.oed.com/view/Entry/65181?redirectedFrom=evaluate#eid.
Farradane, J. (1979). The nature of information. Journal of Information Science, 1(1), 1317.
Fisher, S., & Oulton, T. (1999). The Critical Incident Technique in Library and
Information management research. Education for Information, 17(2), 113-25.
Flanagan, J. (1954). The critical incident technique. Psychological Bulletin, 51( 4), 327.
Fleming-May, R., & Yuro, L. (2009). From student to scholar: the academic library and
social sciences Ph.D. students' transformation. portal: Libraries and the Academy,
9(2), 199-221.
Folster, M. B. (1995). Information seeking patterns: Social sciences. The Reference
Librarian, 23(49-50), 83-93.
Ford, N. (2015). Introduction to information behaviour. London, UK: Facet Publishing.
Forman, S. W. (2009). Characteristics of Successful Community College Students.
(Doctoral dissertation). Retrieved from ProQuest Dissertations & Theses
Global. (Order No. 3396401).
Frade, P. A., & Washburn, A. (2006). The university library: The center of a university
education?. portal: Libraries and the Academy, 6(3), 327-346.
Fredricks, J. A., Blumenfeld, P. C., & Paris, A. H. (2004). School engagement: Potential
of the concept, state of the evidence. Review of Educational Research, 74(1), 59-

264
109.
Fulton, C., & Henefer, J. (2010). Information practice. In M.J. Bates & M.N.
Maack (Eds.), Encyclopedia of Library and Information Sciences (3rd ed.) (25192525). http://dx.doi.org/10.1081/E-ELIS3-120043263
Furrer, C., & Skinner, E. (2003). Sense of relatedness as a factor in children’s academic
engagement and performance. Journal of Educational Psychology, 95 (1), 148–
162.
Gardner, C. C., & Gardner, G. J. (2016). Fast and furious (at publishers): The motivations
behind crowdsourced research sharing. College & Research Libraries, crl16-840.
Gardner, S. K. (2009). Student and faculty attributions of attrition in high and lowcompleting doctoral programs in the United States. Higher Education, 58(1), 97112.
Garson, G.D. (2014). Logistic regression: Binary & multinomial. Asheboro, NC:
Statistical Associates Publishers.
Gatten, J. N. (2004). Measuring consortium impact on user perceptions: OhioLINK and
LibQUAL+™. The Journal of Academic Librarianship, 30(3), 222-228.
George, C., Bright, A., Hurlbert, T., Linke, E. C., St. Clair, G. & Stein, J. (2006).
Scholarly use of information: Graduate students' information seeking behaviour.
Information Research, 11(4), paper 272. Retrieved from
http://informationr.net/ir/11-4/paper272.html
Gessner, G. C., Jaggars, D. E., Rutner, J., & Tancheva, K. (2011). Supporting humanities
doctoral student success: A collaborative project between Cornell University
Library and Columbia University Libraries. Retrieved from
https://clir.org/pubs/ruminations/02cornellcolumbia/report.pdf
Gilstrap, D. L., & Dupree, J. (2008). Assessing learning, critical reflection, and quality
educational outcomes: The critical incident questionnaire. College & Research
Libraries, 69(5), 407-426.
Gratch-Lindauer, B. (2002). Comparing the regional accreditation standards: Outcomes
assessment and other trends. The Journal of Academic Librarianship, 28(1), 1425.
Green, R. (2010). Information illiteracy: Examining our assumptions. The Journal of
Academic Librarianship, 36(4), 313-319.
Green, R., & Macauley, P. (2007). Doctoral students' engagement with information: An
American-Australian perspective. portal: Libraries and the Academy, 7(3), 317-

265
332.
Greenhill, K., & Wiebrands, C. (2012). No library required: The free and easy
backwaters of online content sharing. In Proceedings from VALA2012:
emPowering eFutures, 6-9 February 2012, Melbourne, Australia. Retrieved from
http://vala.org.au/direct-download/vala2012-proceedings/456-vala2012-session11-greenhill-paper/file
Greifeneder, E. (2014, December). Trends in information behaviour research. In
Proceedings of ISIC: The information behaviour conference (No. Part 1), 2-5
September, 2014, Leeds, UK. Retrieved from
http://www.econ.ku.dk/ansatte/emeriti/?pure=files%2F137513587%2FTrends_in_
information_behaviour_research.htm
Green, R., & Macauley, P. (2007). Doctoral students' engagement with information: an
American-Australian perspective. portal: Libraries and the Academy, 7(3), 317332.
Haddow, G. (2013). Academic library use and student retention: A quantitative analysis.
Library & Information Science Research, 35(2), 127-136.
Hamlin, A. T. (1981). The university library in the United States, its origins and
development (p. 11). Philadelphia: University of Pennsylvania Press.
Harris, C. S. (2011). The case for partnering doctoral students with librarians: a synthesis
of the literatures. Library Review, 60(7), 599-620.
Heinstrom, J. (2014). The emotional valence of information experience: Relation to
personality and approach to studying. In Bruce, C., Partridge, H., Davis, K.,
Hughes, H., & Stoodley, I. (Eds.), Information experience (275-294). Bingley,
UK: Emerald Publishing Limited.
Heller, D.E. (2003). Federal funds for higher education. In J.W. Guthrie (Ed.),
Encyclopedia of Education (2nd ed.) (861-864). New York, NY: Macmillan
Reference.
Horlings, E., & Gurney, T. (2013). Search strategies along the academic lifecycle.
Scientometrics, 94(3), 1137–1160.
Hoy, M. B. (2017). Sci-Hub: What librarians should know and do about article
piracy. Medical Reference Services Quarterly, 36(1), 73-78.
Iannuzzi, P. A., & Brown, J. M. (2010). ACRL's standards for libraries in higher
education: Academic library directors weigh in. College & Research Library
News, 17(9), 2.

266
IBM. (2017). K-Means cluster analysis. Retrieved from
https://www.ibm.com/support/knowledgecenter/SSLVMB_20.0.0/com.ibm.spss.s
tatistics.help/idh_quic.htm
Indiana University Center for Postsecondary Research. (2017). Carnegie Classification of
institutions of higher education. Retrieved from
http://carnegieclassifications.iu.edu/
Inger, S., & Gardner, T. (2013). Library technology in content discovery–Evidence from
a large-scale reader survey. Insights, 26(2), 120-127.
Input, n [Def. 2]. (2015). In OED Online. Oxford University Press. Retrieved from
http://www.oed.com/view/Entry/96482
Invisible, adj. and n, [Def. A.1.d.]. (2017). OED Online. Oxford University Press.
Retrieved from
http://www.oed.com/view/Entry/99129?redirectedFrom=invisible+college#eid93
52
Jamali, H. R., & Asadi, S. (2010). Google and the scholar: the role of Google in
scientists' information-seeking behaviour. Online information review, 34(2), 282294.
Johnson, P. C., & Simonsen, J. E. (2015). Do engineering master’s students know what
they don’t know? Exploring abstracting & indexing service use and non-use.
Library Review, 64(1/2), 36-57.
Julien, H., Pecoskie, J. J., & Reed, K. (2011). Trends in information behavior research,
1999–2008: A content analysis. Library & Information Science Research, 33(1),
19-24.
Kaplan, R. S., & Norton, D. P. (1992). The balanced scorecard: measures that drive
performance. Harvard Business Review, January/February, 71-80.
Kelly, L. J. (1996). Implementing Astin's IEO Model in the study of student retention: A
multivariate time dependent approach. Annual Forum of the Association for
Institutional Research, 5-8 May, 1996, Albuquerque, NM, Retrieved from
http://files.eric.ed.gov/fulltext/ED397732.pdf
Kerr, P. A. (2010). Conceptions and practice of information literacy in academic
libraries: Espoused theories and theories-in-use. (Doctoral dissertation).
Retrieved from Dissertations & Theses @ Rutgers University. (Order No.
3418770).
Kerins, G., Madden, R., & Fulton, C. (2004). Information seeking and students studying

267
for professional careers: The cases of engineering and law students in Ireland.
Information Research, 1(1), paper 272. Retrieved from
http://InformationR.net/ir/10-1/paper208.html
Khosrowjerdi, M., & Iranshahi, M. (2011). Prior knowledge and information-seeking
behavior of PhD and MA students. Library & Information Science
Research, 33(4), 331-335.
Kim, S. J., & Jeong, D. Y. (2006). Analyzing the status of theoretical framework by
subfields in Library and Information Science research articles. Journal of the
Korean Society for Information Management, 23(2), 21-37.
King, D. W., & Tenopir, C. (2013). Linking information seeking patterns with purpose,
use, value, and return on investment of academic library journals. Evidence Based
Library and Information Practice, 8(2), 153-162.
Kingma, B., & McClure, K. (2015). Lib-value: Values, outcomes, and return on
investment of academic libraries, phase III: ROI of the Syracuse University
Library. College & Research Libraries, 76(1), 63-80.
Kjelgaard, M. M., & Guarino, A. J. (2012). Assessing clinical and academic performance
in a Master’s level speech language pathology program: A path analysis. Creative
Education, 3(1), 145-148.
Ko, Y. M., Shim, W., & Pyo, S. H. (2016). Factors affecting users’ assessment of the
economic value of university library services. Journal of Librarianship and
Information Science, 48(3), 223-235.
Korobili, S., Malliari, A., & Zapounidou, S. (2011). Factors that influence informationseeking behavior: The case of Greek graduate students. The Journal of Academic
Librarianship, 37(2), 155-165.
Kuh, G. D., & Gonyea, R. M. (2003). The role of the academic library in promoting
student engagement in learning. College & Research Libraries, 64(4), 256-282.
Kyrillidou, M (2010). The ARL library scorecard pilot: Using the Balanced Scorecard in
research libraries. Research Library Issues: A Bimonthly Report from ARL, CNI,
and SPARC, 33-35.
Larson, K. A. (2011). Negotiating romantic and sexual relationships: Patterns and
meanings of mediated interaction (Doctoral dissertation). Retrieved from
ProQuest Dissertations & Theses Global. (Order No. 3460489).
Leech, N.L., Barrett, K.C., & Morgan, G.A. (2011). IBM SPSS for intermediate statistics:
Use and interpretation (4th ed.). New York, NY: Routledge.

268
Leung, S. O. (2011). A comparison of psychometric properties and normality in 4-, 5-, 6-,
and 11-point Likert scales. Journal of Social Service Research, 37(4), 412-421.
Lewis, V., Hiller, S., Mengel, E., & Tolson, D. (2013). Building scorecards in academic
research libraries: Performance measurement and organizational issues. Evidence
Based Library and Information Practice, 8(2), 183-199.
Libutti, P., & Kopala, M. (1995). The doctoral student, the dissertation, and the library: A
review of the literature. The Reference Librarian, 22(48), 5-25.
Light, R. J. (2001). Making the most of college: Students speak their minds. Cambridge,
MA: Harvard University Press.
Lin, K. T. (2009). Maintaining a news perspective remotely through online information
retrieval: Task-based web experiences of foreign news correspondents (Doctoral
dissertation). Retrieved from ProQuest Dissertations & Theses Global. (Order No.
3366490).
Lindauer, B. G. (1998). Defining and measuring the library’s impact on campuswide
outcomes. College & Research Libraries, 59(6), 546-570.
Line, M. B. (2000). Social Science information--The poor relation. IFLA Journal, 26(3),
177-179.
Maddison, T. (2015). A matter of size: Flipping library instruction in various engineering
classrooms. Issues in Science & Technology Librarianship. Retrieved from
http://istl.org/15-fall/refereed2.html.
Maddison, T., Beneteau, D., & Sokoloski, B. (2014). Breaking ground: Improving
undergraduate engineering projects through flipped teaching of literature search
techniques. Issues In Science & Technology Librarianship. Retrieved from
http://www.istl.org/14-fall/refereed2.html.
Mark, A. E., & Boruff-Jones, P. D. (2003). Information literacy and student engagement:
What the national survey of student engagement reveals about your campus.
College & Research Libraries, 64(6), 480-493.
Markey, K. & Atherton, P. (1978). ONTAP: Online training and practice manual for
ERIC data base searchers. Retrieved from
http://files.eric.ed.gov/fulltext/ED160109.pdf
Matell, M. S., & Jacoby, J. (1971). Is there an optimal number of alternatives for Likert
scale items? Study I: Reliability and validity. Educational and Psychological
Measurement, 31(3), 657-674.
Mathews, B.S. (2008). Preemptive reference: Coming out from behind the desk. In S.K.

269
Steiner & M.L. Madden (Eds.), The desk and beyond: Next generation reference
services (91-98). Chicago, IL: ACRL.
Matteson, M. L. (2013). The whole student: Cognition, emotion, and information
literacy. College & Research Libraries, crl13-533.
Matthews, J. (2006). The Library Balanced Scorecard: Is It in Your Future? Public
Libraries-Chicago-Public Library Association, 45(6), 64.
Matthews, J. R. (2007). The evaluation and measurement of library services (132-133).
Westport, CT: Libraries Unlimited.
Matthews, J. R. (2012). Assessing library contributions to university outcomes: The need
for individual student level data. Library Management, 33(6/7), 389-402.
Maughan, P. D. (1999). Library resources and services: A cross-disciplinary survey of
faculty and graduate student use and satisfaction. The Journal of Academic
Librarianship, 25(5), 354-366.
Melo, L. B., & Pires, C. P. (2011). Measuring the economic value of the electronic
scientific information services in Portuguese academic libraries. Journal of
Librarianship and Information Science, 43(3), 146-156.
Mengel, E., & Lewis, V. (2012). Collaborative assessment: North American academic
libraries' experiences using the Balanced Scorecard to measure performance and
show value. Library Management, 33(6/7), 357-364.
Michie, S., van Stralen, M. M., & West, R. (2011). The behaviour change wheel: a new
method for characterising and designing behaviour change interventions.
Implementation Science, 6(1), 42.
Middle States Commission on Higher Education. (2015). Standards for accreditation and
requirements of affiliation: Thirteenth edition. Retrieved from
http://www.msche.org/publications/RevisedStandardsFINAL.pdf
Mikitish, S. E., & Radford, M. L. Initial impressions: Investigating how future faculty
value academic libraries. ACRL 2013: Imagine, Innovate, Inspire, 10-13 April
2013, Indianapolis, Indiana (USA). Retrieved from
http://www.ala.org/acrl/sites/ala.org.acrl/files/content/conferences/confsandpreco
nfs/2013/papers/MikitishRadford_Initial.pdf
Miles, M.B., Huberman, A.M., & Saldaña, J. (2014). Qualitative data analysis: A
methods sourcebook (3rd ed.). Thousand Oaks, CA: Sage.
Miller, K. (2013). Flipping out: Reflections upon landing. Retrieved from
http://acrlog.org/2013/03/28/flipping-outreflections-upon-landing/

270

Miller, K. (2014). Undergraduate use of library databases decreases as level of study
progresses. Evidence Based Library and Information Practice, 9(3), 98-100.
Monroe-Gulick, A., & Petr, J. (2012). Incoming graduate students in the social sciences:
How much do they really know about library research?. portal: Libraries and the
Academy, 12(3), 315-335.
Mu, X., Dimitroff, A., Jordan, J., & Burclaff, N. (2011). A survey and empirical study of
virtual reference service in academic libraries. The Journal of Academic
Librarianship, 37(2), 120-129.
Nelson, S.D. (2003). Federal funding for academic research. In J.W. Guthrie (Ed.)
Encyclopedia of Education (2nd ed.) (853-861).New York, NY: Macmillan
Reference.
Nelson, W.N. (2009). Library standards in higher education. In M.J. Bates & M.N.
Maack (Eds.) Encyclopedia of Library and Information Sciences (3rd ed.) (34493459). http://dx.doi.org/10.1081/E-ELIS3-120043263
Nicholas, D., Watkinson, A., Abdullah, A., Boukacem-Zeghmouri, C., Bravo, B.R.,
Świgoń, M., et al. (2016). Early career researchers: The harbingers of change?:
Year one (2016). Retrieved from http://ciber-research.eu/download/20161120ECR_Year_1_final_report_071116.pdf
Niu, X., & Hemminger, B. M. (2012). A study of factors that affect the
informationǦseeking behavior of academic scientists. Journal of the American
Society for Information Science and Technology, 63(2), 336-353.
Niu, X., Hemminger, B. M., Lown, C., Adams, S., Brown, C., Level, A., et al. (2010).
National study of information seeking behavior of academic researchers in the
United States. Journal of the American Society for Information Science and
Technology, 61(5), 869–890.
Norusis, M. (2017). Cluster analysis. Retrieved from
http://www.norusis.com/pdf/SPC_v13.pdf
Oakleaf, M. (2010). Value of academic libraries: A comprehensive research review and
report. Chicago, IL: Association of College and Research Libraries.
O'Brien, H. L., & Toms, E. G. (2008). What is user engagement? A conceptual
framework for defining user engagement with technology. Journal of the
American Society for Information Science and Technology, 59(6), 938-955.
Okahana, H., Feaster, K., & Allum, J. (2016). Graduate enrollment and degrees: 2005 to
2015. Washington, DC: Council of Graduate Schools. Retrieved from

271
http://cgsnet.org/ckfinder/userfiles/files/Graduate%20Enrollment%20%20Degree
s%20Fall%202015%20Final.pdf
Ondrusek, A. (2008). Information literacy. In M.L. Radford & P. Snelson (Eds.)
Academic library research: Perspectives and current trends. Chicago, IL:
Association of College & Research Libraries.
Ortega, J. L. (2015). Disciplinary differences in the use of academic social networking
sites. Online Information Review, 39(4), 520-536.
Ory, J. C., & Braskamp, L. A. (1988). Involvement and growth of students in three
academic programs. Research in Higher Education, 28(2), 116-129.
Output, n [Def. 2]. (2015). In OED Online. Oxford University Press. Retrieved from
http://www.oed.com/view/Entry/133846
Ovadia, S. (2014). ResearchGate and Academia. edu: Academic social networks.
Behavioral & Social Sciences Librarian, 33(3), 165-169.
Page, J. D. (2010). Activism and leadership development: Examining the relationship
between college student activism involvement and socially responsible leadership
capacity. Retrieved from ProQuest Dissertations & Theses Global. (Order No.
3426286).
Parsons, M.D. (2003). Accreditation in the United States: Higher education. In J.W.
Guthrie (Ed.) Encyclopedia of Education (2nd ed.) (28-35). New York, NY:
Macmillan Reference
Partridge, H. & Yates, C. (2014). Researching information experience: Object and
domain. In Bruce, C., Partridge, H., Davis, K., Hughes, H., & Stoodley, I. (Eds.),
Information experience (19-32). Bingley, UK: Emerald Publishing Limited.
Pascarella E.T., & Terenzini, P. T. (1991). How college affects students. San Francisco,
CA: Josey-Bass.
Pettigrew, K. E., Fidel, R., & Bruce, H. (2001). Conceptual frameworks in information
behavior. Annual Review of Information Science and Technology, 35, 43-78.
Poll, R. (2001). Performance, process, and costs: Managing service quality with the
Balanced Scorecard. Library Trends, 49(4), 709-17.
Poll, R., & Payne, P. (2006). Impact measures for libraries and information services.
Library Hi Tech, 24(4), 547-562.
Pritchard, S. M. (1996). Determining quality in academic libraries. Library Trends, 44(3),
572-594.

272

QSR International. (2017). What is NVivo?. NVivo. Retrieved from
http://www.qsrinternational.com/nvivo-product
Qualtrics. (2017). Qualtrics [computer software]. Retrieved from
https://www.qualtrics.com/
Radford, M. L. (1993). Relational aspects of reference interactions: A qualitative
investigation of the perceptions of users and librarians in the academic library.
Retrieved from ProQuest Dissertations & Theses Global. (Order No. 9333442).
Radford, M. L. (2006). The critical incident technique and the qualitative evaluation of
the connecting libraries and schools project. Library Trends, 55(1), 46-64.
Rempel, H. G. (2010). A longitudinal assessment of graduate student research behavior
and the impact of attending a library literature review workshop. College &
Research Libraries, 71(6), 532-547.
Research Information Network. (2011). The role of research supervisors in information
literacy. Retrieved from
http://www.rin.ac.uk/system/files/attachments/Research_supervisors_report_for_s
creen.pdf
Rockinson-Szapkiw, A. J., & Spaulding, L. S. (2014). Navigating the doctoral journey: A
handbook of strategies for success. New York, NY: Rowman & Littlefield.
Roethlisberger, F. & Dickson, W. (1939). Management and the worker. Cambridge, MA:
Harvard University Press.
Rooney, P. (1992). On values in science: Is the epistemic/non-epistemic distinction
useful? PSA: Proceedings of the Biennial Meeting of the Philosophy of Science
Association, 1992, (13-22). Chicago, IL: University of Chicago Press. Retrieved
from https://www.jstor.org/stable/192740?item_view=read_online
Rose, S. (2013). The value of a college degree. Change: The magazine of higher
learning, 45(6), 24-33.
Rowlands, I., Nicholas, D., Williams, P., Huntington, P., Fieldhouse, M., Gunter, B., et
al. (2008). The Google generation: The information behaviour of the researcher of
the future. In Aslib proceedings 60(4), 290-310.
Rutgers University Libraries. (2017). Social sciences. Retrieved from
http://www.libraries.rutgers.edu/indexes/subjects/indexes_social_sciences
Sadler, E., & Given, L. M. (2007). Affordance theory: a framework for graduate students'
information behavior. Journal of Documentation, 63(1), 115-141.

273

Sandeen, A. (2003). Enhancing student engagement on campus. Lanham, MD:
University Press of America.
Saracevic, T., & Kantor, P. B. (1997). Studying the value of library and information
services. Part I. Establishing a theoretical framework. Journal of the American
Society for Information Science, 48(6), 527-542.
Savolainen, R. (2007). Information Behavior and Information Practice: Reviewing the
“Umbrella Concepts” of Information-Seeking Studies 1. The Library
Quarterly, 77(2), 109-132.
Schullery, N. M., Reck, R. F., & Schullery, S. E. (2011). Toward solving the high
enrollment, low engagement dilemma: A case study in introductory
business. International Journal of Business, Humanities and Technology, 1(2), 19.
Searing, S. E., & Greenlee, A. M. (2011). Faculty responses to library service
innovations: A case study. Journal of Education for Library and Information
Science, 52(4), 279-294.
Senteio, C. R. (2015). Investigating psychosocial factors: Supporting clinical decisions
for outpatient diabetes care (Doctoral dissertation). Retrieved from ProQuest
Dissertations &
Theses Global. (Order No. 3746092).
Shernoff, D. J. (2013). Optimal learning environments to promote student engagement.
New York, NY: Springer.
Shoaf, E. C. (2003). Using a professional moderator in library focus group research.
College & Research Libraries, 64(2), 124-132.
Shreeve, S., & Chelin, J. (2014). Value and impact of librarians’ interventions on student
skills development. New Review of Academic Librarianship, 20(2) 204-232.
Simon, H. A. (1955). A behavioral model of rational choice. The Quarterly Journal of
Economics, 69(1), 99-118.
Sinha, S. (2013). Exploring student engagement and transfer in technology mediated
environments (Doctoral dissertation). Retrieved from ProQuest Dissertations &
Theses Global. (Order No. 3606559).
Snyder, J. A. (2008). An exploration of the effects of student characteristics and
engagement practices on academic success for low-income students (Doctoral
dissertation). Retrieved from ProQuest Dissertations & Theses Global. (Order No.

274
3301775)
Social science, n. (2017). OED Online. Oxford University Press. Retrieved from
http://www.oed.com/view/Entry/183756?redirectedFrom=social+science#eid.
Soria, K. M., Fransen, J., & Nackerud, S. (2013). Library use and undergraduate student
outcomes: New evidence for students' retention and academic success. portal:
Libraries and the Academy, 13(2), 147-164.
Sowell, R., Bell, N., & Kirby, S.N. (2010). PhD completion and attrition: Policies and
practices to promote student success. Washington, DC: Council of Graduate
Schools.
Spady, J. (2002). Outcome based education. In J.W. Guthrie (Ed.) Encyclopedia of
Education (2nd ed.) (1827-1831). New York, NY: Macmillan Reference.
Spezi, V. (2016). Is information-seeking behavior of doctoral students changing?: A
review of the literature (2010–2015). New Review of Academic Librarianship,
22(1), 78-106.
Sprow, A. H. (2011). The role of faculty-student interactions in student leadership
development and engagement based on varying levels of faculty scholarship of
engagement (Doctoral dissertation). Retrieved from ProQuest Dissertations &
Theses Global. (Order No. 3487122).
Standard, n and adj [Def. II, 12]. (2015). In OED Online. Oxford University Press.
Retrieved from http://www.oed.com/view/Entry/188962
Steffy, C. (2013. October 7). The flipped classroom in an information literacy session
[Web log post]. Retrieved from
http:// crdpala.org/2013/10/07/theflippedclassroom-in-an-informationliteracysession/
Stein, J. L. (2007). Peer educators and close friends as predictors of male college
students' willingness to prevent rape. Journal of College Student Development,
48(1), 75-89.
Stevens, M., & Kirst, M. (Eds.). (2015). Remaking College: The Changing Ecology of
Higher Education. Palo Alto, CA: Stanford University Press.
Stimpson, C.R. (2012). Graduate education: The nerve center of higher education. In E.C.
Lagemann & H. Lewis (Eds.), What is college for?: The public purpose of higher
education (132-156). New York, NY: Teachers College Press.
Swab, M., & Romme, K. (2016). Scholarly sharing via Twitter:# icanhazpdf requests for
health sciences literature. Journal of the Canadian Health Libraries

275
Association/Journal de l'Association des bibliothèques de la santé du
Canada, 37(1), 6-11.
Switzer, A., & Wynn Perdue, S. (2011). Dissertation 101: a research and writing
intervention for education graduate students. Education Libraries, 34(1), 4.
Talja, S., Tuominen, K., & Savolainen, R. (2005). " Isms" in information science:
constructivism, collectivism and constructionism. Journal of
Documentation, 61(1), 79-101.
Tenopir, C. (2013). Building evidence of the value and impact of library and information
services: Methods, metrics and ROI. Evidence Based Library and Information
Practice, 8(2), 270-274.
Tenopir, C. (2014). Scholarly reading patterns of US graduate and undergraduate
students. Presented at 2014 Library Assessment Conference: Building effective,
sustainable, practical assessment, 4-6 August 2014, Seattle, Washington (USA).
Retrieved from http://libraryassessment.org/bm~doc/15tenopirpanel-2.pdf
Tenopir, C., King, D. W., & Bush, A. (2004). Medical faculty's use of print and
electronic journals: changes over time and in comparison with scientists. Journal
of the Medical Library Association, 92(2), 233.
Tenopir, C., King, D. W., Christian, L., & Volentine, R. (2015). Scholarly article seeking,
reading, and use: A continuing evolution from print to electronic in the sciences
and social sciences. Learned Publishing, 28(2), 93-105.
Terenzini, P. E., & Pascarella, E. T. (1991). How college affects students: findings and
insights from twenty years of research. San Francisco, CA: Jossey-Bass.
Terenzini, P. T., Springer, L., Pascarella, E. T., & Nora, A. (1995). Influences affecting
the development of students' critical thinking skills. Research in Higher
Education, 36(1), 23-39.
Thelwall, M., & Kousha, K. (2014). Academia. edu: social network or academic
network?. Journal of the Association for Information Science and Technology,
65(4), 721-731.
Town, J. S., & Kyrillidou, M. (2013). Developing a values scorecard. Performance
Measurement and Metrics, 14(1), 7-16.
Tracy, D. G., & Searing, S. E. (2014). LIS graduate students as library users: A survey
study. The Journal of Academic Librarianship, 40(3), 367-378.
Trustees of Indiana University. (2017). About NSSE ®. National Survey of Student
Engagement. Retrieved from http://nsse.indiana.edu/html/about.cfm

276

Trustees of Indiana University. (2014). Engagement indicators. National Survey of
Student Engagement. Retrieved from
http://nsse.iub.edu/html/engagement_indicators.cfm
Twenge, J. M., Campbell, W. K., & Freeman, E. C. (2012). Generational differences in
young adults' life goals, concern for others, and civic orientation, 1966–
2009. Journal of Personality and Social Psychology, 102(5), 1045-1062.
U.S. Census Bureau. (2017). Census regions and divisions of the United States. Retrieved
from https://www2.census.gov/geo/pdfs/maps-data/maps/reference/us_regdiv.pdf
Value, n [Def. 1-2]. (2017). OED Online. Oxford University Press. Retrieved from
http://www.oed.com/view/Entry/221253?rskey=62qWMo&result=1#eid
Vezzosi, M. (2009). Doctoral students' information behaviour: An exploratory study at
the University of Parma (Italy). New Library World, 110(1/2), 65-80.
Webb, K. A., Lu, T., & Black, E. L. (2008). New intersections for student engagement in
libraries: A qualitative exploration of collaborative learning with multimedia
technologies. Evidence Based Library and Information Practice, 3(4), 34-48.
Whitmire, E. (1998). Development of critical thinking skills: An analysis of academic
library experiences and other measures. College & Research Libraries, 59(3),
266-273.
Whitmire, E. (2002). Academic library performance measures and undergraduates' library
use and educational outcomes. Library & Information Science Research, 24(2),
107-128.
Williams, A. E., & Woodacre, M. A. (2016). The possibilities and perils of academic
social networking sites. Online Information Review, 40(2), 282-294.
Williams, P., Nicholas, D., & Rowlands, I. (2010, May). The attitudes and behaviours of
illegal downloaders. In Aslib Proceedings, 62(3), 283-301.
Wong, G., Chan, D., & Chu, S. (2006). Assessing the enduring impact of library
instruction programs. The Journal of Academic Librarianship, 32(4), 384-395.
Wong, S. H. R., & Cmor, D. (2011). Measuring association between library instruction
and graduation GPA. College & Research Libraries, 72(5), 464-473.
Wu, M. D., & Chen, S. C. (2014). Graduate students appreciate Google Scholar, but still
find use for libraries. The Electronic Library, 32(3), 375-389.
You, S., DesArmo, J., Mu, X., & Dimitroff, A. (2014). Balancing factors

277
affecting virtual reference services: Identified from academic librarians'
perspective. In Digital Libraries (JCDL), 2014 IEEE/ACM Joint Conference
(477-478). http://dx.doi.org/10.1109/JCDL.2014.6970233
Young, V. (1993). Focus on focus groups. College & Research Libraries News, 54(7),
391-394.
Yu, L., Hong, Q., Gu, S., & Wang, Y. (2008). An epistemological critique of gap theory
based library assessment: The case of SERVQUAL. Journal of Documentation,
64(4), 511-551.
Zickuhr, K., Purcell, K., & Rainie, L. (2014). From distant admirers to library lovers –
and beyond: A typology of public library engagement in America. Retrieved from
http://www.pewinternet.org/2014/03/13/library-engagement-typology/

