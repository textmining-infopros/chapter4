BOOTSTRAP-BASED CONFIDENCE INTERVALS IN PARTIALLY
ACCELERATED LIFE TESTING

by

AHMED MOHAMED ESHEBLI

A DISSERTATION
Presented to the Faculty of the Graduate School of the
MISSOURI UNIVERSITY OF SCIENCE AND TECHNOLOGY
In Partial Fulfillment of the Requirements for the Degree

DOCTOR OF PHILOSOPHY
in
MATHEMATICS WITH STATISTICS EMPHASIS

2017

Approved
V.A. Samaranayake, Advisor
Robert Paige
Gayla Olbricht
Xuerong Wen
Xiaoping Du

ABSTRACT

Accelerated life testing (ALT) is utilized to estimate the underlying failure
distribution and related parameters of interest in situations where the components under
study are designed for long life and therefore will not yield failure data within a
reasonable test period. In ALT, life testing is carried out under two or more higher than
normal stress levels, with the resulting acceleration of the failure process yielding a
sufficient amount of un-censored life-span data within a practical test duration. Usually
one (or more) parameters of the life distribution is linked to the stress level through a
suitably selected model based on a well-understood relationship. The estimate of this
model is then utilized to determine the life distribution of the components under normal
use (design use) conditions. Partially accelerated life testing (PALT) is preferable over
accelerated life testing (ALT) in situations where such a model linking the stress to the
distribution parameters is unavailable. In this study, parametric and nonparametric
bootstrap based methods for obtaining confidence intervals for the parameters of the life
distribution as well as a the lower confidence bound for the mean life under normal
conditions are developed for both the Weibull and Generalized exponential life
distributions under Type I censoring. Monte-Carlo simulation studies are carried out to
study the performance of the confidence intervals based on the proposed methods against
those of intervals obtained using the traditional delta method. Results show that the
bootstrap-based methods performs as well as or better than asymptotic distribution-based
methods in most cases.

1. INTRODUCTION
When products are designed to be highly reliable and therefore have a long lifespan, standard life testing, where a sample of units is tested under normal use conditions,
will not produce a sufficient number of failures to enable the researcher to obtain good
estimates of the parameters of interest. One solution to the problem is to subject the
specimens in the sample to higher than normal stress levels. The stress factors can be
temperature, humidity, pressure, repetitive flexing at a higher than normal rate, or any
other variable that can accelerate the failure process. Since the goal of the study is to
estimate the parameters of the underlying life distribution and the expected life-span of
the products under normal (design) use conditions, a mathematical model that relate the
stress level to one or more parameters of the life distribution has to be estimated and
then utilized to extrapolate results obtained at high stress levels to those at the normal
level. This model that links stress to the distribution parameter(s), however, must be
based on well-understood and/or empirically verified relationship (Meeker and Escobar
1998, p. 495). When such a model is available, an accelerated life test (ALT) can be
performed where test specimens are subjected to two or more distinct higher than
normal stress levels. The higher stress levels accelerate the failure process, thus yielding
a sufficient number of un-censored failure data within a reasonable test period. When a
reasonable model that links the stress level to distributional parameter(s) is not available,
the partially accelerated life test (PALT) procedure is available as an alternative. In
PALT, the test specimens are subjected to a single high stress level as well as stress at
the normal level.
Each of these accelerated life test methods can be implemented in two different
ways, namely using a constant stress protocol or utilizing a step-stress approach to life
testing. In the constant stress procedure, independent samples of specimens are assigned
to each of the designated high stress levels, and all specimens in a sample are kept at the
assigned stress throughout the experiment. That is, the stress is kept constant within a
sample. For example, in PALT, some specimens may experience normal

stress

throughout the experiment while others are subjected to a higher stress level which is
kept constant during the test period. In step-stress method, all specimens are first

2
subjected to one level of stress for a given period of time, and the test specimens that are
still functional are subjected to a higher stress level. In this study, the focus will be
limited to the constant stress approach so discussions from here on will be on this
method only.
In a certain type of constant stress life testing, a sample of product specimens are
put to test over a pre-specified test period T and the life spans of the items that failed
during this period are recorded. Since not all items on test may fail by time T, the lifespan of some specimens are censored. This type of censoring is called Type I censoring.
Alternatively, the experimenter can wait until a specific number of items fail and then
stop the experiment. For example he/she can wait until 50% of the items fail. In this case
we have what is termed as Type II censoring. Since the experimenter sets a specific time
at which the experiment will end, the Type I censoring approach is preferable over Type
II censoring. The experimenter who conducts a Type II censored experiment will not
have a precise idea when the experiment is going to end because the time it takes for a
specific percentage of items to fail is a random variable. However, the mathematics of
the estimation procedure under Type I censoring can be complicated because the number
of failures, R, is a random variable rather than a fixed number as is the case in Type II
censoring. The work herein centers on experiments conducted under Type I censoring.
1.1. ACCELERATED LIFE TESTS (CONSTANT STRESS CASE)

In Accelerated Life Tests (ALT), the life-span, X, of a product is assumed to have
a distribution (termed the life-distribution) with a probability density function𝑓𝑓�𝑥𝑥, 𝜃𝜃�,
′

where 𝜃𝜃 = �𝜃𝜃1 , 𝜃𝜃2 , ⋯ , 𝜃𝜃𝑝𝑝 � is a vector of parameters associated with the distribution such
that one or more of the parameters in 𝜃𝜃 are related to the stress S through a relationship

whose functional form is known except for a few parameters. For example, θ1 may be

related to 𝑆𝑆 through the function: 𝜃𝜃1 = 𝑔𝑔(𝑆𝑆, ∅0 , ∅1 ) = 𝑒𝑒𝑒𝑒𝑒𝑒{∅0 + ∅1 𝑆𝑆}. It is assumed
that the other parameters in 𝜃𝜃 are not related to S. To estimate the parameters, two

independent samples of specimens of the product are tested, with one sample

undergoing stress at an accelerated level 𝑆𝑆𝐿𝐿 and the other sample subjected to an even

3
higher stress level 𝑆𝑆𝐻𝐻 . If 𝑆𝑆𝐷𝐷 is the stress level at normal (design) use conditions, then we

have the ordering 𝑆𝑆𝐿𝐿 < 𝑆𝑆𝐷𝐷 < 𝑆𝑆𝐻𝐻 . From the experimental data, the parameters of the

function 𝑔𝑔 are estimated (in the example above we estimate ∅0 and ∅1 ). The parameters

𝜃𝜃𝑖𝑖 , 𝑖𝑖 = 2,3, … . 𝑝𝑝 are estimated using combined data from both samples because they do
not depend on the stress level. Then, using the estimated function, g , the value of 𝜃𝜃1 at

�0, ∅
�1 � This yields an
the design stress level is estimated by the relationship 𝜃𝜃�1 = 𝑔𝑔�𝑆𝑆𝐷𝐷 , ∅
estimate of the life-distribution at normal use stress level.
1.1.1

A Brief Review of Relevant Literature. There are a large number of

publications on ALTs and a relatively smaller but an appreciable number also available
for PALTs. For brevity, we will refrain from discussing all of these, but limit the
discussion to a select few of these publications. An excellent coverage of Accelerated
Life tests is given in Nelson (1990). Other books include Mann, Schafer, and
Singapuwalla (1974), Lawless (1982), Viertl (1988), Marvin Rausand and Hsyland
(2004) Michelle, Hoang Jr, and David

(2006),Guangbin Yang (2007), Tobias and

Trindade (2011), and Meeker and Escobar (1998).

One of the more recent publications is Jayawardhana and Samaranayake (2003),
that discussed obtaining lower prediction bounds for a future observation from a Weibull
population at design (normal use) stress level, using Type II censored accelerated life
test data. The scale parameter of the life distribution is assumed to have an inverse
power relationship with the stress level. They showed that the method works well when
the low and high stresses are reasonably far apart. Alferink and Samaranayake (2011)
considered accelerated degradation models and developed confidence intervals for mean
life using the Delta method and the bootstrap, assuming lognormal distribution with
variance dependent on stress. Another interesting paper is Kamal, et al (2013), who
presented a step stress ALT plan that works well. In step stress, the components are first
put at a lower stress and the unfailed components are subjected to higher stress after a
specific period. More recently, Jayawardhana and Samaranayake (2014), obtained
predictive density of a future observation at normal use conditions using ALT method
under lognormal life distribution and Type II censoring with non-constant variance.

4

1.2. PARTIALLY ACCELERATED LIFE TESTS (CONSTANT STRESS CASE)
The main drawback of accelerated life tests is the fact that the functional form of
the model that relates stress to the parameters of the life-distribution has to be known.
The form of this function can be dependent on the nature of the material the product
under study is made of or the construction of the product. For some materials such as
electrical insulators, the functional form of g is well known (Nelson, 1990). For some

products, especially those constructed of new materials, such a function may not be
easily assumed. In many situations, Partially Accelerated Life Tests (PALT) can
overcome this problem. In PALT scenario, one set of product specimens are tested at
normal use conditions while the other set is tested under high stress conditions. Rather
than assume a function that links the model parameter θ1 with stress, it is assumed that

at higher stress, θ1 takes a new value θ1∗ = βθ1 . That is, the acceleration changes
θ1 through a multiplicative constant. While the mathematics behind estimating both θ1

and θ1∗ as well as the other parameters of the life distribution is not simple, the PALT
methodology avoids the assumption of the linkage function g thus eliminating the

chance of using an incorrect functional form. The main drawback of the PALT
procedure is that one set of product specimens has to be tested at the normal use stress
level thus forcing the experimenter to increase the product test time T in order to ensure
that a sufficient number of specimens will fail under normal use conditions. This
method, however, is ideal for life testing products such as chemicals, whose usable lifespan is moderately long but may not run into many years.
Within the PALT, the literature works mention. Saxena and Zarrin (2013) used
the Constant Stress Partially Accelerated Life Test (CSPALT) and assumed Type-I

censoring under the Extreme Value Type-III distribution. The Extreme Value Type-III
distribution has been recommended as appropriate for high reliability components. The
authors used the Maximum Likelihood (ML) method to estimate the parameters of
CSPALT model and confidence intervals for the model parameters were constructed.
Note that the CSPALT plan is used to minimize the Generalized Asymptotic Variance
(GAV) of the ML estimators of the model parameters.

5

Ismail (2013) derived the maximum likelihood estimators (MLEs) of the
parameters of the GE distribution and the acceleration factor when the data are Type-II
censored under constant-stress PALT model.

The likelihood ratio bounds (LRB)

method was used to obtain confidence bounds of the model parameters when the sample
size is small. It is also shown that the maximum likelihood estimators are consistent and
their asymptotic variances decrease as the sample size increases. The numerical results
reported in the paper support the theoretical findings and showed that the estimated
approximate confidence intervals for the three parameters are smaller when the sample
size is larger.
Abdel-Hamid (2009), considered a constant PALT model when the observed
failure times come from Burr(c,k) distribution under progressively Type-II right
censoring. The MLEs of the parameters were obtained and their performance was
studied through their mean squared errors and relative absolute biases. The paper also
showed how to constructed approximate and bootstrap CIs for the parameters. The
bootstrap CIs give more accurate results than the approximate intervals for small sample
sizes, the Student’s-t bootstrap CIs are better than the Percentile bootstrap CIs in the
sense of having smaller widths. However, the differences between the lengths of CIs for
the two methods decrease with the increase in sample size.
In this study, we develop PALT methodologies for constructing confidence
intervals not only for the distribution parameters and the acceleration factor, but also a
lower confidence bound for the mean life, under Type I censoring. Three types of
confidence intervals and bounds are considered. They are the asymptotic
intervals/bounds constructed from the delta-method and those constructed using the
parametric bootstrap or the non-parametric bootstrap. The underlying distributions
considered are the Weibull and the Generalized Exponential (GE).

Methods for

obtaining asymptotic or bootstrap-based confidence bounds for the mean life under
PALT are not discussed in currently available literature for any type of life distribution,
censoring scheme. Also, not available in current literature on PALT are bootstrap-based
methods for constructing confidence intervals for distribution parameters of Weibull and

6
GE distribution and the acceleration factor under Type I censoring. This research aims to
fill this gap.

7
2. BOOTSTRAP-BASED CONFIDENCE INTERVALS IN PARTIALLY
ACCELERATED LIFE TESTING UNDER THE WEIBULL
DISTRIBUTION

2.1 INTRODUCTION
Products which under normal use conditions last for a long period pose a

problem in determining their mean life using standard life tests because only a very
small fraction of them will fail under a testing period of reasonable duration. In such
situations, practitioners resort to accelerated life tests (ALT). As Nelson (1980) puts it:
“Accelerated life testing of a product or material is used to get information quickly on its
life distribution.” In an ALT scenario, test units are run under two or more high stress
levels to accelerate the failure process conditions yielding failure-time data sooner than
under normal (design, field) use conditions. A model is fitted to the accelerated failure
times and then extrapolated to estimate the life distribution under normal conditions.
Alternatively, a known acceleration factor that adjusts a parameter of the life distribution
to account for the higher stress is utilized for this purpose. This is quicker, cheaper, and
more practical than testing at design use conditions. When there exists a mathematical
model, which specifies the life-stress relationship, or an acceleration factor is known, the
ALT is a very suitable approach to quickly obtain information useful for estimating the
life distribution under normal use conditions. However, there are some situations in
which neither the acceleration factor is known nor do life-stress models exist, or are very
hard to assume. In such cases partially accelerated life tests (PALT) provide a better
approach.
Under the PALT method, a portion of the test units are placed under the normal
use stress conditions and the remaining units are tested under a suitably selected higher
than normal stress level. The life distribution under the higher stress level is assumed to
be the same as that under normal use, but with the scale parameter multiplied by an
acceleration factor. This factor is estimated together with the other distribution
parameters. Since there are more failure data from the units that received higher than

8
normal

stress level, the combined data provide better estimates of the common

parameters.
One drawback of the PALT method is that unlike in the ALT, some units have to
be tested under normal use. Thus this method is not suitable for components that are
very long lasting. But items such as chemicals that have shelf-lives that are measured in
months or a year or two can be tested using this method.

In the following, we develop PALT-based methodologies to obtain confidence
bounds for the mean life and confidence intervals for the acceleration factor as well as
the distribution parameters when the underlying distribution is Weibull. Type I
censoring is also assumed. The methodologies considered are asymptotic methods as
well as those relying on the parametric or the non-parametric bootstrap. This research
extends the work of Ismail (2013) who assumed Type II censoring and employed only
the traditional large sample approach to obtaining prediction intervals. While Ismail’s
work assumed a Generalized Exponential distribution as the underlying life distribution,
we assume the Weibull in this study. The performances of the three methods are
compared using a Monte-Carlo simulation study.

2.1.1

A Brief Review of Relevant Literature. Partial accelerated life test

(PALT) is the one of methods used for reliability demonstration and prediction of
components at normal conditions using data obtained at accelerated condition. It is a
type of testing method that enables one to quickly get information over a variety of
conditions, and is therefore an important tool for the reliability engineer. A brief outline
of previous work on PALT is given below.

Nelson (1990) showed that the stress can be applied in two ways; as constant
stress over the test period or in a step-stress fashion. In step-stress partially accelerated
life tests (SS-PALT), a test item is first run at normal use conditions and, if it does not
fail for a specified time, then it is subjected to a higher than normal stress level for

9
another testing period. The SS-PALT were studied extensively by many authors, for
example: Preeti Wanti Srivastava, Mittal (2010), Abdel-Hamid (2009).
However, the constant-stress PALT runs every item at either normal use
condition or accelerated use condition only. Thus, we have two samples and units in
each sample are run at a constant stress level unique to that sample, the levels being
either normal or a pre-determined higher than normal level. Within the literature on
PALTs, the following studies are worth mentioning. Saxena and Zarrin (2013) used the
constant stress Partially Accelerated Life Test (CSPALT) and assumed Type-I
censoring. The underlying life-distribution they incorporated was the Extreme Value
Type-III distribution, which has been recommended as appropriate for high reliability
components. The Maximum Likelihood (ML) method was employed by the authors to
estimate the parameters of CSPALT model and confidence intervals for the model
parameters were also constructed.
Ismail (2013) assumed a constant-stress PALT testing scenario under Type-II
censoring. In addition to asymptotic confidence bounds, likelihood ratio bounds (LRB)
method employed to obtain confidence bounds of the model parameters in small sample
situations. The authors showed that the maximum likelihood estimators are consistent
and their asymptotic variances decrease as the sample size increases. They also
established that the estimated approximate confidence intervals for the three parameters
become narrower with increase in sample size. These asymptotic results were confirmed
using numerical simulations.
A constant PALT model was developed by Abdel-Hamid (2009), for the case
when the underlying life distribution is Burr(c,k). They considered that sample is
subjected to progressive Type-II right censoring. The MLEs of the parameters were
obtained and their performance with respect to their mean squared errors and relative
absolute biases were investigated.

The author also constructed approximate and

parameters bootstrap-based confidence intervals (CIs) for the parameters. It was shown
that the bootstrap CIs gave more accurate results than the approximate intervals for
small sample sizes, and that the Student’s-t bootstrap CIs have smaller widths than the

10
Percentile bootstrap CIs.

The differences between the lengths of CIs for the two

methods, however, decreased with on increase in sample size.
2.1.2

The Weibull Distribution. The proposed PALT method is developed for

the case where the underlying life distribution is Weibull. The Weibull probability
density function is given by:
α

α −1

αx
f ( x;α , λ )
=
λ  λ 

e

x
− 
λ

,

x > 0, α > 0, λ > 0,

(1)

And the cumulative distribution function is:
α

F ( x;α , λ ) = 1 − e

x
− 
λ

,

(2)

where α is the shape parameter and λ the scale parameter.
Note that the Weibull distribution is used extensively in reliability literature because of
the different shapes its hazard function can take based on different shape parameter
values. The hazard (or the failure rate) function of the Weibull distribution is given by:

h (=
x; α , λ )

2.2

f ( x)
αx
=
 
1− F ( x) λ  λ 

α −1

.

(3)

THE PROPOSED PALT METHOD AND BOOTSTRAP INTERVALS

The following assumptions are made regarding the proposed PALT method.
1. The total number of units under test is n .
2. π denotes the proportion of sample units allocated to accelerated condition

nπ units, where π = 1 − π , are allocated to normal (field) use conditions.
3. n (1 − π ) =
4. nπ units are allocated to the high stress condition (subject to acceleration)
2.2.1 Likelihood Function under Type I Censoring and Asymptotic C.I.s.
Under Type I censoring, the censoring time, τ is fixed but the number of failures
observed during the test duration τ is a random variable, say R.
Notation

11
xi : Observed lifetime of item i tested at the normal (field) use conditions.

yj

: Observed lifetime of item j tested at high stress conditions.

δu

: Indicator function denoting the censoring state of ith observation under normal use
δ =1
condition, with ui
if the observation is uncensored.
i

δa

: Indicator function denoting the censoring state of jth observation under high stress
δ =1
condition, with a j
if the observation is uncensored.
j

nu : Number of items that failed at normal use condition.
na : Number of items that failed at high stress condition.

τ: The censoring time of the life test (for all units).

x(1) ≤  ≤ x( nu ) ≤ τ : Ordered failure times at normal use condition.
y(1) ≤  ≤ y( nu ) ≤ τ : Ordered failure times at high stress condition.

β : Denotes the acceleration factor ( β > 1) .
In type I censoring, τ is fixed but the number of failure values observed in time τ is a

random variable. The number of items, R, failing before time τ is assumed to follow a
α

binomial distribution R  Bin ( n, p ) , where p = FX (τ ; α , λ ) = 1 − e

τ 
− 
λ

, under normal

use conditions. Under high stress conditions the number of items failing will have a
α

Binomial Bin ( n, p* ) , distribution where p* = FX (τ ; α , λ , β ) = 1 − e

 βx
−

 λ 

. Then, for

observation i under normal use conditions, we have,

xi ≤ τ
1
=
δ ui =
, i 1, 2, , nπ .
0/w
0
Similarly, for observation j under a high stress condition, we have,
yj ≤ τ
1
δ a j =
, j 1, 2, , nπ ,
=
0/w
0

(5)

δu =
1−δ u ,
δa =
1−δ a ,
i

with

i

(4)

j

j

12
np

δ u  Ber ( p) ⇒ ∑ δ u  Bin(np , p ),
i

i =1

(6)

i

np

δ a  Ber ( p) ⇒ ∑ δ a  Bin(np , p).
j

j =1

(7)

j

We also have, under normal use conditions,
α

Fτ ( x; α , λ τ =
) P ( X ≤ x X ≤ τ =)

x
− 
λ

P( X ≤ x) 1 − e
=
α
τ 
P( X ≤ τ )
− 
1− e  λ 
α
x

− 
1 − e  λ 

α ,
τ 
=
− 
1 − e  λ 
 1,


(8)

x ≤τ
x >τ,

and
α

α −1

αx
λ  λ 

f X ( x; α , λ )
fτ=
( x;α , λ τ ) F=
X (τ ; α , λ )

e

x
− 
λ
α

x
− 
λ

.

(9)

1− e
Thus, given R = nu , the conditional density of the first r failure times under a normal
use condition is equivalent to the joint density of an ordered random sample of size nu
from a truncated Weibull distribution, given by
 α x α −1 − xi 
  i  e  λ 
nu
nu
λ λ
f x(1) , , x( nu ) =
R n=
nu !∏ fτ ( x(i ) ; α , =
λ ) nu !∏   
α
u
τ 
− 
=i 1 =i 1

λ
1− e



α

(

)

nu

α

 xi 

 α  − λ ∑  λ 
  e
λ
nu

i =1

= nu !


1 − e



α

τ 
− 
λ














nu

α −1

 xi 
∏
 
i =1  λ 
nu

.

The joint density of obtaining R = nu ordered observations at the values x(1) , , x( nu )
before time τ may be expressed as

(10)

13

(

)

(

)

=
f x(1) , , x( nu )
f=
x(1) , , x( nu ) R n=
u bin( nu ; np , p )
nu

α

x 

i
nu
λ
 α  −∑

i =1 
e
 
λ
nu !  
nu
α
τ 

−  
1 − e  λ  





α −1

 np  nu
np − nu
  p (1 − p )
 nu 

 xi 
∏
 
i =1  λ 
nu

nu

( nπ )!

−

nu

α

x 

∑  λi 

α 
i =1
  e
n
!
π
(
)
λ

=
( nπ − nu )!  − τ α
1 − e  λ 


nu






α

 xi 

λ
 α  −∑

i =1 
e
=
 
!
π
λ
n
n
−
(

u) 
nu

nu

τ 

− 
1 − e  λ 



α

 − τ 
e λ



α











nπ − nu

nu

 − τ 
e λ



α






nπ − nu

α −1

 xi 
∏
 
i =1  λ 
nu

α −1

 xi 
∏
 
i=1  λ 
nu

.

Therefore, we have

(

f x(1) , , x( nu )

)

nu 
τ 
 α   − λ 
∝  e
λ 


α






nπ − nu

−

e

nu

α

x 

∑  λi 
i =1

α −1

 xi 
∏
 
i =1  λ 
nu

.

In a fashion similar to the argument made about the joint density of observations under
normal use conditions, given R = na the conditional density of the first r failure times
under acceleration is equivalent to the joint density of an ordered random sample of size
na from a truncated accelerated Weibull distribution. Therefore, for an item tested at

accelerated condition, the probability density function is given by
α

α −1

βα  β x 
f ( x; α , λ , β )
=
λ  λ 
where Y = β −1 X .

e

 βx
−

 λ 

,

x > 0, α > 0, λ > 0, β > 1,

14


 aβ
na
na 
λ
f y(1) , , y( na ) =
R n=
na !∏ fτ ( y(i ) ; a , λ , β
=
) na !∏ 
a
=j 1 =j 1 



(

)

 β yj

λ
i =1 
na

∑
 βa 

 e
λ 
= na ! 
a
 βτ 

−

λ


1 − e


na

−

e

 β yj
−
 λ

a

1− e

 βτ 
−

 λ 

a













(11)

a





a −1

 β yj 


∏
j =1  λ 
na






a −1

 β yj 


 λ 

na

.

The joint density of obtaining R = na ordered observations at the values Y(1) , , Y( na )
before time, may be expressed as

(

)

(

)

=
f y(1) , , y( na )
f=
y(1) , , y( na ) R na bin(na ; np , p* )
−

a

 β yj 

λ 
i =1
na

∑ 

 βa 
e

λ 

na !
a
 βτ 

−

λ


1 − e


na






na

 βa 
=
( np − na )!  λ 

na

−

 np  * na
* np − na
  ( p ) (1 − p )
 na 

a

 β yj 

λ 
i =1
na

∑ 

 βa 
e
n
!
p
( )  λ 
( np − na )!  − βτ a
1 − e  λ 


na

( np )!

a −1

 β yj 


∏
j =1  λ 
na

a −1

 β yj 


∏
j =1  λ 
na






na

 − βτ 
e  λ 



a






np − na

−

e

a

 β yj 

λ 
i =1
na

∑ 

 βτ 

−

1 − e  λ 



a






na

 − βτ 
e  λ 



a






a −1

 β yj 


∏
j =1  λ 
na

,

thus,

(

f y(1) , , y( na )

)

  βτ 
 βa   − λ 
∝
 e
 λ  


a

na

(






nπ − na

−

e

a

 β yj 

λ 
i =1
na

∑ 

a −1

 β yj 


∏
j =1  λ 
na

,

and the total likelihood function for x1 ; δ u1 , , xnπ ; δ unπ , y1 ; δ a1 , , ynπ ; δ anπ
expressed as follows:

) can be

np − na

15

(

) (

(

)

=
L L=
α , λ , β x , y Lui α , λ xi , δ ui Lu j α , λ , β y j , δ u j
δu

)

δ

 α x α −1 − xi   i  − τ α  ui nπ  βα  β y α −1 − β y j 
 
j
 λ 
= ∏   i  e  λ   e  λ   ∏ 

 e

λ  λ 
λ  λ 
 

i 1=
 j1

 
α

nπ

α






δu j

 − βτ 
e  λ 



α






δu j

(12)

 nπ  − τ α  nα  βα  β y α −1 − β y j   nπ  − βτ α 
j
 λ  
 ∏ e  λ  ∏ 
e  λ  .

 e
∏


λ  λ 
 i=

 j=

j=
n +1 
 1

 nu +1 
 α 
The MLE’s of the parameters can estimated numerically by minimizing the log
likelihood function.
α

 α  x α −1 − xi 
= ∏  i  e λ 
1 λ  λ 
i=


α

nu

)

(

=
⇒ ln L ln L al
=
, ,β x , y l
a

a

nπ
 xi  nu  xi 
τ 
=
⇒ l ∑ ln ala
− ∑ ln + ( − 1) ∑ ln   − ∑   − ∑  
 lll
 =i 1   =i nu +1  
=i 1
=i 1
=i 1
nu

nu

nu

(13)
a

a
nπ
 β y j  na  β y j 
 βτ 
+ ∑ ln al
− ∑ ln + ∑ ln β + (a − 1) ∑ ln 
−∑ 
 − ∑ 
 .

=j 1
=j 1
=j 1
=j 1
 lll
 =j 1 
 =j na +1 
na

na

na

na

aa

nπ
τ 
 βτ 
⇒ l= ( nu + na )( ln al
− ln ) + na ln β + ∑ψ 7 i + ∑ψ 8 j − ∑   − ∑ 



i=
j=
i=
nu +1  ll
j=
na +1 
1
1

⇒ l=

nu

na

nu

na

nπ

− ln ) + na ln β + ∑ψ 7 i + ∑ψ 8 j
( nu + na )( ln al
=i 1 =j 1

aa

τ 
 βτ 
− ( nπ − nu )   − ( nπ − na ) 
 .
 ll



The score equations are obtained by differentiating the log likelihood with respect to the
parameters and setting them to zero. These equations are:

 na   β y j 
∂l n + na nu   xi 
ln
⇒ = u
+ ∑  ln   − yy
+
−




∑
1i 
2j 

∂aall
i 1=
=
  
 j 1 



(14)

− ( nπ − nu )y 3 − ( nπ − na )y 5 =
0,

a
a
aa
nu
na
 β yj  
∂l a 
 xi 
τ 
 βτ 
1 + ( nπ − nu )   + ( nπ − na ) 
⇒
=
  = 0,
 + ∑   +∑ 
∂llllll
 
=


 
i 1=
j 1
 
and
a
a
na
 β yj  
∂l a 
 βτ 
1 − ( nπ − na ) 
⇒
=
  = 0.
 − ∑
∂β lll



j =1 
 

(15)

(16)

16
Now, we have a system of three nonlinear equations in three unknowns a , λ , and β .

It is clear that a closed form solution is very difficult to obtain. Therefore, an

iterative procedure must be used to find a numerical solution of the above system.
Asymptotic confidence intervals for parameter θ = (α , λ , β ) can be obtained using the
following convergence in distribution result;
Result 2.1
The MLEs obtained from the above procedure has the asymptotic distribution given by
the following convergence result:




n  (αˆ − α ) , λˆ − λ , βˆ − β →  0 , I −1 (α , λ , β )   ,
−



(

)(

)

where the I = (α , λ , β ) is the fisher information matrix given by
 ∂2l
∂2l
∂2l 


∂α∂l ∂α∂β 
∂α 2

 I11 (α ) I12 (αl ) I13 (αβ ) 
2
∂2l
∂2l 

  ∂ l
=
I (α , l , β ) =
I 21 ( lα ) I 22 ( llβ
I
) 23 ( ) 
.
2
∂
∂
∂
∂
l
α
l
β
∂
l

 I 31 ( βα ) I 32 ( βl ) I 33 ( β )  
 ∂2l
∂2l
∂2l 


∂β 2 
 ∂β∂α ∂β∂l
Proof: A proof that shows that the regularity conditions needed for asymptotic normality
for Weibull parameters estimates under Type I censoring in the PALT setup is given in
the appendix. Note that since the Weibull distribution belongs to the log-location-scale
family and the distributions in this family satisfy the regularity conditions needed, the
above asymptotic result does hold for MLE estimators of the Weibull parameters (see
Escobar and Meeker (2000)), but their results do not consider the case where PALT data
are used. Thus, the proof given in the appendix is of importance.

The elements of the 3x3 matrix

()

(

I , I ij (θ ) , i, j = 1, 2,3,

)

I ij θˆ = I ij αˆ , λˆ, βˆ ,

From Eq. (13), we get the following:

∂ 2 l (θ )
I ij θˆ =
∂θi ∂θ j

()

θ =θˆ

.

can be approximated by

17

nu
na
∂ 2 l nu + na
=
+
2
n
π
−
n
ψ
+
2
n
π
−
n
ψ
+
ψ
+
(
)
(
)
∑ 1i ∑ψ 2 j ,
u
3
a
5
2
2
∂aa
=i 1 =j 1

aa


τ 
 βτ 
 nu + na − (a − 1)  ( nπ − nu )   + ( nπ − na ) 

 ll





a
a
na
 β yj  
∂ 2 l −a 
 βτ 
1 + (a − 1)( nπ − na ) 
=
 ,
 + (a − 1) ∑ 
ll
∂β 2 β 2 


j =1 
 


(17)


  ,
 

a
∂2l
=
2
∂ll 2

(18)

(19)

aa

∂2l
τ 
 βτ 
= na ln β + ψ 7 i + ψ 8 j − ( nπ − nu )   − ( nπ − na ) 
 ,
∂a∂β
 ll



nu
na
2

∂ l
−1 
=
 nu + na − ( nπ − nu )ψ 4 − ( nπ − nu )ψ 6 − ∑ψ 4i − ∑ψ 6 j  ,
∂all
∂
=i 1 =j 1


and
a
a
na
 β yj  
a2 
∂2l
 βτ 
= ( nπ − na ) 
 ,
 + ∑
ll
∂β∂llβ 


j =1 
 


(20)
(21)

(22)

where
α

α
 β yj 
 β yj
 xi 
 xi 
yy
=

 ln 
1i
2j
  ln   ,
 llll

 



α


,


α

τ 
τ 
τ  
τ  
=
=
yy
3
4
  ln   ,
  α ln   + 1 ,
 llll

 
  
  
α

α

x  
x  
 βτ 
 βτ
y 4i=  i  α ln  i  + 1 ,
y 5= 
 ln 



 llll
 
  


,


α

α
 β yj  
 β yj  
 βτ  
 βτ  
=
=
y 6   α ln   + 1 ,
y
α
ln




 + 1 ,
6j
 llll
 

 

 

 
α

α
βy  βy 
 xi   xi 
y 7i =
y8 j =
(α − 1) ln  j  −  j  .
(α − 1) ln   −   ,
 llll
  

 

Now by employing the standard z-based confidence interval formulations,

αˆ ± Zγ

2

I11−1 (αˆ ) , λˆ ± Zγ

2

( )

−1 ˆ
I 22
λ , βˆ ± Zγ

2

( )

I 33−1 βˆ ,

we obtain the confidence intervals for the parameters based on the asymptotic
distribution.

18
The asymptotic confidence interval for the mean life at normal use conditions is given
by

µˆ ± Zγ

2

Var ( µˆ ) ,

where Var ( µˆ ) is obtained using the Delta method.
2.3

THE BOOTSTRAP RESAMPLING METHODS AND THE MONTECARLO PROCEDURE
There are two different methods for generating bootstrap sample data. One is the

parametric bootstrap, where once the parameters of the underlying distribution are
estimated, they are plugged into the assumed distribution and pseudo random numbers
then drawn from this estimated distribution to produce the bootstrap sample. The nonparametric bootstrap does not assume a set underlying distribution, but resample from
the sample data to produce new samples. The resampling procedure, of course, should
be adopted to fit the underlying structure of the problem. For example, in a regression
setting, resampling must be done on the residuals of a fitted model rather than from the
original data.
In the following, we combine the bootstrap steps with the steps needed to carry
out a Monte-Carlo comparison of the proposed methods of building confidence bounds
and intervals. The steps for the parametric bootstrap and the non-parametric bootstrap
are given separately. Note that the confidence bounds and intervals based on the
asymptotic distribution can be computed at each Monte-Carlo simulation sample and
does no require bootstrap resampling.
2.3.1 The Proposed Parametric Bootstrap Method and the Monte-Carlo
Procedure for Studying its Performance.The Monte-Carlo procedure employed to
study the performance of the parametric bootstrap method is described below. The steps
for the parametric bootstrap method for obtaining confidence bounds for α , λ , and β
and lower bounds for the mean life are embedded in this procedure and are given in
italics. Note that distributional parameters are varied in the study as follows:

19
=
(a 1.5, nad 2,=
λ 1,=
β 1.5, and 2) with 𝜇𝜇 = 𝜆𝜆Γ �1 + 1 �. The censoring time was set at
𝛼𝛼

𝜏𝜏=1, and1.5. Note that without loss of generality, scale parameter λ can be set at 1.

(1) For fixed values of n and 𝜋𝜋, generate a random sample 𝑥𝑥𝑖𝑖 , 𝑖𝑖 = 1,2, … , 𝑛𝑛𝜋𝜋 from
the Weibull (𝛼𝛼, 𝜆𝜆) distribution. This would be considered data from the normal
use sample. Similarly, generate the data set 𝑦𝑦𝑗𝑗 , 𝑗𝑗 = 1,2, … , 𝑛𝑛𝑛𝑛, representing the
sample under the high stress condition, from the Weibull (𝛼𝛼, 𝛽𝛽𝛽𝛽) distribution.
(2) Use the ML method to estimate the parameters with the same censoring time τ
used for both samples. In this study, the nonlinear equations of the maximum
likelihood estimates were solved iteratively using the Newton Raphson method.

(3) Employ the resulting estimates of the parameters and acceleration factor to
construct asymptotic confidence limits with confidence level at 𝛾𝛾 = 0.95. Also,
plug-in the MLEs into the Fisher Information matrix to obtain the asymptotic
variance and covariance matrix of the estimators and then use them in the delta
method to compute the lower bound for mean life.
(4) Replace the unknown parameters, 𝛼𝛼, 𝜆𝜆, in the Weibull distribution for the normal
use case with their MLEs, 𝛼𝛼�, 𝜆𝜆̂, and utilize the estimated distribution to generate
a bootstrap sample 𝑥𝑥𝑖𝑖∗ , 𝑖𝑖 = 1,2, … , 𝑛𝑛𝜋𝜋 of size 𝑛𝑛𝜋𝜋. Censor the data based on the
censoring time τ.
(5) Similarly replace the unknown parameters, 𝛼𝛼, 𝜆𝜆, 𝛽𝛽 in the Weibull distribution for
the high stress case with their MLEs, 𝛼𝛼�, 𝜆𝜆̂, 𝛽𝛽̂ and utilize the estimated
distribution to generate a bootstrap sample 𝑦𝑦𝑗𝑗∗ , 𝑗𝑗 = 1,2, … , 𝑛𝑛𝑛𝑛 of size 𝑛𝑛𝑛𝑛. Censor
the data based on the censoring time τ.
(6) Re-estimate the Weibull parameters of the normal use distribution were using
the combined bootstrap samples. Denote the bootstrap sample-based MLEs of
𝛼𝛼, 𝜆𝜆, 𝛽𝛽 𝑎𝑎𝑎𝑎𝑎𝑎 𝜇𝜇 obtained at bootstrap step k by 𝛼𝛼� ∗(𝑘𝑘) , 𝜆𝜆̂∗(𝑘𝑘) , 𝛽𝛽̂ ∗(𝑘𝑘) 𝑎𝑎𝑎𝑎𝑎𝑎 𝜇𝜇̂ ∗(𝑘𝑘)
respectively.
(7) Repeat Steps (4) to (6) 1,000 times. Construct the empirical distributions of the bootstrap
estimates 𝛼𝛼� ∗(𝑘𝑘) , 𝜆𝜆̂∗(𝑘𝑘) , 𝛽𝛽̂ ∗(𝑘𝑘) 𝑎𝑎𝑎𝑎𝑎𝑎 𝜇𝜇̂ ∗(𝑘𝑘) , k=1, 2, …, 1,000

(8) Use the empirical distributions obtained from bootstrap estimates to construct,
1−𝛾𝛾
1−𝛾𝛾
confidence interval for 𝛼𝛼, 𝜆𝜆, 𝛽𝛽 using quantiles at � �100% and 1 − � �100% of the
2
2
respective empirical distribution as the lower and upper bounds respectively. Use
the (1 − 𝛾𝛾)100% quantile of the empirical distribution of 𝜇𝜇̂ ∗(𝑘𝑘) , k=1, 2, …, 1,000, as
the lower bound for 𝜇𝜇.

(9) Repeat steps (1) through (8) 1,000 times and compute the average number of
times each parameter fell within the bound(s). This would yield an estimate of the

20
expected coverage for each interval. For each parameter except 𝜇𝜇, the widths of
the two sided interval computed in Steps (3) and (8) are averaged to obtained an
estimate of the expected, width.
2.3.2

The Proposed Nonparametric Bootstrap Method and the Monte-

Carlo Procedure for Studying its Performance.The Monte-Carlo procedure employed
to study the performance of the nonparametric bootstrap method is described below. The
steps for the parametric bootstrap method for obtaining confidence bounds for
𝜶𝜶, 𝝀𝝀, 𝒂𝒂𝒂𝒂𝒂𝒂 𝜷𝜷 and lower binds for the mean life are imbedded in this procedure and given

in italics.

(1) For fixed values of n and 𝜋𝜋, generate a random sample 𝑥𝑥𝑖𝑖 , 𝑖𝑖 = 1,2, … , 𝑛𝑛𝜋𝜋 from the
Weibull (𝛼𝛼, 𝜆𝜆) distribution. This would be considered data from the normal use
sample. Similarly, generate the data set 𝑦𝑦𝑗𝑗 , 𝑗𝑗 = 1,2, … , 𝑛𝑛𝑛𝑛, representing the sample
under the high stress condition, from the Weibull (𝛼𝛼, 𝛽𝛽𝛽𝛽) distribution.

(2) Obtain a bootstrap resample from each of the two samples generated in Step (1)
above, with each bootstrap sample of size π n (or π n ) obtained by sampling with
replacement from the respective sample obtained in (1).

(3) New “bootstrap estimates” 𝛼𝛼� ∗ , 𝜆𝜆̂∗ , 𝑎𝑎𝑎𝑎𝑎𝑎 𝛽𝛽̂∗ are computed from the combined
bootstrap sample using the ML method as described in Step (2) given in Section
2.3.1. Also estimate the mean life µ under normal conditions, accounting for the
censoring.
(4) Repeat the process given in Steps (2) and (3) 1,000 times and obtain the empirical
�𝛽𝛽∗ ..
distributions of 𝛼𝛼� ∗ , 𝜆𝜆̂∗ , 𝑎𝑎𝑎𝑎𝑎𝑎

(5) Using the empirical distributions of the 𝛼𝛼� ∗ , 𝜆𝜆̂∗ , 𝑎𝑎𝑎𝑎𝑎𝑎 𝛽𝛽̂∗ obtained from bootstrap
estimates, construct confidence interval for 𝛼𝛼, 𝜆𝜆, 𝑎𝑎𝑎𝑎𝑎𝑎 𝛽𝛽 using respective quantiles
1−𝛾𝛾
1−𝛾𝛾
at � �100% and 1 − � �100%.
2

2

(6) Using the empirical distributions of the mean 𝜇𝜇̂ ∗ obtained from bootstrap
estimates, construct the lower confidence bound for 𝜇𝜇 is using quantile at
(1 − 𝛾𝛾)100% .
(7) Coverage probabilities were computed based on 1,000 simulation runs by
repeating Steps (1) – (6).

21

Population Or
Process

𝐹𝐹(𝑡𝑡, 𝜃𝜃�)

Actual Sample Data From
Population Or Process
Used to estimate model
parameters

DATA
…………

𝐹𝐹(𝑡𝑡, 𝜃𝜃� )

Simulated censored samples
from F�t, θ��
Draw 1000 samples, each of
size n

𝐷𝐷𝐷𝐷𝐷𝐷𝐴𝐴1∗
………
𝜃𝜃�1∗

𝐷𝐷𝐷𝐷𝐷𝐷𝐴𝐴∗2
………
𝜃𝜃�2∗

𝐷𝐷𝐷𝐷𝐷𝐷𝐴𝐴∗𝐵𝐵
………
𝜃𝜃�𝐵𝐵∗

Figure 2.1.Illustrates the parametric bootstrap resampling method

22

Population Or
Process

𝐹𝐹𝐹𝐹(𝑡𝑡, 𝜃𝜃�)

inference.
Actual Sample Data
From Population Or
Process
Used to estimate model
parameters

DATA
…………

𝜃𝜃�

Simulated censored
samples from F�t, θ��
Draw 1000 samples,
each of size n

𝐷𝐷𝐷𝐷𝐷𝐷𝐴𝐴1∗
………
𝜃𝜃�1∗

𝐷𝐷𝐷𝐷𝐷𝐷𝐴𝐴∗2
………
𝜃𝜃�2∗

𝐷𝐷𝐷𝐷𝐷𝐷𝐴𝐴∗𝐵𝐵
………
𝜃𝜃�𝐵𝐵∗

Figure 2.2. Illustrates the nonparametric bootstrap resampling for parametric
inference.

23

2.4

MONTE-CARLO SIMULATION RESULTS AND DISCUSSION

All simulations results reported here are for α = (1.5, and 2) and λ = 1,

with the

acceleration factor 𝛽𝛽 set at 1.5 and 2.0. The censoring parameter 𝜏𝜏 was set at values 1,
and 1.5

The simulation study was conducted using a computer code written in Matlab, and the
simulation results are reported in Table 2.1a to Table 2.31. Tables 2.1a and 2.1b show
the results of the maximum likelihood estimation of (a , λ , β , and µ ) . The estimated
expected values of the MLEs are reasonably close to the true values, even for n=30.
There is no discernible pattern linking the means of the estimates to changes in the
parameter values, at least over the range of parameter values considered in this study.
Tables 2.2 to 2.31 show the performance of the asymptotic, parametric bootstrap, and
nonparametric bootstrap confidence intervals for (a , λ , and β ) at the 95% confidence
level and the performance of the Asymptotic, Parametric Bootstrap, and Nonparametric
Bootstrap based 95% confidence bound of mean-life under normal conditions.

Table 2.1a Weibull Parameters, Acceleration Factor, and Type I Censoring
π

0.5

τ

α

λ

β

μ

n

α
�

λ�

β�

𝜇𝜇̂

1.516009
1.499799
1.531349
1.522719

0.886497
0.890782
0.919195
0.911857

30
50
1.0 1.5 1.0 1.5 0.9027
75
100

1.576672
1.553463
1.543858
1.537005

1.043488 1.54321 0.949033
1.014072 1.565054 0.920318
0.983789 1.485776 0.891226
1.015341 1.539113 0.918978

30
50
1.5 1.5 1.0 1.5 0.9027
75
100

1.615358
1.564676
1.538481
1.518111

0.981995
0.986138
1.016195
1.008931

24
Table 2.1b Weibull Parameters, Acceleration Factor, and Type I Censoring
π

τ

α

λ

β

μ

1.0 2.0 1.0 1.5 0.88623

n

�
α

λ�

β�

𝜇𝜇̂

30

2.10566

1.005542 1.526783 0.894765

50

2.069241 1.015067 1.528142 0.901825

75

2.03491

1.000411 1.505851 0.887366

100 2.045635 1.009265 1.516721 0.895606

0.5
1.5 2.0 1.0 1.5 0.88623

30

2.144481 0.996427 1.516204 0.885228

50

2.082549 1.005506 1.521205 0.892661

75

2.056499 1.003315

1.51703

0.890216

100 2.035133 0.995743 1.498669 0.883108

1.0 2.0 1.0 1.5 0.88623

30

2.097268 1.000108 1.491671 0.889862

50

2.071283

75

2.059496 1.009989 1.524947 0.896732

1.0045

1.505745 0.892569

100 2.032169 1.002487 1.508073 0.889318

0.667
1.5 2.0 1.0 1.5 0.88623

30

2.056806 0.969927 1.431312 0.861615

50

2.064046 0.992657 1.496456 0.881282

75

2.031231 0.988086 1.474673 0.877013

100 2.021136 0.998281 1.501615 0.885341

0.5

0.667

30 2.08247 1.007274 2.033726 0.894476
50 2.060175 1.009152 2.025427 0.930197
1.0 2.0 1.0 2.0 0.88623
75 2.043235 1.00704 2.02148 0.893325
100 2.030411 1.001163 2.005237 0.888148
30
50
1.5 2.0 1.0 2.0 0.88623
75
100

2.088419 1.002052 2.025996 0.889443
2.056056 1.003584 2.012276 0.890461
2.042821 1.00153 2.015177 0.888306
2.036468 1.00281 2.011049 0.889302

30
50
1.0 2.0 1.0 2.0 0.88623
75
100

2.032135
2.034193
2.042055
2.037142

30
50
1.5 2.0 1.0 2.0 0.88623
75
100

1.970186 0.971658 1.919988 0.863211
1.980244 0.98249 1.942167 0.872111
2.035332 0.99344 1.990141 0.881122
2.023208 1.001917 2.010083 0.888552

0.993783 1.97212 0.882527
1.004697 1.992329 0.89193
1.000766 2.005288 0.887732
1.005378 2.025448 0.891691

25
Table 2.2 Coverage of Asymptotic 95% C.I.s
α =1.5, λ =1, β =1.5, µ=0.9027, π=.5, τ=1
n parameter Lower Bound Upper Bound
width
1.058891
2.297576
1.238686
α
0.737031
1.535213
0.798183
λ
30
1.04402
2.459394
1.415374
β
μ
0.711167
1.042634
2.174402
1.131768
α
0.654653
1.411619
0.756966
λ
50
1.068087
2.288784
1.220697
β
μ
0.658812
1.088779
2.131125
1.042346
α
0.762073
1.275386
0.513312
λ
75
1.075537
2.114673
1.039136
β
μ
0.711217
1.147291
2.019342
0.872052
α
0.78837
1.22566
0.437289
λ
100
1.183164
2.010407
0.827243
β
μ
0.731194

SD(W) Coverage
0.207054
0.967
0.186898
0.956
0.350738
0.969
0.954
0.172591
0.966
0.104981
0.953
0.212695
0.967
0.959
0.141036
0.965
0.069603
0.948
0.184005
0.965
0.9554
0.130529
0.960
0.041496
0.945
0.105109
0.957
0.952

Table 2.3 Coverage of Parametric Bootstrap 95% C.I.s
α =1.5,
n

λ =1, β =1.5, µ=0.9027, π=.5,

τ=1

parameter Lower Bound Upper Bound
1.131354
2.141941
α
0.754875
1.532676
λ
30
1.059377
2.344477
β
μ
0.717816
1.085333
2.017417
α
0.663783
1.426243
λ
50
1.097772
2.200414
β
μ
0.663387
1.143289
1.985733
α
0.777822
1.262373
λ
75
1.109645
2.089899
β
μ
0.722811
1.210594
1.918238
α
0.796675
1.219792
λ
100
1.212475
1.915997
β
μ
0.734488

width
SD(W) Coverage
1.010587 0.135168
0.964
0.777801 0.166295
0.954
1.2851 0.259647
0.968
0.953
0.932085 0.108718
0.962
0.762459 0.090188
0.953
1.102642 0.166013
0.966
0.958
0.842444 0.112366
0.959
0.484551 0.062424
0.947
0.980254 0.140481
0.964
0.953
0.707644 0.091796
0.952
0.423118 0.034572
0.945
0.703522 0.084472
0.952
0.952

26
Table 2.4 Coverage of Nonparametric Bootstrap 95% C.I.s
α =1.5,

λ =1, β =1.5, µ=0.9027, π=.5,

τ=1

n

parameter Lower Bound Upper Bound
1.094575
2.17102
α
0.746554
1.786962
λ
30
1.10094
2.802566
β
μ
0.681494
1.194322
2.240323
α
0.797266
1.480086
λ
50
1.119282
2.155023
β
μ
0.73212
1.125589
2.033443
α
0.743859
1.489184
λ
75
1.191063
2.113575
β
μ
0.712294
1.272253
1.826239
α
0.762935
1.290845
λ
100
1.058365
1.852788
β
μ
0.711974

width
SD(W) Coverage
0.966
1.076446 0.172978
0.965
1.040408 0.252883
0.971
1.701625 0.307192
0.956
0.965
1.046002 0.124307
0.951
0.682821 0.165115
0.965
1.035741 0.269971
0.952
0.961
0.907854 0.130005
0.953
0.745326 0.03947
0.962
0.922512 0.101411
0.954
0.950
0.553986 0.095875
0.949
0.527909 0.071676
0.956
0.794424 0.195381
0.954

Table 2.5 Coverage of Asymptotic 95% C.I.s
α =1.5,
n

λ =1,

β =1.5,

µ=0.9027,

π=.5,

τ=1.5

parameter Lower Bound Upper Bound
width
SD(W) Coverage
0.975
1.033085
2.426617 1.393531 0.180204
α
0.961
0.678737
1.372683 0.693945 0.073595
λ
30
0.972
1.005162
2.32126 1.316098 0.206278
β
μ
0.655996
0.959
0.971
1.14241
2.2658 1.12339 0.129082
α
0.960
0.746816
1.404307 0.657491 0.074644
λ
50
0.970
1.029431
2.096434 1.067004 0.100187
β
μ
0.710759
0.955
0.967
1.159242
2.022276 0.863035
0.1017
α
0.956
0.745781
1.29498
0.549199
0.109312
λ
75
0.970
1.125778
2.184085 1.058307 0.090719
β
μ
0.704224
0.955
0.960
1.231624
1.906322 0.674698 0.125054
α
0.949
0.822826
1.223822 0.400996 0.076077
λ
100
0.962
1.204736
1.944556 0.73982 0.078233
β
μ
0.770035
0.952

27
Table 2.6 Coverage of Parametric Bootstrap 95% C.I.s
α =1.5,

λ =1,

β =1.5,

µ=0.9027,

π=.5,

τ=1.5

n

parameter Lower Bound Upper Bound
1.044993
2.156078
α
0.774367
1.385444
λ
30
1.065791
2.195218
β
μ
0.690502
1.20184
2.135915
α
0.739619
1.346937
λ
50
1.188012
2.0868
β
μ
0.726995
1.198535
1.904932
α
0.788263
1.272586
λ
75
1.075457
1.957574
β
μ
0.711617
1.279874
1.820302
α
0.85188
1.214498
λ
100
1.286863
1.823956
β
μ
0.788831

width
SD(W) Coverage
1.111085 0.1244
0.971
0.611076 0.052272
0.957
1.129427 0.126623
0.971
0.957
0.934076 0.078184
0.969
0.607318 0.068972
0.957
0.898788 0.061109
0.968
0.953
0.706397 0.041833
0.961
0.484323 0.126915
0.953
0.882117 0.093547
0.968
0.955
0.540428 0.06384
0.956
0.362618 0.063717
0.947
0.537092 0.060072
0.955
0.951

Table 2.7 Coverage of Nonparametric Bootstrap 95% C.I.s
α =1.5,
n

λ =1,

β =1.5,

µ=0.9027,

π=.5,

τ=1.5

parameter Lower Bound Upper Bound
1.181807
2.249394
α
0.685653
1.327392
λ
30
1.04436
2.16286
β
μ
0.677046
1.178017
1.970022
α
0.755765
1.348221
λ
50
1.015456
2.094452
β
μ
0.70394
1.215212
1.968906
α
0.800374
1.273839
λ
75
1.166294
2.090804
β
μ
0.713027
1.287767
1.806507
α
0.780335
1.236837
λ
100
1.189513
1.91431
β
μ
0.731749

width
SD(W) Coverage
1.067588 0.209897
0.970
0.641739 0.094274
0.958
1.1185 0.144993
0.971
0.958
0.792005 0.138289
0.964
0.592456 0.108979
0.957
1.078996 0.191347
0.970
0.956
0.753694 0.106314
0.963
0.473465 0.0537
0.953
0.92451 0.11965
0.969
0.954
0.51874 0.097141
0.954
0.456501 0.072061
0.951
0.724797 0.081611
0.962
0.953

28
Table 2.8 Coverage of Asymptotic 95% C.I.s
α =2,

λ =1, β =1.5, µ=0.88623, π=.5,

τ=1

n

parameter Lower Bound Upper Bound
1.428128
2.939773
α
0.770204
1.174486
λ
30
1.108281
1.986048
β
μ
0.710548
1.481771
2.689149
α
0.804323
1.154149
λ
50
1.163985
1.86008
β
μ
0.73394
1.569334
2.524061
α
0.832946
1.08939
λ
75
1.215491
1.788695
β
μ
0.756085
1.615016
2.469886
α
0.828196
1.060944
λ
100
1.219649
1.726205
β
μ
0.752861

width
SD(W) Coverage
1.511645 0.060395
0.978
0.404282 0.031394
0.971
0.877767 0.066968
0.967
0.973
1.207378 0.086403
0.963
0.349826 0.01449
0.963
0.696095 0.035691
0.958
0.966
0.954727 0.029612
0.958
0.256444 0.016714
0.953
0.573204 0.043942
0.951
0.957
0.85487 0.051646
0.952
0.232748 0.011203
0.949
0.506556 0.02449
0.948
0.958

Table 2.9 Coverage of Parametric Bootstrap 95% C.I.s
α =2,
n

λ =1, β =1.5, µ=0.88623, π=.5, τ=1

parameter Lower Bound Upper Bound
1.528454
2.785755
α
0.798505
1.260899
λ
30
1.15374
2.000944
β
μ
0.736512
1.611724
2.595878
α
0.833037
1.237362
λ
50
1.200454
1.871415
β
μ
0.757589
1.658296
2.478763
α
0.860875
1.177373
λ
75
1.256715
1.828712
β
μ
0.779962
1.707784
2.424014
α
0.856417
1.148437
λ
100
1.264424
1.766898
β
μ
0.775597

width
SD(W) Coverage
1.257301 0.077077
0.971
0.462394 0.030082
0.975
0.847204 0.077483
0.964
0.961
0.984154 0.101286
0.958
0.404325 0.008988
0.971
0.670961 0.044053
0.959
0.957
0.820467 0.047923
0.956
0.316498 0.010782
0.955
0.571997 0.047047
0.951
0.949
0.71623 0.038172
0.949
0.29202 0.00501
0.953
0.502474 0.023945
0.947
0.949

29
Table 2.10 Coverage of Nonparametric Bootstrap 95% C.I.s
α =2,

λ =1, β =1.5, µ=0.88623, π=.5,

τ=1

n

parameter Lower Bound Upper Bound
1.529158
2.651451
α
0.783752
1.220464
λ
30
1.125854
1.898379
β
μ
0.715692
1.558791
2.627216
α
0.795752
1.148941
λ
50
1.151173
1.810551
β
μ
0.72729
1.62623
2.416409
α
0.819761
1.118779
λ
75
1.251547
1.776176
β
μ
0.75656
1.620668
2.34255
α
0.841021
1.141333
λ
100
1.232678
1.753693
β
μ
0.763656

width
SD(W) Coverage
1.122293 0.098632
0.965
0.436712 0.027064
0.973
0.772525 0.056806
0.961
0.973
1.068425 0.095078
0.961
0.353189 0.037386
0.964
0.659378 0.056138
0.953
0.967
0.790179 0.051491
0.953
0.299018 0.02911
0.954
0.524629 0.035295
0.949
0.957
0.721882 0.062642
0.949
0.300312 0.025753
0.954
0.521015 0.039505
0.949
0.951

Table 2.11 Coverage of Asymptotic 95% C.I.s
α =2,
n

λ =1, β =1.5, µ=0.88623, π=.5,

τ=1.5

parameter Lower Bound Upper Bound
1.520019
2.922929
α
0.756544
1.245536
λ
30
1.09424
2.028082
β
μ
0.707699
1.559902
2.690285
α
0.802351
1.227825
λ
50
1.16077
1.982451
β
μ
0.738667
1.631716
2.557852
α
0.831179
1.170676
λ
75
1.216213
1.86435
β
μ
0.764202
1.646833
2.521604
α
0.850687
1.157054
λ
100
1.243681
1.82187
β
μ
0.775236

width
SD(W) Coverage
1.40291 0.035159
0.979
0.488992 0.02013
0.978
0.933842 0.039966
0.978
0.969
1.130383 0.033658
0.971
0.425474 0.022293
0.966
0.821681 0.02153
0.967
0.967
0.926136 0.04647
0.966
0.339497 0.004215
0.955
0.648137 0.014805
0.957
0.959
0.874771 0.035506
0.957
0.306367 0.000251
0.953
0.578189 0.014259
0.954
0.957

30
Table 2.12 Coverage of Parametric Bootstrap 95% C.I.s
α =2,

λ =1, β =1.5, µ=0.88623, π=.5,

τ=1.5

n

parameter Lower Bound Upper Bound
1.56777
2.697066
α
0.781595
1.229183
λ
30
1.145747
1.921492
β
μ
0.728119
1.620587
2.521198
α
0.833704
1.218953
λ
50
1.217152
1.90776
β
μ
0.759754
1.670304
2.429366
α
0.85118
1.159789
λ
75
1.258623
1.776935
β
μ
0.781167
1.697873
2.407773
α
0.870352
1.141185
λ
100
1.293153
1.746251
β
μ
0.784996

width
SD(W) Coverage
1.129296 0.070908
0.971
0.447588 0.031947
0.967
0.775745 0.062558
0.964
0.967
0.900611 0.055174
0.964
0.385249 0.031161
0.959
0.690608 0.045046
0.959
0.962
0.759062 0.062678
0.952
0.308609 0.014821
0.953
0.518312 0.023148
0.952
0.955
0.7099 0.053035
0.948
0.270833 0.006514
0.947
0.453098 0.028371
0.946
0.955

Table 2.13 Coverage of Nonparametric Bootstrap 95% C.I.s
α =2,
n

λ =1, β =1.5, µ=0.88623, π=.5,

τ=1.5

parameter Lower Bound Upper Bound
1.606009
2.720998
α
0.794537
1.225881
λ
30
1.168365
1.905797
β
μ
0.735065
1.654509
2.564973
α
0.843868
1.19489
λ
50
1.245075
1.837008
β
μ
0.76032
1.665246
2.443821
α
0.864866
1.16557
λ
75
1.291665
1.780783
β
μ
0.785837
1.689984
2.363461
α
0.874408
1.141715
λ
100
1.287561
1.769521
β
μ
0.796818

width
SD(W) Coverage
1.114989 0.063759
0.970
0.431344 0.026063
0.966
0.737432 0.046977
0.963
0.967
0.910464 0.051808
0.965
0.351022 0.0103
0.957
0.591933 0.035625
0.955
0.961
0.778575 0.055733
0.953
0.300704 0.008555
0.952
0.489118 0.031462
0.948
0.954
0.673477 0.051352
0.943
0.267307 0.010536
0.947
0.48196 0.01726
0.948
0.951

31
Table 2.14 Coverage of Asymptotic 95% C.I.s
α =2,

λ =1, β =1.5, µ=0.88623, π=.667,

τ=1

n

parameter Lower Bound Upper Bound
1.469722
2.896838
α
0.738007
1.349913
λ
30
1.093268
2.083072
β
μ
0.688976
1.492544
2.789179
α
0.771562
1.320843
λ
50
1.136512
2.045376
β
μ
0.713673
1.586767
2.612116
α
0.826597
1.22184
λ
75
1.201475
1.882793
β
μ
0.754699
1.578048
2.545202
α
0.826024
1.216026
λ
100
1.217061
1.897053
β
μ
0.75678

width
SD(W) Coverage
1.427116 0.0385
0.978
0.611906 0.020207
0.978
0.989803 0.057317
0.975
0.967
1.296636 0.052213
0.971
0.549281 0.036053
0.964
0.908865 0.042843
0.975
0.963
1.025349 0.028957
0.962
0.395243 0.026544
0.957
0.681319 0.039577
0.953
0.957
0.967155 0.030645
0.958
0.390002 0.020137
0.956
0.679992 0.037355
0.951
0.955

Table 2.15 Coverage of Parametric Bootstrap 95% C.I.s
α =2,
n

λ =1, β =1.5, µ=0.88623, π=.667,

τ=1

parameter Lower Bound Upper Bound
1.542738
2.695054
α
0.754464
1.35406
λ
30
1.123545
2.012949
β
μ
0.694204
1.566228
2.637608
α
0.784254
1.319521
λ
50
1.14788
1.988201
β
μ
0.718012
1.642439
2.537224
α
0.831346
1.217517
λ
75
1.220708
1.848062
β
μ
0.759773
1.627974
2.437957
α
0.836019
1.218105
λ
100
1.240228
1.865566
β
μ
0.762306

width
SD(W) Coverage
1.152316 0.071666
0.967
0.599596 0.025528
0.975
0.889404 0.071674
0.974
0.965
1.07138 0.073092
0.963
0.535268 0.041696
0.962
0.840321 0.061033
0.964
0.962
0.894786 0.044157
0.954
0.386171 0.030114
0.954
0.627354 0.052174
0.949
0.954
0.809983 0.048325
0.951
0.382086 0.024322
0.953
0.625338 0.051281
0.948
0.953

32
Table 2.16 Coverage of Nonparametric Bootstrap 95% C.I.s
α =2,

λ =1, β =1.5, µ=0.88623, π=.667,

τ=1

n

parameter Lower Bound Upper Bound
1.506926
2.678344
α
0.747277
1.340167
λ
30
1.101348
1.982783
β
μ
0.685901
1.576145
2.52648
α
0.771919
1.302267
λ
50
1.158171
1.992152
β
μ
0.71257
1.656934
2.418502
α
0.825347
1.25867
λ
75
1.224955
1.926176
β
μ
0.750564
1.679754
2.412073
α
0.831369
1.196916
λ
100
1.235504
1.828671
β
μ
0.765924

width
SD(W) Coverage
1.171419 0.089682
0.969
0.59289 0.052143
0.974
0.881435 0.061581
0.973
0.969
0.950336 0.088759
0.957
0.530348 0.063096
0.962
0.833981 0.054018
0.964
0.963
0.761568 0.033334
0.949
0.433323 0.02956
0.959
0.701221 0.030402
0.954
0.959
0.732319 0.076219
0.947
0.365547 0.029107
0.951
0.593167 0.041163
0.945
0.951

Table 2.17 Coverage of Asymptotic 95% C.I.s
α =2,
n

λ =1, β =1.5, µ=0.88623, π=.667,

τ=1.5

parameter Lower Bound Upper Bound
1.511932
2.745184
α
0.71954
1.270625
λ
30
1.052062
1.991453
β
μ
0.6759
1.561383
2.630201
α
0.746884
1.224853
λ
50
1.094703
1.887362
β
μ
0.695307
1.647156
2.589251
α
0.821089
1.201453
λ
75
1.207406
1.860281
β
μ
0.757438
1.654901
2.494256
α
0.821951
1.195714
λ
100
1.218479
1.850327
β
μ
0.753369

width
SD(W) Coverage
1.233251 0.027192
0.978
0.551085 0.021859
0.969
0.939392 0.031368
0.974
0.969
1.068818 0.027701
0.966
0.477969 0.028403
0.967
0.792659 0.035397
0.962
0.967
0.942095 0.035601
0.959
0.380365 0.018444
0.957
0.652875 0.015337
0.956
0.955
0.839355 0.03133
0.954
0.373763 0.019084
0.957
0.631848 0.016362
0.954
0.957

33
Table 2.18 Coverage of Parametric Bootstrap 95% C.I.s
α =2,

λ =1, β =1.5, µ=0.88623, π=.667,

τ=1.5

n

parameter Lower Bound Upper Bound
1.558948
2.582157
α
0.759619
1.252893
λ
30
1.120572
1.905564
β
μ
0.704306
1.628638
2.473594
α
0.772152
1.208786
λ
50
1.155274
1.799511
β
μ
0.717311
1.707015
2.490781
α
0.845906
1.189656
λ
75
1.261705
1.799855
β
μ
0.780221
1.689858
2.378117
α
0.843054
1.185081
λ
100
1.266483
1.791549
β
μ
0.772328

width
SD(W) Coverage
1.023209 0.056101
0.965
0.493274 0.031319
0.967
0.784991 0.048903
0.962
0.966
0.844956 0.044464
0.955
0.436634 0.038431
0.961
0.644237 0.053251
0.955
0.962
0.783766 0.050419
0.952
0.34375 0.028677
0.954
0.53815 0.032134
0.951
0.951
0.688259 0.052034
0.947
0.342027 0.020943
0.954
0.525066 0.02699
0.949
0.953

Table 2.19 Coverage of Nonparametric Bootstrap 95% C.I.s
α =2,
n

λ =1, β =1.5, µ=0.88623, π=.667,

τ=1.5

parameter Lower Bound Upper Bound
1.601104
2.628303
α
0.772562
1.257633
λ
30
1.1283
1.898682
β
μ
0.715079
1.646487
2.515288
α
0.785353
1.204297
λ
50
1.154279
1.85254
β
μ
0.716794
1.675806
2.411131
α
0.820617
1.184242
λ
75
1.224426
1.771015
β
μ
0.747658
1.687046
2.394657
α
0.83771
1.162611
λ
100
1.259962
1.755438
β
μ
0.765106

width
SD(W) Coverage
1.027199 0.040553
0.965
0.48507 0.043927
0.967
0.770382 0.057681
0.961
0.963
0.8688 0.077675
0.957
0.418944 0.03367
0.959
0.698261 0.066559
0.957
0.963
0.735324 0.052411
0.951
0.363625 0.033534
0.956
0.54659 0.039734
0.951
0.959
0.70761 0.050762
0.949
0.324901 0.017601
0.951
0.495477 0.027759
0.947
0.954

34
Table 2.20 Coverage of Asymptotic 95% C.I.s
α =2,

λ =1, β =2, µ=0.88623, π=.5,

τ=1

n

parameter Lower Bound Upper Bound
1.483977
2.964364
α
0.760693
1.289926
λ
30
1.452246
2.822865
β
μ
0.703819
1.541615
2.795729
α
0.815671
1.292058
λ
50
1.529307
2.718522
β
μ
0.75032
1.605842
2.614159
α
0.835775
1.203938
λ
75
1.620502
2.561225
β
μ
0.764811
1.63589
2.569063
α
0.852459
1.192569
λ
100
1.645568
2.488622
β
μ
0.779406

width
SD(W) Coverage
1.480387 0.07662
0.971
0.529233 0.02404
0.969
1.370619 0.059191
0.974
0.969
1.254114 0.046893
0.969
0.476386 0.034501
0.961
1.189215 0.039204
0.963
0.963
1.008317 0.041621
0.958
0.368163 0.032678
0.955
0.940722 0.032304
0.955
0.959
0.933173 0.031881
0.956
0.34011 0.011751
0.952
0.843054 0.03875
0.953
0.954

Table 2.21 Coverage of Parametric Bootstrap 95% C.I.s
α =2, λ =1, β =2, µ=0.88623, π=.5,
n

τ=1

parameter Lower Bound Upper Bound
1.573883
2.778111
α
0.772769
1.29223
λ
30
1.494966
2.659774
β
μ
0.707184
1.523706
2.607722
α
0.785877
1.296655
λ
50
1.550444
2.614866
β
μ
0.791947
1.661268
2.491358
α
0.838859
1.205743
λ
75
1.663064
2.463082
β
μ
0.770055
1.696021
2.442705
α
0.856891
1.192207
λ
100
1.691015
2.419798
β
μ
0.782631

width
SD(W) Coverage
1.204229 0.10896
0.968
0.519461 0.028304
0.967
1.164807 0.074735
0.962
0.969
1.084016 0.234415
0.963
0.510777 0.126723
0.966
1.064422 0.109566
0.957
0.952
0.830089 0.067737
0.952
0.366885 0.038591
0.955
0.800017 0.050784
0.949
0.956
0.746684 0.042784
0.947
0.335317 0.018008
0.951
0.728783 0.05458
0.946
0.953

35
Table 2.22 Coverage of Nonparametric Bootstrap 95% C.I.s
α =2, λ =1, β =2, µ=0.88623, π=.5,

τ=1

n

parameter Lower Bound Upper Bound
1.598029
2.799223
α
0.797661
1.31236
λ
30
1.561397
2.669822
β
μ
0.738896
1.582964
2.624709
α
0.818457
1.280681
λ
50
1.610014
2.661945
β
μ
0.759059
1.697661
2.482775
α
0.847026
1.209156
λ
75
1.653746
2.463097
β
μ
0.769909
1.685963
2.455716
α
0.865267
1.186219
λ
100
1.69385
2.436623
β
μ
0.778681

width
SD(W) Coverage
1.201194 0.065988
0.967
0.514699 0.046478
0.967
1.108425 0.095103
0.961
0.966
1.041745 0.086998
0.959
0.462224 0.042453
0.959
1.051931 0.102445
0.956
0.962
0.785114 0.073166
0.951
0.36213 0.024926
0.954
0.80935 0.060862
0.951
0.957
0.769753 0.067814
0.949
0.320952 0.033396
0.949
0.742773 0.064615
0.948
0.955

Table 2.23 Coverage of Asymptotic 95% C.I.s
α =2,
n

λ =1, β =2, µ=0.88623, π=.5,

τ=1.5

parameter Lower Bound Upper Bound
1.519411
2.909902
α
0.774595
1.217566
λ
30
1.518904
2.700644
β
μ
0.721357
1.594864
2.727219
α
0.81636
1.212945
λ
50
1.555829
2.612693
β
μ
0.751918
1.633685
2.583715
α
0.837753
1.168198
λ
75
1.628016
2.476788
β
μ
0.76741
1.666533
2.507369
α
0.853058
1.160286
λ
100
1.644257
2.446682
β
μ
0.778088

width
SD(W) Coverage
1.390492 0.061313
0.971
0.442971 0.024039
0.969
1.181739 0.041725
0.971
0.969
1.132355 0.051139
0.968
0.396586 0.023425
0.965
1.056864 0.054011
0.965
0.963
0.95003 0.050887
0.963
0.330445 0.015659
0.959
0.848772 0.033739
0.957
0.961
0.840836 0.025476
0.956
0.307228 0.017121
0.954
0.802426 0.018657
0.954
0.956

36
Table 2.24 Coverage of Parametric Bootstrap 95% C.I.s
α =2, λ =1, β =2, µ=0.88623, π=.5,

τ=1.5

n

parameter Lower Bound Upper Bound
1.598659
2.707797
α
0.802635
1.193767
λ
30
1.632064
2.511551
β
μ
0.748624
1.658097
2.545814
α
0.841991
1.200204
λ
50
1.647926
2.457365
β
μ
0.7662
1.683376
2.469675
α
0.862218
1.153746
λ
75
1.69323
2.360561
β
μ
0.780868
1.729485
2.393175
α
0.87499
1.141151
λ
100
1.720341
2.341989
β
μ
0.793004

width
SD(W) Coverage
1.109138 0.092883
0.967
0.391133 0.029194
0.965
0.879487 0.056993
0.959
0.965
0.887717 0.069414
0.958
0.358213 0.032026
0.962
0.809439 0.080083
0.955
0.962
0.786299 0.076359
0.953
0.291528 0.020141
0.951
0.667331 0.05416
0.949
0.954
0.66369 0.045071
0.949
0.266161 0.023524
0.949
0.621648 0.042074
0.949
0.951

Table 2.25 Coverage of Nonparametric Bootstrap 95% C.I.s
α =2, λ =1, β =2, µ=0.88623, π=.5,
n

τ=1.5

parameter Lower Bound Upper Bound
1.575055
2.729891
α
0.80811
1.246761
λ
30
1.59222
2.561914
β
μ
0.739658
1.679571
2.574616
α
0.846712
1.207035
λ
50
1.672726
2.480116
β
μ
0.771552
1.699384
2.426089
α
0.847472
1.1556
λ
75
1.68427
2.42793
β
μ
0.780956
1.6863
2.41247
α
0.863844
1.16041
λ
100
1.703262
2.339657
β
μ
0.784534

width
SD(W) Coverage
1.154836 0.089533
0.969
0.438652 0.029353
0.967
0.969694 0.07735
0.962
0.968
0.895045 0.061833
0.959
0.360323 0.024383
0.963
0.80739 0.055024
0.955
0.958
0.726705 0.074618
0.951
0.308127 0.019242
0.954
0.74366 0.055335
0.951
0.954
0.726169 0.056313
0.951
0.296566 0.023083
0.952
0.636396 0.060274
0.947
0.952

37
Table 2.26 Coverage of Asymptotic 95% C.I.s
α =2,

λ =1, β =2, µ=0.88623, π=.667,

τ=1

n

parameter Lower Bound Upper Bound
1.501868
2.772073
α
0.732235
1.290081
λ
30
1.399317
2.77564
β
μ
0.68061
1.537503
2.674505
α
0.758493
1.270095
λ
50
1.458741
2.702765
β
μ
0.704273
1.634756
2.589415
α
0.803353
1.233541
λ
75
1.58919
2.556285
β
μ
0.742476
1.635246
2.518069
α
0.832208
1.204684
λ
100
1.638842
2.516205
β
μ
0.760237

width
SD(W) Coverage
1.270204 0.038875
0.971
0.557847 0.049982
0.967
1.376323 0.094227
0.971
0.969
1.137002 0.056834
0.969
0.511602 0.026488
0.963
1.244024 0.061638
0.962
0.962
0.954659 0.027532
0.961
0.430188 0.024894
0.954
0.967094 0.048037
0.954
0.955
0.882823 0.036994
0.956
0.372476 0.033863
0.951
0.877364 0.057793
0.951
0.949

Table 2.27 Coverage of Parametric Bootstrap 95% C.I.s
α =2, λ =1, β =2, µ=0.88623, π=.667,
n

τ=1

parameter Lower Bound Upper Bound
1.584251
2.566311
α
0.746065
1.292176
λ
30
1.449592
2.649389
β
μ
0.687468
1.616916
2.536815
α
0.766718
1.266552
λ
50
1.492434
2.609861
β
μ
0.70979
1.706257
2.496656
α
0.809151
1.239306
λ
75
1.629579
2.480842
β
μ
0.749622
1.683559
2.402323
α
0.839364
1.20608
λ
100
1.686397
2.448049
β
μ
0.764513

width
SD(W) Coverage
0.98206 0.07286
0.964
0.546111 0.057218
0.965
1.199797 0.123255
0.959
0.967
0.9199
0.07817
0.958
0.499834 0.03195
0.961
1.117427 0.08488
0.956
0.962
0.790399 0.046235
0.952
0.430156 0.033166
0.954
0.851263 0.063122
0.948
0.953
0.718765 0.043911 0.9948
0.366715 0.039321
0.949
0.761652 0.07715
0.946
0.948

38
Table 2.28 Coverage of Nonparametric Bootstrap 95% C.I.s
α =2, λ =1, β =2, µ=0.88623, π=.667,

τ=1

n

parameter Lower Bound Upper Bound
1.525844
2.653593
α
0.754816
1.329374
λ
30
1.507521
2.7966
β
μ
0.700187
1.566619
2.58238
α
0.78307
1.257077
λ
50
1.514494
2.601731
β
μ
0.71844
1.672603
2.439149
α
0.805391
1.23648
λ
75
1.621088
2.548998
β
μ
0.736908
1.694588
2.415594
α
0.826174
1.186477
λ
100
1.64081
2.412504
β
μ
0.756104

width
SD(W) Coverage
1.127749 0.072596
0.968
0.574558 0.045802
0.969
1.289079 0.106334
0.965
0.963
1.015762 0.06782
0.966
0.474007 0.038776
0.959
1.087237 0.09075
0.955
0.959
0.766546 0.057978
0.950
0.431089 0.035548
0.955
0.92791 0.06989
0.953
0.956
0.721006 0.048877
0.949
0.360303 0.024121
0.948
0.771694 0.043612
0.947
0.951

Table 2.29 Coverage of Asymptotic 95% C.I.s
α =2,
n

λ =1, β =2, µ=0.88623, π=.667,

τ=1.5

parameter Lower Bound Upper Bound
1.478369
2.624401
α
0.698611
1.275153
λ
30
1.377872
2.614177
β
μ
0.662728
1.557089
2.526809
α
0.748954
1.211557
λ
50
1.44547
2.50873
β
μ
0.698295
1.640691
2.544269
α
0.803109
1.18672
λ
75
1.573525
2.446088
β
μ
0.739598
1.674023
2.503273
α
0.832987
1.18701
λ
100
1.634495
2.443882
β
μ
0.762572

width
SD(W) Coverage
1.146033 0.040713
0.971
0.576542 0.025833
0.971
1.236306 0.03808
0.971
0.969
0.96972 0.017892
0.969
0.462603 0.023503
0.963
1.06326 0.073951
0.964
0.965
0.903578 0.030188
0.963
0.383611 0.019288
0.956
0.872562 0.027582
0.957
0.959
0.82925 0.025159
0.958
0.354023 0.013232
0.952
0.809388 0.034866
0.953
0.956

39
Table 2.30 Coverage of Parametric Bootstrap 95% C.I.s
α =2, λ =1, β =2, µ=0.88623, π=.667,

τ=1.5

n

parameter Lower Bound Upper Bound
1.549899
2.467697
α
0.731346
1.250344
λ
30
1.477881
2.474033
β
μ
0.688011
1.632197
2.404597
α
0.783189
1.189062
λ
50
1.511334
2.386952
β
μ
0.720073
1.690757
2.433836
α
0.823991
1.174488
λ
75
1.640487
2.346654
β
μ
0.763419
1.723998
2.398318
α
0.864714
1.171753
λ
100
1.696336
2.349142
β
μ
0.784101

width
SD(W) Coverage
0.917798 0.071158
0.965
0.518998 0.038741
0.967
0.996152 0.061427
0.963
0.967
0.7724 0.043842
0.955
0.405872 0.026159
0.958
0.875618 0.106242
0.957
0.961
0.743078 0.040686
0.952
0.350496 0.023103
0.952
0.706166 0.034308
0.949
0.955
0.67432 0.033039
0.949
0.307039 0.018894
0.949
0.652807 0.04112
0.946
0.951

Table 2.31 Coverage of Nonparametric Bootstrap 95% C.I.s
α =2, λ =1, β =2, µ=0.88623, π=.667,
n

τ=1.5

parameter Lower Bound Upper Bound
1.539082
2.472456
α
0.752717
1.231717
λ
30
1.469029
2.482562
β
μ
0.688344
1.621013
2.448673
α
0.773339
1.248316
λ
50
1.515091
2.502296
β
μ
0.710522
1.70718
2.448783
α
0.841297
1.188695
λ
75
1.687001
2.396295
β
μ
0.770576
1.732012
2.404005
α
0.849432
1.190326
λ
100
1.695365
2.353513
β
μ
0.769226

width
SD(W) Coverage
0.933374 0.05274
0.966
0.479
0.034023
0.965
1.013533 0.051888
0.964
0.967
0.827661 0.08118
0.958
0.474977 0.042215
0.964
0.987205 0.1304
0.963
0.962
0.741603 0.072849
0.952
0.347398 0.026076
0.951
0.709293 0.047098
0.949
0.953
0.671993 0.029558
0.949
0.340894 0.025235
0.951
0.658147 0.060011
0.946
0.954

40

The simulations results show that all the three methods provide at least 95%
coverage in almost all cases. While the asymptotic confidence intervals for the
parameters a and β provide consistently conservative coverage when the sample size is
30 (e.g. 0.967 and 0.969 respectively in Table 2.2) these intervals provide marginally
less conservative coverage for larger sample sizes (see Tables 2.2, 2.5, 2.8, 2.11, 2.14,
2.17, 2.20, 2.23, 2.26 and 2.29). The same pattern of less conservative coverage with
increasing sample size is seen for the asymptotic confidence intervals for β (see Tables
2.2 through 2.29), except showing a near normal coverage for sample size 30 in Table
2.2. The intervals for a , λ , and β based on the parametric and nonparametric bootstrap
show a similar pattern of decreasing coverage with increase in sample size, but in
almost all cases the coverage stays near or above the normal value. In general, the
coverage probabilities of the confidence intervals for a , λ , and β do not differ by much
when compared across the mode of construction.
While the coverage probabilities do not show any distinctive differe3nces
between the three methods, inspection of the widths of the confidence intervals for

a , λ , and β show some slight differences. In general, the intervals based on the
parametric bootstrap are slightly narrower than those based on the asymptotic
distribution (see Tables 2.2 and 2.3, 2.5 and 2.6) when α = 1.5. This phenomenon
disappears when α = 2 except for intervals constructed for α (see Tables 2.8 and 2.9
for example). Other than that, one cannot find any discernible pattern that separates the
two bootstrap methods as far as interval widths are concerned. When the ratio of normal
to accelerated sample sizes changes from 0.5 to 2/3, the widths of the asymptotic
intervals for λ and β increases slightly when the censoring time τ = 1 (see Tables 2.8
and 2.14), but such a pattern is not seen when τ = 1.5 (see Tables 2.11 and 2.17). This
increase is also seen for parametric and nonparametric bootstrap-based intervals when

τ = 1 (Tables 2.12, 2.13, 2.15, and 2.16).
For a practicing chemist or an engineer, estimation of the mean life under normal
conditions is even more important than building confidence intervals for the distribution

41
parameters. Manufacturers of products such as chemicals would like to provide
customers with information on the shelf-life of the product when stored or used under
normal conditions. Usually such assurances are given in terms of a lower confidence
bound for the mean life. Therefore, it is of interest to note how the confidence bounds
for the mean life performed in the Monte-Carlo study. The confidence bounds for mean
life constructed using all three methods show near normal coverage, especially when
the sample size is at or above 75. The coverage probability can be conservative for small
sample sizes, but this occurs only when the shape parameter α = 2. The expected value
of the bounds also do not vary by much across the three methods.
In summary, all three methods produce confidence intervals with reasonable
coverages as well as lower confidence bounds for mean life that are comparable and
provide coverage ranging from conservative to normal . The relative performance of the
asymptotic distribution-based method relative to the bootstrap-based methods is
somewhat surprising, even when the sample size is 30. Further studies, with smaller
sample sizes and a higher level of censoring may differentiate the bootstrap methods
from the asymptotic method.
2.5

CONCLUSIONS AND FUTURE WORK
PALT is a method that is preferable over the ALT procedure when the

accelerating factor is unknown or a suitable model that links parameters of the life
distribution to the stress level is not available. While PALT is not suitable when the
products under test have a very long mean life, it is applicable in situations where the
life-span of tested products is only moderately long. This research extended previous
work to cover Type I censoring in the Weibull case while at the same time developing
bootstrap-based methods for obtaining prediction intervals for distribution parameters
and the acceleration factor. In addition, asymptotic distribution based intervals were also
considered. More importantly, a method of obtaining lower confidence bounds for the
mean life under normal use conditions was also developed. The performance of the
three methods was studies using a Monte-Carlo study. Results show that all methods

42
perform reasonable well under all parameter combinations employed in the Monte-Carlo
study.
Future work studying the performance of the three approaches under additional
parameter combinations and censoring levels is warranted. The performance of the
bootstrap methods when estimates other than MLEs, such as the closed form
approximations introduced by Englehardt (1975), are used would be of interest. A
possible generalization of the proposed procedure is to consider the case where the
censoring times are different for the accelerated and normal use samples. Studies on the
robustness of the three methods in the presence of outliers or distributional missspecification may also be valuable. Extending the proposed methodologies to StepStress PALT experiments as well as Progressive Step-Stress PALT situations would also
be of added value.

43
3.

BOOTSTRAP-BASED CONFIDENCE INTERVALS IN PARTIALLY
ACCELERATED LIFE TESTING UNDER THE GENERALIZED
EXPONENTIAL DISTRIBUTION

3.1 INTRODUCTION
Accelerated life tests (ALT) are often used to obtain information about the life
distribution of products that are designed to last a long time under normal use conditions. This
is because, under normal use conditions, only a very small fraction of them will fail

during a feasible testing period. Nelson (1980) drives this point home in his statement:
“Accelerated life testing of a product or material is used to get information quickly on its
life distribution. Test units are run under severe conditions and fail sooner than under
usual conditions. ….. This is quicker and cheaper than testing at usual conditions, which
is usually impractical because life is so long.” In situations where the acceleration factor
is known or one can find a mathematical model describing the life-stress relationship,
ALT is provides a quick way to get a sufficient amount of information to estimate the
life distribution. However, in situations where neither the acceleration factor is known
nor a reasonable life-stress model can be found, partially accelerated life tests (PALT)
provide a suitable approach to estimating the life distribution and related parameters.
Under the PALT method, a subset of the test units are placed under the normal
use (field use, design use) stress conditions and the remaining units are tested under a
suitably selected higher than normal stress level. This provides a statistically viable
approach by assuming that life distribution of the units under the higher stress level is
the same as that of units under normal use, but with the scale parameter multiplied by
an acceleration factor. This factor is estimated together with the other distribution
parameters by utilizing the combined data set. Since there is more failure data from the
units that received higher than normal stress level, the combined data provide better
estimates of the common parameters.
`
One drawback of the PALT method is that unlike in the ALT, some units have to
be tested under normal use. Thus this method is not suitable for components that are

44
very long lasting. But items such as chemicals that have shelf-lives that are measured in
months or a year or two can be tested using this method.
In this paper, we introduce three approaches for the construction confidence
intervals for model parameters and lower confidence bounds for the mean life under
normal use conditions using Type I censored data from a constant stress PALT when
the underlying distribution is Generalized Exponential (GE). The methods introduced
are, namely, intervals and bounds based on the asymptotic distribution of the model
parameters, the parametric bootstrap,

and the nonparametric bootstrap. While results

based on the asymptotic distribution is available for the case where the PALT is carried
out for GE data, such results are for the Type II censoring scenario. In addition, no
bootstrap-based intervals have been developed for cases where the underlying
distribution is GE or when the censoring mechanism is Type I.
3.1.1 A Brief Review of Relevant Literature. Compared to the large number of
publications on ALTs the publications on PALT is relatively smaller. For brevity, we
will focus only on a limit number of these publications. For details on ALT, we refer the
reader to the excellent coverage of the topic given in Nelson (1990). Other good
references include Mann, Schafer, and Singapuwalla (1974), Lawless (1982), Tobias and
Trindade (2011), and Meeker and Escobar (1998).
A relatively recent publication on ALT is Jayawardhana and Samaranayake
(2003), which discussed obtaining lower prediction bounds for a future observation from
a Weibull population at design (normal

use) stress level, using Type II censored

accelerated life test data. The authors assumed that the scale parameter of the life
distribution have an inverse power relationship with the stress level. They showed that
the method works well when the low and high stresses are reasonably far apart. Alferink
and Samaranayake (2011) considered accelerated degradation models and developed
confidence intervals for mean life using the Delta method and the bootstrap, assuming
lognormal distribution with variance dependent on stress. This contrasts with other
approaches, which assume that the variance if not affected by increasing stress. Another
important publication is Kamal, et al (2013), which presented a step stress ALT plan

45
with good performance. In step stress, the components are first put at a lower stress and
the unfailed components are subjected to higher stress after a specific period. More
recently, Jayawardhana and Samaranayake (2014), obtained predictive density of a
future observation at normal use conditions using ALT method under lognormal life
distribution and Type II censoring with non-constant variance.

Among the publications on PALTs, the following are worth mentioning. Saxena
and Zarrin (2013) used the Constant Stress Partially Accelerated Life Test (CSPALT)
and assumed Type-I censoring under the Extreme Value Type-III distribution. Note that
the Extreme Value Type-III distribution has been recommended as appropriate for high
reliability components. The authors used the Maximum Likelihood (ML) method to
estimate the parameters of CSPALT model and confidence intervals for the model
parameters were constructed. Note that the CSPALT plan is used to minimize the
Generalized Asymptotic Variance (GAV) of the ML estimators of the model parameters.

Abdel-Hamid (2009), considered a constant PALT model when the observed
failure times come from Burr(c, k) distribution under progressively Type-II right
censoring. The MLEs of the parameters were obtained and their performance was

studied through their mean squared errors and relative absolute biases. The paper also
showed how to construct approximate and bootstrap CIs for the parameters. The
bootstrap CIs give more accurate results than the approximate intervals for small sample
sizes, and the Student’s-t bootstrap CIs are better than the Percentile bootstrap CIs in the
sense of having smaller widths. However, the differences between the lengths of CIs for
the two methods decrease with increased sample size.

A publication that motivated the work in this dissertation is by Ismail (2013), who
derived the maximum likelihood estimators (MLEs) of the parameters of the GE
distribution and the acceleration factor when the data are Type-II censored under
constant-stress PALT model. The likelihood ratio bounds (LRB) method was used to
obtain confidence bounds of the model parameters when the sample size is small. It is
also shown that the maximum likelihood estimators are consistent and their variances

46
decrease as the sample size increases. The numerical results reported in the paper
support the theoretical findings and showed that the estimated approximate confidence
intervals for the three parameters are smaller when the sample size is larger.

3.1.2 The Generalized Exponential Distribution. The proposed PALT method
is developed for the case where the underlying life distribution is GE. The generalized
exponential distribution has been introduced and studied quite extensively by Gupta and
Kundu (1999, 2001a, 2001b), and by Ragab and Ahsanullah (2001). The probability
density function and the cumulative distribution function of the generalized exponential
distribution function has the forms:

f ( x; α ,=
λ ) αλ e − λ x (1 − e − λ x )

α −1

F ( x; α , λ )=

(1 − e )

−λ x α

x > 0, α > 0, λ > 0,

,

(1)
(2)

respectively, where 𝛼𝛼 is the shape parameter and λ the scale parameter.

The GE distribution has certain features which are distinct from the Gamma and

Weibull distributions (see Gupta and Kundu (1999, 2001)). The GE model can be used
as a possible alternative for analyzing skewed datasets. An interesting fact is that both
Gamma and GE distributions have the likelihood ratio ordering property while Weibull
does not. On the other hand, GE and Weibull distributions have the common feature of
having closed form expressions for Cumulative Distribution Function (CDF) and the
hazard function. One aspect that makes the GE distribution outperform the Weibull is
the fact that the convergence of MLE’s of Weibull parameters can be very slow
(Bain(1976)) whereas the asymptotic confidence intervals obtained under the GE
assumption maintain normal coverage even for small sample sizes (Gupta and Kundu
(2001)). Gupta and Kundu (2001) also showed that the hazard function of the GE
distribution has proprieties similar to those of the Gamma and Weibull distributions.
These properties are summarized in Table 3.1.

47
Table 3.1 Properties of the Hazard Function 1
Parameters
𝜶𝜶 = 𝟏𝟏
𝜶𝜶 > 𝟏𝟏
𝜶𝜶 < 𝟏𝟏

Gamma
Constant
Increasing
from 0 to λ
Decreasing
from ∞ to λ

Weibull
Constant
Increasing
from 0 to ∞
Decreasing
from ∞ to 0

GE
Constant
Increasing
from 0 to λ
Decreasing
from ∞ to λ

Table 3.1. Note that the Hazard function of the GE distribution is given by

αλ e − λ x (1 − e − λ x )

α −1

h ( x; α , λ ) =

(1 − e−λ x )

α

.

Figure 3.1. Properties of the Hazard Function 2

3.2

THE PROPOSED PALT METHOD AND BOOTSTRAP INTERVALS

The following assumptions are made regarding the proposed PALT method.
1
2

Exponentiated Exponential Family:An Alternative to Gamma and Weibull Distributions(2001)
Exponentiated Exponential Family:An Alternative to Gamma and Weibull Distributions(2001)

48

1. The total number of units under test is n .
2. 𝜋𝜋 denotes the proportion of sample units allocated to accelerated condition

3. 𝑛𝑛(1 − 𝜋𝜋) = 𝑛𝑛𝜋𝜋�of these units are allocated to normal (field) use conditions.

4. nπ units are allocated to the high stress condition (subject to acceleration)

3.2.1 Likelihood Function under Type I Censoring and Asymptotic C.I.s.
Under Type I censoring, the censoring time, 𝛕𝛕, is fixed but the number of failures

observed in the time 𝛕𝛕 is a random variable, say R. We assume that the number of items

failing before time 𝛕𝛕 follows binomial distribution with parameters (𝒏𝒏, 𝒑𝒑) with

𝒑𝒑 = 𝑭𝑭𝑿𝑿 �𝝉𝝉; 𝜽𝜽�, where 𝛉𝛉 is the vector of parameters of the GE distribution.
Notation:
𝑥𝑥𝑖𝑖 : Observed lifetime of item i tested at the normal (field) use condition.
𝑦𝑦𝑗𝑗 : Observed lifetime of item j tested at high stress condition.

𝛿𝛿𝑢𝑢𝑢𝑢 : Indicator function denoting the censoring state of ith observation under

normal .

use condition, with 𝛿𝛿𝑢𝑢𝑢𝑢 = 1 if the observation is uncensored.

𝛿𝛿𝑎𝑎𝑎𝑎 : Indicator function denoting the censoring state of jth observation under high stress

condition, with 𝛿𝛿𝑎𝑎𝑎𝑎 = 1 if the observation is uncensored.
𝑛𝑛𝑢𝑢 : Number of items that failed at normal use condition.

𝑛𝑛𝑎𝑎 : Number of items that failed at a high stress condition.
τ: The censoring time of the life test (for all units).

49
𝑥𝑥(1) ≤ ⋯ ≤ 𝑥𝑥(𝑛𝑛𝑢𝑢 ) ≤ 𝜏𝜏: Ordered failure times at normal use condition.
𝑦𝑦(1) ≤ ⋯ ≤ 𝑦𝑦(𝑛𝑛𝑎𝑎 ) ≤ 𝜏𝜏: Ordered failure times at high stress condition.
𝛽𝛽: Denotes the acceleration factor (𝛽𝛽 > 1).

The number of items failing before time τ follows a binomial distribution R  Bin ( n, p )
where

(1 − e )

− λτ α

p= FX (τ ; α , λ )=

.

We let,

xi ≤ τ
1
=
δ ui =
, i 1, 2, , nπ ,
0/w
0
(4)

yj ≤ τ
1
, j 1, 2, , nπ ,
δ a j =
=
0/w
0

(5)

and δ ui =
1 − δ ui ,
δaj =
1− δ aj .

Then,
np

δ u  Ber ( p) ⇒ ∑ δ u  Bin(np , p ),
i

i =1

(6)

i

np

δ a  Ber ( p) ⇒ ∑ δ a  Bin(np , p ) ,
j

j =1

)
X (τ ; α , λ , β =

where p*= F

(7)

j

(1 − e

− λβτ

)

α

, with

50

Fτ ( x; α , λ τ =
) P ( X ≤ x X ≤ τ =) PP(( XX ≤≤ τx))=
 (1 − e − λ x )α

,
=  (1 − e − λτ )α

 1,

(1 − e )
(1 − e )

−λ x α

− λτ α

(8)

x ≤τ
x >τ,

and
−λ x
−λ x
f X ( x; α , λ ) αλ e (1 − e )
fτ=
( x;α , λ τ ) F=
α
X (τ ; α , λ )
(1 − e−λτ )

α −1

(9)

.

Thus, given 𝑅𝑅 = 𝑛𝑛𝑢𝑢 , the conditional density of the first r failure times is equivalent to
the joint density of an ordered random sample of size 𝑛𝑛𝑢𝑢 from a truncated GE distribution,

(

 αλ e − λ xi 1 − e − λ xi
f x(1) , , x( nu ) =
R n=
nu !∏ fτ ( x(i ) ; α , =
λ ) nu !∏ 
u
− λτ α
=i 1 =i 1 
−
1
e
(
)


(

)

nu

= nu !

nu

(αλ )

nu

−λ

(1 − e )

α −1






(10)

nu

∑ xi
i =1

e

)

− λτ α nu

nu

∏ (1 − e λ )
− xi

i =1

α −1

.


The joint density of obtaining R = nu ordered observations at the values x(1) , , x( nu )
before time, may be expressed as

(

)

(

)

f x(1) , , x( nu )
f=
x(1) , , x( nu ) R nu bin ( nu ; np , p )
=
= nu

(αλ )
!

nu

−λ

e

nu

∑ xi
i =1

(1 − e )

− λτ α nu

∏ (1 − e λ )

− xi α −1

i =1

−λ

u

Therefore we can write,

i =1

u

( np )! αλ n
=
( )
( np − nu )!

  np  p nu (1 − p ) np − nu
  nu 

nu

∑ xi

( np )! (αλ ) e
( np − nu )! (1 − e−λτ )α n
nu

=

nu

(1 − e−λτ )

α nu

1 − (1 − e − λτ )α 



np − nu

∏ (1 − e λ )

− xi α −1

i =1

nu

n
∑ xi
α np − nu u
 1 − e − λ xi
e i=1 1 − (1 − e − λτ ) 
∏



i =1 
−λ

nu

(

)

α −1

.





51

(

)

n
f x(1) , , x( nu ) ∝ (αλ ) u 1 − (1 − e


)

− λτ α




nπ − nu

−λ

nu

∑ xi
i =1

e

nu

∏ (1 − e λ )

− xi α −1

i =1

.


Similarly, given 𝑅𝑅 = 𝑛𝑛𝑎𝑎 the conditional density of the first r failure times is equivalent

to the joint density of an ordered random sample of size 𝑛𝑛𝑎𝑎 from a truncated accelerated
GE distribution.

For an item tested at accelerated condition, the probability density function is given by

f ( x; α , λ ) =
αβλ e − βλ x (1 − e − βλ x )

α −1

x > 0, α > 0, λ > 0, β > 1 ,

where 𝑌𝑌 = 𝛽𝛽 −1 𝑋𝑋 and therefore the conditional joint distribution given R = na is

(

 aλβ e − λβ y j 1 − e − λβ y j
=
f y(1) , , y( nu ) =
R n=
na !∏ fτ ( x( j ) ; a , λ , β
) na !∏ 
a
a
=j 1 =j 1
(1 − e−λβτ )


)

(

na

= na !

na

(aλβ )

na

(1 − e

− λβ

− λβτ

a −1

nu

∑ yj
i =1

e

)

)

a na

(

na

 1 − e − λβ y j
∏

j =1 

)

a −1






(11)

.


The joint density of obtaining 𝑅𝑅 = 𝑛𝑛𝑎𝑎 ordered observations at the values 𝑌𝑌(1) , … , 𝑌𝑌(𝑛𝑛𝑎𝑎 )
before time, may be expressed as

(

)

(

)

=
f y(1) , , y( na )
f=
y(1) , , y( na ) R na bin ( na ; np , p* )
= nu

(aλb )
!

na

− λb

e

na

∑ yj
j =1

(1 − e )

− λbτ a na

j =1

− λb

)

− λb y j a −1

  np  p *na 1 − p* np − na
( )
 
  na 

na

∑ yj

( npaλb
)! ( ) e
( np − na )! (1 − e− λbτ )a n
na

=

∏ (1 − e
na

j =1

a

na

(1 − e )

1 − 1 − e
 (

)

na
− λbτ aa
− λbτ




np − na

∏ (1 − e

( np )! aλb na e− λb ∑j=1 y j 1 − 1 − e−λbτ a  np −na na  1 − e−λb y j
=
( )
(
)  ∏ 

( np − na )!
i =1
Therefor we can write,

(

na

j =1

)

a −1

.


)

− λb y j a −1

52

(

)

f y(1) , , y( na ) ∝ (aλβ ) e
na

− λβ

na

∑ yj
j =1

1 − (1 − e − λβτ )a 



nπ − na

∏ (1 − e
na

− λβ y j

i =1

)

(

a −1

and the total likelihood function for x1 ; δ u1 , , xnπ ; δ unπ , y1 ; δ a1 , , ynnπ ; δ anπ

)

,

is

given by

(

nπ

)

nπ

) ∏ L (α , λ , β

(

L α , λ , β x, y = ∏ Lui α , λ xi , δ ui

uj

=i 1 =j 1

=

nπ

(

αλ e − λ xi 1 − e − λ xi
∏

i =1 
nπ

× ∏ αλβ e

j =1 
=

(1 − e

− λβ y j

nu

α −1 δ ui

)

− λβ y j

(

αλ e − λ xi 1 − e − λ xi
∏


i =1

)

(

nα




α −1

)

1 − (1 − e − λτ )α 



α −1 δ u j

)

y j ,δu j




δ ui

1 − (1 − e − λβτ )α 



δu j

(12)

nπ

 1 − (1 − e − λτ )α 


∏
i =1
nπ

)

− λβ y j
− λβ y j α −1 
1 − (1 − e − λβτ )α .
× ∏ αλβ e
1− e
∏

 i 1 

=j 1 =

The MLE’s of the parameters can estimated numerically by minimizing the log
likelihood function.

(

)

l,
⇒ ln L α , l , β x, y =
=
⇒l

nu

nu

nu

nu

+ ∑ ln − ∑ x + ( − 1) ∑ ln (1 − e l )
∑ ln alla

=i 1 =i 1

i

=i 1

− xi

=i 1

+ ∑ ln + ∑ ln β
) + ∑ ln al
− lβ ∑ y + (a − 1) ∑ ln (1 − e
) − ∑ ln (1 − (1 − e
−

∑ ln (1 − (1 − e )
nπ

na

− lτ a

i=
nu +1
na

=j 1

na

j=
j=
j=
1
1
1

na

j

na

nπ

− lβ y j

=j 1

− lβτ

=j na +1

)

a

),

na
 nu

=
⇒ l nu ( ln alal
+ ln ) + na ( ln + ln + ln β ) − l  ∑ xi − β ∑ y j 
=
 i 1 =j 1 
na
 nu
− lβ y j 
− l xi
+ (a − 1)  ∑ ln 1 − e
+ ∑ ln 1 − e

=
 i 1 =j 1


(

(

(

)

− ( nπ − nu ) ln 1 − (1 − e − lτ )

)

) − ( nπ − n ) ln (1 − (1 − e

(13)

)

aa
− lβτ

The Score equations become

a

).

53
1 − e − lτ ) ln (1 − e − lτ )
(
∂l nu + na nu
− l xi
=
+ ∑ ln 1 − e
+ ( nπ − nu )
a
∂aa
i =1
1 − (1 − e − lτ )

(

(

na

+ ∑ ln 1 − e
j =1

− lβ y j

a

)

)

(

) + ( nπ − n ) (

(14)

) ln (1 − e ) =
0,
(1 − (1 − e ) )

1− e

− lβτ a

− lβτ

− lβτ a

a

nu
e − lτ (1 − e − lτ )
xi e − l xi
∂l nu + na nu
=
+ ∑ xi + (aa
− 1) ∑
+ ( nπ − nu )
a
− l xi
∂ll
=i 1 =i 1 1 − e
1 − (1 − e − lτ )

a −1

(

na

na

yi e

(

)

e − lβτ (1 − e − lβτ )

a −1

− lβ y j

− β ∑ y + (aa
− 1) ∑
+ ( nπ − na )
a
− lβ y j
1− e
1 − (1 − e − lβτ )

j
=j 1 =j 1

(

)

(

)

)

0,
=

− lβ y
na
na
e − lβτ (1 − e − lβτ )
l yi e j
∂l na
=
− laalτ
+
y j + ( − 1) ∑
( nπ − na )
∑
a
− lβ y j
∂=
β β
j 1 =j 1 1 − e
1 − (1 − e − lβτ )

(15)

a −1

(

(

)

)

= 0.

(16)

Now, we have a system of three nonlinear equations in three unknowns 𝛼𝛼, 𝜆𝜆, and 𝛽𝛽. It is

clear that a closed form solution is intractable. Therefore, iterative procedure can be used
to find a numerical solution of the above system.
The asymptotic confidence intervals for the parameters θ = (α , λ , β ) can be obtained
using following hypothesized convergence in distribution result:




n  (αˆ − α ) , λˆ − λ , βˆ − β →  0 , I −1 (α , λ , β )   ,
−


where the I = (α , λ , β ) is the Fisher information matrix given by

(

)(

)

 ∂2l

∂α 2

 I11 (α ) I12 (αl ) I13 (αβ ) 
 ∂2l

=
I (α , l , β ) =
I 21 ( lα ) I 22 ( llβ
) I 23 ( ) 
∂l∂α
 I 31 ( βα ) I 32 ( βl ) I 33 ( β )  
 ∂2l

 ∂β∂α

∂2l
∂α∂l
∂2l
∂l 2
∂2l
∂β∂l

∂2l 

∂α∂β 
∂2l 
.
∂l∂β 
∂2l 

∂β 2 

54
Note that Gupta and Kundu (1999), focusing on the three parameter GE
distribution (in our case it is assumed that the location parameter is zero), stated the
asymptotic normality of the MLEs under the assumption that the shape parameter α > 2 ,
and mentioned that further investigation is needed for the case α ≤ 2 . They indicate that
the regularity conditions can be established using techniques similar to those employed
for the gamma and the Weibull families. The above authors, however, studied the
behavior of the estimators for α ≤ 2 using Monte-Carlo simulation in Gupta and Kundu
(2000), and did not detect any anomalous behavior when α ≤ 2. Also, their results are
for Type II censored data and does not consider the PALT scenario. Ismail (2013),
however, assumed the above asymptotic result and obtained reasonable confidence
intervals for distribution parameters under Type-II censoring. Based on these empirical
findings, we will assume that the above result holds in the PALT situations under Type I
censoring and also when α ≤ 2 . As simulation results given later show, this assumption
does not lead to poorly performing confidence intervals.

(

)

The elements of the 3x3 matrix I , I ij αˆ , λˆ, βˆ , i, j = 1, 2,3, can be approximated by

(

)

I ij αˆ , λˆ, βˆ , where

∂ 2 l (θ )
I ij θˆ =
∂θi ∂θ j

()

θ =θˆ

.

From Eq. (12), we get the following:

aa
2
2
nu + na ( nπ − nu )ψ 2 ln (ψ 2 ) ( nπ − na )ψ 6 ln (ψ 6 )
∂2l
=
+
+
,
2
2
∂aa
ψ 42
ψ 52

(17)

2
alτ
+ e − − 2alτ
e− }
 nu 2 − l xi  τ a ( nπ − nu )yay
∂ 2 l nu + na
2 {−
=
+ (a − 1) ∑ xi e  −
2
2
2
∂ll
 −1 + e − lτ − 1 e − lτ a  e − lτ − 1 2
 i =1

(
)
)

 (

(

τ β a ( nπ − na )yay
+e
− 6e
6 {−
2

−

2

−
alβτ
2

alβτ
−
2
6 e
1 − yy
 6

−
alβτ

)

}+

 nu

(a − 1) β 2 ∑ y 2j e−lβ y
 i =1

j





(18)

55
alβτ
nu
+ e − − 6alβτ
τ 2 β 2a ( nπ − na )yay
e− }
na
∂2l
6 {−
2 
2 − lβ y j 
=
+ (al
− 1) ∑ y j e
−
2
−
alβτ
∂β 2 β 2
1 − yy
 62
 i =1

6 e
(19)
− lβ y
aaa
2
nu
l y 2j e j τl ( nπ − na ) ayyyy
∂2l
6 ln ( 6 ) + 6 − 6
= ∑
−
(20)
∂a∂β i =1 y 3 j
e− − 1
y 5alβτ

{

− lβ y j

nu
nu
β y 2j e
xi2 e − l xi
∂2l
=
+∑
∑
∂alyy
∂
=
i 1=
i 1
1i
3j

+

(

aa
τa ( nπ − nu )yayy
2 {− ln ( 2 ) + 1 − 2 }

y 4alτ
( e− − 1)

{

y 52 ( e − lβτ − 1)
− lβ y

− lβ y j

nu 
β y j e j β y 2j e
∂2l
=
−
(a − 1) ∑ 
∂β∂lyy
i =1 
3j
3j


+

−

)

alβτ
−
τβ ( nπ − na )yay
) + 1 − 6a
6 − ln (1 − e

+

}

−

(21)

},

βl y 2j e

−2 lβ y j

y 32 j

{





}

alβτ
τa ( nπ − na ) yalτβ
− e − + 1 + βlτ e − lβτ − βlτ )
6 (−

y 5 ( e − lβτ − 1)

aaa
τ 2 β ( nπ − na )yyay
6 {1 − 6 −
6 }

y 52 ( e − lβτ − 1)

2

2

(22)

,

where
− λβ y j

1 − e−λ x ,
1 − e − λτ ,
1− e
,
yyy
1i =
2 =
1j =
i

− λτ
− λβτ
,
yyy
) , 5 =1 − (1 − e−λβτ ) ,
4 =1 − (1 − e
6 =1 − e

α

α

and employing the standard z-based confidence interval formulations,

αˆ ± Zγ

2

I11−1 (αˆ ) , λˆ ± Zγ

2

( )

−1 ˆ
I 22
λ , βˆ ± Zγ

2

( )

I 33−1 βˆ .

The asymptotic confidence interval for the mean life at normal use condition is given by

µˆ ± Zγ

2

Var ( µˆ ) ,

where Var ( µˆ ) is obtained using the standard delta method.
3.3

THE BOOTSTRAP SAMPLING METHODS

There are several different methods for generating the needed bootstrap samples data

56
3.3.1

The Proposed Parametric Bootstrap Method and the Monte-Carlo

Procedure. The Monte-Carlo procedure used for the simulation study is given below.
The steps for the parametric bootstrap method that can be utilized to obtain confidence
bounds for 𝜶𝜶, 𝝀𝝀, 𝒂𝒂𝒂𝒂𝒂𝒂 𝜷𝜷 and lower binds for the mean life is imbedded in this procedure
and are given in italics.

Distribution parameters are varied in the study as following (𝛼𝛼 = 1.5, 𝑎𝑎𝑎𝑎𝑎𝑎 2 , 𝜆𝜆 =

1 , 𝛽𝛽 = 1.5, and 2) and 𝜇𝜇 = 1/𝜆𝜆 [𝜓𝜓(𝛼𝛼 + 1) − 𝜓𝜓(1)] where 𝜓𝜓(. ) Digamma function is
presented here. The censoring time was set at 𝜏𝜏 =1, and 1.5. The 𝑛𝑛 test items were
divided into (a) equal sample proportions by setting 𝜋𝜋 = 0.5 , such that 1/2 the items are
allocated at accelerated condition and the remaining 1/2 are allocated to the normal use

condition and (b) by setting 𝜋𝜋 = 0.667 such that 1/3 the items are allocated at accelerated

condition and the remaining 2/3 are allocated to the normal use condition.

(1) Generate random samples from the GE distribution by using the transformation
−1

(1⁄𝛼𝛼 )

𝑥𝑥𝑖𝑖 = � � ln�1 − 𝑢𝑢𝑖𝑖
𝜆𝜆

�, 𝑖𝑖 = 1,2, … , 𝑛𝑛 where 𝑢𝑢𝑖𝑖′ 𝑠𝑠 are random sample from a

uniform (0, 1) distribution. Similarly, generate data for the high stress condition
by replacing λ with βλ. Employ censoring time τ for both samples.
(2) Employ Maximum likelihood method to estimate the parameters with the same
censoring time τ used for both samples. [The nonlinear equations of the maximum
likelihood estimates were solved iteratively using Newton Raphson method.]
(3) Use the resulting estimates of the parameters and acceleration factor to construct
asymptotic confidence limits with confidence level at 𝛾𝛾 = 0.95 and also the
asymptotic variance and covariance matrix of the estimators (for use in the delta
method based confidence bounds).


(4) Used the estimated parameters 𝛼𝛼�, 𝜆𝜆̂, and β to generate data from the estimated

normal use and accelerated GE distribution using the transformation


1
(1⁄𝛼𝛼
�)
�− �� ln�1 − 𝑢𝑢𝑖𝑖
� , [ λ is replaced by βλ for the accelerated sample.]
𝜆𝜆

(5) Repeat Step (4) to obtain 1,000 bootstrap samples.

𝑥𝑥𝑖𝑖 =

57
(6) Obtain MLEs of the GE parameters and the acceleration factor using each
bootstrap sample and label these 𝛼𝛼� ∗ , 𝜆𝜆̂∗ , 𝑎𝑎𝑎𝑎𝑎𝑎 𝛽𝛽̂ ∗ .

(7) Using the empirical distributions of the estimates 𝛼𝛼� ∗ , 𝜆𝜆̂∗ , 𝑎𝑎𝑎𝑎𝑎𝑎 𝛽𝛽̂ ∗ obtained from

bootstrap estimates, construct confidence interval for 𝛼𝛼, 𝜆𝜆, 𝛽𝛽 using respective
quantiles at �

1−𝛾𝛾
2

1−𝛾𝛾

�100% and 1 − �

2

�100%.

(8) Using the empirical distribution of the estimated means 𝜇𝜇̂ ∗ , obtained from

bootstrap samples, construct lower bound confidence bound for 𝜇𝜇 using quantile
at (1 − 𝛾𝛾)100%

(9) Coverage probabilities were computed based on 1,000 simulation runs by
repeating Steps (1) through (7) 1,000 times.
3.3.2

The Proposed Nonparametric Bootstrap Method and the Monte-

Carlo Procedure.The Monte-Carlo procedure used for the simulation study is given
below. The steps for the nonparametric bootstrap method that can be utilized to obtain
confidence bounds for 𝜶𝜶, 𝝀𝝀, 𝒂𝒂𝒂𝒂𝒂𝒂 𝜷𝜷 and lower bounds for the mean life is imbedded in
this procedure and are given in italics.

(1) Generate random samples from the GE distribution by using the transformation
−1

(1⁄𝛼𝛼 )

𝑥𝑥𝑖𝑖 = � � ln�1 − 𝑢𝑢𝑖𝑖
𝜆𝜆

�, 𝑖𝑖 = 1,2, … , 𝑛𝑛 where 𝑢𝑢𝑖𝑖′ 𝑠𝑠 are random sample from a

uniform (0, 1) distribution. Similarly, generate data for the high stress condition
by replacing λ with βλ. Employ censoring time τ for both samples.
(2) Obtain a bootstrap resample from each of the two samples generated in Step (1)
above, with each bootstrap sample of size π n (or π n ) obtained by sampling with
replacement from the respective sample obtained in (1).
(3) New “bootstrap estimates” 𝛼𝛼� ∗ , 𝜆𝜆̂∗ , 𝑎𝑎𝑎𝑎𝑎𝑎 𝛽𝛽̂∗ are computed from the combined
bootstrap sample using the ML method. Also estimate the mean life µ under
normal conditions, accounting for the censoring.
(4) Repeat the process given in Steps (2) and (3) 1,000 times and obtain the empirical

distributions of 𝛼𝛼� ∗ , 𝜆𝜆̂∗ , 𝛽𝛽̂∗ , and 𝜇𝜇̂ ∗ .

58
(5) Using the empirical distributions of the 𝛼𝛼� ∗ , 𝜆𝜆̂∗ , 𝑎𝑎𝑎𝑎𝑎𝑎 𝛽𝛽̂∗ obtained from bootstrap
estimates, confidence interval for 𝛼𝛼, 𝜆𝜆, 𝑎𝑎𝑎𝑎𝑎𝑎 𝛽𝛽 is constructed using respective
1−𝛾𝛾

quantiles at �

2

1−𝛾𝛾

�100% and 1 − �

2

�100%..

(6) Using the empirical distributions of the mean 𝜇𝜇̂ ∗ obtained from bootstrap
estimates, construct the lower bound confidence interval for using quantile at
(1 − 𝛾𝛾)100%.

(7) Coverage probabilities were computed based on 1,000 simulation runs obtained
by repeating Steps 1 -7.

59

Population Or
Process

𝐹𝐹𝐹𝐹(𝑡𝑡, 𝜃𝜃�)

Actual Sample Data
From Population Or
Process
Used to estimate model
parameters

DATA
…………

𝐹𝐹(𝑡𝑡, 𝜃𝜃� )

Simulated censored
samples from F�t, θ��
Draw 1000 samples,
each of size n

𝐷𝐷𝐷𝐷𝐷𝐷𝐴𝐴1∗
………
𝜃𝜃�1∗

𝐷𝐷𝐷𝐷𝐷𝐷𝐴𝐴∗2
………
𝜃𝜃�2∗

𝐷𝐷𝐷𝐷𝐷𝐷𝐴𝐴∗𝐵𝐵
………
𝜃𝜃�𝐵𝐵∗

Figure 3.2.Illustrates the parametric bootstrap resampling method

60

Population Or
Process

𝐹𝐹𝐹𝐹(𝑡𝑡, 𝜃𝜃� )

Actual Sample Data
From Population Or
Process
Used to estimate model
parameters

DATA
………
𝜃𝜃�

Simulated censored
samples from F�t, θ��
Draw 1000 samples,
each of size n

𝐷𝐷𝐷𝐷𝐷𝐷𝐴𝐴1∗
………
𝜃𝜃�1∗

𝐷𝐷𝐷𝐷𝐷𝐷𝐴𝐴∗2
………
𝜃𝜃�2∗

𝐷𝐷𝐷𝐷𝐷𝐷𝐴𝐴∗𝐵𝐵
………
𝜃𝜃�𝐵𝐵∗

Figure 3.3. Illustrates the nonparametric bootstrap resampling for parametric
inference.

61
3.4

RESULTS AND DISCUSSION
Only select results from the simulation experiments are reported below for

brevity. All simulations results reported here are for α = (1.5, and 2) and λ = 1, with the
acceleration factor β is set at 1.5 and 2.0. The censoring parameter τ was set at values 1,
and 1.5

By conducting the steps given in Section 3.2 using a computer program written in
the Matlab, the simulation results reported in Tables 3.2 to Tables 3.32 are obtained.
Tables 3.2 and 3.3 show the maximum likelihood estimates of (α, λ, β, and µ). The

estimated expected value of the MLEs for α are close to the true value for when the
sample size is 100 but show a slight upwards bias for smaller sample sizes. A similar
pattern is observed for estimates of β when the censoring time is 1. The results improve
when the censoring time increases to 1.5 or when α increases to 2. Estimates of µ and

λ are quite reasonable when the sample size is greater than 30. In general, when the
sample size increases the estimates of the parameters approach the true values. Tables
3.4 to 3.32 show the simulation result of (asymptotic, parametric bootstrap, and
nonparametric bootstrap) of 95% confidence interval for (α, λ, and β) and the lower 95%

confidence bound of (asymptotic, parametric bootstrap, and nonparametric bootstrap)
for the mean.

Table 3.2a GE Parameters, Acceleration Factor, and Type I Censoring
π

τ

λ

β

μ

n

α
�

λ�

β�

µ�

30
50
1.5 1 1.5 1.2804
75
100

1.740536
1.675096
1.634216
1.570929

1.098127
1.082763
1.062605
1.027731

30
50
1.5 1.5 1 1.5 1.2804
75
100

1.708948
1.673527
1.622074
1.568747

1.071374 1.57073 1.33327
1.073493 1.548163 1.312707
1.052761 1.536937 1.305793
1.023732 1.534235 1.305156

1
.5

α

1.625868 1.385353
1.570141 1.330857
1.532303 1.314904
1.546379 1.30996

62
Table 3.2b GE Parameters, Acceleration Factor, and Type I Censoring
π

τ

1
.5

β

μ

n

α
�

λ�

β�

𝜇𝜇̂

30 2.318859 1.095341 1.553225 1.551502
50
2.24871 1.079897 1.52247 1.528549
2 1 1.5 1.5
75 2.134716 1.041066 1.529489 1.527969
100 2.122318 1.038666 1.533429 1.52009

30 2.282045 1.095051 1.538302
50 2.179126 1.05482 1.519219
1.5 2 1 1.5 1.5
75
2.09156 1.019907 1.527478
100 2.117973 1.039584 1.516357

1.498027
1.509462
1.526625
1.502626

30 2.278941 1.080764 1.590074
50
2.20633 1.05488 1.564727
2 1 1.5 1.5
75 2.115322 1.030532 1.525772
100
2.1418 1.036304 1.551029

1.579644
1.561578
1.543245
1.536084

1
.667

α λ

30
50
1.5 2 1 1.5 1.5
75
100

1

2 1

1

2 1

1.541497 1.537559
1.554918 1.54164
1.521313 1.519886
1.531034 1.524627

2

2.399464 1.120903 2.124475 1.589719
2.250541 1.06822 2.098928 1.571614
2.172331 1.065452 2.028721 1.511861
2.160057 1.048692 2.034841 1.526222

2

30
50
1.5
75
100

2.385585
2.258429
2.173423
2.111775

1.097882
1.075495
1.061296
1.041332

2

30
50
1.5
75
100

2.400406
2.238159
2.134428
2.110595

1.136162 2.12515 1.599587
1.096672 2.054929 1.544576
1.039408 2.051435 1.562317
1.037786 2.033213 1.529881

2

30 2.339819 1.1254 2.008896 1.517782
50 2.23878 1.095246 1.991892 1.501189
1.5
75 2.147047 1.056632 2.029514 1.516827
100 2.106217 1.03097
2.0268 1.518701

.667
1.5 2 1

1.062217
1.040722
1.037873
1.024071

30
50
1.5
75
100

.5
1.5 2 1

2.211647
2.175302
2.122398
2.096505

2.105311
2.035044
2.028121
2.008467

1.553023
1.513903
1.502438
1.499085

63
Table 3.3 Coverage of Asymptotic 95% C.I.s
α =1.5,

λ =1, β =1.5, µ= 1.2804, π=.5,

τ=1

n

parameter Lower Bound Upper Bound
0.886303
3.665721
α
0.423517
1.949804
λ
30
1.000001
2.749131
β
μ
0.906484
0.955879
2.983422
α
0.550844
1.806479
λ
50
1.001205
2.310694
β
μ
0.946696
1.013576
2.686825
α
0.587548
1.653909
λ
75
1.031394
2.206177
β
μ
1.006098
1.103747
2.330024
α
0.664616
1.473539
λ
100
1.106806
2.099396
β
μ
1.053292

width
SD(W) Coverage
2.779418 0.738984
0.978
1.526287 2.016739
0.965
1.74913 2.097398
0.973
0.956
2.027543 1.469277
0.974
1.255635 1.977485
0.960
1.309489 2.494733
0.961
0.955
1.673249 0.764873
0.968
1.066361 1.619368
0.954
1.174782 1.70078
0.957
0.955
1.226278 0.547898
0.959
0.808923 1.27331
0.949
0.99259 0.979626
0.952
0.948

Table 3.4 Coverage of Parametric Bootstrap 95% C.I.s
α =1.5,
n

λ =1, β =1.5, µ= 1.2804, π=.5,

τ=1

parameter Lower Bound Upper Bound
1.331254
2.149819
α
0.391843
1.80441
λ
30
0.743076
2.50866
β
μ
1.014195
1.308252
2.041941
α
0.439819
1.725707
λ
50
0.828421
2.311862
β
μ
1.012725
1.311399
1.957033
α
0.473016
1.652194
λ
75
0.897685
2.16692
β
μ
1.011334
1.309674
1.832185
α
0.54133
1.514132
λ
100
1.022915
2.069844
β
μ
1.017121

width
SD(W) Coverage
0.818565 0.145201
0.949
1.412567 0.049314
0.963
1.765583 0.115011
0.973
0.954
0.733689 0.083439
0.947
1.285888 0.04242
0.961
1.483442 0.105412
0.964
0.954
0.645634 0.10657
0.946
1.179178 0.050176
0.957
1.269235 0.068122
0.961
0.954
0.522512 0.081731
0.945
0.972802 0.038495
0.951
1.046929 0.03674
0.953
0.953

64
Table 3.5 Coverage of Nonparametric Bootstrap 95% C.I.s
α =1.5,

λ =1, β =1.5, µ= 1.2804, π=.5,

τ=1

n

parameter Lower Bound Upper Bound
1.252881
2.228192
α
0.256597
1.939656
λ
30
0.574031
2.677705
β
μ
1.045158
1.238005
2.112187
α
0.316702
1.848824
λ
50
0.686389
2.453894
β
μ
1.047231
1.249583
2.018849
α
0.360116
1.765094
λ
75
0.776162
2.288443
β
μ
1.0460111
1.259646
1.882213
α
0.44819
1.607272
λ
100
0.922677
2.170082
β
μ
1.047725

width
SD(W) Coverage
0.975311 0.173224
0.951
1.683059 0.061136
0.969
2.103674 0.179796
0.974
0.951
0.874182 0.132915
0.950
1.532122 0.061149
0.965
1.767505 0.15137
0.973
0.951
0.769266 0.127347
0.948
1.404978 0.05818
0.962
1.512281 0.08068
0.964
0.951
0.622567 0.107027
0.946
1.159083 0.050324
0.956
1.247405 0.044463
0.960
0.949

Table 3.6 Coverage of Asymptotic 95% C.I.s
α =1.5,
n

λ =1, β =1.5, µ= 1.2804, π=.5,

τ=1.5

parameter Lower Bound Upper Bound
0.933868
3.407471
α
0.567705
1.774346
λ
30
1.000349
2.445438
β
μ
0.970362
0.981381
2.82447
α
0.594318
1.639513
λ
50
1.030054
2.31746
β
μ
0.984372
1.071548
2.518645
α
0.668835
1.543877
λ
75
1.064394
2.180605
β
μ
1.01071
1.108671
2.288746
α
0.705641
1.397243
λ
100
1.165785
2.031982
β
μ
1.065617

width
SD(W) Coverage
2.473602 1.290271
0.984
1.206641 1.799216
0.96
1.445089 5.322325
0.975
0.957
1.843089 0.605814
0.980
1.045195 1.346216
0.962
1.287407 1.840845
0.969
0.956
1.447097 0.757173
0.975
0.875043 2.094424
0.960
1.116211 2.583192
0.963
0.955
1.180075 0.361684
0.964
0.691602 0.940337
0.955
0.866197 1.344257
0.959
0.952

65
Table 3.7 Coverage of Parametric Bootstrap 95% C.I.s
α =1.5,

λ =1, β =1.5, µ= 1.2804, π=.5,

τ=1.5

n

parameter Lower Bound Upper Bound
1.382336
2.03556
α
0.505363
1.637384
λ
30
0.694585
2.446875
β
μ
1.013786
1.410145
1.93691
α
0.558426
1.58856
λ
50
0.834799
2.261527
β
μ
1.012222
1.390069
1.854078
α
0.552144
1.553378
λ
75
0.855868
2.218006
β
μ
1.012077
1.373397
1.764096
α
0.588992
1.458472
λ
100
0.933072
2.135398
β
μ
1.010425

width
SD(W) Coverage
0.653224 0.106312
0.954
1.13202 0.049502
0.963
1.75229 0.046163
0.979
0.955
0.526765 0.088477
0.949
1.030134 0.054323
0.962
1.426728 0.050832
0.974
0.955
0.464009 0.065731
0.948
1.001234 0.04045
0.961
1.362138 0.069633
0.973
0.955
0.390699 0.046292
0.947
0.869481 0.02252
0.959
1.202326 0.040381
0.965
0.955

Table 3.8 Coverage of Nonparametric Bootstrap 95% C.I.s
α =1.5,
n

λ =1, β =1.5, µ= 1.2804, π=.5,

τ=1.5

parameter Lower Bound Upper Bound
1.319793
2.098102
α
0.396978
1.745769
λ
30
0.526812
2.614647
β
μ
1.045852
1.35971
1.987344
α
0.459796
1.68719
λ
50
0.698198
2.398128
β
μ
1.045817
1.345643
1.898504
α
0.456281
1.64924
λ
75
0.72545
2.348423
β
μ
1.047438
1.33599
1.801504
α
0.505743
1.54172
λ
100
0.817956
2.250514
β
μ
1.047012

width
SD(W) Coverage
0.778309 0.1715
0.957
1.34879 0.073165
0.971
2.087835 0.068938
0.981
0.954
0.627635 0.117548
0.953
1.227394 0.056736
0.967
1.699931 0.066081
0.978
0.954
0.552861 0.088861
0.951
1.192959 0.047648
0.965
1.622973 0.102484
0.978
0.953
0.465514 0.043429
0.948
1.035977 0.030483
0.962
1.432558 0.080309
0.974
0.953

66
Table 3.9 Coverage of Asymptotic 95% C.I.s
α =2,

λ =1,

β =1.5,

µ=1.5, π=.5, τ=1

n

parameter Lower Bound Upper Bound
1.124812
5.488589
α
0.45653
2.015863
λ
30
1.000825
2.534655
β
μ
1.065479
1.212398
4.513783
α
0.522441
1.854566
λ
50
1.001737
2.321992
β
μ
1.081032
1.263107
3.871243
α
0.574729
1.664997
λ
75
1.104767
2.196196
β
μ
1.163424
1.31801
3.525307
α
0.604603
1.584901
λ
100
1.110492
2.120666
β
μ
1.189367

width
SD(W) Coverage
4.363777 0.255789
0.981
1.559333 0.069615
0.961
1.53383 0.20974
0.958
0.957
3.301385 2.134716
0.976
1.332125 1.041066
0.957
1.320255 1.529489
0.957
0.956
2.608136 0.093859
0.972
1.090268 0.058044
0.954
1.091429 0.087538
0.954
0.954
2.207297 0.104948
0.968
0.980298 0.041411
0.952
1.010174 0.112572
0.953
0.953

Table 3.10 Coverage of Parametric Bootstrap 95% C.I.s
α =2, λ =1, β =1.5, µ=1.5, π=.5, τ=1
n

parameter Lower Bound Upper Bound
1.694165
2.943554
α
0.320408
1.870273
λ
30
0.595046
2.511404
β
μ
1.0316127
1.780996
2.716424
α
0.478922
1.680873
λ
50
0.811726
2.233213
β
μ
1.0316179
1.745389
2.524044
α
0.494851
1.582482
λ
75
0.957679
2.101298
β
μ
1.0326788
1.744137
2.5005
α
0.511075
1.571056
λ
100
0.962567
2.104292
β
μ
1.032988

width
SD(W) Coverage
1.249389 4.30871
0.956
1.549865 5.301323
0.961
1.916358 7.503472
0.967
0.958
0.935428 0.974083
0.953
1.201951 1.425656
0.956
1.421487 1.743399
0.957
0.958
0.778655 0.785926
0.948
1.087631 1.278039
0.954
1.143619 1.105818
0.954
0.958
0.756363 0.85133
0.947
1.059981 1.492294
0.954
1.141725 1.437931
0.954
0.958

67
Table 3.11 Coverage of Nonparametric Bootstrap 95% C.I.s
α =2, λ =1, β =1.5, µ=1.5, π=.5, τ=1
n

parameter Lower Bound Upper Bound
1.251818
4.411799
α
0.478803
1.943141
λ
30
1.005472
2.691726
β
μ
1.064006
1.288635
3.74735
α
0.570361
1.633126
λ
50
1.046718
2.272096
β
μ
1.127359
1.367648
3.21649
α
0.608303
1.53222
λ
75
1.109874
2.164214
β
μ
1.180841
1.421612
3.17387
α
0.653574
1.464142
λ
100
1.125632
2.122699
β
μ
1.203878

width
SD(W) Coverage
3.159981 0.351353
0.979
1.464338 0.085403
0.958
1.686254 0.244308
0.964
0.957
2.458715 0.241691
0.971
1.062765 0.080249
0.954
1.225378 0.127038
0.956
0.955
1.848842 0.109827
0.966
0.923917 0.055685
0.951
1.05434
0.1211
0.954
0.953
1.752258 0.127457
0.965
0.810568 0.042688
0.949
0.997067 0.057624
0.952
0.952

Table 3.12 Coverage of Asymptotic 95% C.I.s
α =2,
n

λ =1,

β =1.5, µ=1.5, π=.5, τ=1.5

parameter Lower Bound Upper Bound
1.203329
4.645745
α
0.582958
1.742203
λ
30
1.032079
2.33018
β
μ
1.11376
1.269906
3.929247
α
0.636697
1.620818
λ
50
1.060733
2.146322
β
μ
1.1796
1.35047
3.349937
α
0.653978
1.486139
λ
75
1.098455
2.027858
β
μ
1.212948
1.412011
3.294311
α
0.708374
1.456289
λ
100
1.151063
2.002362
β
μ
1.212369

width
SD(W) Coverage
3.442416 0.130657
0.979
1.159245 0.072167
0.965
1.298101 0.039626
0.966
0.956
2.659341 0.142574
0.974
0.984122 0.056806
0.963
1.085589 0.060346
0.964
0.955
1.999467 0.05589
0.972
0.83216 0.024997
0.958
0.929403 0.038901
0.961
0.952
1.882299 0.089669
0.971
0.747916 0.029385
0.954
0.8513 0.045278
0.959
0.952

68
Table 3.13 Coverage of Parametric Bootstrap 95% C.I.s
α =2,

λ =1,

β =1.5,

µ=1.5,

π=.5,

τ=1.5

n

parameter Lower Bound Upper Bound
1.919609
2.644481
α
0.662232
1.447409
λ
30
0.977084
2.09952
β
μ
1.32115
1.852541
2.505711
α
0.703134
1.486967
λ
50
0.984396
2.054043
β
μ
1.320038
1.859323
2.323797
α
0.731617
1.308196
λ
75
1.126363
1.928592
β
μ
1.338596
1.913367
2.322579
α
0.794288
1.28488
λ
100
1.188489
1.844225
β
μ
1.323883

width
SD(W) Coverage
0.724872 0.523763
0.953
0.785177 0.952636
0.956
1.122436 1.199089
0.965
0.951
0.653171 1.305673
0.951
0.783833 2.348957
0.956
1.069647 2.643919
0.964
0.951
0.464474 0.37725
0.947
0.576579 0.685785
0.949
0.80223 1.217863
0.957
0.949
0.409212 0.217596
0.946
0.490591 0.46561
0.948
0.655736 0.695918
0.951
0.951

Table 3.14 Coverage of Nonparametric Bootstrap 95% C.I.s
α =2,
n

λ =1,

β =1.5,

µ=1.5,

π=.5,

τ=1.5

parameter Lower Bound Upper Bound
α
1.251818
4.411799
λ
0.478803
1.943141
30
β
1.005472
2.691726
μ
1.064006
α
1.288635
3.74735
λ
0.570361
1.633126
50
β
1.046718
2.272096
μ
1.127359
α
1.367648
3.21649
λ
0.608303
1.53222
75
β
1.109874
2.164214
μ
1.180841
α
1.421612
3.17387
λ
0.653574
1.464142
100
β
1.125632
2.122699
μ
1.203878

width
SD(W) Coverage
0.978
3.159981 0.351353
0.967
1.464338 0.085403
0.969
1.686254 0.244308
0.958
0.973
2.458715 0.241691
0.964
1.062765 0.080249
0.966
1.225378 0.127038
0.956
0.971
1.848842 0.109827
0.961
0.923917 0.055685
0.964
1.05434
0.1211
0.954
0.970
1.752258 0.127457
0.957
0.810568 0.042688
0.963
0.997067 0.057624
0.953

69
Table 3.15 Coverage of Asymptotic 95% C.I.s
α =2,

λ =1,

β =1.5,

µ=1.5,

π=.667,

τ=1

n

parameter Lower Bound Upper Bound
1.154164
5.039237
α
0.43768
1.97847
λ
30
1.001028
2.757582
β
μ
1.039089
1.229204
4.292912
α
0.489315
1.841252
λ
50
1.014078
2.484245
β
μ
1.07771
1.30664
3.55098
α
0.552958
1.620801
λ
75
1.064717
2.252631
β
μ
1.140656
1.377051
3.475573
α
0.603089
1.597809
λ
100
1.11633
2.204177
β
μ
1.15184

width
SD(W) Coverage
3.885073 0.324596
0.979
1.54079 0.092184
0.966
1.756553 0.117573
0.969
0.958
3.063708 0.160972
0.978
1.351937 0.089279
0.961
1.470167 0.08339
0.963
0.956
2.244339 0.164655
0.972
1.067843 0.042779
0.957
1.187913 0.142825
0.958
0.954
2.098522 0.071369
0.971
0.994721 0.041832
0.955
1.087848 0.071244
0.957
0.953

Table 3.16 Coverage of Parametric Bootstrap 95% C.I.s
α =2,
n

λ =1,

β =1.5,

µ=1.5,

π=.667,

τ=1

parameter Lower Bound Upper Bound
1.881184
2.676698
α
0.544195
1.617332
λ
30
0.825775
2.354374
β
μ
1.316223
1.865775
2.546885
α
0.57059
1.53917
λ
50
0.884242
2.245212
β
μ
1.319241
1.852858
2.377786
α
0.638843
1.42222
λ
75
0.969072
2.082472
β
μ
1.324147
1.898036
2.385564
α
0.675433
1.397174
λ
100
1.034355
2.067703
β
μ
1.328961

width
SD(W) Coverage
0.795514 0.771997
0.951
1.073136 1.379114
0.957
1.528599 1.759849
0.966
0.949
0.68111 0.537325
0.948
0.96858 1.159222
0.954
1.36097 1.510346
0.961
0.949
0.524928 0.27227
0.947
0.783377 0.73819
0.951
1.113401 1.056698
0.958
0.948
0.487527 0.25388
0.946
0.721741 0.653684
0.949
1.033348 0.996751
0.956
0.948

70
Table 3.17 Coverage of Nonparametric Bootstrap 95% C.I.s
α =2,

λ =1,

β =1.5,

µ=1.5,

π=.667,

τ=1

n

parameter Lower Bound Upper Bound
1.26568
4.129411
α
0.484636
1.840151
λ
30
1.001162
2.504954
β
μ
1.058091
1.390639
3.947453
α
0.541281
1.748228
λ
50
1.012852
2.395613
β
μ
1.077546
1.434628
3.28381
α
0.650661
1.560505
λ
75
1.111393
2.134884
β
μ
1.161951
1.406637
3.110757
α
0.630473
1.502043
λ
100
1.143815
2.129617
β
μ
1.184248

width
SD(W) Coverage
2.863731 0.271449
0.977
1.355515 0.13168
0.961
1.503792 0.092099
0.965
0.957
2.556814 0.133658
0.975
1.206947 0.082081
0.959
1.382761 0.148703
0.962
0.956
1.849182 0.179964
0.970
0.909844 0.102515
0.952
1.023491 0.066214
0.956
0.952
1.70412 0.122568
0.968
0.87157 0.082322
0.952
0.985802 0.077929
0.955
0.951

Table 3.18 Coverage of Asymptotic 95% C.I.s
α =2,
n

λ =1,

β =1.5,

µ=1.5,

π=.667,

τ=1.5

parameter Lower Bound Upper Bound
1.232809
4.457629
α
0.54125
1.733208
λ
30
1.001427
2.382551
β
μ
1.083422
1.260234
3.903778
α
0.562567
1.635615
λ
50
1.06169
2.281618
β
μ
1.133477
1.337075
3.459852
α
0.638029
1.539833
λ
75
1.086239
2.087624
β
μ
1.171499
1.399008
3.275216
α
0.669059
1.449411
λ
100
1.140491
2.050325
β
μ
1.212545

width
SD(W) Coverage
3.22482 0.181657
0.979
1.191958 0.052937
0.956
1.381125 0.068153
0.968
0.958
2.643544 0.07673
0.977
1.073047 0.071044
0.963
1.219928 0.053243
0.965
0.956
2.122777 0.084385
0.975
0.901804 0.042428
0.957
1.001385 0.065787
0.961
0.953
1.876208 0.072845
0.974
0.780353 0.037839
0.955
0.909834 0.048918
0.957
0.951

71
Table 3.19 Coverage of Parametric Bootstrap 95% C.I.s
α =2,

λ =1,

β =1.5,

µ=1.5,

π=.667,

τ=1.5

n

parameter Lower Bound Upper Bound
1.905518
2.517776
α
0.739691
1.384743
λ
30
1.0212
2.061794
β
μ
1.321983
1.911204
2.439399
α
0.757291
1.324153
λ
50
1.083335
2.026501
β
μ
1.325385
1.915097
2.3297
α
0.805611
1.270135
λ
75
1.152093
1.890533
β
μ
1.325509
1.9096
2.283411
α
0.812185
1.235958
λ
100
1.193668
1.868399
β
μ
1.327446

width
SD(W) Coverage
0.612258 0.226052
0.950
0.645052 0.601005
0.951
1.040595 1.295443
0.962
0.949
0.528195 0.237054
0.948
0.566862 0.7951
0.949
0.943166 1.481413
0.959
0.948
0.414603 0.185886
0.946
0.464524 0.644159
0.947
0.73844 1.358236
0.954
0.948
0.373811 0.108726
0.945
0.423774 0.439657
0.946
0.674732 0.878456
0.952
0.947

Table 3.20 Coverage of Nonparametric Bootstrap 95% C.I.s
α =2,
n

λ =1,

β =1.5,

µ=1.5,

π=.667,

τ=1.5

parameter Lower Bound Upper Bound
1.269203
3.87178
α
0.547979
1.70635
λ
30
1.004247
2.260736
β
μ
1.083247
1.410204
3.556727
α
0.642983
1.556018
λ
50
1.090268
2.235839
β
μ
1.145845
1.407773
3.034033
α
0.662992
1.443012
λ
75
1.127706
2.140591
β
μ
1.190708
1.446856
2.916606
α
0.69255
1.394713
λ
100
1.14157
1.985018
β
μ
1.2051

width
SD(W) Coverage
2.602577 0.134755
0.977
1.158371 0.0731
0.965
1.256489 0.097995
0.966
0.958
2.146523 0.139628
0.975
0.913035 0.064789
0.958
1.145571 0.072421
0.964
0.955
1.62626 0.123123
0.971
0.78002 0.046011
0.955
1.012885 0.059599
0.962
0.952
1.46975 0.093843
0.969
0.702163 0.045603
0.953
0.843448 0.066669
0.956
0.951

72
Table 3.21 Coverage of Asymptotic 95% C.I.s
α =2,

λ =1,

β =2,

µ=1.5,

π=.5,

τ=1

n

parameter Lower Bound Upper Bound
1.180678
5.254714
α
0.506592
1.988138
λ
30
1.247111
3.310564
β
μ
1.066164
1.254238
4.207019
α
0.525802
1.756541
λ
50
1.35092
3.189649
β
μ
1.139075
1.345242
3.560474
α
0.630792
1.641162
λ
75
1.41278
2.846945
β
μ
1.166284
1.423207
3.352306
α
0.656122
1.526605
λ
100
1.503427
2.710758
β
μ
1.229383

width
SD(W) Coverage
4.074036 0.982995
0.983
1.481546 1.347648
0.963
2.063453 2.46987
0.972
0.958
2.952782 0.779648
0.978
1.230739 0.921677
0.957
1.838729 1.702697
0.967
0.955
2.215231 0.641804
0.974
1.01037 0.984202
0.955
1.434165 1.982925
0.962
0.953
1.929098 0.740477
0.970
0.870483 1.059187
0.952
1.207332 1.952878
0.956
0.952

Table 3.22 Coverage of Parametric Bootstrap 95% C.I.s
α =2,
n

λ =1,

β =2,

µ=1.5,

π=.5,

τ=1

parameter Lower Bound Upper Bound
1.892279
2.906649
α
0.557248
1.684558
λ
30
1.152335
3.096615
β
μ
1.16297
1.841297
2.659786
α
0.620751
1.515689
λ
50
1.31134
2.886517
β
μ
1.139042
1.828085
2.516577
α
0.661838
1.469066
λ
75
1.331545
2.725896
β
μ
1.120542
1.866915
2.453198
α
0.72914
1.368244
λ
100
1.483533
2.586148
β
μ
1.110236

width
SD(W) Coverage
1.01437 0.235433
0.955
1.12731 0.066727
0.955
1.944281 0.160542
0.971
0.953
0.818489 0.172752
0.950
0.894938 0.04958
0.952
1.575178 0.101799
0.964
0.955
0.688491 0.130191
0.947
0.807228 0.041491
0.949
1.394352 0.07054
0.961
0.956
0.586283 0.075365
0.945
0.639104 0.024837
0.946
1.102614 0.048198
0.955
0.956

73
Table 3.23 Coverage of Nonparametric Bootstrap 95% C.I.s
α =2,

λ =1,

β =2,

µ=1.5,

π=.5,

τ=1

n

parameter Lower Bound Upper Bound
1.795159
3.00377
α
0.449315
1.792492
λ
30
0.96618
3.28277
β
μ
1.323013
1.762931
2.738152
α
0.535066
1.601375
λ
50
1.160525
3.037332
β
μ
1.31011
1.762166
2.582496
α
0.58455
1.546353
λ
75
1.198043
2.859398
β
μ
1.340352
1.810782
2.509332
α
0.66795
1.429435
λ
100
1.377964
2.691717
β
μ
1.329761

width
SD(W) Coverage
1.208611 0.361801
0.956
1.343178 0.093729
0.959
2.31659 0.241459
0.975
0.951
0.975221 0.2239
0.954
1.066309 0.060284
0.955
1.876807 0.110246
0.969
0.951
0.82033 0.180288
0.950
0.961803 0.047957
0.954
1.661355 0.092702
0.965
0.949
0.69855
0.0834
0.947
0.761486 0.029565
0.948
1.313753 0.069997
0.958
0.951

Table 3.24 Coverage of Asymptotic 95% C.I.s
α =2,
n

λ =1,

β =2,

µ=1.5,

π=.5,

τ=1.5

parameter Lower Bound Upper Bound
1.11577
5.913243
α
0.517366
1.982876
λ
30
1.194996
3.683123
β
μ
1.04588
1.339687
3.971925
α
0.669776
1.636548
λ
50
1.394634
2.880582
β
μ
1.178838
1.413564
3.616671
α
0.713172
1.538877
λ
75
1.462322
2.740709
β
μ
1.202326
1.467187
3.153735
α
0.754687
1.399557
λ
100
1.528815
2.568824
β
μ
1.263944

width
SD(W) Coverage
4.797474 0.580694
0.984
1.46551 0.931705
0.970
2.488127 1.731279
0.975
0.959
2.632239 0.310943
0.978
0.966772 0.619589
0.961
1.485948 1.649991
0.967
0.952
2.203107 0.372951
0.974
0.825706 0.556209
0.957
1.278387 1.557488
0.964
0.951
1.686548 0.109747
0.972
0.64487 0.290948
0.952
1.04001 0.56538
0.961
0.949

74
Table 3.25 Coverage of Parametric Bootstrap 95% C.I.s
α =2,

λ =1,

β =2,

µ=1.5,

π=.5,

τ=1.5

n

parameter Lower Bound Upper Bound
2.020808
2.750363
α
0.766721
1.429044
λ
30
1.406384
2.804238
β
μ
1.166141
1.991935
2.524924
α
0.808887
1.342104
λ
50
1.492788
2.577299
β
μ
1.136777
1.951479
2.395367
α
0.842167
1.280425
λ
75
1.594817
2.461425
β
μ
1.12091
1.950381
2.273168
α
0.881478
1.201186
λ
100
1.716333
2.300601
β
μ
1.105877

width
SD(W) Coverage
0.729555 0.223073
0.955
0.662323 0.056178
0.953
1.397854 0.138132
0.967
0.953
0.532989 0.1023
0.949
0.533217 0.035152
0.949
1.084511 0.079342
0.962
0.954
0.443888 0.142408
0.948
0.438259 0.071572
0.948
0.866607 0.088798
0.959
0.955
0.322786 0.078887
0.945
0.319709 0.030296
0.945
0.584269 0.068257
0.951
0.956

Table 3.26 Coverage of Nonparametric Bootstrap 95% C.I.s
α =2,
n

λ =1,

β =2,

µ=1.5,

π=.5,

τ=1.5

parameter Lower Bound Upper Bound
1.950957
2.820214
α
0.703307
1.492458
λ
30
1.272547
2.938076
β
μ
1.311975
1.940904
2.575955
α
0.757834
1.393157
λ
50
1.388952
2.681135
β
μ
1.321858
1.908979
2.437867
α
0.800206
1.322386
λ
75
1.511844
2.544398
β
μ
1.333867
1.919476
2.304073
α
0.850867
1.231797
λ
100
1.660392
2.356542
β
μ
1.332677

width
SD(W) Coverage
0.869257 0.350217
0.959
0.789151 0.065922
0.956
1.665529 0.180675
0.971
0.947
0.635051 0.17827
0.952
0.635322 0.037498
0.952
1.292183 0.111022
0.965
0.946
0.528888 0.176183
0.949
0.522181 0.100848
0.949
1.032553 0.11485
0.961
0.945
0.384596 0.101437
0.947
0.380929 0.04092
0.947
0.69615 0.094245
0.954
0.945

75
Table 3.27 Coverage of Asymptotic 95% C.I.s
α =2,

λ =1,

β =2,

µ=1.5,

π=.667,

τ=1

n

parameter Lower Bound Upper Bound
1.18527
5.169248
α
0.485836
2.144273
λ
30
1.197337
3.762314
β
μ
1.019534
1.265367
4.094663
α
0.533431
1.84381
λ
50
1.301443
3.129461
β
μ
1.074997
1.34338
3.477807
α
0.594233
1.613714
λ
75
1.404189
2.959669
β
μ
1.154882
1.441018
3.172844
α
0.66922
1.529088
λ
100
1.449447
2.746254
β
μ
1.191878

width
SD(W) Coverage
3.983978 0.764236
0.983
1.658437 1.246684
0.969
2.564977 1.838645
0.976
0.961
2.829295 0.53232
0.979
1.310379 0.761906
0.962
1.828018 1.449937
0.972
0.957
2.134427 0.554648
0.973
1.019481 1.09164
0.956
1.55548 2.097117
0.967
0.953
1.731826 0.210842
0.971
0.859869 0.425829
0.953
1.296807 0.910654
0.961
0.951

Table 3.28 Coverage of Parametric Bootstrap 95% C.I.s
α =2,
n

λ =1,

β =2,

µ=1.5,

π=.667,

τ=1

parameter Lower Bound Upper Bound
2.008872
2.79194
α
0.697514
1.574811
λ
30
1.37921
2.871089
β
μ
1.163706
1.924886
2.551433
α
0.750658
1.442686
λ
50
1.412114
2.697744
β
μ
1.13942
1.856146
2.41271
α
0.712056
1.366759
λ
75
1.433622
2.669249
β
μ
1.118694
1.906126
2.315065
α
0.815369
1.260203
λ
100
1.584649
2.481777
β
μ
1.108328

width
SD(W) Coverage
0.783068 0.216999
0.951
0.877297 0.059573
0.954
1.491879 0.340091
0.964
0.952
0.626548 0.084447
0.946
0.692029 0.039608
0.948
1.28563 0.106689
0.959
0.955
0.556563 0.117915
0.945
0.654704 0.050946
0.947
1.235628 0.088742
0.957
0.956
0.408939 0.069336
0.941
0.444834 0.04016
0.943
0.897128 0.084253
0.954
0.956

76
Table 3.29 Coverage of Nonparametric Bootstrap 95% C.I.s
α =2,

λ =1,

β =2,

µ=1.5,

π=.667,

τ=1

n

parameter Lower Bound Upper Bound
1.933897
2.866914
α
0.613517
1.658807
λ
30
1.236371
3.013929
β
μ
1.329451
1.864897
2.611422
α
0.6844
1.508944
λ
50
1.289022
2.820836
β
μ
1.319644
1.802858
2.465998
α
0.649371
1.429444
λ
75
1.315317
2.787554
β
μ
1.325531
1.866972
2.354219
α
0.772779
1.302794
λ
100
1.498753
2.567672
β
μ
1.322481

width
SD(W) Coverage
0.933017 0.263416
0.955
1.04529 0.077511
0.956
1.777558 0.431662
0.971
0.948
0.746525 0.112463
0.949
0.824545 0.049949
0.952
1.531815 0.144096
0.965
0.949
0.663139 0.147549
0.947
0.780072 0.061241
0.951
1.472237 0.120466
0.963
0.948
0.487247 0.08616
0.944
0.530015 0.055875
0.945
1.068919 0.12288
0.956
0.948

Table 3.30 Coverage of Asymptotic 95% C.I.s
α =2,
n

λ =1,

β =2,

µ=1.5,

π=.667,

τ=1.5

parameter Lower Bound Upper Bound
1.237376
4.600557
α
0.608433
1.870604
λ
30
1.221473
3.041747
β
μ
1.05579
1.347492
3.843713
α
0.656041
1.675728
λ
50
1.346022
2.92537
β
μ
1.111593
1.392674
3.386375
α
0.681337
1.554317
λ
75
1.402039
2.776832
β
μ
1.153385
1.463351
3.086464
α
0.724027
1.397377
λ
100
1.571962
2.667108
β
μ
1.259611

width
SD(W) Coverage
3.36318 0.449652
0.981
1.262172 0.932927
0.969
1.820275 1.995731
0.975
0.959
2.496221 0.185363
0.978
1.019688 0.571424
0.965
1.579348 1.208899
0.974
0.955
1.993702 0.111297
0.976
0.872979 0.47853
0.963
1.374793 1.352452
0.972
0.952
1.623113 0.069089
0.974
0.673351 0.277008
0.957
1.095146 0.701327
0.966
0.949

77
Table 3.31 Coverage of Parametric Bootstrap 95% C.I.s
α =2,

λ =1,

β =2,

µ=1.5,

π=.667,

τ=1.5

n

parameter Lower Bound Upper Bound
2.024841
2.654797
α
0.823314
1.427485
λ
30
1.42803
2.589763
β
μ
1.164098
1.996617
2.480943
α
0.884618
1.305873
λ
50
1.617316
2.441713
β
μ
1.138449
1.945004
2.349089
α
0.868486
1.244779
λ
75
1.58077
2.403015
β
μ
1.128913
1.945608
2.266826
α
0.898642
1.163298
λ
100
1.741905
2.311696
β
μ
1.107509

width
SD(W) Coverage
0.629956 0.174726
0.956
0.60417 0.065627
0.955
1.161733 0.105384
0.968
0.951
0.484325 0.121479
0.951
0.421255 0.030382
0.949
0.824397 0.113162
0.959
0.953
0.404085 0.128183
0.948
0.376293 0.055322
0.947
0.822246 0.056835
0.959
0.954
0.321218 0.041146
0.946
0.264656 0.025534
0.944
0.569791 0.037611
0.954
0.956

Table 3.32 Coverage of Nonparametric Bootstrap 95% C.I.s
α =2,
n

λ =1,

β =2,

µ=1.5,

π=.667,

τ=1.5

parameter Lower Bound Upper Bound
1.964526
2.715112
α
0.765468
1.485331
λ
30
1.3168
2.700993
β
μ
1.305051
1.950246
2.527314
α
0.844285
1.346206
λ
50
1.538384
2.520645
β
μ
1.319397
1.906316
2.387778
α
0.832458
1.280807
λ
75
1.502044
2.481741
β
μ
1.32296
1.914853
2.297581
α
0.873303
1.188637
λ
100
1.68735
2.36625
β
μ
1.332709

width
SD(W) Coverage
0.750586 0.258982
0.957
0.719862 0.073161
0.958
1.384193 0.143171
0.972
0.947
0.577069 0.144537
0.954
0.501921 0.037447
0.952
0.982261 0.149884
0.965
0.946
0.481462 0.167801
0.951
0.448349 0.071815
0.950
0.979697 0.081917
0.964
0.946
0.382727 0.047641
0.947
0.315334 0.027154
0.946
0.6789 0.043447
0.957
0.945

78
Looking at the performance of the confidence intervals for the distribution
parameters and the acceleration factor, one clear observation that can be made is that the
coverage of all three types of intervals are conservative for small sample sizes and that
the coverage probabilities drop towards the normal or slightly below normal levels as
the sample size increases. When the sample size is 30, the asymptotic intervals for the
shape parameter α remains above normal, and in some cases it becomes highly
conservative (see Tables 3.3, 3.6 for example). In addition, the width of the interval for

α is quite wide relative to the widths of intervals based on the bootstrap methods. While
this width decreases with sample size, it remains much wider than the bootstrap intervals
for any of the sample sizes considered in this study. Among the bootstrap methods, the
parametric intervals for α are consistently narrower than the intervals based on the
nonparametric method even though occasionally the coverage dips slightly below the
normal

level (e.g. Table 3.13, sample size ≥ 75 ). The latter intervals for α are

conservative even for large sample sizes (e.g. Table (3.17), but in other cases they tend
to be slightly liberal (e.g. Table 3.5, n=100). Overall, for intervals estimation of the
shape parameter α , the parametric bootstrap method has good properties that mean the
width of the confidence interval is narrowest compared to the intervals based on other
methods while at the same time, the coverage does not drop much below the normal
level. If slightly liberal intervals for α are a concern, then the recommendation is to use
nonparametric intervals when the sample size is fifty or more, and use the parametric
bootstrap intervals when the sample size falls below fifty.
Results on the intervals for the scale parameter λ show that, in general, the
parametric bootstrap methods yields narrower intervals for small sample sizes 3.15 –
3.17), but there are a few exceptions (e.g. Tables 3.9 – 3.11). In addition, the coverage of
the parametric bootstrap-based intervals does not fall below normal for small sample
sizes. When the sample size is ≥ 75 , however, the asymptotic distribution-based
intervals are narrower than those obtained using the other two methods when the sample
size is 100 (e.g. Tables 3.3 – 3.5), but this is not always the case (for example see Tables
3.12 and 3.13; Tables 3.27 and 3.28). However, in some instances when this happens,
the coverage of the parametric bootstrap intervals is liberal. The nonparametric

79
bootstrap-based intervals tend to provide intervals that are wider than their parametric
counterparts but they also tend to be come liberal for large sample sizes (e.g. Table
3.29). Therefore, for intervals estimation of λ , parametric intervals are recommended
for sample sizes below 75 and the asymptotic distribution based intervals are
recommended for larger sample sizes.
Intervals estimates for the acceleration factor β show conservative coverage for
all three types of intervals when the sample size is 30. This conservative coverage
decrease as the sample size increases, but never becomes liberal as was the case for
other parameters. For sample size 30 with the shape parameter α = 1.5 , the asymptotic
and parametric bootstrap methods provide less conservative coverage than the
nonparametric bootstrap-based intervals. When the shape parameter in increased to 2,
the parametric bootstrap-based intervals are the narrowest in general when the sample
size is 30, while maintaining appropriate coverage (Tables 3.9 through 3.32). When

α = 2, the parametric bootstrap-based intervals are narrower than the other two types of
intervals for sample sizes 50 and 75, and they maintain coverage at or above the normal
level. When α = 1.5 and the sample size is 100, the asymptotic distribution-based
intervals are narrowest (see Tables 3.3 – 3.8). When the shape parameter is equal to 2
and the sample size is 100, the parametric bootstrap method tends to consistently
produce narrower intervals. Overall, the parametric bootstrap-based intervals can be
recommended for the acceleration factor.
For constructing 95% lower confidence bounds for the mean life under normal
use conditions, no discernible difference is seen between the three methods when

α = 1.5 (see Tables 3.3 – 3.8). The estimates of the expected value of the lower bound
are very close to one another for sample sizes 75 and 100. The asymptotic distributionbased bounds are somewhat lower than the bounds based on the bootstrap methods when
= 2 and the censoring time =1, both the
the sample size is 30 or 50. When at

asymptotic method and the nonparametric bootstrap-based method provide bounds with
slightly above normal coverage with expected values are close to each other while the
parametric bootstrap-based bounds display slightly lower expected values with slightly

80
higher than normal coverage (Tables 3.9 – 3.11). When the censoring time increases to
1.5 with α remaining at the value 2, the asymptotic and nonparametric methods
provides bounds with very close expected values and coverages which are slightly above
normal , but the parametric method yields bounds that are higher with closer to normal
coverage (Tables 3.12 – 3.14). The above results were obtained for the case where the
size of the normal use and accelerated samples are the same and the acceleration factor
is set at 1.5. When the proportion of the sample allocated to the accelerated condition
was increased from ½ to 2/3, the same pattern is seen irrespective of the censoring time,
but the coverage of the parametric bootstrap-based bounds drops slightly below normal
when the acceleration factor remains at 1.5 (Tables 3.15 – 3.17 and 3.18 – 3.20). When
the acceleration factor is increased to 2, however, it is the nonparametric bootstrap that
yields higher bounds with slightly above normal coverage decreasing to slightly below
normal as the sample size increases. (Tables 3.21 – 3.23). From the above results it is
apparent that the bounds based on the asymptotic method would suffice if slightly
conservative bounds that in some cases are less sharper than other types of bounds are
acceptable. However, the bootstrap methods provide sharper bounds in some cases but
which of the bootstrap methods perform better depends on the values of the underlying
parameters. Since the practitioner will have no idea what the true value of α and the
acceleration factor β are, but have control over the censoring time π , it is
recommended to use the parametric bounds when using a relatively high censoring time
but the sample is divided equally between the normal use and accelerated use. When π
is close to 2/3 and one has some an idea that the acceleration factor should be high, the
nonparametric bootstrap may be a good choice.

3.5 CONCLUSIONS AND FUTURE WORK
PALT have advantages over the ALT procedure under two scenarios: (1) when
the accelerating factor is unknown or (2) a suitable model that links parameters of the
life distribution to the stress level is not available. However, a drawback to PALT is that
it is not suitable when the products under test have a very long mean life. This is because
part of the sample is tested under normal use conditions and components with a very

81
long expected life span may not fail at all during a reasonably chosen test period. It is.
However, applicable in situations where the life-span of tested products is only
moderately long. This is mostly the case in the chemical industry, where the shelf-life of
a specialty chemical may be only a few months to an year long. This research
contributes to the area of PALT by generalizing an existing procedure that considers
testing products with a generalized exponential distribution. While the previous work
considered Type II censoring, the more difficult case of Type I censoring was
considered in this paper. In addition, this paper develops two bootstrap-based methods
for obtaining confidence intervals for the distribution parameters and the acceleration
factor. Moreover, it utilizes the three methods to obtain lower confidence bounds for the
mean life of the product under normal use conditions. Monte-Carlo simulation Results
show that one or more of the methods perform very well under a wide variety of
conditions.
Future work would involve developing a theoretical justification for using the
result of asymptotic normality for maximum likelihood estimates derived from a PALT
scenario under Type I censoring scheme, while at the same time extending the results for
the case where the shape parameter is less than or equal to two. Additional extensions
would involve generalizing the test situation to include two censoring times for the two
sub-samples, and investigating the behaviour of the proposed procedures under a wider
set of distributional and test parameters.

82
4.

CONCLUSION

Partially accelerated life tests (PALT) have advantages over the accelerated life
test (ALT) procedure when either (1) the accelerating factor is unknown or (2) a suitable
model that links parameters of the life distribution to the stress level is not available.
PALT, however, has a drawback in the sense that it is not suitable when the products
under test have a very long expected life. If the products have a long life, then the portion
of the test sample that is tested under normal use conditions may not produce a few
failures at best during a reasonably chosen test period. It is, however, applicable in
situations where the life-span of tested products is only moderately long. This is mostly
the case in the chemical industry, where the shelf-life of a specialty chemical may be only
a few months to a year long.
This research consisted of two main studies. The first study extended a currently
available method, for construction confidence intervals for distributional parameters of
the underlying Weibull distribution and the acceleration factor, to cover Type I
censoring case. It also developed two bootstrap-based methods for obtaining prediction
intervals for distribution parameters and the acceleration factor. In addition, asymptotic
distribution based intervals were also considered. More importantly, a method of
obtaining lower confidence bounds for the mean life under normal use conditions was
also developed. The performance of the three methods was studies using a Monte-Carlo
study. Results show that all methods perform reasonable well under all parameter
combinations employed in the Monte-Carlo study.
The second study contributes to the area of PALT by generalizing an existing
procedure that considers testing products with a generalized exponential distribution.
While the previous work considered Type II censoring, the more difficult case of Type I
censoring was considered in this paper. In addition, this paper develops two bootstrapbased methods for obtaining confidence intervals for the distribution parameters and the
acceleration factor. Moreover, it utilizes the three methods to obtain lower confidence
bounds for the mean life of the product under normal use conditions. Monte-Carlo

83
simulation Results show that one or more of the methods perform very well under a
wide variety of conditions.

84
APPENDIX

Result 2.1 given in Section 2 can be proved using the Theorem B.41 in Meeker and
Escobar (1998) which gives the regularity conditions necessary for the asymptotic
normality of the MLEs.
Regularity Conditions for Location-Scale Distributions
When Y [or a transformation of T such as Y=log(T)] is location-scale with pdf,
fY =
( y; θ )

1  y−µ 
f
=
,θ
σ  σ 

( µ , σ ) , − ∞ < y < ∞, − ∞ < µ < ∞, σ > 0, the “regularity”

conditions can be expressed as follows:
• φ ( z ) > 0 for all −∞ < z < ∞
• The following limits hold:

∂φ ( z ) 
lim  z 2 ×
0
=
z →±∞
∂z 

• The second derivative ∂ 2φ ( z ) ∂z 2 is continuous.

• The matrix

 ∂ 2 log φ ( z )  
E −
,
′
θ
θ
∂
∂



is positive definite and all its elements are finite.

First we show that the log of the Type I censored Weibull variables have a location-scale
Family.
Let X  W ( λ , α ) ,

Y = ln ( X ) , and τ 0 = ln (τ )

Y has a location-scale distribution, namely its cumulative distribution function (cdf) is

85

)

(

τ 0 ) P ln ( X ) ≤ y ln ( X ) ≤=
τ0
P (Y ≤ y Y ≤=

P(ln ( X ) ≤ y )
P(ln ( X ) ≤ τ 0 )

  e y α 
1 − exp −   
y
l
P( X ≤ e )
   
= =
τ0
P( X ≤ e )
  eτ 0 α 
1 − exp −   
  l  
  y − ln l  
( ) 

1 − exp −e 
 1 − exp −e  y − u  
1
 



 
α  
  b  ,
= =
  τ − ln l  
( )   1 − exp −e  τ 0 − u  

1 − exp −e  0

  b 
1

 
α

1
with location parameter u = ln ( l ) and scale parameter b =
since
α


τ −u 
y −u
H ( y τ 0 ) = P (Y ≤ y Y ≤ τ 0 ) = P ( u + bZ ≤ y u + bZ ≤ τ 0 ) = P  Z ≤
Z≤ 0

b
b 

 y − u τ0 − u 
=
Φ
Φ z τ*
=
b 
 b

( )

  y −u  
1 − exp −e b  
z

 1 − exp −e
.
=   τ −u   =
*
  0b   1 − exp −eτ
1 − exp −e  



{ }

{ }

( )

where Φ z τ ∗ is the CDF of a Gumbel with Type I censoring. Therefore,

{ }
{ }

{ }
{ }

( z) 
( z)
z

∂Φ ( z ) ∂  1 − exp −e  e exp −e
=
=
zτ
φ=
,
*
*
∂z
∂z  1 − exp −eτ  1 − exp −eτ



( )
*

( )

{ }
{ }

{

}(

)

( z) 
( z)
 z
ez −1
∂φ z τ *
∂  e exp −e  − exp z − e
,
= =
*
*
∂z
∂z  1 − exp −eτ 
1 − exp −eτ



( )

{ }

where φ z τ * is the standard Gumbel Distribution with Type I censoring.

( )  =
0,


∂φ z τ *
2

We need show lim z ×
z →±∞ 
∂z





86


( )  =
lim  z


∂φ z τ *
2

which is equivalent to lim z ×
z →∞ 
∂z

and therefore,

∂φ z τ *
2
lim  z ×
z →−∞ 
∂z








z →∞

2

( )  =lim  z



z →−∞




{

}(

)

}(

)

− exp z − e( z ) e z − 1 
=
×
0,
τ*

1 − exp −e


2

{ }

{

− exp z − e( z ) e z − 1 
=
×
0.
τ*

1 − exp −e


{ }

( ) is continuous. Note that

∂ 2φ z τ *
We also need to show that

( )

∂z 2

{

}
{ }
exp { z − e( ) } ( −3e + e
=
1 − exp {−e }

{

}(

) 

( z) 

 − exp z − e( z ) e z − 1
∂ 2φ z τ *
∂ 2  exp z − e
∂


=
=
*
*
∂z 2
∂z 2  1 − exp −eτ  ∂z 
1 − exp −eτ



z

z

τ

2z

)

{ }




+1

*

.

The distribution under the acceleration factor β is:

(

)

Y = β −1 X .=
T ln=
(Y ) ln β −1 X . Therefore,

X  W ( λ,α )

Y has a location-scale distribution, and its cumulative distribution function (cdf) is

(

)

P (T ≤ t =
T ≤ t 0 ) P ln ( β −1 X ) ≤ t ln ( β −1 X=
) ≤ t0

P(ln ( β −1 X ) ≤ t )

P(ln ( β −1 X ) ≤ t 0 )

  β et α 
1 − exp − 
 
−1
t
t
P( β X ≤ e )
P( X ≤ β e )
  l  

=
=
=
0
) P( X ≤ β e 0 )
P( β −1 X ≤ ett
  β et 0 α 
1 − exp − 
 
  l  

 t − ln ( l ) + ln ( β )  



 t − u 
1 − exp − exp 
1 − exp − exp 
1




α
 β 



=
=




t
ln
ln
l
β
+
−
( ) ( )   1 − exp − exp  t 0 − u  

1 − exp − exp  0

 β 

1



α


1
with location parameter
=
u ln ( l ) − ln ( β ) and scale parameter b = .
α
Now let T = bZ + b

⇒Z =

T −u
. Then
b

87


t −u 
t −u
P (T ≤ t T ≤ tt
Z≤ 0
0 ) = P ( u + bZ ≤ t u + bZ ≤ 0 ) = P  Z ≤

b
b 


 t − u 
1 − exp − exp 

 t − u t0 − u 
 b 

∗
=
Φ
Φ zt =
=
b 

 t − u 
 b
1 − exp − exp  0

 b 


( )

=

1 − exp {− exp ( z )}

{

( )}

1 − exp − exp t ∗

,

τ0 − u

( )

where Φ z τ ∗ is the CDF of a Gumbel with Type I censoring and τ ∗ =

b

.

z
∂Φ ( z ) ∂  1 − exp {− exp ( z )}  e exp {− exp ( z )}
zτ
=
=
This implies that φ =
∂z
∂z  1 − exp − exp τ *  1 − exp − exp τ *



( )
*

{

( )

( )}

{

{

}

( )}

z
 z

∂φ z τ *
∂  e exp {− exp ( z )}  − exp z − e exp ( z ) − 1
=
=
and
that
,
∂z
∂z  1 − exp − exp τ * 
1 − exp − exp τ *



{

( )

( )}

{

( )}

where φ z τ * is the standard Gumbel Distribution with Type I censoring.
Then we show that the condition

( )  =
0

holds as follows:


∂φ z τ *
2

lim z ×
z →±∞ 
∂z


∂φ z τ *
2
lim  z ×
z →∞ 
∂z


∂φ z τ *
2

lim z ×
z →−∞ 
∂z


( )

( )



z


−e z − e e z − 1
2
=
lim  z ×
 z →∞ 
1 − exp − exp τ *


z −e z


−
e
ez −1
2

=
lim z ×
 z →−∞ 
1 − exp − exp τ *



{

( ) is continuous. Note that

(

)


=
0 and



=
0.



( )}
( )
{ ( )}

∂ 2φ z τ *
Also we need show that

∂z 2

( )

{

}
( )}

{

}

z




∂ 2φ z τ *
exp z − e z
∂2 
∂  − exp z − e exp ( z ) − 1 

=
=

∂z 2
∂z 2  1 − exp − exp τ *  ∂z 
1 − exp − exp τ *





{

{

}

exp z − e z  −3e z + e 2 z + 1
.
=
1 − exp − exp τ *

{

( )}

{

( )}

88
Clearly, the above function is continuous.
In addition, we wish to show that

( )  =lim  z


∂φ z τ *
2
lim  z ×
z →−∞ 
∂z





z →−∞




(

×

{

lim  z 2 × −e z −e
z →−∞ 
z
Applying the quotient rule, we write lim  z 2 × −e z −e
z →−∞ 
z

(

)

(

)

 lim − z 2 e z e z − 1 
z →−∞
=

lim e z


z →−∞
e



(

) 

 lim − z 2 e z e z − 1
 z →−∞
=
e0







=
0.



( )}
( e − 1) .
( e − 1) as

1 − exp − exp τ *

In order to show this we examine

 lim − z 2 e z e z − 1 
z →−∞
=

z


lim ee
z →−∞



)

−e z − e e z − 1
z

2

z

z

(

) 

 lim − z 2 e z e z − 1
 z →−∞
−∞

ee


(




)

=
lim − z 2 e z e z − 1 .
z →−∞

By the product rule,

(

)

lim z 2 e z=
1 − ez

z →−∞

1 − e ) ( lim z e ) .
(1 − e ) ( lim z e ) (=
( lim z e ) lim=
2 z

z →−∞

z

2 z

z →−∞

−∞

z →−∞

2 z

z →−∞

To prepare product z 2 e z for solution by L'Hôpital's rule, we write it as

z2
.
e− z

Applying L'Hôpital's rule, we obtain,
d 2
z
z2
2z
=
−2 lim ze − z .
lim − z =
lim dz
lim − z =
lim − 2 ze − z =
z →−∞ e
z →−∞ d
z →−∞ −e
z →−∞
z →−∞
−z
e
dz

To prepare the product ze − z for solution by L’Hôpital’s rule, write as

z
:
e− z

Applying L'Hôpital's rule, we obtain,
d
z
1
z
2 lim e z 2e −∞ =
0,
−2 lim − z =
−2 lim dz =
−2 lim − z ==
→−∞
z →−∞ e
z →−∞ d
z
z →−∞
−
e
−z
e
dz

(

)

z

−e z − e e z − 1
2
which implies lim  z ×
z →−∞ 
1 − exp − exp τ ∗


{

( )}


=
0.



(1)

89


( )  =
lim  z


∂φ z τ *
2
In order to show that lim  z ×
z →∞ 
∂z





z →∞

)

(




×

(

)

−e z − e e z − 1
z

2

{

( )}

1 − exp − exp τ *


=
0 , we look at



e 
 lim  − z 2 e 2 z −e + z 2 e z −=
lim  z 2 × −e z −e e z − 1=
lim z 2 e z −e − lim z 2 e 2 z −e
 z →∞ 
 z →∞
z →∞ 
z →∞
2 z
2 2z
ze
ze
= lim e z − lim e z .
z →∞
z →∞
e
e
z

z

z

z

Applying L'Hôpital's rule, we obtain,
d 2 z
d 2 2z
ze
ze
dz
dz
lim e z − lim e z = lim
− lim
z →∞
z →∞
z →∞ d
z →∞
d ez
ez
e
e
e
e
dz
dz
2 ze z + z 2 e z
2 ze 2 z + 2 z 2 e 2 z
lim
= lim
−
z
z
z →∞
z →∞
ee + z
ee + z
2 ze z + z 2 e z
ze 2 z + z 2 e 2 z
2
lim
= lim
−
z
z
z →∞
z →∞
ee + z
ee + z
z 2e z

z 2e2 z

= lim

(

ez 2z + z2

z →∞

ee

z

+z

) − 2 lim e ( z + z )
2z

z →∞

ee

z →∞

z

+z

( 2 z + z ) − 2 lim ( z + z ) .
2

= lim

2

ee

2

z

z →∞

ee

z

−z

Also, applying L'Hôpital's rule, we obtain,
d
d
2z + z )
(z + z )
(
z+z )
2z + z )
(
(
= lim
− 2 lim
= lim dz
− 2 lim dz
2

z →∞

ee

z

2

2

z

−z

2

z →∞
d ez
d ez − z
e
e
dz
dz
(1 + 2 z ) .
( 2 + 2z )
= lim z + e z − 2 lim e z − z z
z →∞
z →∞
e
e
e −1

z →∞

ee

z →∞

(

Again, applying L'Hôpital's rule, we get that

)

z

90

lim
z →∞

( 2 + 2z )
e z +e

z

d
d
(1 + z )
(1 + 2 z )
dz
dz
− 2 lim e z − z z
=
2 lim
− 2 lim
z
z
z →∞
z →∞ d
z →∞ d
e
e −1
e z +e
ee − z e z − 1
dz
dz
1
2
= 2 lim e z − z z
− 2 lim z
2
z
z →∞
z →∞ e − z z
e
e +1
e e + e z − 1 ee − z

(1 + 2 z )

(

)

(

(

=

(

2

e

e∞ −∞

(e

)

z

−e z − e e z − 1
2
And therefore lim  z ×
z →∞ 
1 − exp − exp τ ∗


{

( )}

)

)

∞

)

+1

−

(

)

4
ee

∞

−∞ ∞

(

)

2

e + e∞ − 1 ee

∞

−∞

=

2 4
− = 0,
∞ ∞


=
0.



(2)

From (1) and (2) we obtain,

(

)

z −e z

−
e
ez −1 
2

=
lim z ×
0.
∗
z →±∞ 
1 − exp − exp τ 


2
 ∂ log φ ( z )  
We need to show further that E −
 is positive definite and all its
∂θ∂θ ′



{

( )}

elements are finite.
First we need find ML function of the location-scale Distribution.
In Type I censoring, τ is fixed but the number of failure values observed in time τ is a

random variable. The number of items, R, failing before time τ is assumed to follow a
binomial distribution R  Bin ( n, p ) , where


 τ − u 
p=
FZ (τ 0 ; u , b ) =
1 − exp − exp  0
1 − exp − exp (τ * ) , under nominal use
 =
 b 


{

}

conditions. Under high stress conditions the number of items failing will have a
Binomial Bin ( n, p* ) , distribution where


 τ 0 − u *  
p =
FZ (τ 0 ; u , b ) =
1 − exp − exp 
1 − exp − exp (τ * ) . Then, for observation
 =

 b  
*

*

i under nominal use conditions, we have,

{

}

91

xi ≤ τ
1
=
δ ui =
, i 1, 2, , nπ .
0/w
0

yj ≤τ
1
=
δ a j =
, j 1, 2, , nπ ,
0
0
/
w


δu =
1−δ u ,
δa =
1−δ a ,
i

i

j

j

with
np

δ u  Ber ( p) ⇒ ∑ δ u  Bin(np , p),
i

i =1

i

np

δ a  Ber ( p) ⇒ ∑ δ a  Bin(np , p).
j

j

j =1

We also have, under nominal use conditions,


 t − u 
 1 − exp − exp 

 b 


,
 t − u t0 − u  

t 0 − u 

Φ
1 − exp − exp 
=

b  
 b 
 b


 1,


t≤

t>

( z)
z
∂Φ ( z ) ∂  1 − e − e  e z e − e
zτ
φ=
=
 =
* 
* ,
∂z
∂z  1 − e − eτ  1 − e − eτ

( )
*

( )

where φ z τ * is the standard Gumbel distribution.
Also, letting z =

t −u
we have,
b


 t −u 
 t − u 
exp 
 exp − exp 

 t − u t0 − u 
 b 
 b 

.
=
φtφ
(z 0) =


b 

 t 0 − u 
 b
1 − exp − exp 

 b 


t0 − u
b

t0 − u
b

92
Thus, given R = nu , the conditional density of the first r failure times under the
nominal use condition is equivalent to the joint density of an ordered random sample of
size nu from a truncated Weibull distribution, given by

(

 t − u t0 − u 

b 
 b

)

*
R n=
R n=
nu !∏
zi =
φφφtφ
( z(1) ,, z(n ) =
( t(1) ,, t(n ) =
u)
u)
tttt

nu

u

u

i =1

nu

nu
∑ zi




i =1
e exp − exp ∑ ( zi ) 
zi

nu 
e exp {− exp ( zi )}
i =1

 .

 nu !
= n=
u !∏
nu



−

u
t


i =1

0

 t 0 − u  
−
−
1
exp
exp



 

1 − exp − exp 
 
 b  


 b  



The joint density of obtaining R = nu ordered observations at the values z(1) , , z( nu )
before time τ may be expressed as,

(

)

(

)

nu bin(nu ; n , p )
φφp
=
=
τ z(1) , , z( nu )
τ z(1) , , z( nu ) R
nu

∑ zi

nu


exp − exp ∑ ( zi ) 
i =1


= nu !
nu


 τ − u  
1 − exp − exp  b  

 



e

i =1

 np  nu
np − nu
  p (1 − p )
 nu 

nu

∑ zi

nu


e i=1 exp − exp ∑ ( zi ) 
( np )!
i =1

 1 − exp − exp τ *
=
{ ( )}
nu
*
!
n
n
p
−
(

u ) 1 − exp − exp τ
(
)



{

=

( np )!

( np − nu )!

nu

e

∑ zi
i =1

}

(

( {

})

nu


exp − exp ∑ ( zi )  exp − exp (τ * )
i =1



) ( exp {− exp (τ )})
nu

*

np − nu

np − nu

.

Therefore, we can state that,
nu

( np )! e∑ z exp − exp n z  exp − exp * np −n
,
,
z
z
φt

=
( i ) ( {
( )})
(
)

∑
(1)
(n )
t
( np − nu )!
i =1


u

i

i =1

u

u

φφ
( z(1) ,, z(n ) ) = ( t(1) ,, t(n ) ) ∝ e
tt
u

u

nu

 t −u 



∑ zi  i b
i =1

nu


 t − u  
 t 0 − u  
exp − exp ∑  i
   exp − exp 
 
b  
 b  
i =1 



np − nu

.

Similarly to the argument made about the joint density of observations under nominal use
conditions, given R = na the conditional density of the first r failure times under
acceleration is equivalent to the joint density of an ordered random sample of size na

93
from a truncated accelerated Weibull distribution. Therefore, for an item tested at
accelerated condition, the probability density function is given by


 t ′ − u′  
 1 − exp − exp 

t − u′
 b 


t′ ≤ 0
,
 t ′ − u′ t 0 − u′  
b

 t − u′  
Φ
1 − exp − exp  0

=

b  
 b 
 b


t − u′
t′ > 0
 1,
b

z′ )
(
z′
∂Φ ( z ′ ) ∂  1 − e − e  e z′e − e
which yields φ =
=
z ′ τ *′
 =
* 
* ,
∂z ′
∂z ′  1 − e − eτ ′  1 − e − eτ ′

( )

( )

where φ z ′ τ *′ is the standard Gumbel distribution.

=
Also, z ′

t 0 − u′
t ′ − u′
, and t *′
=
, and therefore,
b
b


 t ′ − u′ 
 t ′ − u′  
exp 
exp − exp 


b 
b 
 t ′ − u′ t 0 − u′ 



*′
=
=
z′
,
φtφ


b 

 t 0 − u′ 
 b
1 − exp − exp 

 b 


( )

The joint density of obtaining R = na ordered observations at the values Y(1) , , Y( n

a)

before time, may be expressed as,

(

)

(

)

′
′
′
′
=
=
nu bin(nu ; n , p )
φφp
τ 0 z(1) , , z( nu )
τ 0 z(1) , , z( nu ) R
nu

nu
∑ zi′


e i=1 exp − exp ∑ ( zi′ ) 
i =1

  np  p nu 1 − p np − nu
nu !
(
)

nu 


 τ 0 − u ′    nu 
1 − exp − exp 
 
 b  


nu

∑ zi′

nu


e i=1 exp − exp ∑ ( zi′ ) 
nu
np − nu
( np )!
i =1

 1 − exp − exp τ *′   exp − exp τ *′ 
=




 

( np − nu )! 1 − exp − exp τ *′  nu 



{ ( )}

=

( np )!

nu

{ ( )}

{ ( )}

np − nu
nu
∑ zi′


.
e i=1 exp − exp ∑ ( zi′ )   exp − exp τ *′ 

( np − nu )!
i =1



Therefore, we can state that

{ ( )}

94
nu

}

{

( np )! e∑ z′ exp − exp n z′   exp − exp *′
′
′

=
φt
z
z
,
,
( i ) 

∑
t ( (1)
(n ) )
( )
( np − nu )!
i =1


u

i

i =1

u

0





np − nu

φφ
( z(1)′ ,, z(′n ) ) = ( t(1)′ ,, t(′n ) )
tt
u

0

u

nu

∝e

 t ′ −u ′ 



∑ zi′  i b
i =1

nu


 t ′ − u′   
 t 0 − u′   
exp − exp ∑  i
   exp − exp 
 
b  
 b  
i =1 



(

and the total likelihood function for t1 , δ u1 ,..., tnπ , δ unπ , t1′, δ a1 ,..., tn′π , δ anπ

(

) (

)

np − nu

,

) is as follows,

(

=
L L=
b t ′j , δ u j
Lui ( u , b ti , ) Lu j u , b, b ′ t ′j , δ u j
(θ t , t ′) Lui u, b ti , δ ui Lu j u′,=
  ti b−u 

 t − u  
=
e  exp − exp  i
∏
 
b

 
i =1 


np

δ ui

  t ′j −u′ 

 t ′j − u ′  
b
× ∏ e  exp − exp 
 
b

j =1 

 


np



 t 0 − u  
 exp − exp 
 
 b    



δa j

)

δ ui



 t 0 − u′    
 exp − exp 
 
 b    



δa j

  ti b−u 


 ti − u   np 
 t 0 − u  


exp − exp 
L=
e
∏
  ∏  exp − exp 
 
 b  =i nu +1 
 b    
=i 1 



nu

  t ′j −u′ 
 t ′j − u ′   np 


 t 0 − u′    
b
× ∏ e  exp − exp 
  ∏  exp − exp 
 
 b    
j =1 

 b   i = na+1 


na

nu

 t −u 

∑  i b 
 nu

 t − u   
 t 0 − u  
exp −∑ exp  i
e i=1
=
   exp − exp 
 
 b   
 b    

 i =1
na

×e

 t ′j −u ′ 

b 

∑ 
j =1

np − nu

na
 t ′j − u ′   


 t 0 − u′    
exp − exp ∑ 
   exp − exp 
 
b   
 b    
j =1 



np − na

.

The MLE’s of the parameters can be estimated numerically by minimizing the log
likelihood function. The log likelihood is expressed as,

95
=
L (θ t , t ′ ) l ,
ln L ln=
np − nu
 ∑u  ti −u 

nu




−

−

t
t
u
u




b 

i
0
=
i
1
e

−
exp −∑ exp 
exp
exp

  

 


 b   
 b    

 i =1
l = ln  na ′ ′

 t −u 
np − na
na
 ∑  j b 

′
′



−

t
u


′



−

t
u


 0

j
exp − exp ∑ 
×e j=1

   exp − exp 
 
b   
 b    
j =1 




n
na
 nu  t − u  nu
 t ′j − u ′  
 ti − u  a  t ′j − u ′ 
= ∑  i
−
+
−
exp
exp
∑



 ∑

 ∑
b 
 b  i1
 b  j 1  b=
=
=
j 1

 i 1=
np − na
np − nu
 





 tt
 0 − u′    
0 − u 
× ln   exp − exp 
−
exp
exp













 b    
 b    


 


n
n
 nu  t − u  nu
 t ′j − u ′  
 ti − u  a  t ′j − u ′  a
= ∑  i
−
+
−
exp
 ∑ exp 

 ∑

 ∑
b  j1
 b  i1
 b  j 1 =
=
=
 b 
 i 1=
−u 
 tt
 0 − u′ 
− ( npp
− nu ) exp  0
 − ( n − na ) exp 
,
 b 
 b 
n

where u ′= u + ln ( β ) .
The normal equations are obtained by differentiating the log likelihood with respect to
the parameters and setting them to zero. The Score equations are:
na
 t ′j − u ′ 
∂l −nu 1 nu
 t − u  na 1
exp
=
+ ∑ exp  i
−
+


∑

du
b
b i 1=
b 
 b  b b
j 1
=

+

− nu )
( npp
− u  ( n − na )
 tt
 − u′ 
exp 0
exp 0
+

 b 


nu
na
 t ′j − u ′ 
−nu − na 1
 t −u  1
=
+ ∑ exp  i
+ exp ∑ 


b
b i 1=
b 
 b  b
j 1
=
− nu 
 npt
 − u  ( np − na )
 t − u′ 
exp  0
exp  0
+
+


 = 0,
b
 b 
 b 
 b 
b




b




b

nu
n
n
n
 t ′j − u ′ 
∂l
 t −u  u  t −u 
 t − u  a  t ′j − u ′  a  t ′j − u ′ 
=
−∑  i 2  + ∑  i 2  exp  i
−
+
∑
 2  ∑  2  exp 


db
b  j 1 b 
 b  i 1 b 
 b  j 1 =
i 1=
=
=
 b 

−u 
 tttt
 −u 
 − u′ 
 − u′ 
0,
+ ( npp
− nu )  0 2  exp  0
+ ( n − na )  0 2  exp  0

=
 b 
 b 
 b 
 b 

96

and

na
 t ′j − u ′  ( np − na )
n 1
∂l
 t − u′ 
exp  0
0.
=
− a + exp ∑ 
+

=
du ′
b b
b 
b
 b 
j =1 

We also derive the second derivatives:
n
nu + na 1 nu  ti − u 
∂ 2l
 ti − u  1 u
 t −u 
exp
exp  i
=
−
−
∑




2
2 
2 ∑
b
b i 1=
∂u∂b
 b 
 b  b i1
 b 
=

 t ′j − u ′  1 na
 t ′j − u ′ 
1 na  t ′j − u ′ 
− ∑  2  exp 
 − 2 ∑ exp 

b j 1=
=
 b 
 b  b j1
 b 
−

− nu )  ttt
( npp
 − u  ( n − nu )
 −u 
0 −u 
exp 0
exp 0
−

 b2 
 b 
 b 
b2






′
( np − na )  ttt
 − u ′  ( np − na )
 − u′ 
0 −u 
−
exp  0
exp  0
−



,
2
b
b
 b 
 b 
 b 
b

na
 t ′j − u ′  ( np − na )
−na
∂ 2l
 t − u′ 
=
−
exp
exp  0
∑


,
2
2
∂u∂u ′ b
b 
b
 b 
j =1 

na
 t ′j − u ′ 
 t ′j − u ′ 
− u′ 
n
∂ 2l
 tt
 − u′ 
=a2 − 2∑  3  exp 
− 2 ( np − na )  0 3  exp  0

,
∂b∂u ′ b
b 
 b 
 b 
j =1 
 b 

n
n
− nu )
− u  ( n − na )
 t ′j − u ′  ( npp
 ti − u 
 tt
 − u′ 
=
−
−
−
n
exp
exp
exp  0
exp  0
∑
 3 
−
,
u∑


2
3
2
2
∂u
b 
b
b
 b 
 b 
 b 
=
i 1=
j 1

∂ 2l

u

a

and
2
nu
n
∂ 2l
 ti − u  u
 ti − u   ti − u 
 ti − u  
=−
+
2
exp
2

∑
 b3  ∑
 b   b2 
 b3  
∂b 2

 i1

 


 
i 1=
=
2
 t ′j − u ′  na
 t ′j − u ′   t ′j − u ′ 
 t ′j − u ′  
+ 2∑  3  − ∑ exp 
+
2

  2 
 3 
=j 1 =
 b  j1
 b   b 
 b  
na

2

 tt
 t 0 − u  
0 − u   0 − u 
− ( np − nu ) exp 
2
+

  2 
 b3  
 b   b 

 
2
′ 
′
 ttt
 0 − u ′  
0 − u   0 − u 
2
− ( np − na ) exp 
+

  2 
 b3   ,
 b   b 

 

97
na
 t ′j − u ′  ( np − na )
∂ 2l −1
 t − u′ 
exp
exp  0
=
−
∑



2
2
2
∂u ′
b
b 
b
 b ,
j =1 

na
 na
 t ′j − u ′  ( np − na )
 −∂ 2l 
 t − u′  
E
E
exp
exp 
=
+

∑



 > 0 ,
2
2
b 
b
 b  
j =1 
 ∂u∂u ′ 
 b

na
 t ′j − u ′ 
 −∂ 2l 
nu nu
 ti − u  1
E  2  E{ 2 ∑ exp 
exp
=
+


∑
 2
b i 1=
b 
 b  b
j 1
=
 ∂u 

×

− nu )
( npp
− u  ( n − na )
 tt
 − u′ 
+
exp 0
exp 0
} > 0,
b2




b




b2




b




na
1
 t ′j − u ′  ( np − na )
 −∂ 2l 
 t − u′  
exp  0
E 2 =
E  2 exp ∑ 
+

 > 0
2
b 
b
 b 
j =1 
 ∂u ′ 
b

2
nu
n
 −∂ 2l 
 ti − u  u
 ti − u   ti − u 
 ti − u  
2
E 2 =
E{−2∑  3  + ∑ exp 
+

  2 
 b3  
 b  i1
 b   b 

 
i 1=
 ∂b =

2
 t ′j − u ′  na
 t ′j − u ′   t ′j − u ′ 
 t ′j − u ′  
− 2∑  3  + ∑ exp 
  2  + 2  3  
=j 1 =
 b  j1
 b   b 
 b  
na

2
 t 0 − u   tt
 − u 
0 −u 
+ ( np − nu ) exp 
 2  + 2  0 3  

 b   b 
 b  
2
− u ′   0 − u ′ 
 ttt
 − u′  
+ ( np − na ) exp  0
 2  + 2  0 3  },

 b   b 
 b  
Further simplifying the first and second parts,

2
n
 ti − u  u
 ti − u   ti − u 
 ti − u  
−2∑  3  + ∑ exp 
 2  + 2  3  

 b  i1
 b   b 
 b  
=i 1 =
nu

2
n
  ti − u 2
 ti − u  u   ti − u  1  ti − u 
 t − u 
=
−2∑  3  + ∑ 1 + 
+ 
+   2  + 2  i 3  


 b  i 1   b  2!  b 
 b  
=i 1 =
  b 
nu

98
nu
 t −u 
= −2∑  i 3 

i =1  b
2
2
2
2
nu  

 t −u 
 t − u    t − u 
 t − u    t − u   ti − u 
 ti − u   1  ti − u 
+ ∑   i 2  + 2  i 3   +  i 2  + 2  i 3    i
+
+
2
+ 
 2 




3 
 b    b 
 b    b   b 
 b   2!  b 
i =1  

  b 
nu
nu
 t −u 
 t −u 
=
−2∑  i 3  + 2∑  i 3 
b



i =1
i =1  b
2
2
  t − u  2   t − u  2

 ti − u    ti − u   ti − u 
 ti − u   1  ti − u 
i
i
+ ∑   2   +  2  + 2  3   
+  2  + 2  3   
+ 


 b    b   b 
 b   2!  b 
i =1  

  b    b 
nu

2
2
 t − u  2  t − u  2

 t i − u   1  ti − u 
 t i − u    ti − u    t i − u 
i
i
+
+
2
+
+
2





∑
 b3  2!  b  + 
 b2 
 b2 
 b3   b   b 2 
 


  
 


  

i =1 


nu

=

2
2
 t − u  2  t − u  2

 ti − u    ti − u   ti − u 
 ti − u   1  ti − u 
i
i
= ∑  2  +  2  + 2  3   
+  2  + 2  3   
+ 


  b 
 b    b   b 
 b   2!  b 
i =1  b


nu

2
2
2
3
nu 

 ti − u 
 ti − u 
 ti − u    ti − u  1  ti − u  1  ti − u 
2
+
+
+
+
+  .



∑








2 
2 
3 
b  i 1  b 
 b    b  2!  b  3!  b 
=i 1 =


=

nu

∑ 

Similar si9mplification of the third and fourth parts yield:
2
2
nu

 −∂ 2l 
 ti − u 
 ttt
 − u 
0 −u   0 −u 
+
−
p
E=
E
n
n
{
exp
(
 2  + 2  0 3  
∑
u)
 2 



2 

 b   b 
 b  
i =1  b
 ∂b 
2
3
  ti − u  2

 ti − u    ti − u  1  ti − u  1  ti − u 

+ ∑  2  + 2  3   
+
+
+






 b    b  2!  b  3!  b 
i =1 

 b 
nu

2
′ 
′
 t ′j − u ′ 
 ttt
 0 − u′  
0 −u   0 −u 
+
2
+ ∑  2  + ( np − na ) exp 

  2 
 b3  
 b   b 

 
j =1  b

2

na

2
3
 t ′j − u ′ 2

 t ′j − u ′    t ′j − u ′  1  t ′j − u ′  1  t ′j − u ′ 

+ ∑  2  + 2  3   
+
+
+
} > 0,





b    b  2!  b  3!  b 
j =1  b





na

n
 −∂ 2l 
−nu − na 1 nu  ti − u 
 ti − u  1 u
 t −u 
E
=
E
+
+
{
exp
exp  i
∑





2
2 
2 ∑
∂u∂b 
b
b i 1=
 b 
 b  b i1
 b 
=
 t ′j − u ′  1 na
 t ′j − u ′ 
1 na  t ′j − u ′ 
+ ∑  2  exp 
 + 2 ∑ exp 

b j 1=
=
 b 
 b  b j1
 b 

+

− nu )  ttt
( npp
 − u  ( n − nu )
 −u 
0 −u 
+
exp 0
exp 0

 b 
 b2 
 b 
b2






− na )  ttt
′
( npp
 − u ′  ( n − na )
 0 − u′ 
0 −u 
+
exp  0
+
exp



 b }.
b
b2
 b 
 b 


b

99
Expanding the second and forth parts using the Taylor series we obtain,
2
n
1 nu

 ti − u  1 u   ti − u  1  ti − u 
exp 
= 2 ∑ 1 + 
+ 
+ 



2 ∑
b i 1=
 b  b i 1   b  2!  b 
=

2
nu
n
 n

 ti − u  1 u  ti − u 
u

=
+
+
+
 2 ∑
,
∑


 b  2! i 1  b 
i 1=
=
 b

2

 t ′j − u ′  1 na   t ′j − u ′  1  t ′j − u ′ 
1 na
=
+
+
+
exp
1

 




∑  b  b2 ∑
b2 j 1 =
b  2!  b 
=
j 1



 
2
na
 t ′j − u ′  1 na  t ′j − u ′ 
 na

=
+
+
+

 2 ∑
,



∑
2!
b
b
b
=
=
j
j
1
1







and
2

nu
n
 −∂ 2l 
1 nu  ti − u 
 ti − u  nu nu
 ti − u  1 u  ti − u 
=
E
E
−
+
+
+
{
exp
∑

 b  b 2 b 2 ∑  b  2! ∑  b  + 
u∂b 
b i 1  b 2 





=
i 1=
i 1
 ∂=

 t ′j − u ′  na na na  t ′j − u ′  1 na  t ′j − u ′ 
1 na  t ′j − u ′ 
+ ∑  2  exp 
 − 2 + 2 + ∑
 + ∑
 +
b j 1 b 
=
j 1=
 b =
 b b
 b  2! j 1  b 
2

+

− nu )  ttt
( npp
 − u  ( n − nu )
 −u 
0 −u 
+
exp 0
exp 0

 b2 
 b 
 b 
b2






− na )  ttt
′
( npp
 − u ′  ( n − na )
 − u′ 
0 −u 
+
+
exp  0
exp  0



},
2
b
b
 b 
 b 
 b 
b

2

n
n
 −∂ 2l 
1 nu  ti − u 
 ti − u  u  ti − u  1 u  ti − u 
Thus,
{
exp
E
E
=
+
+
∑

 b  ∑  b  2! ∑  b  + 
u∂b 
b i 1  b 2  =

 i 1=



i 1
 ∂=

 t ′j − u ′  na  t ′j − u ′  1 na  t ′j − u ′ 
1 na  t ′j − u ′ 
+ ∑  2  exp 
 + ∑
 + ∑
 +
b j 1 b  =
=
 b  j 1=
 b  2! j 1  b 
2

+

( np − nu )  t 0 − u  exp  tt
( np − nu ) exp  0 − u 
0 −u 
+

 b2 
 b 
 b 
b2






− na )  ttt
′
( npp
 − u ′  ( n − na )
 − u′ 
0 −u 
exp  0
exp  0
+
+



} > 0.
2
b
b
 b 
 b 
 b 

b

Thus,
na
 −na
 t ′j − u ′ 
 t ′j − u ′ 
 ∂ 2l 
− u′ 
 tt
 − u′  
E
E
2
exp
=
+
+ 2 ( np − na )  0 3  exp  0

∑






2
3
b 
 b 
 b 
j =1 
 ∂b∂u ′ 
 b 
b

100

 τ − u′ 
 τ − u′ 
 τ − u′ 
because 2 ( npp
2 ( n − na )  0 3 
− na )  0 3  exp  0
=

 b 
 b 
 b 
2
  τ 0 − u ′  1  τ 0 − u ′ 

× 1 + 
+ 
+ 


  b  2!  b 


2 ( npp
− na )  τ 0 − u ′  ( n − na )  τ 0 − u ′ 
=
 b +
 b  +
2
b
b2




2

2 ( nπ − na )

na
τ − u′
and 0
>0 we obtain,
2
b
b
b
na
 t ′j − u ′ 
 t ′j − u ′ 
 ∂ 2l 
−na
+
E =
E
{
2
exp
∑





b2
b3 
j =1 
 ∂b∂u ′ 
 b 

Because

2

>

− na )  tt
2 ( npp
′ ( n − na )  0 − u ′ 
0 −u 
+

+
 b  + } > 0.
2
b
b2
 b 


2

This completes the proof of Result 2.1.

