The Pennsylvania State University
The Graduate School

INFORMATION EXTRACTION AND METADATA ANNOTATION
FOR ALGORITHMS IN DIGITAL LIBRARIES

A Dissertation in
Computer Science and Engineering
by
Suppawong Tuarob

c 2015 Suppawong Tuarob


Submitted in Partial Fulﬁllment
of the Requirements
for the Degree of

Doctor of Philosophy

May 2015







ProQuest Number: 10609615






All rights reserved


INFORMATION TO ALL USERS
The quality of this reproduction is dependent upon the quality of the copy submitted.


In the unlikely event that the author did not send a complete manuscript
and there are missing pages, these will be noted. Also, if material had to be removed,
a note will indicate the deletion.









ProQuest 10609615


Published by ProQuest LLC (2017 ). Copyright of the Dissertation is held by the Author.




All rights reserved.
This work is protected against unauthorized copying under Title 17, United States Code
Microform Edition © ProQuest LLC.



ProQuest LLC.
789 East Eisenhower Parkway
P.O. Box 1346
Ann Arbor, MI 48106 - 1346

The dissertation of Suppawong Tuarob was reviewed and approved∗ by the following:

C. Lee Giles
David Reese Professor of Information Sciences and Technology
Dissertation Co-Advisor, Co-Chair of Committee
Raj Acharya
Professor of Computer Science and Engineering
Co-Chair of Committee
Prasenjit Mitra
Associate Professor of Information Sciences and Technology
Dissertation Co-Advisor
Robert Collins
Associate Professor of Computer Science and Engineering
Wang-Chien Lee
Associate Professor of Computer Science and Engineering
Conrad Tucker
Assistant Professor of Industrial and Manufacturing Engineering
Lee Coraor
Associate Professor of Computer Science and Engineering
Director of Academic Aﬀairs

∗

Signatures are on ﬁle in the Graduate School.

Abstract

Algorithms are developed in computer science and related disciplines to provide
concise step-by-step instructions for solving particular problems. For example, Dijkstra’s algorithm was developed by computer scientist Edsger Dijkstra in 1956 to
ﬁnd a shortest path in a graph with a single source node. As science advances,
problems in other disciplines are often transformed into algorithmic ones so that
standard algorithms can be applied. For example, algorithms for stock portfolio optimization are used for diversifying search results in information retrieval
systems.
Though algorithm collections such as encyclopedias (e.g. Wikipedia) and algorithm textbooks manually catalog algorithms and make them available, such
collections only contain standard algorithms (algorithms which are well deﬁned
and well known). Examples of standard algorithms include Dijkstra’s algorithm,
Binary search, Fast Fourier transform, etc. Despite the well-established nature of
these standard algorithms, they hardly satisfy the needs of algorithm users and
researchers who seek cutting-edge solutions to their problems. Fortunately, researchers constantly develop new algorithms to solve new problems that have not
been solved before, or new algorithms that improve upon the existing ones. A
signiﬁcant number of scholarly articles in computer science and related disciplines
hence contain high-quality algorithms developed by researchers. However, manually searching for algorithms in a pile of papers would be tedious. Therefore, it
would be useful to have a system that automatically collects and enables the search
for this ever increasing collection of algorithms.
Despite the development of various search engine systems for entities in scholarly documents such as tables, ﬁgures, mathematical expressions, etc., systems that
collect and index algorithms from scholarly documents never existed. The diﬃculty of automatically identifying algorithm representations and extracting their
metadata make this problem challenging. While the needs for algorithm search
iii

engines are apparent, literature on the possibility to build such systems is still in
the infant stage.
This dissertation discusses AlgorithmSeer, a search engine for algorithms. AlgorithmSeer collects algorithms from scholarly documents in the forms of pseudocodes and algorithmic procedures. It then extracts and indexes the algorithm
metadata, allowing algorithms to be searched. We raise the issue of metadata
annotation, which can be solved by transferring the annotation from the wellannotated corpus via topical modeling. Finally, we discuss the potential use of
algorithm citations in order to enhance the search capacity beyond traditional
text based search.

iv

Table of Contents

List of Figures

ix

List of Tables

xi

Acknowledgments

xiii

Chapter 1
Introduction

1

Chapter 2
Analyzing Semantic Structure of Scholarly Documents
2.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . .
2.2 Background and Related Works . . . . . . . . . . . . . .
2.3 Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.4 Identifying Section Boundaries . . . . . . . . . . . . . . .
2.4.1 Balancing Training Data . . . . . . . . . . . . . .
2.4.2 Classiﬁcation Algorithms . . . . . . . . . . . . .
2.4.3 Experiment Results and Discussions . . . . . . .
2.4.3.1 Results and Discussion . . . . . . . . . .
2.4.3.2 Impact of Diﬀerent Types of Features .
2.4.3.3 Impact of Data Balancing Methods . . .
2.4.3.4 Impact of Flexible Window Sizes . . . .
2.4.3.5 Impact of Each Feature . . . . . . . . .
2.5 Identifying Standard Sections . . . . . . . . . . . . . . .
2.5.1 Experiments and Results . . . . . . . . . . . . . .
2.6 Building a Hierarchy of Sections . . . . . . . . . . . . . .
2.7 Conclusions . . . . . . . . . . . . . . . . . . . . . . . . .

v

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

9
9
10
13
14
14
16
17
17
18
19
20
21
21
22
23
24

Chapter 3
Identifying and Extracting Algorithm Representations
3.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.2 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.3 System Overview . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.4 Detecting Pseudo-codes (PCs) . . . . . . . . . . . . . . . . . . .
3.4.1 Rule Based Method (PC-RB) . . . . . . . . . . . . . . . .
3.4.2 Machine Learning Based Method (PC-ML) . . . . . . . . .
3.4.2.1 Sparse Box Extraction . . . . . . . . . . . . . . .
3.4.2.2 Feature Sets for PC Box Classiﬁcation . . . . . .
3.4.2.3 Classiﬁcation Models . . . . . . . . . . . . . . . .
3.4.3 Combined Method (PC-CB) . . . . . . . . . . . . . . . . .
3.5 Detecting Algorithmic Procedures (APs) . . . . . . . . . . . . . .
3.5.1 Rule Based Method (AP-RB) . . . . . . . . . . . . . . . .
3.5.2 Machine Learning Based Method (AP-ML) . . . . . . . . .
3.5.2.1 Feature Sets . . . . . . . . . . . . . . . . . . . . .
3.5.2.2 Classiﬁcation Models . . . . . . . . . . . . . . . .
3.6 Linking Algorithm Representations . . . . . . . . . . . . . . . . .
3.6.1 Identifying Sections in Scholarly Documents . . . . . . . .
3.6.2 Using Document Sections for Linking Representations of Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.7 Experiments and Discussion . . . . . . . . . . . . . . . . . . . . .
3.7.1 Data Sets . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.7.2 Evaluation of Pseudo-code Detection . . . . . . . . . . . .
3.7.3 Evaluation of Algorithmic Procedure Detection . . . . . .
3.7.4 Evaluation on Algorithm Representation Linking . . . . .
3.8 Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

25
25
29
30
31
31
32
33
34
36
36
37
37
38
38
39
39
39

.
.
.
.
.
.
.

40
41
41
41
43
45
46

Chapter 4
Algorithm Metadata Extraction, Annotation, Indexing, and
Searching
4.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.2 Related Literature . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.2.1 Automatic Document Annotation . . . . . . . . . . . . . . .
4.2.2 Automatic Tag Recommendation . . . . . . . . . . . . . . .
4.3 Algorithm Textual Metadata Extraction via Document Element
Summarization . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.3.1 Summary of the Document Element Summarization Algorithm
4.3.2 Experiments and Results . . . . . . . . . . . . . . . . . . . .
4.4 Automatic Algorithm Metadata via Probabilistic Topic Modeling .
vi

47
47
53
53
54
55
56
57
60

4.4.1

4.5

4.6

Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . .
4.4.1.1 Cosine Similarity . . . . . . . . . . . . . . . . . .
4.4.1.2 Term Frequency-Inverse Document Frequency . .
4.4.1.3 Latent Dirichlet Allocation . . . . . . . . . . . .
4.4.2 Summary of the Original Probabilistic Topic Modeling based
Metadata Annotation Algorithm . . . . . . . . . . . . . .
4.4.2.1 Document Similarity Measures . . . . . . . . . .
4.4.3 Automatic Annotation of Algorithm Textual Metadata . .
4.4.4 Experiments, Results and Discussion . . . . . . . . . . . .
4.4.4.1 Dataset: CiteseerX Algorithm Metadata . . . . .
4.4.4.2 Results . . . . . . . . . . . . . . . . . . . . . . .
AlgorithmSeer Search System . . . . . . . . . . . . . . . . . . . .
4.5.1 Related Search Engines . . . . . . . . . . . . . . . . . . . .
4.5.2 Indexing Algorithm Metadata . . . . . . . . . . . . . . . .
4.5.3 Query Interface and Result Ranking . . . . . . . . . . . .
4.5.3.1 An Example Search Session . . . . . . . . . . . .
Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

Chapter 5
Analyzing Algorithm Citations
5.1 Capturing Similarities among Algorithms using the Algorithm CoCitation Network . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.1.1 Observations and Motivations . . . . . . . . . . . . . . . .
5.1.2 Related Work . . . . . . . . . . . . . . . . . . . . . . . . .
5.1.3 Algorithm Co-Citation Network . . . . . . . . . . . . . . .
5.1.3.1 Algorithm Citation Detection . . . . . . . . . . .
5.1.3.2 Constructing the Algorithm Co-Citation Network
5.1.3.3 Clustering the Algorithm Co-Citation Network .
5.1.4 Experiment and Evaluation on
Clustering Results . . . . . . . . . . . . . . . . . . . . . .
5.1.5 Suggested Applications . . . . . . . . . . . . . . . . . . . .
5.1.5.1 Algorithm Recommendation . . . . . . . . . . . .
5.1.5.2 Improving Ranking . . . . . . . . . . . . . . . . .
5.1.6 Conclusions and Future Work . . . . . . . . . . . . . . . .
5.2 Studying Algorithm Evolution and Inﬂuence Over Time using Algorithm Citation Network . . . . . . . . . . . . . . . . . . . . . .
5.2.1 Background and Related Works . . . . . . . . . . . . . . .
5.2.1.1 Understanding Document Elements . . . . . . . .
5.2.1.2 Citation Context Classiﬁcation . . . . . . . . . .

vii

.
.
.
.

60
60
61
62

.
.
.
.
.
.
.
.
.
.
.
.

63
64
65
66
66
69
71
72
72
73
74
75
76

.
.
.
.
.
.
.

76
77
78
78
80
81
82

.
.
.
.
.

84
86
87
87
87

.
.
.
.

88
89
90
90

5.2.2

5.2.3
5.2.4

Proposed Classiﬁcation Scheme for Algorithm Citation Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.2.2.1 Annotation Scheme . . . . . . . . . . . . . . . . . .
5.2.2.2 Methodology . . . . . . . . . . . . . . . . . . . . .
5.2.2.3 Algorithm Citation Function in Diﬀerent Sections .
Potential Future Works . . . . . . . . . . . . . . . . . . . . .
Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . .

91
91
92
94
95
95

Chapter 6
Conclusions

96

Bibliography

98

viii

List of Figures
1.1
1.2

2.1
2.2
2.3
2.4
2.5

A majority of algorithms in scientiﬁc literatures are represented
with pseudo-codes and algorithmic procedures. . . . . . . . . . . . .
Two representations of Dijkstra’s algorithm with (1.2(a)) and without (1.2(b)) caption. 1.2(a) has rich extracted synopsis, while 1.2(b)
has no synopsis since the synopsis extraction algorithm relies on the
presence of captions. . . . . . . . . . . . . . . . . . . . . . . . . . .
A document includes a header and subsequent sections. . . . . . . .
Comparison of diﬀerent classiﬁers trained with diﬀerent feature types.
Comparison of diﬀerent data balancing methods on each classiﬁer. .
Comparison of ﬂexible window sizes (K) on each classiﬁer. . . . . .
A scholarly document is normally organized into a hierarchy of sections. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.1
3.2
3.3
3.4

Example pseudo-code (PC), from [27] . . . . . . . . . . . . . . . .
Example algorithmic procedure (AP), from [71] . . . . . . . . . .
Example pseudo-code without a caption, taken from [2] . . . . . .
Overview diagram of our proposed algorithm representation discovery system. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.5 A grammar for document-element captions . . . . . . . . . . . . .
3.6 Example of sparse regions (i.e. sparse boxes), taken from [46]. The
left ﬁgure is an actual PDF page. The right ﬁgure illustrates the
extracted text. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.7 Distribution of upper boundary deltas of PC boxes . . . . . . . .
3.8 Distribution of lower boundary deltas of PC boxes . . . . . . . . .
3.9 Comparison of the ensemble methods against the best base classiﬁers in PC-ML and PC-CB . . . . . . . . . . . . . . . . . . . . .
3.10 Comparison of the ensemble methods against the best base classiﬁer
in AP-ML . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

ix

4

5
11
18
19
20
23

.
.
.

26
27
27

.
.

30
31

.
.
.

32
34
34

.

43

.

45

4.1

4.2
4.3

4.4
4.5
4.6
4.7
4.8
4.9
4.10

4.11
4.12
4.13
5.1
5.2
5.3
5.4
5.5

Sample of primary textual metadata (i.e. caption text and reference
sentences) associated with Algorithm INIT-PDA, represented by a
pseudo-code in [98]. . . . . . . . . . . . . . . . . . . . . . . . . . . .
Distribution of number of words in primary metadata of the extracted algorithms. . . . . . . . . . . . . . . . . . . . . . . . . . . .
Distribution of numbers of processed (clean) words in primary metadata of the extracted algorithms. Note that the processed words
are taken from pre-processed primary textual metadata by removing
stopwords, non-word characters, and stemming. This preprocessing
step has shown to eliminate noise and unuseful information in the
textual metadata and improve the overall search quality. . . . . . .
Distribution of number of words in synopsis metadata of the extracted algorithms. . . . . . . . . . . . . . . . . . . . . . . . . . . .
Distribution of number words in primary and synopsis metadata
(combined). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Sample extracted synopsis corresponding to Algorithm INIT-PDA
in Figure 4.1, represented by a pseudo-code in [98] . . . . . . . . . .
A high-level illustration of the metadata annotation algorithm proposed by [85]. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Example algorithm (represented with a pseudo-code), taken from [27].
Precision, Recall, F1, and Precision vs Recall of the TF-IDF, TM,
KEA (baseline) algorithms on the Algorithm textual metadata. . .
Sample top 30 suggested (stemmed) words by TFIDF, TM, and
KEA (Baseline) algorithms on the metadata record of Algorithm
3.44 mentioned in [29]. . . . . . . . . . . . . . . . . . . . . . . . .
High level description of the algorithm search engine system, AlgorithmSeer. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Screenshot showing results for the query “shortest path”. Along
with search results, associated metadata is also shown to the user. .
Screenshots showing algorithm page displayed on clicking the ﬁrst
result. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
CiteseerX algorithm search engine. . . . . . . . . . . . . . . . . .
Distribution of cluster sizes with granularity parameters of 1.4, 3.0,
and 5.0. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Example of algorithm citation context, consisting an algorithm citation sentence (in italic) and the sentences surrounding it. . . . .
Distribution of diﬀerent algorithm citation functions found in 300
randomly selected algorithm citation contexts. . . . . . . . . . . .
Distribution of algorithm citation functions over diﬀerent sections.
x

49
50

51
58
58
59
63
66
67

69
71
74
74

.

79

.

83

.

93

.
.

93
94

List of Tables
1.1

2.1

2.2

2.3
2.4

2.5
3.1

3.2
3.3

3.4
4.1

Approximate number of algorithms published in diﬀerent computer
science conferences during 2005 - 2009. Reproduced from the study
by Bhatia et al. [6]. . . . . . . . . . . . . . . . . . . . . . . . . . . .
Features set for section header classiﬁcation can be divided into
3 groups–pattern based (PAT), style based (STY), and structure
based (STR) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Performance of each classiﬁer with best data balancing techniques
(in terms of F1) ATT denotes Average Training Time per fold (in
seconds). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Features for section header classiﬁcation, ranked by their information gain scores. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Regular expressions used to capture standard and general section
headers. GEN-WN: General Section with numbers. GEN-WON:
General section without numbers. . . . . . . . . . . . . . . . . . .
Accuracy on each standard section classiﬁcation. . . . . . . . . . .

1

.

15

.

17

.

22

.
.

22
23

.

35

.

38

.

42

.

44

Statistics about primary textual metadata of the extracted 6285
algorithms. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

48

Features for pseudo-code box classiﬁcation can be divided into 4
groups: font style based (FS), context based (CX), content based
(CN), and structure based (ST) . . . . . . . . . . . . . . . . . . .
Features for detecting AP indication sentences can be divided into
2 groups: content based (CN) and context based (CX). . . . . . .
Precision, recall, and F1 of the best classiﬁcation models (in terms
of F1) used for the pseudo-code (PC) and algorithmic procedure
(AP) methods. (‘!’ denotes Majority Voting (VOTE), ‘+’ denotes
Probability Averaging (PAVG)) . . . . . . . . . . . . . . . . . . .
Features for pseudo-code detection, ranked by their information
gain scores. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

xi

4.2

Statistics of the algorithm metadata (ALGO) dataset. . . . . . . . .

5.1

Output cluster and average cluster sizes generated using diﬀerent
granularity parameters. . . . . . . . . . . . . . . . . . . . . . . . .
Precision calculated from the sample clusters using granularity parameter 1.4. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Precision calculated from the sample clusters using granularity parameter 3.0. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Precision calculated from the sample clusters using granularity parameter 5.0. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Proposed algorithm function classiﬁcation scheme consists of 9 categories, divided into 3 groups based on the authors’ attitudes: Favorable, Neutral, Critical. . . . . . . . . . . . . . . . . . . . . . .
Sample annotated algorithm citation contexts. The cited algorithms are denoted with red/bold phrases. The sentences with
underlined/italic font are the algorithm citation sentences. . . . .

5.2
5.3
5.4
5.5

5.6

xii

69

.

84

.

85

.

86

.

86

.

92

.

92

Acknowledgments
This dissertation is a result of cumulative successful collaboration among many
individuals. I would not have been able to ﬁnish this dissertation without people
around me to give me guidance and support.
First of all, I would like to thank my advisors, Dr. C. Lee Giles and Dr.
Prasenjit Mitra, for allowing me to join their research teams and work with them.
As a young scientist, mastering research profession was one of the biggest challenges
in my life. Their constant eﬀort to provide me with guidance and enthusiasm
enabled me to achieve what I never had deemed possible.
Next, I would like to thank Dr. Conrad Tucker for allowing me to join his lab
and a chance to expand my horizon by applying my research skills on diﬀerent
areas of research in the industrial engineering and healthcare domains. Working
with him has exposed me to a wide range of real-world research opportunities
where my skill sets would be useful. His guidance and support have been a great
addition to my knowledge. In addition, I would like to thank collaborators, Dr.
Nilam Ram (HDFS, Penn State) and Dr. Marcel Salathe (Biology, Penn State),
for their guidance on the health informatic projects.
This dissertation would not have been completed without the honorable members of my committee, Dr. Raj Acharya, Dr. Wang-Chien Lee, and Dr. Robert
Collins. I would like to thank them for their constructive criticism, reviews and
feedback that have helped in shaping up my research into its current form.
I would like to thank my internship supervisors, Dr. Kevin Glass (PNNL), Dr.
Line C Pouchard (Purdue), Dr. Natasha Noy (Stanford), Dr. Jeﬀery S Horsburgh
(Utah State University), Giri Palanisamy (ORNL), Sergei Alonichau (Microsoft),
and Dr. Ray Strong (IBM), for allowing me to foster and apply my research skills
on real-world problems.
During the course of progressing through my PhD, I would like to give special
gratitude to Dr. Sumit Bhatia (IBM) and Dr. Sujatha Das Gollapalli for microadvising me and leading me through the proper path of research, in addition to
being such good friends. I consider myself fortunate for having a chance to become

xiii

acquaintance with many people in State College. Their support and friendship
have been valuable. Speciﬁcally, I would like to thank my lab mates: Alexander
Ororbia, Andrew Depenbusch, Bryan Dickens, Chen Liang, Chinmay Sane, Dayu
Yuan, Derauk Gibble, Gautam Manohar, Haibin Liu, Hung-Hsuan Chen, Ishan
Behoora, Jian Wu, Juan Pablo Fernandez, Kyle Williams, Madian Khabsa, Nitish Vasudevan, Paronkasom Indradat, Peifeng Yin, Pradeep Teregowda, Prakhar
Biyani, Puck Treeratpituk, Sagnik Ray Choudhury, Saurabh Kataria, Shibamouli
Lahiri, Suja John, Sungwoo Kang, Wenyi Huang, Yixiang Han, Zackary Ridall,
and Zhaohui Wu. In addition to exchanging technical opinions with them, their
presences have made the labs fun and productive places to work.
Finally, I would like to thank my family, Sa-ngad Tuarob (Dad), Chantana
Tuarob (Mom), and Sopawan Tuarob (Sister), for their continuing support on
whatever decisions I have made. They have been an ultimate source of motivation
that prevented me from giving up.
It has been a great fun chunk of four and a half years at Penn State. I have
been and will be cherishing every moment that I had there.

xiv

Chapter

1

Introduction
Algorithms are ubiquitous in the Computer Science literature. They oﬀer concise
stepwise instructions for solving many computing problems such as searching, sorting, hashing, clustering, decoding, machine learning, etc. Furthermore, in various
ﬁelds other than Computer Science, eﬃcient solutions to important problems involve transforming the problem into an algorithmic one often using fairly standard
algorithms from other ﬁelds. For example, algorithms for portfolio optimization
in stock market are used for diversifying search results in information retrieval
systems [100]. Likewise, in Bio-informatics Hirschberg’s algorithm [34] is widely
used to ﬁnd maximal global alignments of DNA and protein sequences. A thorough knowledge of state-of-the-art algorithms is also crucial for developing eﬃcient
software.
Table 1.1: Approximate number of algorithms published in diﬀerent computer
science conferences during 2005 - 2009. Reproduced from the study by Bhatia et
al. [6].
Conference
SIGIR
SIGMOD
STOC
VLDB
WWW

No. of Algorithms
75
301
74
278
142

Researchers constantly develop new algorithms to either solve new problems
that have not been solved before, or algorithms that improve upon the existing

2
ones. A signiﬁcant number of scholarly articles in computer science and related
disciplines contain high-quality algorithms developed by researchers. Bhatia et
al. provide an estimate of the number of algorithms published in some major
computer science conferences during 2005 - 2009 (Table 1.1). With dozens of new
algorithms being reported in these conferences every year, it would be useful to
have systems that eﬃciently and automatically identify, extract, index and search
this ever increasing collection. Such systems could provide an alternative source for
researchers and software developers looking for cutting-edge algorithmic solutions
to their problems.
Finding well-known standard algorithms is not a hard problem as they usually are already cataloged and made searchable, especially those in online catalogs. We deﬁne a standard algorithm as an algorithm that is well known and is
usually recognized by its name. Examples of standard algorithms include Dijkstra’s shortest-path algorithm, Bellman-Ford algorithm, Quicksort algorithm, and
Knuth-Morris-Pratt algorithm. Standard algorithms are usually collected and cataloged manually in algorithm textbooks (e.g. [21,47]), encyclopedias (especially the
ones available online such as Wikipedia1 ), and websites targeted at computer programmers (e.g. Rosettacode.org2 ). From an initial survey by parsing Wikipedia
algorithm pages in 2010, we found that roughly 1,765 standard algorithms are
cataloged in Wikipedia.org. The National Institute of Standards and Technology
(NIST)3 also has a dictionary of over 289 standard algorithms. However, unlike
these well-known standard algorithms, newly published algorithms are not cataloged by the sources mentioned above, because they are simply too new. The explosion of newly developed algorithms in scientiﬁc and technical documents makes
it diﬃcult to manually catalog them.
Searching for these newly published algorithms is not a trivial task and would
require a great deal of manual searching. Researchers and others who aim to discover eﬃcient and innovative algorithms would have to actively search and monitor
relevant new publications in their ﬁelds of study in order to keep abreast with latest
algorithmic developments. Having to read entire documents would also slow down
the process of ﬁnding the right algorithms. The problem can become even more
1

http://www.wikipedia.org/
http://rosettacode.org/wiki/Rosetta Code/
3
http://xlinux.nist.gov/dads/
2

3
aggravated if algorithm searchers are novices in document search, especially those
who use unuseful search keywords. Thus, automatic identiﬁcation and extraction of algorithms from digital documents could greatly assist successful algorithm
search.
This dissertation addresses the problem of developing a system for collecting,
indexing, and searching for algorithms in a collection of scholarly documents available in digital libraries such as CiteSeer [50]. To the best of our knowledge, we
are the ﬁrst to explorer this research problem. Academic documents have been
used previously as a document source for various tasks [37,51] as they oﬀer several
advantages:
1. Academic documents, in general, follow a structure that is easier for a machine to parse and analyze.
2. They are generally peer reviewed ensuring high quality.
3. They are often the best resource for knowledge about the latest developments
in a ﬁeld.
Although popular academic literature search engines, such as Google Scholar4
and CiteSeerX 5 , oﬀer the capability to search academic documents, these systems
are not geared towards algorithm search. Speciﬁcally, these systems cannot discriminate between a document that contains an algorithm and a document that
does not. As a result, a user searching for query “shortest path” will be oﬀered
many documents that contain the words shortest and path but do not discuss
algorithmic aspects of the shortest path problem.
Our proposed system analyzes a document to identify any algorithms that may
be present in the document. If an algorithm is found in a document, the document
text is further analyzed to extract additional metadata for the algorithm. All the
algorithms and their associated metadata are then indexed and made available
for searching through a text query interface. For a given user query, the system
utilizes evidence from multiple sources to assign relevance scores to algorithms and
results are presented to the user in decreasing order of relevance.
4
5

http://scholar.google.com/
http://citeseerx.ist.psu.edu

4
The possibility to discover and harvest algorithms and their metadata is ﬁrst
explored. We address the following question: Can algorithms and their metadata be automatically identiﬁed and extracted from scholarly documents?

(a) Pseudo-code (from [27])

(b) Algorithmic Procedure (from [71])

Figure 1.1: A majority of algorithms in scientiﬁc literatures are represented with
pseudo-codes and algorithmic procedures.
We ﬁrst investigate the organization of scholarly documents and ﬁnd that most
algorithms are described in certain sections in a paper. We ﬁnd that these sections
could be helpful when identifying algorithms. Then, we ﬁnd that a majority of
algorithms are usually condensed and represented using pseudo-codes (e.g., Figure
1.1(a)) and algorithmic procedures (e.g. Figure 1.1(b)). Though previous work
has shown that document elements including pseudo-codes can automatically be
identiﬁed by detecting the presence of their captions [6], we argue that a majority of
algorithm representations do not have captions and that they can be identiﬁed more
eﬀectively by combining both content and context features. We explore potential
features including writing styles, locations (in the papers), and surrounding text.
For example, a pseudo-code is usually written in a sparse manner and composed
with mathematical symbols and programming keywords. Similarly, an algorithmic
procedure usually begins with a sentence that urges the reader about the algorithm
and is composed with a step-by-step format and a certain algorithmic language.
We claim that we can employ these features to automatically detect the presence
of these algorithm representations.

5

(a) With Caption (from [35])

(b) Without Caption (from [26])

Figure 1.2: Two representations of Dijkstra’s algorithm with (1.2(a)) and without
(1.2(b)) caption. 1.2(a) has rich extracted synopsis, while 1.2(b) has no synopsis
since the synopsis extraction algorithm relies on the presence of captions.
The conventional search engine methodology usually handles textual documents. However, detected algorithm representations have no or little corresponding
text. We next explore the possibility to generate textual metadata for algorithm
representations so they can be indexed and searched. For pseudo-codes with accompanied captions, textual description can be derived using the document element summarizing technique proposed by Bhatia and Mitra [6], which generates a
textual snippet for a document element using its caption to draw and rank relevant
sentences in the paper. However, such a method does not work with pseudo-codes
without captions and algorithmic procedures (which are not treated as document
elements). Figure 1.2 shows an example of two similar algorithms (i.e. Dijkstra’s
algorithm) represented by a pseudo-code with an accompanied caption (1.2(a))
and a pseudo-code without a caption (1.2(b)). The algorithm representation 1.2(a)
would have rich textual metadata since the it has both a caption and reference sentences in the paper which enable the document summarization method to generate
the corresponding synopsis. However, the summarization technique would fail to
retrieve the textual metadata for the algorithm representation 1.2(a) simply due

6
to the absence of its caption and reference sentences. In order to generate textual
metadata for these algorithm representations, we transform the problem into the
metadata annotation problem. We ﬁnd that some extracted algorithms can be similar or derived from other algorithms that already have rich textual metadata. We
ask this question: Can we then use the annotation from textually rich
metadata records in order to annotate textually poor ones? We ﬁrst
propose a simple algorithm that utilizes pairwise similarity between textually rich
and poor metadata records building upon TF-IDF similarity measure. However,
the textual content in poorly annotated metadata is both too sparse and noisy
(with unique jargons) that the method is not able to capture the pairwise similarity well, resulting in poor results. We ﬁnd that topical representation derived
via Latent Dirichlet Allocation (LDA) [10] is suitable for this task. We show that
textually poor metadata records can automatically be enriched using a corpus of
the textually rich ones.
We are able to build a demo of the algorithm search engine by using the conventional text based information retrieval techniques. However, this algorithm search
engine only retrieves and ranks relevant algorithms solely on textual similarity between search queries and algorithm metadata. We argue that algorithms exhibit
certain properties that can be useful for searching. For example, understanding
what problems an algorithm is designed to solve could help algorithm searchers look
for the right algorithms for their target problems. Furthermore, researchers utilize
existing algorithms in multiple ways to develop new ones. For example, Erkan and
Radev proposed LexRank which extended PageRank [24]. Similarly, Walker et
al. introduced CiteRank which also extended PageRank [99]. Recently, Su et al.
proposed PrestigeRank, an extension of PageRank, and compared the results with
both PageRank and CiteRank [72]. Being able to capture and model this algorithm evolution would provide a great push for the current algorithm search engine
techniques to expand their capabilities beyond just textual matching. We observe
that the algorithm usage can be captured by analyzing the algorithm citation context, and then try to answer this question: Can citations help understand
semantics of algorithms? We ﬁrst explore the algorithm co-citation network,
and ﬁnd that groups of algorithms that address similar problems can be obtained
via clustering such a network. Next we attempt to understand the evolution of

7
algorithms. We ﬁnd that citation contexts contain useful information pertaining to how the citing paper uses previously published algorithms, and propose a
classiﬁcation scheme for algorithm usage in scientiﬁc literature.
Listed below are our key contributions that address the questions raised so far:
• We identify important sections in scholarly documents and propose a set of
hybrid algorithms to represent a scholarly document as a hierarchy of sections
(Chapter 2). Speciﬁcally, we ﬁrst propose a machine learning based approach
to identify section headers and use them as section boundaries. Second,
each section is categorized into scientiﬁc relevant sections (i.e. Abstract,
Introduction, etc.). Finally, our algorithm identiﬁes sub-section relationship
in order to build a hierarchy of sections. Insights from this work are used as
location-based features to identify algorithm representations.
• We identify that a majority of algorithms in scholarly documents are represented with pseudo-codes and algorithmic procedures. We propose a set
of ensemble machine learning based approaches that use both content and
context features to identify and extract them (Chapter 3). We also ﬁnd that
two or more algorithms representations may represent the same algorithm;
hence we also propose a simple heuristic to link algorithm representations
that belong to the same algorithm together.
• We develop a set of mathematical models extending Latent Dirichlet Allocation and TF-IDF that learn metadata annotation from textually rich
metadata records in order to annotate the textually poor ones (Chapter 4).
We frame the problem in to the metadata annotation problem which can be
solved by our proposed topical modeling based tag recommendation method.
We show that our method is eﬀective in ﬁlling in missing information for
poorly-annotated algorithm metadata.
• Finally, we present the limitation of the traditional text-based search on
algorithms. We identify that citation could be the key to understand algorithm speciﬁc semantics in scholarly works. We ﬁrst show that algorithms
that address similar problems can be identiﬁed by clustering the algorithm
co-citation network. We make the case that the evolution of algorithmic

8
development can be studied via using the algorithm citation contexts to
identify how an existing algorithm is used in a citing paper, and propose a
classiﬁcation scheme for algorithm citation functions (Chapter 5).
Chapter 6 concludes this dissertation along with suggesting some future directions based on the ﬁndings in this research.

Chapter

2

Analyzing Semantic Structure of
Scholarly Documents
Scholarly documents are usually composed in sections, each of which serves a diﬀerent purpose in conveying speciﬁc context. However, scholarly documents in digital
libraries, such as CiteseerX , are mostly PDF ﬁles with no structural information.
Being able to automatically identify sections would enable the study of various
semantics in documents such as what was in the introduction, methodologies used,
experimental types, trends, etc. We propose a set of hybrid algorithms to 1) automatically identify section boundaries, 2) recognize standard sections, 3) build
a hierarchy of sections. The methodology achieves the performance of 92.38%
F-measure in section boundary detection, 96% accuracy (average) on standard
section recognition, and 99.51% in accuracy in the section positioning task.

2.1

Introduction

The content in a scholarly document is typically organized into various sections.
For examples, the Abstract section provides a complete but succinct summary of
the paper. The Introduction usually covers the background, a high-level description of the research questions and proposed solutions and possibly the motivation.
The Experiments or Methodology section describes the methodology and experimental procedure in detail, etc. The Conclusions is a short description of what
was in the paper and often future work. Other sections are also possible. The

10
capability to automatically identify and separate these sections and their content
has various applications in knowledge extraction and document analysis. Cornelia
et al. utilized the information in the Abstract sections to classify documents [14].
Nguyen et al. [62] proposed a keyphrase extraction algorithm based on the intuition that Abstract and Methodology sections contain higher distribution of useful
terms. Treeratpituk et al. [78] also proposed a keyphrase extraction algorithm
from scientiﬁc documents and notice that useful keyphrases often cooccur in the
Introduction, Abstract, and Conclusion sections. Fleischer [25] proposes a method
for extraction of key sections from a document. The method, however, requires a
document to already be sectionized.
Multiple document segmentation algorithms have been proposed; however,
most of them tend to focus on document zoning analysis. Here, we describe a
strategy to automatically build a semantic hierarchical structure of sections of a
scholarly paper. A section is deﬁned as a pair of section header and its textual content. Figure 2.1 illustrates the ﬁrst page of a sample document, including a header
and 3 sections. Oftentimes, sections in scientiﬁc papers conform to standard sections (i.e. Abstract, Introduction, Background and Related Work, etc.). Hence, we
also propose a rule-based approach to recognize these standard sections.
Speciﬁcally, this chapter has 4 key contributions:
1. We propose a machine learning based strategy to identify boundaries in a scholarly
document by detecting section headers and use them as section boundaries.
2. We construct a set of regular expressions to recognize standard sections.
3. We propose a rule-based strategy to build a hierarchical structure of sections.
4. We validate our methods using empirical evaluation, with standard precision, recall, F-measure, and accuracy metrics.

2.2

Background and Related Works

Document segmentation has long been an area of active research. Multiple document segmentation techniques have been proposed; however, most of them focus
on document zoning analysis, i.e. identifying similar regions in a document. In

11

Header
Improvement of Matching and Evaluation in Handwritten Numeral Recognition
Using Flexible Standard Patterns
Hirokazu MURAMATSU, Takashi KOBAYASHI, Takahiro SUGIYAMA, and Keiichi ABE
Graduate School of Information, Shizuoka University
3-5-1, Johoku, Hamamatsu-Shi, 432-8011, Japan
Contact at: abe@cs.inf.shizuoka.ac.jp

Abstract

learning the standard pattern and of ﬂexible matching [14].
This paper describes mainly the recognition stage with the
improvements made after the report.

The purpose of this study is to develop a ﬂexible matching method for recognizing handwritten numerals based on
the statistics of shapes and structures learned from learning samples. In the recognition method we reported before,
there were problems in matching of the feature points and
evaluation of matching. To solve them, we propose a new
matching method supplementing contour orientations with
convex/concave information and a new evaluation method
considering the structure of strokes.
With these improvements the recognition rate rose to
96.0% from the earlier ﬁgure 91.9%. We also made a recognition experiment on samples from the ETL-1 database and
obtained the recognition rate 95.2%.

2. Overview of the recognition system
A schematic diagram of the recognition system proposed
is illustrated in Figure 1. The system consists of two parts,
each dealing with the learning stage and the recognition
stage. In the learning stage it learns the standard patterns
and their ranges of variations from the given set of learning
samples. We obtain standard line patterns for each subcategory of ten numeral categories, augmented by statistics of
constituting strokes. This stage is brieﬂy explained in the
next section.
In the recognition stage it recognizes input unknown
characters by matching them with the elastically deformable standard patterns. Recognition is performed in

1. Introduction
In off-line recognition of handwritten characters statistical or neural network methods based on various features extracted from input characters are widely used[1][2]. On the
other hand, humans are supposed to recognize characters
based on not such features but structures of characters. In
this sense the current main stream of character recognition
may differ from the way of human character understanding
and recognition.
An alternative way of machine character recognition
is structural approach[3][4][5][6], especially matching an
input character elastically to standard patterns, or vice
versa[7][8][9][10]. Character recognition or shape learning using deformable templates is also a closely related
attempt[11][12][13]. Recognition by elastic matching compares the shapes and structures themselves between the input character and the standard pattern; thus it would be
closer to the way of human recognition than feature extraction/statistical classiﬁcation approach is.
We have reported our earlier work with the methods of

Learning sample "3"

Unknown patterns

Learning
...

Matching

...

Standard patterns

Evaluation

Input pattern

"3"
Result of recognition

Figure 1. Overview of the recognition system

1

Section
Figure 2.1: A document includes a header and subsequent sections.
this section, we brieﬂy discuss work related to identifying sections in scholarly
documents. The content-based approaches tend to involve analyzing the similarity between each chunk of texts to determine the section border lines. The

12
block-based approaches mostly involve image analysis to determine the structures
of documents. Hearst et al. propose the TextTiling algorithm, which segments
text into multi-paragraph units that represent passages [31]. This algorithm ﬁrst
breaks the document into units of ﬁxed length and then calculates the cohesion
scores between units. The cohesion scores are used to determine topic changes and
boundaries of sections. Generally, topic changes are calculated based on the lexical
co-occurrence patterns in the block and nearby blocks. This algorithm, however,
relies on the assumption that a signiﬁcant portion of a set of terms changes when
a topic changes. This assumption may not be true for scholarly documents where
two sections can have overlap sets of terms, especially when they are both subsections of the same section. Tieldemann et al. [76] modify Deemter’s co-reference
chain model [97] for a document segmentation task. A co-reference is a relationship between two noun phrases both of which are interpreted as referring to the
same entity in the context in which they appear. They evaluate their algorithm
on a corpus of Flemish weekly news magazines. Their method would work with a
document that contain multiple distinct sections that discuss completely diﬀerent
stories (like news and magazines). However, a scholarly document usually focus
on one topic. Hence, their method is likely to fail as references in a scholarly
document tend to refer to the same objects across the document.
Treeratpituk et al. proposed a section detection algorithm as part of their
keyphrase extraction work [78]. They use a set of regular expressions to detect 6
common sections including Title, Abstract, Introduction, Related Work, Methodology + Experiments, and Conclusion + Future Work. They, however, do not publish
the regular expression rules nor the related experimental results. Moreover, their
algorithm only identiﬁes common sections, while our algorithm aims to identify all
the sections. Similarly, Nguyen et al. have successfully used a Maximum Entropy
(ME) classiﬁer to classify a section header into one of 14 common section header
types [62]. Their method, however, requires section headers to be pre-identiﬁed
using a preprocessing technique which they omit to address. Denny et al. [22] proposed a method that automatically identiﬁes section headers in history and physical
examination documents. Their method, however, is designed speciﬁcally for medical notes and have a high reliance on medical keywords. Hence, their method
would only detect sections whose headers contain medical terms, which is not a

13
nature in scholarly documents in which section headers can have arbitrary titles.
Recently, Ramakrishnan et al. proposed LA-PDFTEXT to facilitate extraction
of text from PDF ﬁles [67]. Though their system can also detect section headers,
it is only accurate when identifying common sections (e.g. Abstract, Introduction,
etc.). Moreover, their system is rule-based and is designed to handle documents
in medicine ﬁelds. Our proposed methodology, on the other hand, can accurately
identify section headers with arbitrary titles, and can be trained to handle diverse
corpora of documents.

2.3

Dataset

117 PDF scholarly documents are randomly selected from CiteseerX 1 repository,
containing various types of scholarly documents namely conference papers, journals, theses, and academic articles. We extract textual and font information from
each document using PDFBox2 . On average, a document has 993 lines, 20 pages,
and 17 sections. Note that images are disregarded as we are only interested in
textual data.
We tag each line in the document with the following labels:
0 Unclassiﬁed, Default
ABS Abstract section header, e.g. ‘ABSTRACT’.
INT Introduction section header, e.g. ‘1. Introduction’ and ‘I. INTRODUCTION’.
REL Background and Related Works section header, e.g. ‘2. Background and
related work’ and ‘II. PRELIMINARIES’.
RAD Experiment, Result and Discussion section header, e.g. ‘5 Experiments and
Evaluation’, ‘V. EXPERIMENTAL RESULTS’, and ‘7 Discussion’.
CON Conclusion section header, e.g. ‘6. Conclusions and future work’ and ‘6.
CONCLUDING REMARKS’.
1
2

http://citeseer.ist.psu.edu
http://pdfbox.apache.org/

14
ACK Acknowledgment section header, e.g. ‘ACKNOWLEDGEMENTS’ and ‘8
Acknowledgements’.
REF References section header, e.g. ‘References’ and ‘Bibliography’.
APX Appendix section header, e.g. ‘Appendix A: Seven principles...’.
GEN Other section header not classiﬁed as a standard section listed above.
Note that even though published datasets about document segmentation exist,
such as GROTOAP [77] and PSET [57], these datasets are for document zone
analysis problems, and hence would not directly accommodate our experiment.

2.4

Identifying Section Boundaries

We observe that scholarly documents have the following properties: 1) Each section
has a section header, usually with a section number. 2) Section headers usually
have diﬀerent font styles from the surrounding content. 3) A majority sections are
common sections such as Abstract, Introduction, Background, Conclusions, and
References. Such observations lead us to identify 22 features that characterize
section headers. The features can be divided into 3 groups: pattern based (PAT),
style based (STY), and structure based (STR). Table 2.1 lists all the features. The
PAT features are used for capturing section headers which are standard sections
or section headers with section numbers. The STY features ﬁlter out lines that
look like section headers but are actually fragments of sentences, lines in tables
of content, or textual fragments from tables/diagrams. The STR features mostly
concern the locations of the section headers. For example, lines that occur between
the Abstract/Introduction section and the References section are better candidates
than those outside this region. Moreover, the IS FIRST LINE OF PAGE and IS
LAST LINE OF PAGE features are used to ﬁlter out page headers and footers.

2.4.1

Balancing Training Data

Our dataset is highly skewed with only 1.75% of positive samples; hence multiple
data balancing techniques are explored to alleviate the class imbalance problem [4].
These techniques include:

15
Table 2.1: Features set for section header classiﬁcation can be divided into 3
groups–pattern based (PAT), style based (STY), and structure based (STR)
Grp

Feature

Descripon

IS SEC HEADER W/ NUM

Whether the line matches the number-leading secon header paern.

IS UPPER SEC HEADER W/ NUM

Whether the above line matches the #-leading secon header paern.

IS LOWER SEC HEADER W/ NUM Whether the lower line matches the #-leading secon header paern.
IS SEC HEADER W/O NUM

Whether the line matches the secon header without number paern.

IS UPPER SEC HEADER W/O NUM Whether the above line matches the secon header without paern.

PAT IS LOWER SEC HEADER W/O

IS CAPTION

Whether the lower line matches the secon header without number
paern.
Whether the line is one of the standard secon headers (Abstract,
Intro, Background, Results and Discussion, Conclusion,
Acknowledgment, or References)
Whether the line is a capon or a ﬁgure, table, or algorithm.

MODE FONTSIZE

Mode fontsize (in Pt.) of all the characters in the line

NUM
IS STANDARD SEC

FRACTION MODE FONTSIZE TO
DOC AVG FONTSIZE
FRACTION MODE FONTSIZE TO
DOC MODE FONTSIZE
FRAC UPPER GAP TO MODE GAP

Fracon of the mode fontsize in the line to the average fontsize in the
document.
Fracon of the mode fontsize in the line to the mode fontsize in the
document.
Fracon of the gap space between the line and the upper line (in cm)
to the mode gap space between two lines in the document.
FRAC LOWER GAP TO MODE GAP Fracon of the gap space between the line and the lower line (in cm)
to the mode gap space between two lines in the document.
STY
FRAC UPPER GAP TO AVG GAP
Fracon of the gap space between the line and the upper line (in cm)
to the average gap space between two lines in the document.
FRAC LOWER GAP TO AVG GAP Fracon of the gap space between the line and the lower line (in cm)
to the average gap space between two lines in the document.
ARE ALL CHARS BOLD
Whether all the characters in the line have boldface font style.
ARE ALL WORDS CAPITALIZED

Whether all the words in the line are capitalized.

FRACTION NUMWORDS TO AVG Fracon of the number of words in the line to the average number of
NUMWORDS
words per line of the document.
IS AFTER ABS INT
Whether the abstract or and introducon secon header has already
been detected using the regular expression described in Table I.
IS BEFORE REF
Whether the reference secon header has NOT yet been detected using
STR
the regular expression described in Table I.
IS FIRST LINE OF PAGE
Whether the line is the ﬁrst line of page.
IS LAST LINE OF PAGE

Whether the line is the last line of page.

None. No data balancing is applied.
Weighting (WEIGHT). Let npRatio =

# −ve instances
.
# +ve instances

A positive instance is

given a weight of npRatio, while a negative instance is given a weight of 1.
Random Over-sampling (ROver). Minority class instances are randomly selected and duplicated until the populations of the two classes are equal.

16
Random Under-sampling (RUnder). Majority class instances are randomly
selected and removed until the populations of the two classes are equal.
Resampling (ReS). A random subsample of the training data is reproduced using sampling with replacement. The new training data has the same number
of total samples as the old one, but equal populations of both classes.
SMOTE. The minority class samples are over-sampled using the Synthetic Minority Oversampling TEchnique (SMOTE) [15]. The SMOTE algorithm avoids
the overﬁtting problem by forming new minority class examples by interpolating between several minority class examples that lie together.

2.4.2

Classiﬁcation Algorithms

In our experiment, we employ 4 classiﬁcation algorithms:
Random Forest (RF) is a tree-based ensemble classiﬁer consisting of many decision trees [11]. We use 100 trees for each RF classiﬁer as suggested by [41].
Support Vector Machine (SVM) is a function based classiﬁer built upon the
concept of decision planes that deﬁne decision boundaries [9]. In our experiment we use the linear kernel SVM with C = 1.0.
Repeated Incremental Pruning to Produce Error Reduction (RIPPER)
is a rule-based classiﬁer which implements a propositional rule learner [20].
For each RIPPER classiﬁer, we set the number of folds to 3, and the minimum weight of instances to 2.0.
NaiveBayes (NB) is a simple probabilistic classiﬁer implementing Bayes’ theorem [36].
We use LibSVM3 implementation for SVM, and Weka4 implementation for the
other classiﬁers.
3
4

http://www.csie.ntu.edu.tw/ cjlin/libsvm/
http://www.cs.waikato.ac.nz/ml/weka/

17

2.4.3

Experiment Results and Discussions

We train each classiﬁer with the 22 features and balance the training data using
diﬀerent data balancing techniques described in Section 2.4.1. For each text line
in a test document, the classiﬁer determines whether it is a section header or not.
Ten fold document-wise cross validation is used to validate our methods.
2.4.3.1

Results and Discussion

Standard precision, recall, and F-measure are used as our evaluation metrics. For
a testing document, a text line is said to be correctly classiﬁed as a section header
if the classiﬁer identiﬁes it as a section header and it is within K (ﬂexible window
size) lines above or below the actual section header line. Note that K = 0 allows
strict evaluation, while K > 0 allows the classiﬁer to predict fuzzy boundaries
(which is more practical in general IR applications). In our experiment we set K
= 2. Let Tp be the set of all the lines correctly classiﬁed as section headers, Tr
the set of all lines classiﬁed as section headers, and Tg the set of all actual section
header lines. We deﬁne precision, recall, and F-measure as:
precision =

|Tp |
2 · precision · recall
|Tp |
, recall =
,F =
|Tr |
|Tg |
precision + recall

Table 2.2: Performance of each classiﬁer with best data balancing techniques (in
terms of F1) ATT denotes Average Training Time per fold (in seconds).
Classiﬁer
RF
RIPPER
SVM
NB

Balancing
WEIGHT
none
WEIGHT
none

Pre
0.9374
0.9118
0.8570
0.4420

Rec
0.9106
0.8623
0.8828
0.8744

F1
0.9238
0.8863
0.8697
0.5872

ATT
52.96
62.39
6.74
1.19

Table 2.2 lists the best result of each classiﬁcation algorithm (in terms of F1),
when combined with the best data balancing method. Note that tree-based classiﬁers such as Random Forest perform relatively better than other classiﬁers. This
is because most of the features are binary features. In fact, the best performance
in terms of F-measure is yielded by the Random Forest classiﬁer trained with
weighted balancing data.

18
2.4.3.2

Impact of Diﬀerent Types of Features

1.0
0.9
0.8

F-Measure

0.7
0.6

PAT

0.5

STY

0.4

STR

0.3

ALL

0.2
0.1
0.0
RF

SVM

RIPPER

NB

Figure 2.2: Comparison of diﬀerent classiﬁers trained with diﬀerent feature types.
Here we study how diﬀerent types of features interplay to help the classiﬁers
learn. Figure 2.2 compares the performance in terms of F-measure of each classiﬁer trained with diﬀerent types of features. It is evident that the pattern based
features are most useful. The structure based features alone do not provide much
information about the data; however, they can be additionally useful when combined with other types of features. SVM classiﬁers turn out to beneﬁt only little
from the style based features compared to other classiﬁers, possibly due to the data
normalization problem since most of such features are numeric values. Hence combining the style based features with other features impedes the overall performance
of SVM classiﬁers. The same analysis can apply to the NaiveBayes classiﬁers–the
overall performance of the NaiveBayes classiﬁers drops signiﬁcantly when trained
with the style based features.

19

1
0.9
RF

0.8
F-Measure

SVM
0.7

RIP

0.6

NB

0.5
0.4
0.3
0.2
none

WEIGHT

ROver

RUnder

ReS

SMOTE

Balance
Method

Figure 2.3: Comparison of diﬀerent data balancing methods on each classiﬁer.
2.4.3.3

Impact of Data Balancing Methods

How data balancing techniques aﬀect the classiﬁcation results is also studied. Figure 2.3 displays the performance (in terms of F-measure) of each classiﬁer trained
with data balanced by diﬀerent balancing strategies, with ﬁxed K = 2. There are
4 points to note:
1. The performance of all classiﬁers drops signiﬁcantly when trained with RUnder’ed data, compared to other balancing techniques. This is because, RUnder method removes roughly 96.50% of negative samples, causing the classiﬁer to obtain insuﬃcient knowledge about the negative class samples which
naturally have a much wider variety in style than that of the positive samples.
2. It is interesting to note that the performance of Random Forest does not
change much when trained with balanced data (except RUnder’ed data).
This is because such an algorithm already incorporates sampling techniques
and cost matrices to handle class-imbalance [16]. Hence additional data
balancing would not be further helpful. Not surprisingly, a similar empirical
study also encourages the use of Random Forest algorithm for imbalance

20
data [41].
3. All balancing techniques which involve duplicating/removing instances such
as ROver, RUnder, ReS, and SMOTE seem to reduce the learning ability of
SVM and NaiveBayes classiﬁers. This suggests that such techniques distort
the distribution of population of the data in the training set, causing the
classiﬁer to perceive wrong distribution of the data.
4. The performance of NaiveBayes classiﬁers decreases with all balancing techniques.
2.4.3.4

Impact of Flexible Window Sizes

0.95
0.9
0.85
F-measure

0.8
0.75
0.7
RF

0.65

SVM

0.6

RIP
0.55
NB
0.5
0

5

10

15

20

25

30 K

Figure 2.4: Comparison of ﬂexible window sizes (K) on each classiﬁer.
In this section, we investigate how varying the values of K impacts the classiﬁcation performance. K = 0 means the detected section headers are correctly
classiﬁed if they exactly match the line numbers of the actual section headers. In
general, the classiﬁcation performance should increase as K increases, as it allows
higher matching ﬂexibility. Figure 2.4 compares the performance (in terms of F1)
of each classiﬁer when evaluated with diﬀerent values of K.

21
The performance of RF, SVM, and RIPPER classiﬁers do not change much
as the window size increases. These results imply that our proposed features are
eﬀective in discriminating section header lines from their surrounding context. It
is interesting to note that NaiveBayes performance increases signiﬁcantly as the
window grows large. This is not surprising since NaiveBayes performs very poorly
at lower K; hence the eﬀect of window size increase is prominent in NaiveBayes’
performance. We also try other Bayes based classiﬁers such as NaiveBayes Updatable, NaiveBayesMultinomial, ComplementNaiveBayes and BayesNet, all of which
we ﬁnd to perform similar to NaiveBayes. As a result, we conclude that NaiveBayes
and other Bayes based classiﬁcation algorithms are not suitable for our task.
2.4.3.5

Impact of Each Feature

In order to analyze the importance of each feature, this section presents a feature
selection analysis by quantifying the worth of a feature by measuring the information gain with respect to the class. Table 2.3 lists the results. We can see
that important features are content based ones that represent the appearance of
section headers in scholarly documents including whether it is led with numbers,
font styles, and gaps between the line with the section body. It is interesting to
note that location based features are not as important.

2.5

Identifying Standard Sections

Scientiﬁc articles often use standard sections to organize contents. These standard
sections include ABS (Abstract), INT (Introduction), REL (Background and Related Work), RAD (Experiment, Results, and Discussion), CON (Conclusions),
ACK (Acknowledgment), and REF (References). Being able to identify and recognize these standard sections could potentially shed light on semantic discovery
applications in scientiﬁc literature.
In this section, we propose to recognize these standard section using a set of
regular expressions displayed in Table 2.4. These regular expressions are derived
by observing on the writing variation of section headers.

22
Table 2.3: Features for section header classiﬁcation, ranked by their information
gain scores.
Score
0.077527945
0.059999133
0.057769496
0.056546918
0.051614183
0.051585803
0.051318366
0.035688542
0.026156727
0.016191286
0.006836211
0.003252936
0.001107869
0.000451518
0.000186848
6.03424E-05
4.24127E-05
1.32332E-05
0.000004601
0.000000658
4.06E-08
0

Feature
is sec header with number
upper gap ratio to mode gap
mode fontsize ratio to mode fontsize
mode fontsize
upper gap ratio to avg gap
lower gap ratio to mode gap
mode fontsize ratio to avg fontsize
lower gap ratio to avg gap
is standard section
are all words capitalized
are all chars bold
is before ref
is ﬁrst line of page
is last line of page
is caption
is upper line sec header without number
is sec header without number
is after abs or intro
is lower line sec header with number
is upper line sec header with number
is lower line sec header without number
numwords ratio to avg numwords

Table 2.4: Regular expressions used to capture standard and general section headers. GEN-WN: General Section with numbers. GEN-WON: General section without numbers.

2.5.1

Experiments and Results

We frame the standard section recognition into a binary classiﬁcation task. For
example, in order to see if a section header is an Abstract section or not, we check

23
Table 2.5: Accuracy on each standard section classiﬁcation.
Section
ABS
INT
REL
RAD
CON
ACK
REF

Acc.%
100
100
90.48
88.46
94.44
100
100

if it matches with the Abstract regular expression rule or not. We use standard
accuracy as the evaluation metric for this task.
Accuracy =

|Correctly Classif ied Section Headers|
|All Section Headers|

(2.1)

Table 2.5 report the accuracy in percentage for each standard section on the
standard recognition task. Our method works well on capturing almost all standard
sections, except for the RAD section whose accuracy is the lowest. This is because
there can be a much wider variety of word choices to compose Experiment, Results
and Discussion sections.

2.6

Building a Hierarchy of Sections





 



  

 !"##$%

&* %  $%+ , ##$%

& !"##$%

&*-,!

&*&($% (!$

 '#!!

(

)   !

&*'($ -!

Figure 2.5: A scholarly document is normally organized into a hierarchy of sections.
The section layouts in scholarly articles are usually not plain – they are composed by sections and sub-sections (See example in Figure 2.5). This section

24
proposes a simple set of heuristics that builds a hierarchy of sections from the
extracted section headers.
We notice that there are two type of section headers: numbering (e.g., 3.1 The
High Level Algorithm) and non numbering (e.g., Spammer Detection Model).
For the former case, the numberings are directly used to identify sub-section relationship. For the latter case, we use the indentation spacing from the column
margin to infer sub-section relationship.
We evaluate the section hierarchy building task by framing the problem into
the section positioning task. Speciﬁcally, a section is correctly positioned if it is
placed under the right super-section in the hierarchy. Standard accuracy is used
as the evaluation metric:
Accuracy =

|Correctly P ositioned Sections|
|All Sections|

(2.2)

Our section positioning algorithm is 95.51% accurate on the evaluation dataset.

2.7

Conclusions

Most scholarly documents in modern digital libraries are in PDF format which
does not maintain section structures. In attempt to study structural semantics of
scholarly documents, we propose a set of methods to build a semantic hierarchy of
sections of a scholarly document. The problem is divided into three subtasks: 1)
identifying section headers, 2) recognizing standard sections (i.e., Abstract, Introduction, etc), and 3) building a hierarchy of sections. In the ﬁrst task, we employ
machine learning based classiﬁcation techniques to detect section headers and use
them as section boundaries. In the second task, we use a set of regular expressions
that capture lexical patterns in standard sections. For the ﬁnal task, we show that
a set of simple heuristics can be eﬀectively used to capture sub-section relationship.
Future work could explore ensemble strategies and strengthen the evaluation by
comparing with other state-of-the-art algorithms such as LA-PDFTEXT [67].

Chapter

3

Identifying and Extracting Algorithm
Representations
Advancements in algorithmic development are usually published in scholarly articles, especially in the computational sciences and related disciplines. If we can
automatically ﬁnd and extract these algorithms in scholarly digital documents,
then it would be possible to do algorithm indexing, searching, discovery, and analysis. Recently, AlgorithmSeer, a search engine for algorithms, has been investigated
as part of CiteSeerX with the intent of providing a large algorithm database. Currently, over 200,000 algorithms have been extracted from over 2 million scholarly
documents. Here, we propose a novel set of scalable techniques used by AlgorithmSeer to identify and extract algorithm representations in a heterogeneous pool of
scholarly documents. We determine two types of algorithm representations commonly used, namely pseudo-codes (PCs) and algorithmic procedures (APs), and
develop a set of ensemble machine learning strategies to discover them. Finally,
we describe how diﬀerent algorithm representations are linked to produce a set of
unique algorithms.

3.1

Introduction

Identifying and extracting various informative entities from scholarly documents
is an active area of research. For algorithm discovery in digital documents, Bhatia
et al. brieﬂy described methods for automatic detection of pseudo-codes (PCs) in

26

Figure 3.1: Example pseudo-code (PC), from [27]
Computer Science publications [7]. Their method assumes that each PC is accompanied by a caption (e.g., Figure 3.1). Such a PC can then be identiﬁed using a set
of regular expressions to capture the presence of the accompanied caption [7, 8].
However, such an approach is limited in its coverage due to reliance on presence of
PC captions and wide variations in writing styles followed by diﬀerent journals and
authors. Further, even though PCs are commonly used in scientiﬁc documents to
represent algorithms, we found that a majority of algorithms are also represented
using algorithmic procedures (APs). From our sample data set (DS2) of 258 scholarly documents (see Sect. 3.7.1), we found 275 PCs and 86 APs. Examples of a
PC and an AP are given in Figure 3.1 and 3.2 respectively. We found that 25.8%
(71 out of 275) of the PCs did not have accompanied captions (e.g., Figure 3.3),
and would remain undetected by their proposed approach. Furthermore, their
technique cannot be adopted to identify APs as they diﬀer from PCs in following
ways:

27

Figure 3.2: Example algorithmic procedure (AP), from [71]

Figure 3.3: Example pseudo-code without a caption, taken from [2]
• Writing Style. PCs are usually written in a programming style, with details
omitted. Symbols, Greek letters, mathematical operators, and programming
keywords (such as ‘for’, ‘begin’, ‘end’, ‘return’, etc.) are usually used to
compose PCs. On the other hand, APs are usually written in a listing style,
with a descriptive manner. Each stepwise description usually begins with a
bullet point, or a number. APs lack the power to express complex nested
loops and are less concise than PCs, but they are easier to comprehend by
general readers who do not have programming background.
• Location in Documents. PCs are usually not part of the running text;

28
they may appear anywhere in the documents. Because of this, most PCs have
identiﬁers which the context in the document can refer to. These identiﬁers
include captions (e.g. ‘Figure 3: The hill-climbing algorithm.’), function
names (e.g. ‘APPROXMAX-SAT(g, S , p)’), and algorithm names (e.g.
‘Algorithm BuildGalledNetwork ’). On the contrary, algorithmic procedures
mostly appear as part of the running text, and hence do not have unique
identiﬁers. Hence, detecting APs would require a diﬀerent set of techniques.
Since algorithms represented in documents do not conform to speciﬁc styles,
and are written in arbitrary formats, this becomes a challenge for eﬀective identiﬁcation and extraction. Here we propose a novel methodology based on ensemble
machine learning for discovery of algorithm representations such as PCs and APs.
Moreover, we observe that two or more algorithm representations may be used
to describe the same algorithms. Hence, we also propose a simple heuristic that
links diﬀerent algorithm representations which contribute to the same algorithm
together. Automatic discovery and extraction of these algorithm representations
will give rise to useful applications in digital library and document engineering
ﬁelds.
This chapter has the following key contributions:
1. We propose three variations of a methodology for detecting pseudo-codes
in scholarly documents, including an extension of the existing rule based
method proposed by Bhatia et al. [7], one based on ensemble machine learning
techniques, and a hybrid of these two.
2. We propose a rule based method and a machine learning based method for
detecting algorithmic procedures in scholarly documents.
3. We propose a heuristic that links diﬀerent algorithm representations referring
to the same algorithms together.
4. We evaluate our proposed methodology using empirical evaluation on a dataset
of 258 scholarly documents selected from CiteseerX repository.
The rest of the paper is organized as follows. Section 3.2 provides an overview
of the related works. Section 3.3 describes the overview of our proposed system.

29
Section 3.4 discusses three variations of the pseudo-code detection methodology.
Section 3.5 describes our proposed algorithmic procedure detection method. Section 3.6 explains how algorithm representations are linked to produce unique algorithms. Section 3.7 discusses the dataset construction, evaluation, and results.
Section 3.8 concludes the paper.

3.2

Related Work

Though the literature on identifying document elements is extensive, only limited
works have been explored regarding algorithm identiﬁcation and extraction. Hence,
we brieﬂy discuss works closely related to ours.
Identifying and extracting informative entities such as mathematical expressions [3, 73, 105], tables [52, 55], ﬁgures [18, 38], and tables of contents [103] from
documents have been extensively studied. Kataria et al. [38], employed image
processing and Optical Character Recognition (OCR) approaches for automatic
extraction of data points and text blocks from 2-D plots. They also proposed a
method to index and search for the extracted information. Liu et al. [51] presented
TableSeer, a framework that automatically identiﬁes and extracts tables in digital
documents. They used a tailored vector-space model based ranking algorithm,
TableRank, to rank the search results. Sojka and Lı́ška proposed MIaS (Math
Indexer and Searcher) that collects and interprets mathematical expressions [69].
Since their system only handles documents in the MathML (Mathematical Markup
Language) format where mathematical expressions are already marked up, their
approach is inapplicable to our problem. Bhatia et al. [6] proposed a set of methods used for detecting document-elements which have accompanied captions, e.g.
tables, ﬁgures, and pseudo-codes. They identify a document-element by detecting
the presence of the corresponding caption using a set of regular expressions. In
one of their papers, they proposed an algorithm search engine for software developers [8]. Their search engine, however, only contains pseudo-codes extracted from
scientiﬁc documents. Our proposed methodology addresses the limitation posed
by their pseudo-code detection method and extends the capability to also capture
algorithmic procedures.

30

3.3

System Overview

Figure 3.4: Overview diagram of our proposed algorithm representation discovery
system.
Figure 3.4 displays the high level diagram of the proposed system. The system
speciﬁcally handles PDF documents since a majority of articles in modern digital
libraries including CiteseerX are in PDF format. First, plain text is extracted from
the PDF ﬁle. Inspired by Hassan [30], we use PDFBox1 to extract text and modify
the package to extract object information such as font and location information
from a PDF document. Then, three sub-processes operate in parallel, including
document segmentation, PC detection, and AP detection. The document segmentation module identiﬁes sections in the document. The PC detection module
detects PCs in the parsed text ﬁle. The AP detector ﬁrst cleans extracted text
and repairs broken sentences (since the method assumes that a document is represented with a sequence of sentences), then identiﬁes APs. After PCs and APs are
identiﬁed, the ﬁnal step involves linking these algorithm representations referring
the same algorithms together. The ﬁnal output is then a set of unique algorithms.
1

http://pdfbox.apache.org/

31

<CAPTION> ::= <DOC_EL_TYPE> <Integer> <DELIMITER> <TEXT>
<DOC_EL_TYPE> ::= <FIG_TYPE> | <TABLE_TYPE> | <ALGO_TYPE>
<FIG_TYPE> ::= FIGURE|Figure|FIG.|Fig.
<TABLE_TYPE> ::= TABLE|Table
<ALGO_TYPE> ::= Algorithm|algorithm|Algo.|algo.
<DELIMITER> ::= : | .
<TEXT> ::= <A String of Characters>

Figure 3.5: A grammar for document-element captions

3.4

Detecting Pseudo-codes (PCs)

Most scientiﬁc documents use PCs for compact and concise illustrations of algorithms. PCs are normally treated as document elements separated from the running text, and usually are accompanied with identiﬁers such as captions, function
names, and/or algorithm names. Since PCs can appear anywhere in a document,
these identiﬁers usually serve the purpose of being anchors which can be referred
to by context in the running text. Here, three approaches for detecting PCs in
scholarly documents are presented: a rule based method (PC-RB ), an ensemble
machine learning based method (PC-ML), and a combined method (PC-CB ). Note
that though OCR based techniques have been explored, textual content can be directly and quite accurately extracted from most PDF ﬁles. It seems that converting
documents into images and applying OCR algorithms would just add more noise
to the extracted text.

3.4.1

Rule Based Method (PC-RB)

Recently, Bhatia et al. [8] proposed a rule based PC detection approach, which
utilizes a grammar for document-element captions to detect the presence of PC
captions (See Figure 3.5). We refer to their method as our baseline (PC-BL) for
the PC detection task. Here, our proposed PC-RB extends the baseline by adding
the following rules to improve the coverage and reduce the false positives:
• A PC caption must contain at least one algorithm keyword, namely pseudocode, algorithm, and procedure.
• Captions in which the algorithm keywords appear after prepositions (e.g.

32
‘Figure 15: The robust envelope obtained by the proposed algorithm’ ) are
excluded, as these are not likely captions of PCs.
Given a document (tagged with line numbers), the PC-RB method outputs a
set of line numbers, each of which represents a PC caption, used to locate the PCs
in the document.

3.4.2

Machine Learning Based Method (PC-ML)
       

!  "$%&'* +/: ;<==* >= ?=?>=?@ ?<
= ?=?=?<J
 "K= @* ?=>Q  %'@/&'*'@/ +'@// ?<  '@/&
>='%'@//
X&'*X +X/&Z
=@*<=
'@/&>'@/Z*'@/&[@\Z+'@/&Z
+?<
> 
&[@*][\^<JX'@/&_\Z
=@ <=
&> ?'@/Z
' /&' /`Q ;[j '@/' @/\Z
'@/' @/?
*' /&*' /*'@/Z
+' /&+' /[' @/\+'@/Z
?<
?<
Q=@@= =QXZ
?*X&[\Z

$J=Q_"$J=Q ? %= ?J@?*
q$J=Q{ <=?  QK=Q  =?
%==@ =? >=Q :=Q  ? >=Q: ? <<=? 
>  Q%?:=:=:?  ==@ =? >=Q ?J= 
J=Q=? >=Q%=??=?:? >  Q=Q  =? ?<
 =? @=?= Q>=Q   <? <!? |}~
%= =??< J >%&'*X +X/= =? ?==
==?J= J >?% Q ;Q;>=?

@*X>'@/

j` +X'/
=:=?<==:?J ?=?='/"
="` ='/&Q ; >'@/'j` '//
%&'*X +X/%@*X+X
: @='/&j ?==: 
@*>'@/

&
j` +'/


=  ?%&'* +/% Q % ?=>Q = =?=
 =? %=? =@=  =@@  =%= =? %
=:  Q  ?=%_&'*_ +_/%   J=@
@  _ ?:>= =? %%?: @

>'@/_& @*_j >'@/'j` '//
j` +_
'/
@*_+_
{  =? <=? J ?='/&j


ǀĞƌĂŐĞ
>ŝŶĞ>ĞŶŐƚŚ
сϯϬŚĂƌƐ

ĐƚƵĂůW&WĂŐĞ

ǆƚƌĂĐƚĞĚdĞǆƚ

Figure 3.6: Example of sparse regions (i.e. sparse boxes), taken from [46]. The
left ﬁgure is an actual PDF page. The right ﬁgure illustrates the extracted text.
The PC-RB yields a high precision, however it still suﬀers from a low coverage resulting in a poor recall. We found that 25.8% of PCs in our data set do
not have accompanied captions. These PCs would remain undetected using the
PC-RB method. To get around this issue, we propose a machine learning based
(PC-ML) method that directly detects the presence of PC contents (instead of

33
their captions). Our motivation originated from the observation that most PCs
are written in a sparse manner, resulting in sparse regions (we call them sparse
boxes) in documents. Figure 3.6 shows an example of sparse boxes on a sample
scientiﬁc article page. The PC-ML ﬁrst detects and extracts these sparse boxes,
then classiﬁes each box whether it is a PC box or not. A PC box is a sparse box
that contains at least 80% content (in terms of number of lines) of a PC. The
following subsections explain how sparse boxes are identiﬁed, the feature sets, and
the classiﬁcation models. The output of the PC-ML is a tuples of start, end line
numbers of the detected PCs.
3.4.2.1

Sparse Box Extraction

We deﬁne a sparse box as a set of at least N consecutive sparse lines. A sparse line
is a line that meets the following criteria: 1) the ratio of the number of non-space
characters to the average number of characters per line is less than the threshold M,
2) not footers/headers, and 3) enclosed by sparse lines. We found that N = 4 and
M = 0.8 work best for our dataset. The right sub-ﬁgure of Figure 3.6 illustrates
extracted text lines which are sparse lines (highlighted in yellow), and non-sparse
lines (highlighted in white). Each set of consecutive sparse lines composes a sparse
box as illustrated in the left sub-ﬁgure. The eﬃcacy of the sparse box extraction
method is evaluated in two perspectives: coverage and accuracy. Given a set of
sparse boxes B extracted from a document d, the coverage is deﬁned as following:
Coverage =

|{l|l ∈ b, b ∈ B, l is positive}|
|{l|l ∈ b, b ∈ B}|

The line-wise recall is utilized to quantify how much PC content can be captured
within each extracted sparse box. Our sparse box extraction method yields a
coverage of 92.99%. Among all the sparse boxes detected in our DS2 dataset, we
found 237 (out of 275 (86.18%) actual PCs) PC boxes.
The accuracy evaluation quantiﬁes how precisely each PC is cut into a sparse
box. For each PC box, we measure both the upper boundary delta (the start line
number of the actual PC minus the start line number of the sparse box) and lower
boundary delta (the end line number of the actual PC minus the end line number
of the sparse box). Figure 3.7 and 3.8 show the upper and lower boundary delta
distributions of the 237 PC boxes. 76.37% and 70.89% of PC boxes have upper

34

Upper Boundary Delta Distribuon

% of # Pseudocode Boxes

60%
50%
40%
30%
20%
10%
0%
14

13

11

10

9

8

7

6

5

4

3

2

1

0 -1
Line Deltas

Figure 3.7: Distribution of upper boundary deltas of PC boxes

Lower Boundary Delta Distribuon

70%
% of # Psedocode Boxes

60%
50%
40%
30%
20%
10%
0%
1

0

-1

-2

-3

-4

-5

-6

-7

-8

-9 -10 -11 -12 -13
Line Deltas

Figure 3.8: Distribution of lower boundary deltas of PC boxes
and lower line deltas of ±2 lines respectively, suggesting that most PC boxes are
fully and precisely extracted by the proposed sparse box cutting technique.
3.4.2.2

Feature Sets for PC Box Classiﬁcation

We extract 47 features (listed in Table 3.1) from each extracted sparse box. These
features are classiﬁed into 4 groups: font-style based (FS ), context based (CX ),
content based (CN ), and structure based (ST ). The FS features capture various
font styles used to compose PCs. The CX features detect the presence of PC

35
Table 3.1: Features for pseudo-code box classiﬁcation can be divided into 4 groups:
font style based (FS), context based (CX), content based (CN), and structure based
(ST)
Grp

FS

Fe ature
INDENTATIO N VARIANCE

De scription
Variance of the positions of the first character in each

FIRST 4CHARS INDENTATIO N

Variance of the average of the positions of the first 4

NUM DIFF FO NTSTYLES

# of different font styles. Ex. 'XX XX ' has 2 font styles.

NUM FO NTSTYLE CHANGES
FRAC NUM FO NTSTYLE CHANGES

# of char pairs whose font styles are different. Ex.
'XX XX ' has 3 font style changes.
Fraction of number of font style changes to number of

HAS CAPTIO N NEARBY

Has a caption near (within 3 upper/lower lines) the

CX HAS PC CAPTIO N NEARBY
HAS PC NAME NEARBY

Whether there is a pseudo-code caption near the sparse
Has is an algorithm name (e.g. `Algorithm ABC ') near

NUM PC WO RDS

# of PC keywords (e.g. forall , for, if, else, iftrue, endif,

FRAC PC WO RDS TO NUMWO RDS

Fraction of pseudo-code keywords to number of words.

FRAC PC WO RDS TO NUMLINES

Fraction of pseudo-code keywords to number of lines.

NUM ALGO WO RDS

# of algorithm keywords (e.g. algorithm, pseudo-

FRAC ALGO WO RDS TO

Fraction of # of algorithm keywords to # of words

CN FRAC NUM ALGO WO RDS TO
NUM LINES BEGIN WITH PC WO RDS

Fraction of # of algorithm keywords to # of lines
# of lines beginning with a pseudo-code keyword

FRAC NLINES BEG. W/ PCWO RDS TO Fraction of # of lines beginning with a PC word to # of
NLINES
lines
NUM FUNCTIO NS
# of functions. Ex. Scan(f, x)
FRACTIO N NUM FUNCTIO NS TO
NUMLINES
NUM CHARS

Fraction of # of functions to # of lines

FRAC NUM CHARS TO NUMLINES

Fraction of # of characters to # of lines

NUM SYMBO LS

# of symbols

FRAC NUM SYMBO LS TO

Fraction of # of symbols to # of characters

NUM ALPHABETS

# of alphabets

FRAC NUM ALPHABETS TO

Fraction of # of alphabets to # of characters

NUM DIGITS

# of digits

FRAC NUM DIGITS TO NUMCHARS

Fraction of # of digits to # of characters

NUM ALPHANUMERS

# of alphanumeric characters

FRAC NUM ALPHANUMBERS TO

Fraction of # of alphanumeric characters to # of

# of characters

NUM NO N-ALPHANUMBERS

# of non-alphanumeric characters

FRAC NO N-ALPHANUMBERS TO

Fraction of # of non-alphanumeric characters to # of

NUM GREEKCHARS

# of Greek characters

FRAC NUM GREEK TO NUMCHARS

Fraction of # of Greek characters to # of characters

# of arrow symbols
ST NUM ARRO WS
FRAC NUM ARRO WS TO NUMCHARS Fraction of # of arrow characters to # of all characters
NUM MATHO PS

# of math operators (e.g. +,-,Σ,×, etc.)

FRAC NUM MATHO PS TO

Fraction of # of math operators to # of characters

NUM 1-CHAR WO RDS

# of 1-character words (e.g. `x xx x' has 2 1-character

FRAC NUM 1-CHAR WO RDS TO

Fraction of # of single-char words to # of lines

FRAC NUM 1-CHAR LINES TO

Fraction of # of 1-character lines to # of all lines

NUM IJK

# of characters `i', 'j', and `k'

FRAC NUM IJK TO NUMLINES

Fraction of # of `i', 'j', `k' characters to # of lines

NUM CO DING SYMBO LS

# of coding symbols (e.g. \{,\},[,],@,/)

FRAC NUM CO DING TO NUMLINES

Fraction of # of coding symbols to # of lines

NUM LINES END WITH DO T

Number of lines ending with `.'

FRAC NUMLINES EWDO T TO

Fraction of # of lines ending with `.' to # of lines

NUM LINES BEGIN WITH NUMBER

# of lines beginning with a number

FRAC LINES BWNUMBER TO

Fraction of # of lines beginning with anumber to # of

captions. The CN features capture the PC speciﬁc keywords and coding styles.
The ST features characterize the sparsity of PCs and the symbols used.

36
3.4.2.3

Classiﬁcation Models

Each detected sparse box is then classiﬁed whether it is a PC box or not. We train
12 base machine learning classiﬁcation algorithms with the features described in
Table 3.1. These algorithms include Logistic Model Trees (LMT), Multinomial
Logistic Regression (MLR), Repeated Incremental Pruning to Produce Error Reduction (RIPPER), Linear Logistic Regression (LLR), Support Vector Machine
(SVM), Random Forest (RF), C4.5 decision tree, REPTree, Decision Table (DT),
Random Tree (RT), Naive Bayes (NB), and Decision Stump (DS).
In addition to the base classiﬁers listed above, we also combined them using
standard ensemble techniques such as uniform weighted majority voting (VOTE)
and probability averaging (PAVG) [45]. First, the 12 base classiﬁers are tested with
10% held-out data from the training data and ranked by their F1 scores. Then,
the ﬁrst 2, 3, ..., 12 ranked classiﬁers in each ranked list are combined using the ensemble methods. Note that other ensemble techniques such as Adaboost, Bagging,
and Rotation Forest were also explored but overall the VOTE and PAVG methods
performed much better, agreeing with a prior study of ensemble classiﬁcation by
Kittler et al. which found that these two ensemble methods outperformed others (i.e. multi-staging, product, maximum, median, and minimum rules) on the
identity veriﬁcation and the handwritten digit recognition tasks [45].

3.4.3

Combined Method (PC-CB)

Though the PC-ML methods can capture PCs even though they do not have
accompanied captions, some PCs which are not ﬁrst captured in a sparse box
would still remain undetected. Mostly, such PCs are either written in a descriptive
manner (hence do not result in sparse regions in the document), or ﬁgures (the
text extractor cannot extract images). In our dataset DS2 (See Sec. 3.7.1), 35
PCs (out of 275 actual PCs) cannot be captured using the sparse box extraction.
However, 27 (out of 35) of these undetected PCs have accompanied captions and
hence might still be detected using the PC-RB method. We propose a combined
method (PC-CB ) of the PC-RB and the PC-ML using a simple heuristic as follows:
STEP1 For a given document, run both PC-RB and PC-ML.
STEP2 For each PC box detected by PC-ML, if a PC caption detected by PC-RB

37
is in proximity, then the PC box and the caption are combined.

3.5

Detecting Algorithmic Procedures (APs)

An algorithmic procedure (AP) is used to represent a relatively simpler algorithm,
or describe an algorithm in a higher level, since it lacks the power to precisely
express complex computational operations such as nested loops, recursion, and
functions. An AP is usually composed in an order-listing style, with descriptive wordings. Most APs usually have a sentence at the beginning to introduce
the algorithm enclosed in such an AP. We call such a sentence an algorithmic
procedure indication sentence (e.g. To solve the subproblem, we propose a
pseudopolynomial dynamic programming algorithm as follows:). Our AP
detection approaches aim to detect such indication sentences, and use them to
locate the accompanied APs.
Two methods are developed for detecting AP indication sentences: a rule based
method (AP-RB ) and a machine learning based method (AP-ML). Both methods
rely on the sentences correctly extracted from the document.
Since most scholarly documents are multi-columned and contain document elements such as tables and diagrams, it is often seen that the extracted text may
contain garbage/noisy lines and broken sentences. Hence, before extracting sentences, garbage text fragments are removed from the document. We also develop
a heuristic that stitches up an incomplete sentence which are broken into multiple lines. After the extracted text is cleaned and broken sentences are mended,
sentences are extracted using LingPipe2 sentence extractor.

3.5.1

Rule Based Method (AP-RB)

Often, AP indication sentences exhibit certain common properties:
• The sentences usually end with follows:, steps:, algorithm:, follows:, following:, follows., steps:, below:.
• The sentences usually contain at least an algorithm keyword.
2

http://alias-i.com/lingpipe/

38
We create a set of regular expression rules that capture sentences according to
the rules above.

3.5.2

Machine Learning Based Method (AP-ML)

Unfortunately, some AP indication sentences do not conform to the rules described
in 3.5.1. We hence propose an alternative approach based on machine learning that
directly learns to capture the characteristics of the APs.
3.5.2.1

Feature Sets

Table 3.2: Features for detecting AP indication sentences can be divided into 2
groups: content based (CN) and context based (CX).
Grp

Feature

Description

MATCH AP SENTENCE RULES Wether it is an AP indication sentence
HAS ALGO WO RDS
Wether it contains an algorithm keyword (e.g.
'algorithm', 'procedure', etc.)
HAS STEP WO RDS
Wether it contains a stepwise indication word
(e.g.`followings', `steps', etc.)
END WITH CO LO N
Whether it ends with a colon (i.e. `:')
Whether it ends with a stepwise keyword
CN END WITH STEP WO RDS
END WITH LISTING PLURAL Whether it ends with plural list-indicating noun (e.g.
`properties', `results', etc.)
IS CAPTIO N
Wether it is a caption
NUM WO RDS
Number of words
IS NUMWO RDS LESSTHAN T Whether the number of words is less than the threshold
T (we use T = 35)
NUM STEP SENTENCES
# of sentences starting with a bullet point, a listing
number/alphabet, or the word `Step'
NUM PC KEYWO RDS
# of pseudo-code keywords (e.g.
FRAC PC WO RDS TO
Fraction of # of pseudo-code keywords to # of lines
NUMLINES
Number of algorithm keywords (e.g. algorithm,pseudoNUM ALGO WO RDS
code,procedure,etc.)
FRAC ALGO WO RDS
Fraction of number of algorithm keywords to # of words
NUMWO RDS
NUM SYMBO LS
Number of symbols
FRAC SYMBO LS TO
Fraction of number of symbols to number of lines
CX NUMLINES
NUM GREEKCHARS
Number of Greek characters
FRAC GREEKCHARS TO
Fraction of number of Greek characters to # of lines
NUMCHARS
NUM 1-CHAR WO RDS
Number of single-character words (e.g. `x xx x' has 2
single-character words)
FRAC 1-CHAR WO RDS
Fraction of number of single-char words to # of lines
NUMLINES
NUM MATHO PS
Number of math operators (e.g. +,-, S, ¸, etc.)
FRAC NUM MATHO PS
Fraction of number of math operators to # of lines
NUMCHARS
NUM NO N-ALPHANUMBERS Number of non-alphanumeric characters
FRAC N-ALPHANUMBERS
Fraction of number of non-alphanumeric characters to

We extract 26 features from each extracted sentence. The feature can be cate-

39
gorized into two groups: content based features (CN ) and context based features
(CX ). The content based features are extracted from the sentence itself, and are
designed to learn the characteristics of the AP indication sentences. The context
based features are extracted from the 28 lines below the line where the sentence
appears. The number 28 is the average number of lines of APs observed in our
data set. Table 3.2 lists all the features.
3.5.2.2

Classiﬁcation Models

We use the same set of base classiﬁers and ensemble methods as described in section
3.4.2.3.

3.6

Linking Algorithm Representations

This is the ﬁnal phase of our algorithm detection and extraction system. A heuristic
is implemented to link algorithm representations referring to the same algorithm
together. Speciﬁcally, two algorithm representations are linked if:
1. They represent the same algorithm. For example, an AP may be used to
provide more detail to a PC.
2. They are part of the same algorithm. For example, an algorithm may be
broken into sub-parts, each is represented using a diﬀerent algorithm representation.
For simplicity, we make an assumption that there can be at most one algorithm
contained in a document section. Our linking algorithm ﬁrst assigns each algorithm
representation to a section. Algorithm representations which fall into the same
section are then linked. We ﬁrst brieﬂy discuss how a document is segmented into
sections, then explain how we assign algorithm representations to corresponding
sections so that we can link them.

3.6.1

Identifying Sections in Scholarly Documents

A novel machine learning based approach is developed to construct a sequence of
sections from a document. The algorithm ﬁrst detects section headers, then uses

40
them as section boundaries.
Most scholarly documents have following common properties: 1) Each section
has a section header, usually with a section number. 2) Section headers usually
have distinct font styles from the surrounding content. 3) A majority of sections
are common sections such as Abstract, Introduction, Background, Conclusions, and
References. According to such properties, 22 features are identiﬁed to characterize
section headers. The features can be divided into 3 groups: pattern based (PAT ),
style based (STY ), and structure based (STR). The pattern based features are
used for capturing section headers which are standard sections or section headers
with section numbers. The style based features ﬁlter out lines that look like section headers but are in fact fragments of sentences, lines in tables of contents, or
textual fragments from tables/diagrams. The structure based features concern the
locations of the section headers. For example, lines that occur between the Abstract/Introduction and the References sections are better candidates than those
outside this region. These features also ﬁlter out footers and headers.
A number of data balancing techniques and classiﬁcation algorithms were considered. We found that the Random Forest classiﬁcation algorithm [11] with 100
trees, trained with these 22 features using a variant of the weighting balancing
techniques (where minority class instances are given higher weight), achieved the
best classiﬁcation performance with 93.74% Precision, 91.06% Recall, and 92.38%
F1. We hence use this learner for our document segmentation task. This algorithm
is described in detail in Chapter 2.

3.6.2

Using Document Sections for Linking Representations of Algorithms

Assigning an AP to a section is easy, since it is part of the running text. Unlike
APs, PCs are normally located separately from the running text; hence, we have
no direct way to identify the exact section to which a PC should be assigned. To
get around this problem, we map a PC to the section which contains the largest
number of reference sentences that refer to it. Once each algorithm representation
is assigned to a section, we then link the algorithm representations which are
mapped to the same section together into a set of algorithm representations that

41
represents a unique algorithm in the document.

3.7

Experiments and Discussion

We divide the experiments into three parts: PC detection, AP detection, and
algorithm representation linking. All the experiments are performed on a Windows machine with an Intel Core i7-2600 CPU (3.4GHz) and 16 GB of ram. We
use the LibSVM3 implementation for SVM, and Weka4 implementation for other
classiﬁcation algorithms.

3.7.1

Data Sets

Two datasets are used:
DS1 consists of 100 scholarly documents selected from the CiteseerX repository to
represent a diverse types of scholarly articles and algorithm representations.
This data set is used to construct rules and regular expressions for our rule
based methods, and determine feature sets for our machine learning based
methods.
DS2 consists of 258 scholarly documents randomly selected from the CiteseerX
repository. This data set consists of 275 PCs, 86 APs, and 282 unique algorithms. We use this data set to evaluate our proposed models.

3.7.2

Evaluation of Pseudo-code Detection

We evaluate the three variants of PC detection approaches on the data set DS2,
using 10-fold document-wise cross validation. Standard precision, recall, and F1
are used for evaluating the performance. Let Tg be the set of all PCs, Tr be the

set of detected PCs, so that the correctly detected PCs are Tg Tr . These metrics
are deﬁned as follows:
3
4

http://www.csie.ntu.edu.tw/ cjlin/libsvm/
http://www.cs.waikato.ac.nz/ml/weka/

42
Table 3.3: Precision, recall, and F1 of the best classiﬁcation models (in terms of
F1) used for the pseudo-code (PC) and algorithmic procedure (AP) methods. (‘!’
denotes Majority Voting (VOTE), ‘+’ denotes Probability Averaging (PAVG))
Method
PC-BL
PC-RB
PC-ML
PC-CB
AP-RB
AP-ML

Model
Baseline
RuleBased (Improved Baseline)
!LMT-RF-RIPPER-MLR
!LMT-RF-RIPPER
RuleBased
+NB-LMT-LLR-MLR-RT

Pr%
70.46
87.12
85.31
87.37
64.24
69.56

Re%
35.96
44.57
57.04
67.17
28.00
49.00

F1%
47.62
58.97
68.37
75.95
39.00
57.50



|Tg Tr |
|Tg Tr |
2 · precision · recall
precision =
, recall =
,F1 =
|Tr |
|Tg |
precision + recall
Table 3.3 lists notable results of our proposed methods against the PC detection
baseline (PC-BL). As expected, our rule-based method (PC-RB ) yields high precision with a cost of low recall. Using machine learning techniques (PC-ML), the
overall performance (in terms of F1) is improved. The combined method (PC-CB )
of PC-RB and a majority voting of LMT, Random Forest, and RIPPER base classiﬁers performs the best in terms of F1, outperforming the state-of-the-art baseline
by 28.33% (in terms of F1).
It is worth noting that the ensemble methods result in greater improvement
compared to only using individual experts. Figure 3.9 compares the performances
(in terms of F1) between the ensemble methods and the best base classiﬁers for PCML (i.e. with MLR classiﬁer) and PC-CB (i.e. with LMT classiﬁer). The X-axis
denotes the ﬁrst k base classiﬁers used in each ensemble method. Note that the fact
that the combination of LMT, Random Forest, and RIPPER accumulatively yields
the best result means that each individual expert learns a diﬀerent aspect of the
data, and hence is able to correct each other when making collective decisions. Not
surprisingly, these base classiﬁers are inherited from diﬀerent classes of machine
learning algorithms (function, tree, and rule based respectively). We conclude that
the ensemble methods are useful for these speciﬁc problems, when the best base
classiﬁers are combined. However, the performance of the ensemble methods can

43

0.76
0.74

F-Measure

0.72
0.7
0.68
0.66
0.64
PC-CB (VOTE)
PC-ML (VOTE)

0.62
1

2

3

4

PC-CB (AVG)
PC-ML (PAVG)

5

6

7

PC-CB (LMT)
PC-ML (MLR)

8

9

10

# Base Classiﬁers
11
12
13

Figure 3.9: Comparison of the ensemble methods against the best base classiﬁers
in PC-ML and PC-CB
decrease as the number of base classiﬁers grows. This might be because bad base
classiﬁers can impede the collective decisions of the good ones. Unlike traditional
document classiﬁcation techniques where the feature space can grow large as the
number of documents increases (to handle the pattern and lexical diversity, etc.),
all of our proposed methods scale well with document growth as the feature size
is ﬁxed.
In order to see what features are more important than the others, all the features are evaluated the worthiness by measuring the information gain with respect
to the class. Table 3.4 lists the features ranked by their information gain scores.
We can see that important features involve those that characterize PC composition
such as the presence of programming keywords and symbols, and the presence of
PC captions nearby. Numbers of common digits and characters are not as important, and hence ranked the lowest.

3.7.3

Evaluation of Algorithmic Procedure Detection

Each sentence in the dataset DS2 is labelled whether it is an AP indication sentence
or not. The dataset DS2 contains 86 AP indication sentences and 74,278 nonindication sentences. 10-fold document-wise cross validation is used to evaluate

44
Table 3.4: Features for pseudo-code detection, ranked by their information gain
scores.
Info Gain Score
0.1666
0.14455
0.12043
0.10659
0.08703
0.07421
0.03439
0.03301
0.0299
0.02822
0.02818
0.02748
0.02736
0.02582
0.02569
0.02515
0.02515
0.02497
0.02496
0.02383
0.02365
0.02325
0.02308
0.02262
0.02247
0.02144
0.01959
0.01878
0.0172
0.0171
0.0163
0.01586
0.01484
0.01454
0.01348
0.01345
0.0128
0.01275
0.00995
0.00976
0.00732
0.00714
0.00503
0.00478
0
0
0

Feature
num lines beginwith pc keywords
num lines beginwith pc keyword ratio to num denoisedlines
num pckeywords ratio to num words
has pccaption nearby
num pckeywords
num pckeywords ratio to num denoisedlines
num fontstyle changes
num arrowchars
num arrowchars ratio to num chars
num algokeyword ratio to num words
num codingsymbols ratio to num denoisedlines
num codingsymbols
num fontstyle changes ratio to num denoisedlines
has caption nearby
num funtions
num algokeywords
num algokeyword ratio to num denoisedlines
has pclabel nearby
num mathops ratio to num chars
num digits ratio to num chars
num alphabets ratio to num chars
indentation variance
num diﬀ fontstyles
num functions ratio to num denoisedlines
num symbols
num nonalphanumers
num alphabets
num ijk
num 1charwords ratio to num denoisedlines
num mathops
num alphanumers ratio to num chars
num 1charwords
num alphanumers
num chars
num ijk ratio to num denoisedlines
num 1charlines ratio to num lines
num chars ratio to num denoisedlines
num symbols ratio to num chars
num digits
num nonalphanumers ratio to num chars
num lines end with dot
ﬁrst 4chars indentation variance
num greekchars
num lines end with dot ratio to num denoisedlines
num greekchars ratio to num chars
num lines begin with number
num lines begin with number ratio to num denoisedlines

our methods, using standard precision, recall, and F1 deﬁned in Section 3.7.2 as
the evaluation metrics. Table 3.3 lists notable results.
The best performance in terms of F1 is achieved by the ensemble machine
learning based method (AP-ML) with the probability averaging of NB, LMT, LLR,

45

0.6

F-Measure

0.55

0.5

0.45

0.4
AP-VOTE

AP-AVG

AP-LMT

# Base Classiﬁers

0.35
2

3

4

5

6

7

8

9

10

11

12

13

Figure 3.10: Comparison of the ensemble methods against the best base classiﬁer
in AP-ML
MLR, and RT base classiﬁers, yielding 69.56% precision, 49.00% recall, and 57.50%
F1. Figure 3.10 compares the performances of the ensemble methods against the
best base classiﬁer (i.e. LMT classiﬁer). The X-axis denotes the ﬁrst k base
classiﬁers used in each ensemble method. Similar to the analysis of the ensemble
methods used in the PC detection, the ensemble of diverse base classiﬁers from distinct machine learning families allows individual experts to learn diﬀerent aspects
of the data, resulting in better collective decisions. The classiﬁcation performance
increases up until 5 base classiﬁers, then begins to decrease as addition of bad base
classiﬁers could impede the ensemble decisions.

3.7.4

Evaluation on Algorithm Representation Linking

We evaluate our linking strategy on data set DS2 in term of accuracy, deﬁned as:
Accuracy =

# Algorithm Representations Correctly Grouped
# All Algorithm Representations

We ﬁrst identify all the PCs and APs from each document, then link them into
groups of unique algorithms. Our linking algorithm achieves the accuracy of
85.15%.

46

3.8

Conclusions

Algorithms play an important part in solving research problems. Scientiﬁc publications host a tremendous amount of such high-quality algorithms developed by
professional researchers. In digital libraries, being able to extract and catalog these
algorithms would introduce a number of exciting applications including algorithm
searching, discovering, and analyzing. We have described machine learning based
methodology for discovery of algorithm representations in scholarly documents.
The proposed system takes a PDF document as an input, and outputs a set of
unique algorithms, each of which is a group of algorithm representations (pseudocodes, algorithmic procedures, or both). The empirical evaluation on a collection of
258 scholarly documents shows that our proposed methods outperform the baseline
by 28.33% (in pseudo-code detection), and are eﬀective in discovering pseudo-codes
and algorithmic procedures. Future work would be to further explore the semantic
analysis of algorithms, their trends, and how algorithms inﬂuence each other over
time.

Chapter

4

Algorithm Metadata Extraction,
Annotation, Indexing, and Searching
Computational and related sciences have become more advanced and complex, requiring advanced algorithms to solve emerging problems. In the previous chapter,
we discuss how algorithm representations and their metadata are extracted from
a collection of scholarly documents. A synopsis is generated for each extracted algorithm representation using the document summarization algorithm proposed by
Bhatia and Mitra [6]. This synopsis provides additional useful textual information
that could be useful for retrieval. The synopsis generation method does not work
well with algorithms that lack certain properties, urging the need to investigating
into automatic annotation techniques for these algorithms.
This chapter discusses textual metadata extraction for the extracted algorithms, along with demonstrating the demo version of the algorithm search system.

4.1

Introduction

The algorithm search engine system retrieves relevant algorithms by matching the
search query to the textual information of the algorithm metadata records. The
retrieved algorithms are ranked based on the TD-IDF similarity level between the
query and the textual metadata of the algorithms. Having rich and meaningful
textual metadata is therefore crucial for eﬀective retrieval.
In Chapter 3, we show that it is possible to detect and extract algorithm rep-

48
resentations in forms of pseudo-codes and algorithmic procedures from scholarly
documents. Though the proposed techniques are eﬀective at detecting algorithm
representations, they are still not capable of interpreting the semantics of the
extracted algorithm representations. In order to do so, advanced techniques on interpreting pseudo-codes would need to be further investigated. Furthermore, most
PDF-to-Text extraction algorithms are not designed for handling mathematicallike contents such as equations and pseudo-codes, due to complex compositions of
symbols, font styles, and untypical decorations. In order to get around the problem, techniques similar to non-textual object searches are explored. Examples of
non-textual object search systems include image search and video search, where
textual information that appears in proximity of an object is used as the representative textual component of such an object. These textual components allow each
object to be indexed and retrieved via textual matching with a given query.
For algorithms represented by pseudo-codes, textual metadata could include
their captions and reference sentences. A reference sentence is a sentence in the
paper that refers to the pseudo-codes. For algorithms represented by algorithmic
procedures, their textual metadata could include the indication sentences. Figure
4.1 illustrates the textual metadata relevant to the given pseudo-code that can be
directly extracted. Such metadata includes the pseudo-code’s caption text and its
reference sentences.
Table 4.1: Statistics about primary textual metadata of the extracted 6285 algorithms.
Total Number of Algorithms
Total Number of Words in Caption Text (for Pseudo-codes)
Total Number of Words in Reference Sentences (for Pseudo-codes)
Total Number of Words in Primary Textual Metadata
Total Number of Clean Words in Primary Textual Metadata
Total Number of Sentences in Primary Metadata
Avg Number of Words in Caption Text (for Pseudo-codes)
Avg Number of Words in Reference Sentences (for Pseudo-codes)
Avg Number of Words in Primary Textual Metadata
Avg Number of Clean Words in Primary Textual Metadata
Avg Number of Sentences in Primary Metadata

6,285
96,526
605,606
702,132
304,233
58,661
15.36
96.36
111.72
48.41
9.33

Adequate and meaningful textual metadata allows eﬀective retrieval. How-

49
Pseudo-code

Figure 1:

Caption Text
The Partial-topology Dissemination
Reference Sentences

The INIT-PDA procedure in Fig. 1 initializes the tables of a router at startup time; all
variables of type distance are initialized to infinity and those of type node are initialized
to null. Sim- ilarly, in Fig. 10, the delays obtained using MP routing for NET1 are within
28% envelopes of delays obtained using OPT routing. Fig. 11 compares the average delays
of MP and SP for CAIRN. In Fig. 12, for NET1, MP routing performs even better; average
delays of SP are as much as five to six times those of MP routing which is due to higher
connectivity available in NET1. For CAIRN, Fig. 13 show the effect of increasing T l
when T s and the input traffic is fixed. Similarly, for NET1, delays for SP increased
significantly while there is negligible change in delays of MP as can be observed in Fig.
14, respectively. This becomes evident in Fig. 15, which shows a typical response in NET1
when the flow rate is a step function (i.e.., the flow rate is increased from 0 to a finite
amount at time 0). Fig. 18 shows the delays for SP and MP.

Figure 4.1: Sample of primary textual metadata (i.e. caption text and reference
sentences) associated with Algorithm INIT-PDA, represented by a pseudo-code
in [98].
ever, not all algorithms have as rich textual metadata as the example in Figure
4.1. From here on, we refer to the combination of caption text and reference sentences of an algorithm as its primary textual metadata. Table 4.1 breaks down the
statistics of the primary textual metadata of the collection of 6,285 algorithms extracted from 20,567 scholarly documents. Figures 4.2 and 4.3 plot the distribution

50

Word Distribuon of Primary Textual Metadata
1200

100.00%

90.00%

1027
1000

80.00%

Frequency
Cumulave %

Frequency (# of Algorithms)

849

Expon. (Frequency)

800

70.00%

60.00%
602
600

50.00%
507

504

40.00%
371

400

30.00%

318
204
200

20.00%

179 190
136
99 103 92

78

91
51

56

60

10.00%
47

44

40

36

24

23

29

29

21

29

17

0

0.00%
10

20

30

40

50

60

70

80

90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300
Number of Words (Bin)

Figure 4.2: Distribution of number of words in primary metadata of the extracted
algorithms.
of words and clean words of the primary textual metadata records. Note that another separate statistics for clean primary textual metadata are presented because
clean textual information, taken from pre-processed primary textual metadata by
removing stopwords, non-word characters, and stemming, has shown to eliminate
noise and unuseful information in the textual metadata and improve the overall
search quality. From Table 4.1, the average number of clean words in the primary
textual metadata of each algorithm is only 48. Even worse, according to Figure
4.3, roughly 50% of the extracted algorithms have only 20 or fewer words in their
primary textual metadata. These algorithms with poor textual metadata would
likely be missed when being searched.
A natural question would be whether it is possible to use the textual algorithm
content (i.e. the actual content in the pseudo-codes and algorithmic procedures) as
another piece of textual information. By examining a large number of pseudo-codes
and algorithmic procedures, we found that most of their content discusses about
what each step is taken in order to solve the target problems. While most algorithm search queries tend to include the descriptions of their target problems (e.g.

51

Processed (Clean) Word Distribuon of Primary Textual Metadata
100.00%

1800

90.00%

1563

1600
1454

Frequency

1400

80.00%

Cumulave %

Frequency (# of Algorithms)

2 per. Mov. Avg. (Frequency)

70.00%

1200
60.00%
1000
910

50.00%
800
40.00%
600

537

30.00%
365

400

20.00%
237
169

200

147

121

94

10.00%
65

55

59

43

42

32

26

21

22

20

16

13

12

14

5

4

10

3

3

7

0

0.00%
10

20

30

40

50

60

70

80

90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300
Number of Words (Bin)

Figure 4.3: Distribution of numbers of processed (clean) words in primary metadata of the extracted algorithms. Note that the processed words are taken from
pre-processed primary textual metadata by removing stopwords, non-word characters, and stemming. This preprocessing step has shown to eliminate noise and
unuseful information in the textual metadata and improve the overall search quality.
“shortest paths”, “high dimensional text classification”, etc.), this content in the users queries mostly does not appear in the algorithm content itself,
making this kind of information useless for indexing. Plus, the content in pseudocodes is normally composed with symbols and programming codes, which poses
challenge for tokenizing and indexing processes.
Recently, Bhatia and Mitra have proposed an algorithm for generating comprehensive descriptions for document elements (including tables, ﬁgures, algorithms,
etc.) that allows the end-user to quickly understand the content of the document
element without having to download and read the entire document [6]. We refer
to this comprehensive description as a synopsis. For a given document element,
the algorithm operates in a supervise fashion that includes three main steps: 1)
it retrieves the caption and reference sentences from the document, 2) it extracts
representative features and trains a classiﬁer, and 3) it classiﬁes the remaining

52
sentences in the documents whether they are relevant or irrelevant to the given
document element. The relevant sentences are then put together as the synopsis.
The main drawback of the document summarization algorithm is that it requires the presence of a document caption and at least a reference sentence for
a given document element. In our dataset, we ﬁnd that 25.8% of the extracted
pseudo-codes do not have accompanied captions. Furthermore, algorithmic procedures do not also have captions as they are naturally part of running text. The
document summarization algorithm hence would fail to generate synopses for these
algorithm representations. Recently, Tuarob et al. have proposed an algorithm
that is capable of automatically annotate a metadata record with poor metadata
by learning topical knowledge from another collection of well annotated metadata
records [85]. The algorithm works well when these two criterion are met:
1. Each metadata record must have a main meaningful textual content. The
main textual component allows the algorithm to ﬁnd similar documents in
the training collection to generate annotation.
2. Each metadata record that needs annotated must have some relationship
with the collection of well-annotated metadata. This will guarantee that the
annotation will be relevant since it is modeled from the training data with
similar topics.
We examine some sample extracted algorithms with poor primary textual metadata and found that each of these algorithm representations has an associated small
piece of text composed with technical meaningful keywords. For pseudo-codes
without captions, this would be the function names and surrounding sentences.
For algorithmic procedures, this would be indication sentences and surrounding
sentences. This characteristic meets the ﬁst criteria above. Furthermore, recent
studies show that algorithms are not normally composed from scratch, but are
rather relevant to each other. Some algorithms inherit properties from existing
ones [82]. Likewise, some algorithms are designed to solve similar sets of problems [80]. This also makes the algorithm metadata meets the other criteria.
In this chapter, we explore the possibility of using the document element summarization algorithm [6] and the metadata annotation algorithm [82] to generate

53
textual metadata for algorithms. We also discuss how the metadata is indexed and
search via a text-based search engine interface.

4.2

Related Literature

The literature on document annotation is extensive. Hence we only present the
work closely related to ours.

4.2.1

Automatic Document Annotation

Newman et al. discuss approaches for enriching metadata records using probabilistic topic modeling [61]. Their approach treats each metadata record as a bag
of words, and consists of two main steps: i) generate topics based on a given corpus of metadata, and ii) assign relevant topics to each metadata record. Hence a
metadata record is annotated by the top terms representing the assigned topics.
They propose three variations of their approaches. The ﬁrst method, which they
use as the baseline, uses full vocabulary (every word) from the corpus. The remaining two methods ﬁlter out the vocabulary by deleting useless words resulting
in more meaningful topics. They compare the three approaches in three aspects:
% of usable topics, % enhanced records, and average coverage by the top 4 chosen topics. They acquire the datasets from 700 repositories, hosted by OAISter
Digital Library. The results show that, overall, the second method performs the
best. However, such methods require manual modiﬁcation of the vocabulary, hence
would not scale well. The third method performs somewhere in between.
Bron et al. address the problem of document annotation by linking a poorly
annotated document to well annotated documents using TF-IDF cosine similarity
[12]. One corpus consists of textually rich documents (As ) while the other contains
sparse documents (At ). They address two research problems: document expansion
and term selection. For the document expansion task, each targeted document
(a document in sparse set) is mapped to one or more documents in the rich set,
using simple cosine-similarity measure. Top N documents are chosen from the rich
corpus, and the texts in these documents are added to the targeted documents as
supplemental content. The term selection task was introduced because using the

54
whole documents from the source corpus to enrich the targeted document might
be too spurious and have a fair chance of topic drifts. This term selection task
aims to select only meaningful words from each document in the source corpus to
add to the targeted documents. Basically, top K% of the words in each document,
ranked by TF-IDF scores, are selected as representative words of the document.
This work has a similar problem setting to ours, except that we aim to annotate
a query document with keywords taken from the library, while their approaches
extract keywords from the full content of documents.
Witten et al. propose KEA, a machine learning based key phrase extraction
algorithm from documents [101]. The algorithm can also be applied to annotate documents with relevant keyphrases. Their algorithm ﬁrst selects candidate
keyphrases from the document. Two features are extracted from each candidate
keyphrase: TF-IDF score and distance of the ﬁrst occurrence of the keyphrase
from the beginning of the document. A binary NaiveBayes classiﬁer is trained
with the extracted features to build a classiﬁcation model, which is used for identifying important keyphrases. The algorithm is later enhanced by Medelyan et al.
to improve the performance and add more functionality such as document annotation and keyphrase recommendation from control vocabulary, where the available
keyphrases to be recommend are already deﬁned in the vocabulary [58]. In our
research, we use keyphrase recommendation with control vocabulary feature of this
improved version of the KEA algorithm as our baseline.

4.2.2

Automatic Tag Recommendation

Since we transform the metadata annotation problem into a tag recommendation problem, we brieﬂy cover related literature. Tag recommendation has gained
substantial amount of interest in recent years. Most work, however, focuses on
personalized tag recommendation, suggesting tags to a user’s object based on the
user’s preference and social connection. Mishne et al. employ the social connection
of the users to recommend tags for weblogs, based on similar weblogs tagged by
the same users [60]. Wu et al. utilize the social network and the similarity between the contents of objects to learn a model for recommending tags [102]. Their
system aims towards recommending tags for Flickr photo objects. While such

55
personalized schemes have been proven to be useful, some domains of data have
limited information about authors (users) and their social connections. Liu et al.
propose a tag recommendation model using machine translation. Their algorithm
trains the translation model to translate the textual description of a document
in the training set into its tags [53]. Krestel et al. employ topic modeling for
recommending tags. They use the Latent Dirichlet Allocation algorithm to mine
topics in the training corpus where tags are used as the textual content [48]. They
evaluate their method against the association rule based method proposed in [33].
Their method, however, is designed for tag recommendation for social documents
where the network of users is assumed to exist, while our methods do not rely on
such an assumption.

4.3

Algorithm Textual Metadata Extraction via
Document Element Summarization

A document element is deﬁned as an entity in the document that is not part of
the running text and representing complementary information to the document.
Examples of such document elements include ﬁgures, tables, and algorithms. Since
not being part of the running text, a document element is usually accompanied
with a caption so that the content in the running text can be used to refer to. While
some authors compose tailored and detailed captions for document elements, generally, readers are assumed to have read the entire document in order to make sense
of the document element within. Consequently, a short “synopsis” of this information presented along with the document-element that helps end-users examine the
document element would be useful. Having access to the synopsis allows the enduser to quickly understand the content of the document-element without having to
download and read the entire document as examining the synopsis takes a shorter
time than ﬁnding information about a document element by downloading, opening and reading the ﬁle. Furthermore, it may allow the end-user to examine more
results than they would otherwise. Pseudo-codes are another type of algorithm
representations typically used in scholarly documents to condense and express algorithmic instructions. Typically, pseudo-codes are treated as document elements.

56
Like other document elements, oftentimes, captions do not provide much comprehensive information for their accompanied pseudo-codes. Less textual information
could impede eﬀective retrieval of algorithms. Bhatia and Mitra have proposed
an algorithm for automatic retrieval of synopsis for a document element that has
a caption and reference sentences [6]. In this section, we explore the possibility
of applying their document summarization technique on the extracted algorithm
metadata in order to provide additional meaningful textual information for each
algorithm.

4.3.1

Summary of the Document Element Summarization
Algorithm

The document element summarization algorithm was developed by Bhatia and
Mitra [6]. This section brieﬂy describes the algorithm. The algorithm employs a
machine learning based technique that learns the features from the caption and
reference sentences, in order to discover further relevant information in the paper
that would be put together as the synopsis. Concretely, given a document element e = caption, ref erenece sentences in the document d = {sentences}, the
algorithms:
STEP 1 Retrieve the accompanied caption and reference sentences of e in the
document d.
STEP 2 Extract features from the caption and the reference sentences.
STEP 3 Train a NaiveBayes classiﬁer with the extracted features.
STEP 4 Classify each sentence (excluding the reference sentences) in the document by giving a score of relevance.
STEP 5 The sentences with high relevance scores are selected and ordered by the
order that they appear in the paper.
There are two types of features: content based and context based features.
Content based features include:

57
• Similarity with Caption. This feature utilizes information cues present
in the caption. It is a score assigned to each sentence based on its similarity
with the caption.
• Similarity with Reference Sentence (RefSYM). Like captions, the reference sentences also contain important cues providing information about the
document elements. For all the reference sentences of a document element,
their similarity scores with all the other sentences are computed.
• Cue Words and Phrases(CP). There are certain cue words and phrases
that are used frequently by authors while describing a document-element.
For example, certain verbs are used typically to describe the purpose of
document-elements (“shows”, “describes”, “illustrates”, etc.).
Context based features include:
If Reference Sentence. It is a binary feature with a value of 1 if a sentence is
a reference sentence for the document-element. Otherwise, it has value 0.
Paragraph Location. It is again a binary feature and has a value 1 if a sentence
belongs to the same paragraph as the reference sentence. Otherwise, the
value is 0.
Proximity. This feature captures the fact that a sentence closer to the reference
sentence has a higher probability of being related to the document-element
than a sentence located far away from the reference sentence. The ﬁrst ten
sentences on either side of a reference sentence are assigned a feature value
of 1. All other sentences are assigned a feature value of 0.
Interested readers are encouraged to consult [6] for the detailed description of
the algorithm.

4.3.2

Experiments and Results

The document element summarization technique is applied on the extracted 6,285
algorithms. The average number of words per each generated synopsis is 190.
Figure 4.4 plots the distribution of the sizes (in terms of number of words) of the

58

Word Distribuon of Algorithm Synopsis Metadata
100.00%

1800

1600

90.00%
1535

80.00%

1400

Frequency (# of Algorithms)

70.00%
1200

Frequency
Cumulave %

1000

60.00%

2 per. Mov. Avg. (Frequency)
50.00%

800
40.00%
600
30.00%
400

20.00%
215

200

139
97

168

234 239

257
205

215 222

185 180

166 158

137 129 136 124

112 108 106

57
0

1

6

25
5 12 19

10.00%
83 77 73

52 56 55 58 51 39 38 37 37

18 26 24 26 33 22 14 18

0

0.00%
10

30

50

70

90

110 130 150 170 190 210 230 250 270 290 310 330 350 370 390 410 430 450 470 490
Number of Words (Bin)

Figure 4.4: Distribution of number of words in synopsis metadata of the extracted
algorithms.
Word Distribuon of Primary and Synopsis Metadata
800

100.00%

90.00%

700
Frequency

80.00%

600
Frequency (# of Algorithms)

Cumulave %

70.00%

2 per. Mov. Avg. (Frequency)
500

60.00%

50.00%

400

40.00%

300

30.00%
200
20.00%
100

10.00%

0.00%

0

Number of Words (Bin)

Figure 4.5: Distribution of number words in primary and synopsis metadata
(combined).

59
generated synopses. There are 1535 (24.42%) algorithms whose synopses could
not be generated. As expected, these are mostly pseudo-codes without captions or
algorithmic procedures that would cause the document summarization algorithm
to fail to operate.
After the synopses are generated as part of the algorithm metadata, the average
number of words per each overall textual metadata (synopsis+primary textual
metadata) has become 302, increasing by 170%. Figure 4.5 plots the distribution
of the sizes of combined textual metadata (in terms of number of words).
Synopses provide additional meaningful textual information to each algorithm;
however, the algorithms whose synopses cannot be generated would remain poorly
annotated. In the next section, we propose a way to get around this problem by
using topic modeling to learn the annotation from the algorithm metadata records
(whose synopses are available) in order to annotate those with poorly annotated
metadata (having no synopses or little textual information).
Generated Synopsis
The INIT-PDA procedure in Fig. 1 initializes the tables of a router at startup time; all
variables of type distance are initialized to infinity and those of type node are initialized
to null. ... Similarly, in Fig. 10, the delays obtained using MP routing for NET1 are
within 28% envelopes of delays obtained using OPT routing. ... Fig. 11 compares the
average delays of MP and SP for CAIRN. ... In Fig. 12, for NET1, MP routing performs even
better; average delays of SP are as much as five to six times those of MP routing which is
due to higher connectivity available in NET1. ... When connectivity is low or network load
is light, MP routing cannot offer any advantage over SP. Avg. Delay in milliseconds Flow
IDs Comparison of MP and SP delays ’OPT’ ’MP-TL-10-TS-10’ ’MP-TL-10-TS-2’ Avg. Delay in
milliseconds Flow IDs Comparison of MP and SP delays ’OPT’ ’MP-TL-10-TS-10’ ’MP-TL-10-TS-2’
l and Ts The performance of MP depends on the update intervals T l and T s . ... Just
the long-term routes with load-balancing, without short-term routing parameter updates,
11.5 2 2.5 3 3.5 4 4.5 5 5.5 6 0 1 2 3 4 5 6 7 8 9 10 Avg. delay in milliseconds Flow IDs
Comparison of MP and SP delays ’MP-TL-10-TS-4’ ’MP-TL-20-TS-4’ ’SP-TL-10’ is kept constant
and TL is increased in CAIRN. 2 4 6 8 10 12 14 16 18 20 22 0 1 2 3 4 5 6 7 8 9 Avg. delay
in milliseconds Flow IDs Comparison of MP and SP delays ’MP-TL-10-TS-4’ ’MP-TL-20-TS-4’
’SP-TL-10’ is kept constant and TL is increased in NET1. seem to give significant gains; the
major gains here are due to the mere presence of multiple successors and load-balancing. ...
This becomes evident in Fig. 15, which shows a typical response in NET1 when the flow rate
is a step function (i.e.., the flow rate is increased from 0 to a finite amount at time 0).
The dampened response of the network using MP indicates the fast responsiveness of MP, making
it suitable for dynamic environments. ... The reason SP delays of these flows are better
than those of MP is because of uneven distribution of load in the network and low loads in
some sections of the network in low-load environments SP can perform slightly better than MP.

Figure 4.6: Sample extracted synopsis corresponding to Algorithm INIT-PDA in
Figure 4.1, represented by a pseudo-code in [98]
Figure 4.6 shows a sample generated synopsis corresponding to Algorithm
INIT-PDA in Figure 4.1, represented by a pseudo-code in [98]. We can see that

60
this particular synopsis provide relevant information to its corresponding algorithm
that extends the information provided by the caption and reference sentences.

4.4

Automatic Algorithm Metadata via Probabilistic Topic Modeling

In the era of Web 2.0, the use of tags to label documents has been shown to
enable eﬀective retrieval [42]. Tuarob et al. describe an approach for automatic
annotation of tag-like metadata via transferring topical knowledge from the wellannotated corpus [85]. In this section, we illustrate how their algorithm can be
adopted to annotate metadata records with auxiliary textual information. The
auxiliary textual component typically serves as a description for the document. As
a few examples of such metadata, the textual Abstract section is typically used as
part of the metadata of a scientiﬁc publication. Furthermore, product listings on
online retailers usually have textual product description as part of their metadata.

4.4.1

Preliminaries

Our proposed solution is built upon the concepts of Cosine Similarity, Term Frequency - Inverse Document Frequency (TF-IDF), and Latent Dirichlet Allocation
(LDA). We brieﬂy introduce them here before going further.
4.4.1.1

Cosine Similarity

Cosine similarity is a measure of similarity between two vectors obtained by measuring the cosine of the angle between them. Given two vectors A and B, the
cosine similarity is deﬁned using a dot product and magnitude as:
A·B
= 
CosineSim(A, B) =
A B
N

N

×
i=1 Ai

2
i=1 (Ai ) ×

Bi
N

(4.1)

2
i=1 (Bi )

In information retrieval literature [56], the cosine similarity is heavily used to
calculate the similarity between two vectorized documents. An assumption is made
that each element in a document vector is a real non-negative number (such as term

61
frequency, TF-IDF score, etc.), hence CosineSim(A,B) outputs [0,1], with the
value indicating the level of similarity.
4.4.1.2

Term Frequency-Inverse Document Frequency

TF-IDF, used extensively in the information retrieval ﬁeld [56, 91, 92], quantiﬁes
how important a term is to a document in a corpus. TF-IDF has two components:
the term frequency (TF) and the inverse document frequency (IDF). The TF is the
frequency of a term appearing in a document. The IDF of a term measures how
important the term is to the corpus, and is inversely proportional to the document
frequency (the number of documents in which the term appears). Formally, given
a term t, a document d, and a corpus (document collection) D:

count(t, d)

|D|
|d ∈ D; t ∈ d|

tf (t, d) =
 
log

idf (t, D) =

T F IDFT erm (t, d, D) = T F (t, d) · IDF (t, D)

(4.2)
(4.3)
(4.4)

We can then construct a TF-IDF vector for a document d given a corpus D as
follows:
T F IDFDoc (d, D) =
T F IDFT erm (t1 , d, D), · · · , T F IDFT erm (tn , d, D)

(4.5)

Consequently, if one wishes to compute the similarity score between two documents d1 and d2 , the cosine similarity can be computed between the TF-IDF
vectors representing the two documents:
DocSimT F −IDF (d1 , d2 , D) =
CosineSim (T F IDFDoc (d1 , D), T F IDFDoc (d2 , D))

(4.6)

62
4.4.1.3

Latent Dirichlet Allocation

In text mining, Latent Dirichlet Allocation (LDA) [10] is a generative model that
allows a document to be represented by a mixture of topics. Past literature [39,90,
93–96] demonstrates successful usage of LDA to model topics from given corpora.
The basic intuition of LDA is that an author has a set of topics in mind when
writing a document. A topic is deﬁned as a distribution of terms. The author
then chooses a set of terms from the topics to compose the document. The whole
document can then be represented using a mixture of diﬀerent topics. LDA serves
as a means to trace back the latent topics in the author’s mind before the document
is written. Mathematically, the LDA model is described as follows:
|Z|

P (ti |d) =

P (ti |zi = j) · P (zi = j|d)

(4.7)

j=1

P (ti |d) is the probability of term ti being in document d. zi is the latent
(hidden) topic. |Z| is the number of all topics. This number needs to be predeﬁned.
P (ti |zi = j) is the probability of term ti being in topic j. P (zi = j|d) is the
probability of picking a term from topic j in the document d.
Essentially, the LDA model is used to ﬁnd P (z|d), the topic distribution of
document d, with each topic being described by the distribution of term P (T |z).
After the topics are modeled, we can assign a distribution of topics to a given
document using statistical inference [1]. A document then can be represented with
a vector of numbers, each of which represents the probability of the document
belonging to a topic.
Inf er(d, Z) = z1 , z2 , ..., zQ ; |Z| = Q

(4.8)

Where Z is a set of topics, d is a document, and zi is a probability of the
document d falling into topic i. Since a document can be represented using a
vector of real non-negative numbers, one can then compute the topic similarity
between two documents d1 and d2 using cosine similarity as follows:

63

DocSimT M (d1 , d2 , Z) =
CosineSim (Inf er(d1 , Z), Inf er(d2 , Z))

4.4.2

(4.9)

Summary of the Original Probabilistic Topic Modeling based Metadata Annotation Algorithm

Figure 4.7: A high-level illustration of the metadata annotation algorithm proposed by [85].
In this section, we describe the high-level concept of the topic modeling based
metadata algorithm proposed in [85]. Interested readers are encouraged to consult
the original paper for details.
Figure 4.7 illustrates a ﬂow of the score propagation algorithm on a simple
example. In the example, we have a document query for which the system will
recommend tags. Three documents in the source are annotated with tags {water,
seagull}, {seagull, soil, bird}, and {bird, air} respectively. The algorithm
proceeds as follows:
STEP1 The document similarity score is computed between the document query

64
and each document in the source.
STEP2 The scores then are propagated to the tags in each source document.The
scores are combined if a tag receives multiple scores. In the example, tags
seagull and bird obtain multiple scores (0.7+0.5) and (0.5+0.3) respectively.
STEP3 The tags are ranked by the scores. Then the top K tags are returned as
suggested tags.
The following subsection describes the diﬀerent document similarity score functions that we use.
4.4.2.1

Document Similarity Measures

We explore two diﬀerent document similarity measures (TFIDF and TM based)
when computing the similarity between the document query and the documents
in the source. Though a proﬁle-based document similarity [81] has been proposed,
such a scheme requires external ontology knowledge to map the extracted topics,
which is not available for the datasets that we use.
TFIDF Based. The ﬁrst measure relies on the term frequency-inverse document frequency. In our setting, D is the document source. In order to compute the
IDF part of the scheme, all the documents in the source need to ﬁrst be indexed.
Hence the training phase (preprocess) involves indexing all the documents. We
then compute the similarity between the query q and a source document d using
DocSimT F IDF (q, d, D) as deﬁned in Equation 4.6. We use LingPipe1 to perform
the indexing and calculating the TFIDF based similarity.
TM Based. The second document similarity measure utilizes topic distributions of the documents. Hence the training process involves modeling topics from
the source using LDA algorithm. We use Stanford Topic Modeling Toolbox2 with
the collapsed variational Bayes approximation [1] to identify topics in the source
documents. For each document we generate uni-grams, bi-grams, and tri-grams,
and combine them to represent the textual content of the document. The algorithm takes two input parameters: the number of topics to be identiﬁed and the
1
2

http://alias-i.com/lingpipe/
http://nlp.stanford.edu/software/tmt/tmt-0.4/

65
maximum number of the training iterations. After some experiments on varying
the two parameters we ﬁx them at 300 and 1,000 respectively. For assigning a
topic distribution to a document, we use the inference method proposed by [1].

4.4.3

Automatic Annotation of Algorithm Textual Metadata

ALGORITHM 1: A simple algorithm that transforms a collection of documents with auxiliary textual metadata (D ) into a collection of documents
with tag-like metadata (D).

1
2
3
4
5
6
7
8
9
10
11
12

Input: D = {d1 , d2 , ..., dN } where d =c, a
Output: D = {d1 , d2 , ..., dN } where d=c, e, T = Tag Library
initialization;
T=;
D=;
foreach d’ ∈ D’ do
c, a = d’;
a’ ← Clean(a);
tokens ← Tokenize(a’);
e ← RemoveDuplicates(tokens);
Add e to T;
Add c, e to D;
end
return D, T ;

In order to allow the proposed algorithms to annotate such textual metadata,
we could ﬁrst transform the textual component into tags. Given a collection of
documents with auxiliary textual metadata D = {d1 , d2 , ..., dN } where d ∈ D =
c, a (c is the main textual part of the document, and a is the associated auxiliary
textual component), we would like to transform D into D = {d1 , d2 , ..., dN } where
d ∈ D = c, e (c remains the main textual part of the document, and e is the
associated tags). This can be achieved using Algorithm 1.
Algorithm 1 takes the collection of documents with auxiliary textual components (D ) as input and outputs the corresponding collection of documents with
tag-like metadata (D) along with the tag library (T ). Loosely speaking, the algorithm ﬁrst cleans the auxiliary textual component by removing invalid characters,
removing stopwords, and stemming, then treats each cleaned word as a tag. This

66
transformation allows us to apply the proposed algorithms in Section 4.4 on documents with auxiliary textual components.

4.4.4

Experiments, Results and Discussion

4.4.4.1

Dataset: CiteseerX Algorithm Metadata

Figure 4.8: Example algorithm (represented with a pseudo-code), taken from [27].
For the experiments in this section, we use the algorithm metadata records
collected from the algorithm search engine project as part of CiteseerX scholarly
digital library [8, 83, 104], where the algorithms (represented with pseudo-codes)
and their metadata are extracted from a collection of scholarly documents using
the pseudo-code detection technique proposed in [79]. Figure 4.8 illustrates an
example of an extracted pseudo-code.

67
1
0.8

0.5
0.4

0.6
Recall

Precision

0.6

TFIDF
TM
KEA

0.4

0.3
0.2

0.2
0

0.1

0

50

100
150
200
Number of Recommended Tags

250

0

300

0

50

100
150
200
Number of Recommended Tags

(a) Precision

250

300

(b) Recall

0.3

1

0.25

0.8

F1

Precision

0.2
0.15

0.6
0.4

0.1
0.2

0.05
0

0

50

100
150
200
Number of Recommended Tags

250

300

0

0

0.1

(c) F1

0.2

0.3

0.4

0.5

0.6

0.7

Recall

(d) Precision vs. Recall

Figure 4.9: Precision, Recall, F1, and Precision vs Recall of the TF-IDF, TM,
KEA (baseline) algorithms on the Algorithm textual metadata.
We notice that algorithms, like documents, can be semantically related to each
others. For example, they may address similar sets of problems [80]. Furthermore,
a recent study shows that algorithms are not typically composed from scratch, but
researchers rather utilize existing algorithms in multiple ways in order to compose
new ones [82]. If we can infer these similarities between algorithms, then it would
be possible to learn the annotation of the well annotated algorithm metadata
records in order to automatically enrich the poorly annotated ones.
Each algorithm metadata record comprises of four main pieces of information:
1. Location. Speciﬁc page number of the paper where the algorithm appears.
2. Caption Text. A pseudo-code usually has an accompanied caption to which
the running text in the paper uses to refer.
3. Reference Sentences. These are sentences in the paper that refer to the
algorithm.

68
4. Synopsis. The synopsis of an algorithm is generated using the document
element summarization technique proposed by Bhatia and Mitra [6]. Given
a pseudo-code, the algorithm extracts features from its caption and reference
sentences. The algorithm then uses a machine learning based technique to
retrieve relevant pieces of textual information within the paper and put them
together as the synopsis.
While the caption texts and reference sentences can be retrieved quite easily,
the document summarization algorithm would still fail to generate high quality
synopses for some algorithms due to the following problems:
• Some pseudo-codes do not have accompanied captions. Tuarob et al. found
that roughly 26% of pseudo-codes have this characteristic [79]. Fortunately,
these algorithms are normally part of the running text in the papers, where
surrounding text can be used as reference sentences. Regardless of which,
the lack of captions makes the document element summarization algorithm
fail to operate.
• Even though captions and reference sentences are available, they may contain
only so small amount of textual information that extracted features are not
meaningful, resulting in retrieval of irrelevant sentences.
Our objective here is to use the algorithm metadata records which have rich
synopses to annotate the ones which have poor or no synopses. We group the
caption text and reference sentences together as the document main textual information, and treat the synopsis component as the auxiliary textual component.
Algorithm 1 is used to transform the collection of original algorithm metadata
records into the one which the proposed annotation techniques can be directly
applied on.
Table 4.2 shows the statistics of the algorithm metadata (ALGO) dataset used
in this section. The tag utilization of 7.13 (below that of the DAAC and KNB
datasets, but well above that of the DRYAD and TreeBASE datasets) not only
suggests that researchers use similar sets of keywords when describing algorithms,
but also suggests that the automatic document annotation algorithm could perform
well on this dataset.

69
Table 4.2: Statistics of the algorithm metadata (ALGO) dataset.
# Docs
#All Tags
Avg Tags/Doc
#Uniq. Tags
Tag Util.
#All Words
Avg Words/Doc

3,052
164,316
53.84
23,044
7.13
160,150
52.47

Out of 3,052 algorithm metadata records, we randomly select 2,000 records for
our experiments using document-wise 10 fold cross validation to make sure that
algorithms in the same paper are either in the training or testing sets in each fold.
4.4.4.2

Results
  
,_LqKQ^K~JKZRTTKLqRLqK^_~OOKMOKJK_
L~L^R_ RQ ^JQ^OJLM~KR_LqKYKQLz^LqzíLJ
~_^JLqK_^^K^_LRz^LJLO^_|JK~ZqRQLqKJ~TK
YK_|LqJRLq~L  Izí‫ۅۅ‬I‫ۅ‬I$J~_
K~TMYK'ZRRO^_~LKJ^_LqK^_~OQ^KYZ~JK|^K
$'§OKU^O^_| ORU|qY z«,_Lq^JZ~JKLqK
KMR_K_L~OO~QRO ^JMORZKJJKUJ^_|^_LKOYK~^_|
$Y|RO^LqT z^Lq |^K_  z   
í ~_MR^_LJ |^K_  í z
«)RORUOZ~JK~_QROOKY~L^KYTRKJL~TRU_LJRQ
JLRO~|KLqKJ^_|YKL~YKZRTTKLqR^J~TR_|LqK
Q~JLKJL~_Z~_KUJKLRRL~^_TK~_^_|QUYRMKO~L^R_
ZRU_LZRTM~O^JR_JR^_LTUYL^MY^Z~L^R_ZRJLJ
R^_LJ(&RMKO~L^R_J)^KYRMKO~L^R_JDKLqR
&RRO^_~LKJzJLROK$'D,BRL~Y~ _ _Rz_ «

B),')

    
JKL
  JLKM     
        
MKOQROTOKJUYLRULMUL  
    
JKZL^R_   QU_ZL^R_
OKLUO_JqRz 

BD

   


     

      ^_LK|
         
     JLKM 
    RULMUL

I($

   OKMK~L  
   ZR_JKZUL  
MKOQROT^KYJMKKUMRO^_~O^
Q~JLKOQROTUY~LO~KJ^R_ JY^|qL
JU~O    TR^Q^
      
~ZZKYKO ZRT^_ JLO~LK|^ ^TMOR

Figure 4.10: Sample top 30 suggested (stemmed) words by TFIDF, TM, and KEA
(Baseline) algorithms on the metadata record of Algorithm 3.44 mentioned in [29].
The tag prediction evaluation protocol is carried out to evaluate the performance of our proposed TFIDF and TM algorithms along with the baseline KEA
algorithm on this speciﬁc auxiliary textual metadata annotation task. The TM

70
algorithm is trained with 300 topics and 3,000 iterations. Figure 4.9 plots the precision, recall, F1, and precision-vs-recall at K (up to 300). From the results, while
it is apparent that the TM algorithm outperforms the other two, the performance
on the precision are not signiﬁcantly diﬀerent among the three algorithms. The
KEA algorithm has an increasing recall until around K = 30, where it starts to
remain steady, while both the TFIDF and TM algorithms continue to suggest relevant keywords (hence their recall rates continue to increase). According to Figure
4.9(c), the diminishing returns (i.e. the peaks) in F1 measures of the TFIDF, TM,
and KEA algorithms start to appear at K = 96, 109, and 26 respectively. This
suggests that the TFIDF and TM algorithms start to be less eﬀective after roughly
100 recommended terms onwards. The number is relatively smaller for the KEA
algorithm.
Figure 4.10 shows sample annotation of the three algorithms on the algorithm
metadata of “Algorithm 3.44 Fixed-base comb method for point multiplication”
that appears in “Guide to Elliptic Curve Cryptography” [29]. The top left part
of the ﬁgure shows the actual pseudo-code. The top right part illustrates the
partial actual synopsis. The bottom part of the ﬁgure shows the top 30 terms
recommended by the three algorithms which aim to predict the actual synopsis
text. Note that these terms are stemmed as part of the preprocessing. The red
bold terms are the correctly predicted ones.
In this particular example, the TM algorithm seems to be most eﬀective since it
can correctly predict 27/30 words. The KEA algorithm seems to recommend more
meaningful terms, however most of them are incorrectly predicted. Please note
that, since the tag prediction protocol used for the evaluation measures how well
an annotator guesses the omitted text, it is not necessary the case that incorrectly
predicted terms are not relevant. They just do not appear in the actual synopsis.
The TFIDF algorithm performs better than the KEA algorithm, but is not as
eﬀective as the TM algorithm.
The results from this set of experiments agree with the earlier experiments
[84–89] in the sense that the TM algorithm tends to perform well on the datasets
whose documents have high tag utilization and enough textual information (in
terms of average number of words/document). These criterion allow the method to
model meaningful topics and give popular terms higher chance to be recommended.

71

4.5

AlgorithmSeer Search System
Metadata Extracon

Algorithm
Idenﬁcaon

Document
Collecon

Document Metadata
Synopses Generaon

Index

Query
Results

Query Processing
and
Result Ranking

Figure 4.11: High level description of the algorithm search engine system, AlgorithmSeer.
In this section, a prototype of an algorithm search engine, AlgorithmSeer, is
presented. The ﬁrst version of the system was implemented by Bhatia et al. [7, 8].
In this work, the system was improved to incorporate the new pseudo-code detection technique described in Chapter 3. Note that, the textual metadata extraction
and annotation discussed in Chapter 4 have not been fully integrated into the
system yet. Figure 4.11 illustrates the high level of the proposed system. First,
algorithm representations in a collection of documents are identiﬁed. Then, the
metadata associated with each detected algorithm representation is extracted and
indexed. On the user’s side, the user’s query is parsed and processed. The system
then interacts with the index to retrieve, rank, and display relevant algorithms to
the user.
In the rest of this article, we ﬁrst discuss related literature on diﬀerent types
of search engines. Next, we describe our recent contributions towards building the
initial version of AlgorithmSeer.

72

4.5.1

Related Search Engines

Aside from well known web search engines such as Google3 and Microsoft’s Bing4 ,
various vertical search engines have been proposed. CiteSeer5 , now CiteSeerX , was
developed as a scientiﬁc literature digital library and search engine which automatically crawls and indexes scientiﬁc documents primarily in the ﬁeld of computer
and information science [49]. Liu et al. presented TableSeer, a tool which automatically identiﬁes and extracts tables in digital documents [51]. They used a tailored
vector-space model based ranking algorithm, TableRank, to rank the search results. An implementation of TableSeer that extracts and searches for tables in the
CiteSeerX document repository has been included in the CiteseerX suite. BioText6
search engine, a specialized search engine for biology documents, also oﬀers the capability to extract ﬁgures and tables, and make them searchable [32]. Khabsa
et al. described AckSeer, an acknowledgement search engine which extracts, disambiguates, and indexes more than 4 million mentioned entities from 500,000
acknowledgments from documents in CiteSeerX [40]. Chen et al. emphasized the
importance of scientiﬁc collaboration and introduced CollabSeer, a search engine
for discovering potential collaborators for a given author or researcher by analyzing
the structure of the coauthor network and the user’s research interests [17]. To
the best of our knowledge, we are the ﬁrst to explore the possibility of building a
search engine for algorithms. Recently, Choudhury et al. proposed a ﬁgure search
engine architecture for a chemistry digital library [19]. Their system indexes ﬁgure
caption and mentions extracted from the PDF in documents using a custom built
extractor.

4.5.2

Indexing Algorithm Metadata

If an algorithm is present in a document, the document text is then further processed to extract the algorithm’s synopsis – the set of sentences from the document
that are related to the algorithm. For algorithms whose synopses cannot be extracted, their textual metadata is generated via the metadata annotation approach
3

https://www.google.com/
http://www.bing.com/
5
http://citeseerx.ist.psu.edu
6
http://biosearch.berkeley.edu
4

73
discussed earlier. We also extract additional metadata including the document title, author names, publication year and page on which the algorithm is present.
For this, we adopt the tools available from the SeerSuite toolkit7 . All the extracted
algorithms from a document and their associated metadata are then indexed using
a SOLR8 based indexer.

4.5.3

Query Interface and Result Ranking

The proposed system provides a free text based query interface to the user. The
user interface is implemented using SeerSuite and extends CiteSeerX ’s query interface. The results for a given query are presented to the user as a ranked list
of algorithms along with the associated metadata. For algorithm ranking, we use
a TF-IDF based cosine similarity ranking function [56] found in SOLR. The total similarity score for an algorithm is a linear combination of the following three
similarity scores, with an equal weight given to each component.
1. Similarity between user query and algorithm caption
2. Similarity between user query and algorithm’s reference sentences
3. Similarity between user query and algorithm’s synopsis
The algorithms are presented to the user in decreasing order of their scores.
Experiments comparing the performance of our proposed system with other stateof-the-art search engine systems have shown superiority of our approach in terms of
precision and ranking performance. We selected a set of 20 popular algorithms as
test queries (e.g. topological sort, breadth ﬁrst search etc.) and tested them with
our proposed system, Google Scholar and Google Web Search. A returned result
page was considered as relevant if it contained a valid algorithm/pseudo-code. The
relevance judgments were provided by two human evaluators not associated with
the project. Our proposed system achieves a precision of 81% at top 10 ranks as
compared to 41% and 44% achieved by Google Web Search and Google Scholar,
respectively.
7
8

http://citeseerx.sourceforge.net/
http://lucene.apache.org/solr/

74
4.5.3.1

An Example Search Session

Figure 4.12: Screenshot showing results for the query “shortest path”. Along with
search results, associated metadata is also shown to the user.

Figure 4.13: Screenshots showing algorithm page displayed on clicking the ﬁrst
result.

75
Figure 4.12 shows the screenshot of the result page for the query shortest path.
The top 10 algorithms for the query, along with their associated metadata are
presented to the user. Note that the results returned to the user provide a good
coverage of a variety of shortest path algorithms such as the heuristic algorithm
for shortest path, the Berge shortest path algorithm, in addition to the standard
shortest path algorithm. The algorithm caption is presented in bold and clicking
on it directly takes the user to the PDF page of the related document in which the
algorithm is present. This is illustrated in Figure 4.13.

4.6

Conclusions

Here, a set of methods for extracting textual metadata from detected algorithms
are discussed, including the synopsis generation and automatic document annotation methods. These techniques are evaluated on a set of 6,285 algorithms extracted from 20,567 scholarly documents suggest that these methods could be
successfully applied to algorithm metadata. Finally, a demo search engine for algorithms in large scale scholarly documents is presented. Future work could allow
public access to such a system so that user evaluation on such a system is possible.

Chapter

5

Analyzing Algorithm Citations
Algorithms are an essential part of computational science. An algorithm search
engine, which extracts pseudo-codes and their metadata from documents, and
makes it searchable, has recently been developed as part of the CiteseerX suite [7,8].
However, this algorithm search engine only retrieves and ranks relevant algorithms
solely on textual similarity. This chapter presents two preliminary investigations
on the usefulness of citation analysis in algorithm search. First, we propose a
method for using the algorithm co-citation network to infer the similarity between
algorithms. We apply a graph clustering algorithm on the network for algorithm
recommendation and make suggestions on how to improve the current CiteseerX
algorithm search engine. In the second work, we propose a classiﬁcation scheme
for algorithm citation functions in scholarly works.

5.1

Capturing Similarities among Algorithms using the Algorithm Co-Citation Network

Computer science is often about algorithms. Searching for the right algorithms
for a speciﬁc problem can be a challenging task, as there has not been a way
for automatically interpreting the semantics of algorithms. Methods for searching
for algorithms have been implemented by applying traditional search engine techniques on algorithm metadata. Bhatia et al. [7] developed an algorithm extractor
which extracts pseudo-codes along with their metadata such as captions, reference

77
sentences and synopses [5, 6]. Indexing such algorithm metadata makes it searchable. However, the search is done by text based matching of user queries with the
metadata.
The ability to infer the similarity of algorithms could also be beneﬁcial when
searching for algorithms. One may want to know if there are other available algorithms which address the same problem as a known algorithm. For example,
one might want to know if there are other algorithms that ﬁnd shortest paths in
a graph like the Dijkstra’s algorithm. Researchers often would want to search for
existing algorithms so that they can develop a better algorithm or use these algorithms as baselines for their experiments. Detection of the similarity of algorithms
could also lead to the discovery of newly emerging algorithms which are not yet
well known.
We hypothesize that the similarity between algorithms can be captured using an
algorithm co-citation network. We generate the algorithm co-citation network from
scientiﬁc documents in the CiteseerX 1 repository. We apply a clustering algorithm
on a sample subset of the algorithm co-citation network which produces groups
of relevant algorithm-proposing documents. We evaluate the clustering results by
varying the clustering granularity levels and measure the meaningfulness of each
cluster.

5.1.1

Observations and Motivations

In an algorithm-proposing document, there is usually a paragraph (in the introduction or the related work sections) devoting to addressing past work. In this case,
we are interested in algorithms used in previous documents. From reading multiple
documents throughout our own research, we ﬁnd that if multiple algorithms are
mentioned in a paper, then it is likely that these algorithms address similar problems. From these observations, we hypothesize that, if two algorithms are co-cited
multiple times, then there could exist a relationship between them which can be
used to infer their similarity. Such a relationship could be represented using the
algorithm co-citation graph, which is explained in a later section.
1

http://citeseerx.ist.psu.edu

78

5.1.2

Related Work

Even though the document co-citation has been extensively studied, to the best
of our knowledge, there has not been any work on algorithm co-citation analysis.
Hence, the related work that we present here are mostly related to the current
CiteseerX algorithm search engine.
The current CiteseerX algorithm search engine uses an automated algorithm
extractor to extract pseudo-code which appears as document-elements with captions in scientiﬁc documents. The automated algorithm extractor extracts related
information of each pseudo-code such as its caption, reference sentences, and year
of publication (if available). A pseudo-code reference sentence is a sentence in the
document which mentions the pseudo-code. A synopsis is also generated as part of
the metadata to provide an overview for each pseudo-code extracted [5]. A synopsis of a pseudo-code is generated from the set of its reference sentences, constructed
by heuristics and machine learning techniques. All the extracted metadata information is then fed to the indexer. When a user inputs a query, TF-IDF based
cosine similarity scores are computed to retrieve and rank the relevant pseudocodes. Each search result gives a pointer to the document where the pseudo-code
resides. Figure 5.1 shows the results returned by the CiteseerX algorithm search
engine using query “shortest path.” The current CiteseerX algorithm search engine
only retrieves pseudo-codes based on textual similarities between user queries and
pseudo-codes. This method is eﬀective if the user uses the right keywords. However, there are some cases where the desired algorithms are proposed in diﬀerent
ﬁelds of studies, resulting in a diﬀerent set of context that might not be familiar
to the user. The current search engine would treat these algorithms as unrelated
to the search query.

5.1.3

Algorithm Co-Citation Network

The algorithm co-citation network is an undirected, weighted graph where each
node is a document that proposes some algorithms, and each edge weight is the
frequency of algorithm co-citation. Formally, for the set of all documents D, the
algorithm co-citation network G is deﬁned as follows:

79

Figure 5.1: CiteseerX algorithm search engine.
G=  V,E 
V={d | d ∈ D, d proposes one or more algorithms}
E={(a,b) | a,b ∈ V}
Weight((a,b))=|{d | d ∈ D,d cites algorithms in both a and b,(a,b)∈E}|

An algorithm co-citation network is diﬀerent from a document co-citation network deﬁned in [68] in the sense that each node in the network is a document which
proposes algorithms, and one or more of the algorithms are cited in the document.

80
The weight of each link represents the frequency that the algorithms in at least
two documents are co-cited. We describe how we detect algorithm citations and
construct the algorithm co-citation network in the next sub-sections.
5.1.3.1

Algorithm Citation Detection

Given an input document, the algorithm citation detection returns the cited documents whose algorithms are mentioned in the input document. A document is
treated as an ordered set of sentences. Brieﬂy, the algorithm citation detector
ﬁrst extracts the set of algorithm-citation sentences in a document. An algorithm
citation sentence is deﬁned as a sentence which contains at least one or more algorithm keywords (i.e. ‘algorithm’, ‘method’, and ‘procedure’), and at least a citation
symbol. An example of a sentence where an algorithm is cited is:
Optimization methods for ECG compression were developed, such as
the cardinality constrained shortest path (CCSP) algorithm presented
in [1], [8], [24], [25].
We make an assumption that if both an algorithm keyword and one or more
citations appear in a sentence, then it is likely that the citing document mentions
the algorithms proposed in the cited documents.

81
Algorithm 5.1.1: AlgoCiteDetection(D)
ALGO KEY W ORDS = { algorithm , method , procedure }
DOC ALGOID M AP = {}
D := setof alldocuments
ST D ALGOS := listof standardalgorithmnames
Begin :
for each document d in D :
body ← body part of d
ref ← ref erence part of d
for each sentence s in body :
S = {}
if an algorithm keyword appears in s :
S ← citation symbols in s
End If
IDs = {}
for each s in S :
c ← citation ref erred by s in ref
id ← document of citation c
(query f rom the CiteSeerX Database)
Add id to IDs, if IDs does not already contain id
Add (d, IDs) to DOC ALGOID M AP
End.
After obtaining the set of algorithm-citation sentences, the detector extracts
the set of documents cited in such sentences. We use document IDs to represent
documents.
5.1.3.2

Constructing the Algorithm Co-Citation Network

The algorithm co-citation network is constructed by taking the algorithm citations
from each document, ﬁnding the corresponding document ID for each citation,
and creating for each pair of the cited documents an edge of weight 1. If the
edge already exists, the weight is incremented by 1. Algorithm 5.1.2 describes

82
the network construction process. D is the set of input documents from which
we extract algorithm citations and construct the algorithm co-citation network.
We generate the algorithm co-citation network from roughly 1,370,000 documents
in the CiteseerX repository. The network created contained 9,409,433 edges and
roughly 1 million nodes.
Algorithm 5.1.2: AlgoCoCiteNetworkConstruct(D)
Initialization :
V = {}
E = {}
G = <V,E>
Begin :
for each document d in D:
N ← list of algorithm-proposing documents cited in d
F or each (a, b) where a, b∈V,a = b:
if edge (a, b) ∈E:
Increase weight of edge (a, b) by 1
Else :
Add edge (a, b) to E, and set the weight to 1
End If
Rerurn G
End.
5.1.3.3

Clustering the Algorithm Co-Citation Network

The algorithm co-citation network captures the similarity between two documents
that propose algorithms, based on the assumption that if two algorithms are cited
together, then they are likely to be used for similar problems. Such similarity is
reﬂected by the weight of the edge linking the two documents in which the co-cited
algorithms are mentioned. Based on such a relationship, the network is clustered to
produce groups of similar algorithm-proposing documents. A number of clustering
tools can be considered; however, the MCL2 tool worked well for this task. The
2

http://micans.org/mcl/man/mcl.html

83
MCL clustering tool implements the Markov Cluster Algorithm. The algorithm is
unsupervised and is based on the simulation of network ﬂow. MCL is designed to
speciﬁcally cluster large and preferably undirected weighted networks [23].
The other clustering tools that we have considered include Weka3 , Gephi4 ,
GraphClust5 , and Graclus6 . Weka [28] supports datasets where all the data points
have the same set of attributes. Such data points are diﬀerent from nodes in the
algorithm co-citation network where the only information associated with a node is
the similarities between the node and its neighbors. GraphClust is also a network
clustering tool; however, it only supports undirected graphs with uniform edge
weights. Gephi and Graclus presented problems for large datasets such as ours.
For our experiments, they all crashed on an input network of 600,000 edges and
33,601 nodes.

Figure 5.2: Distribution of cluster sizes with granularity parameters of 1.4, 3.0,
and 5.0.
3

http://www.cs.waikato.ac.nz/ml/weka/
http://gephi.org/
5
http://cs.nyu.edu/shasha/papers/GraphClust.html
6
http://www.cs.utexas.edu/users/dml/Software/graclus.html
4

84
Table 5.1: Output cluster and average cluster sizes generated using diﬀerent
granularity parameters.
Granularity Level
1.4
3.0
5.0

5.1.4

# clusters
30,862
73,779
105,329

Avg. # documents/cluster
14.89
6.23
4.36

Experiment and Evaluation on
Clustering Results

The MCL tool was the most suitable tool we found for clustering large graphs.
However, the complete algorithm co-citation network generated from the whole
CiteseerX repository was still too large for the tool to handle. As such, for experimental purposes, we generated a subgraph by randomly selecting 3,000,000 edges
from the complete network. The selected subgraph contains 459,585 nodes. Figure
5.2 shows the distributions of the cluster sizes with diﬀerent clustering granularity
levels. With granularity parameter of 1.4 (I = 1.4), the clustering results tend to
be coarse-grained, leading to the smallest number of clusters, but highest average
number of documents per cluster. At the other extreme where the granularity parameter is 5.0 (I = 5.0), the clustering results tend to be ﬁne-grained, resulting in
the largest number of output clusters, and smallest average number of documents
per cluster. The granularity parameter of 3.0 (I = 3.0) results in values somewhere
in between. Table 5.1 lists the numbers of output clusters and average cluster sizes
for each granularity parameter.
This experiment aims to measure the meaningfulness of the clustering results.
The experiment and evaluation use the following steps:
1. Run the MCL clustering algorithm with granularity parameters of 1.4, 3.0,
and 5.0 on the sample network with 3,000,000 edges and 459,585 nodes.
2. Randomly choose 10 clusters from each clustering result.
3. For each cluster:
(a) Retrieve the paper titles of all the documents in the cluster.

85
(b) From all the document titles, run the term frequency count to determine
top 5 keywords. We use these keywords to describe the cluster.
(c) Examine each document (manually) and determine the number of documents in the cluster which are related to these 5 keywords.
(d) Calculate the precision of the cluster, where the precision is the ratio of
the number of related documents to the number of all the documents
in the cluster.
4. Calculate the average precision of all the 10 clusters.
5. Compare the average precisions of the results from the three granularity
levels.
Table 5.2, 5.3, and 5.4 lists the results of the precision measurement of a sample
of 10 clusters selected from each of the clustering results generated with diﬀerent
granularity parameters. C# is the cluster number, #D is the number of documents in the cluster, #RD is the number of documents relevant to the 5 chosen
keywords, and Pr(%) is the precision in percent.
Table 5.2: Precision calculated from the sample clusters using granularity parameter 1.4.
C# Keywords
#D #RD Pr(%)
1
integration, structural, analysis, dynamic, time
41 29
70.73
2
walking, biped, control, robot, locomotion
60 49
81.67
3
technology, programmable, error, cmos, performance
41 20
48.78
4
geometric, constraint, rigidity, graph, system
45 30
66.67
5
mammogram,detection,microcalciﬁcation,digital,clustering 43 30
69.77
6
requirement, diagram, model, object-oriented, statechart 46 23
50.00
7
knowledge, ontology, system, design, management
40 31
77.50
8
neural, parallel, network, mapping, architecture
41 32
78.05
9
scheduling, crew, system, transport, driver, transit
43 34
79.07
10 reduction, eigenvalue, power, dominant, system
46 35
76.09
Avg
44.6 31.3 70.73

It is worth noting that the highest average precision is achieved in the clustering
result using the granularity parameter of 1.4. This granularity level produces the
most coarse-grained clustering among all the three parameters. It is also interesting

86
Table 5.3: Precision calculated from the sample clusters using granularity parameter 3.0.
C# Keywords
#D #RD Pr(%)
1
reduction, model, eigenvalue, power, system
40 13
32.50
2
element, ﬁnite, superconvergence, analysis, recovery
52 46
88.46
3
recovery, distribute, rollback, synthesis, system
54 28
51.85
4
java, program, analysis, compiler, object-oriented
58 38
65.52
5
programming,approximation,problem,algorithm,application 48 30
62.50
6
algorithm, system, stability, matrix, linear
52 42
80.77
7
spectral, system, analysis, estimation, identiﬁcation
46 25
54.35
8
partition, parallel, architecture, language, dataﬂow
40 30
75.00
9
distributed, signal, sparse, linear, sensor
49 30
61.22
10 feature, selection, analysis, learn, approach
56 26
46.43
Avg
49.5 30.8 61.86

Table 5.4: Precision calculated from the sample clusters using granularity parameter 5.0.
C# Keywords
#D #RD Pr(%)
1
clustering, data, algorithm, sampling, spatial
47 30
63.83
2
nonlinear, equation, dynamic, solution, schrodinger
41 21
51.22
3
animation, method, simulation, model, realistic
52 32
61.54
4
convex, version, enumeration, facet, version
48 26
54.17
5
match, recognition, contour, object, shape
56 32
57.14
6
waveguide, microwave, multiplexer, design, analysis
51 31
60.78
7
language, entropy, maximum, model, statistical
55 32
58.18
8
testing, protocol, conformance, generation, machine
41 34
82.93
9
learning,reinforcement,stochastic,markov,approximation 43 22
51.16
10 motion, estimation, image, structure, optical
58 33
56.90
Avg
49.2 29.3 59.78

to see that the average precision tends to decrease as the clustering results are more
ﬁne-grained.

5.1.5

Suggested Applications

The clustering on the algorithm co-citation network provides us with groups of
related algorithm-proposing documents. We can create clusters of similar algorithms by grouping together pseudo-codes extracted from the documents in the
same clusters. There are a number of applications that can be employed from such
algorithm clustering. Here, we propose two possible applications which can poten-

87
tially be used to improve the CiteseerX algorithm search engine or any similar full
text digital library.
5.1.5.1

Algorithm Recommendation

Suppose every algorithm in each cluster addresses similar problems. When the
algorithm search engine retrieves an algorithm, the algorithms in the same cluster
can be displayed as recommended algorithms.
5.1.5.2

Improving Ranking

Assuming that algorithms in the same cluster are similar in the sense that they
address similar problems, then when an algorithm is textually matched with the
search query, the algorithms in the same cluster could receive extra scores so they
can be ranked higher. To do this, we propose a technique similar to query expansion:
1. First, the search engine pre-generates a list of keywords that represent each
cluster. The keywords may be chosen from the documents which propose the
algorithms in the cluster, or they may be chosen from the information (i.e.
captions, reference sentences, and synopses) of the algorithms in the clusters.
2. When a user inputs a query, the query engine modiﬁes the original query
such that if the original query contains at least one word that belong to the
list of keywords of a cluster, then some or all of the keywords that represents
such cluster are added to the original query. Adding cluster keywords to
an original query would make sure that when at least an algorithm matches
the original search query, ranking scores are also given to the rest of the
algorithms in the same cluster.

5.1.6

Conclusions and Future Work

The algorithm co-citation network captures the similarity between algorithm proposing documents. The similarity comes from the observation that if two algorithms
are cited together, then it is likely that these algorithms address similar problems.
Based on this assumption, clustering the network would result in groups of similar

88
algorithm-proposing documents. The clustering results could be applied in many
applications to improve the current CiteseerX or any algorithm search engine, such
as improving the ranking and recommending related algorithms. There are several
aspects of the methodology that could be improved. For example content analysis
techniques can be used to identify additional features which would infer similarity between two algorithm proposing documents. This will generate a diﬀerent
type of algorithm graph. It would be useful to implement clustering algorithms
which can handle large networks such as our dataset making it possible to perform
experiments on the complete algorithm co-citation network generated from the
whole CiteseerX repository. Using the methods proposed here, many algorithm
recommendation systems should be feasible.

5.2

Studying Algorithm Evolution and Inﬂuence
Over Time using Algorithm Citation Network

Algorithms are the foundation of Computer Science. They oﬀer step-by-step instructions for solving diverse types of computational problems such as sorting,
searching, ranking, etc. Moreover, important problems in various ﬁelds are usually
transformed into computational ones so they can be solved by applying eﬃcient
algorithms. Researchers are constantly developing new algorithms to either solve
new problems or to improve upon the existing ones. These newly emerging algorithms are not manually cataloged because they are simply both too new and too
many, behooving systems that automatically identify, extract, index, and search
this ever growing collection of algorithms.
To facilitate such needs, an algorithm search engine has been implemented as
part of CiteseerX suite [8]. The system extracts pseudo-codes along with their
metadata from scholarly documents, indexes them with Apache Solr/Lucene7 , and
makes them searchable via full text search. The metadata of a pseudo-code incudes
its caption, reference sentences, synopsis, etc. A reference sentence is a sentence
in the paper that mentions the pseudo-code. A synopsis of a pseudo-code is gener7

http://lucene.apache.org/solr/

89
ated from the set of its reference sentences, constructed by heuristics and machine
learning techniques [6]. However, the search is done merely by text based matching
of user query with the metadata, and the results are ranked based on the TF-IDF
scores.
The limitation of the traditional text-based search urges the needs for semantic
understanding of algorithms extracted from scholarly works, such as knowing how
researchers utilize the existing algorithms in their works. For example, Walker et
al. extend PageRank algorithm [63] to create CiteRank algorithm which utilizes
the characteristics of citation networks to rank academic publications [99]. PhilippFoliguet et al. use the ELBG (Enhanced Linde-Buzo-Gray) algorithm proposed by
Patané and Russo [64] as a baseline to compare their algorithm against on the
tasks 4 and 5 of the TECHNOVISION/ImagEval contest [66]. Kiss et al. suggest
that the algorithm proposed in [44] could be applied to solve the problem of “the
mismatch between the MASH stages” [43].
Here, we propose a classiﬁcation scheme for algorithm citation function in scholarly works. We make an assumption that authors indicate how a previous algorithm is utilized in the context where the algorithm is cited, which we call it an
algorithm citation context. We propose a classiﬁcation scheme with 9 classes for an
algorithm citation function, divided into 3 groups based on the authors’ attitudes.
We then manually classify a set of 300 algorithm citation contexts randomly selected from 2,000 papers from CiteseerX 8 repository, and report the distribution.
Discovering the functionality of a cited algorithm could provide an insight into
how and where the algorithm is used, and could give rise to multiple interesting
applications such as algorithm trend discovery and algorithm ranking.

5.2.1

Background and Related Works

To the best of your knowledge, we are the ﬁrst who attempt to identify functions of
algorithm citation in scholarly works. Hence we present only the literature closely
related to ours.
8

http://citeseer.ist.psu.edu

90
5.2.1.1

Understanding Document Elements

In scholarly documents, authors use a number of document elements to report or
summarize the experimental results (i.e. graphs, plots), to visualize concepts (i.e.
ﬁgures), and to describe algorithms (pseudo-codes). Since such document elements
are usually not part of the running text, they are often accompanied by captions
which serve as anchors to which the content in the document can refer. Since
pseudo-codes are also document elements, the relevant works on understanding
the semantic of document elements are brieﬂy discussed.
Bhatia et al. propose an approach that automatically generates a textual description for a document element [6]. Their method ﬁrst collects the reference
sentences corresponding to the document element. Such reference sentences are
used as seeds to further collecting relevant sentences from the document using a
machine learning based classiﬁcation algorithm. Tuarob et al. propose to use the
algorithm co-citation network to identify groups of similar algorithm proposing
papers [80]. An algorithm co-citation network is a weighted indirect graph where
each node is a paper proposing one or more algorithms, and the edge weight represents the frequency of co-citation. They claim that groups of algorithms which
address similar problems can be obtained by clustering such a network. Lu et
al. propose 5 categories for ﬁgures in scholarly documents including Photo, 2-D,
3-D, Diagram, and Others [54]. They also propose a machine learning based algorithm that learns the features extracted from the graphical content of the ﬁgures.
Browuer et al. study 2-d plots in scholarly documents by proposing an approach
that identiﬁes the axis labels, legend and the data points from a 2-D plot [13].
Liu et al. present TableSeer, a search engine for tables [51]. The system identiﬁes
tables in scholarly documents along with their metadata such as the information of
the document where a table is located, table captions, layouts, cell content, etc. A
tailored vector-space model based ranking algorithm, TableRank, is used to rank
the search results.
5.2.1.2

Citation Context Classiﬁcation

Since our method involves analyzing citation contexts. The relevant works are presented here. Peritz proposes a classiﬁcation scheme for citation functions in social

91
science ﬁelds. Her scheme consists of 8 classes including Setting Stage, Background
Info., Methodology, Comparative, Argumental, Documentary, Historical, and Casual [65]. Spiegel-Rösing deﬁnes 13 categories for citations based on motivation
of the authors [70]. Teufel et al. analyze a corpus of scientiﬁc articles in computational linguistics and propose an annotation scheme for citation function [75],
which is an adaption of the scheme proposed by Spiegel-Rösing. Their scheme
consists of 12 categories, concentrating on contrast, weaknesses of other work,
similarities between work and usage of other work. Later, they develop a machine
learning based algorithm for automatic classiﬁcation of citation function [74].
We ﬁnd Teufel et al.’s work the most closely related to ours. However, their
scheme is for annotation of citation function in general. To our knowledge, we
are the ﬁrst who study the algorithm citation function in scholarly works using
citation contexts.

5.2.2

Proposed Classiﬁcation Scheme for Algorithm Citation Function

Authors employ existing solutions to their works in multiple ways such as directly
using, modifying, comparing with, or just mentioning them. The ability to automatically identify how an existing algorithm is utilized could give an insight into
the nature of both the cited algorithm and the works that use it. For example, if
an algorithm is being used as baselines multiple times, then this may indicate an
emergence of new algorithms that address the similar problems. Or, an algorithm
which is extensively used as building blocks for more complex algorithms tends to
be a generalized one. Moreover, study of the inﬂuence of algorithms could lead to
further interesting studies which we shall explore later in Section 5.2.3.
5.2.2.1

Annotation Scheme

Table 5.5 illustrates our proposed scheme for classiﬁcation of algorithm citation
function in scholarly works, along with descriptions. The scheme consists of 9
classes representing possible ways in which an algorithm can be used. The 9
classes can be divided into 3 groups based on the authors’ attitude towards the
cited algorithms: favorable, neutral, and critical. Authors tend to feel ‘favorable’

92
Table 5.5: Proposed algorithm function classiﬁcation scheme consists of 9 categories, divided into 3 groups based on the authors’ attitudes: Favorable, Neutral,
Critical.
Atude

Category
Extension
Favorable
Direct-use
Suggeson
Similarity
Neutral Diﬀerence
Analysis
Menon
Baseline
Crical
Argument

Descripon
The cited algorithm is extended or modiﬁed by the cing work.
Part or whole of the cited algorithm is used as part of the cing work. (No modiﬁcaons to the cited algorithm)
The authors suggest a potenal improvement using the cited algorithm.
The authors point out the similarity between the cited algorithm and their work.
The authors point out the diﬀerence(s) between the cited algorithm and their work.
The cing work performs further study (usually proofs) on the cited algorithm.
The context just menons the cited algorithm exists, usually along with some related descripon.
The authors compare the performance of their algorithms with the cited algorithm.
The authors bring up disadvantages or ﬂaws in the cited algorithm, usually to emphasize the advantages of the
cing work.

Table 5.6: Sample annotated algorithm citation contexts. The cited algorithms
are denoted with red/bold phrases. The sentences with underlined/italic font are
the algorithm citation sentences.
Category

Example Ciataon Context
The a ppl i ca ons were s pl i t i nto two pa rts : the communi ca on pa rt (wri en i n ALua ) a nd thethe proces s i ng kernel pa rt (wri en i n C). The implementaon

Extension of a soluon for the N-BodySimulaon Problem was based on the Barnes-Hut al-gorithm [2] . More s peci ca l l y, our i mpl ementa on wa s ba s ed on [7].

We ca n l ea rn to cl a s s i fy text i ns ta nce us i ng a l ea rner tha t i s a bl e to i nduce ﬁrs t-order rul es . The learning algorithm that we use in our system is Quinlan's

Direct-use Foil algorithm [5] . Foi l i s a greedy coveri ng a l gori thm for l ea rni ng funcon-free Horn cl a us es deﬁni ons of a rel a on i n terms of i ts el f a nd other
Suggeson
Similarity
Diﬀerence

Analysis

Menon
Baseline
Argument

rel a ons .
The ca s e of mul va ri a te di s tri buons i s deﬁni tel y more di ﬃcul t. For general distribuons and support sets, rejecon methods [33] can be used. There a re two
ki nds of rejecon methods : The ﬁrs t one i s ba s ed on the concept of rejecon from a “domi na ng dens i ty.”
The focus i s thus nor on obta i ni ng a new es ma tor, but ra ther on ga i ni ng i ns i ght on the beha vi or of the s tudi ed cl a s s of methods . The esmator that is
analyzed here is close to the algorithm presented in [11] . Here, we extend the a na l ys i s i n [I] a nd s how tha t, by ca reful l y expl oi ng the s tructure, compa ct
expres s i ons for the es ma on error cova ri a nce ca n i n fa ct be obta i ned.
Li ke other a pproa ches , Kea extra cts keyphra s es us i ng the two s teps des cri bed a bove: ﬁrs t i denfyi ng ca ndi da te phra s es i n a document’s text, a nd then
ra nki ng the ca ndi da tes ba s ed on whether they a re l i kel y to be keyphra s es . However, we diﬀer from previous work in that we rank candidates using a model
built with the Naive Bayes machine learning algorithm [25] . The s teps i nvol ved i n tra i ni ng a nd extra con a re i l l us tra ted i n Fi g. 2.
The energy-s tretch i s a va ri a nt of the wel l -known mea s ure of di s ta nce-s tretch, whi ch for a pa th P i s the ra o of the l ength of P to the the mi ni mum
di s ta nce between u a nd v. We show that a simple local-control algorithm, proposed by Li et al [32] , idenﬁes an O(1)-degree graph N on V such that for any two
nodes u and v, there exists a path in N between u and v that has O(1) energy-stretch. For the s peci a l ca s e of ci vi l i zed gra phs , i n whi ch i t i s a s s umed tha t the
ra o of the ma xi mum edge l ength to the mi ni mum edge l ength i s bounded by a cons ta nt, the s a me a l gori thm a chi eves O(1) di s ta nce-s tretch for a ny two
nodes u a nd v.
[6] a l s o compa re ontol ogi es us i ng s i mi l a ri ty mea s ures , wherea s they compute the s i mi l a ri ty between l exi ca l entri es . [7] describes the use of FOIL
algorithm in classiﬁcaon and extracon for construcng knowledge bases from the web. Current web i s evol vi ng to s ema nc web.
In thi s s econ, s evera l experi ments a re ca rri ed out to s how the eﬃci ency a nd eﬀecvenes s of our propos ed a l gori thm for fa ce recogni on. We compare
our algorithm with the Eigenface(PCA) [8], Fisherface (LDA) [1], and Laplacianface (LPP) [5] methods, three of the most popular linear methods for face recognion.
Two fa ce da ta ba s es were us ed.
An eﬀecve and widely used method for removing spurious pits in digital elevaon models consists of ﬁlling them unl they overﬂow[1] . However, thi s method
s omemes crea tes l a rge ﬂa t regi ons whi ch i n turn pos e a probl em for the determi na on of a ccura te ﬂow di recons .

towards algorithms that they choose to cooperate into their works; while they tend
to feel ‘critical’ about algorithms that they use as baselines or those that cannot
solve the problems they address. Such criticism is often reﬂected in discussions
about the disadvantages of the cited algorithms. Table 5.6 shows sample labelled
algorithm citation contexts.
5.2.2.2

Methodology

A document is represented by a sequence of sentences. An algorithm citation context is a tuple of an algorithm citation sentence and the sentences that immediately

93
... What we are planning to do in this paper, we show that
this formula (1) can indeed be theoretically justified.
Our justification for this formula will use methods
motivated by the neural network approach (see, e.g., [10]).
We must choose a family of functions, not a single function
A. ...

Figure 5.3: Example of algorithm citation context, consisting an algorithm citation
sentence (in italic) and the sentences surrounding it.
precede and follow it. An algorithm citation sentence is a sentence in which one
or more algorithms are cited. Speciﬁcally, an algorithm citation sentence must
have both at least a citation and at least an algorithm keyword (i.e. ‘algorithm’,
‘method’, ‘procedure’). Figure 5.3 shows an example of an algorithm citation context taken from [59]. We manually label 300 algorithm citation contexts randomly
selected from a random set of 2,000 scholarly documents drawn from CiteseeX
repository, using the scheme in Figure 5.5.
42.33

Number of citaon contexts (%)

45
40
35
30

24.67

25
20
15
7.67

10
5

3.00

7.67
3.33

6.00
1.67

3.67

0

Figure 5.4: Distribution of diﬀerent algorithm citation functions found in 300
randomly selected algorithm citation contexts.
Figure 5.4 represents the distribution of algorithm citation functions over different classes. Based on the results, most cited algorithms belong to the ‘Mention’
and ‘Argument’ classes. This supports the observation that authors usually just
mention and/or discuss the disadvantages of the existing algorithms to make the

94
readers aware of the motivation and the completeness of the proposed works.
5.2.2.3

Algorithm Citation Function in Diﬀerent Sections
100%
90%
80%

ABS

70%

INT

60%

BCK

50%

RAD

40%

CON

30%
20%

OTHER

10%
0%

Figure 5.5: Distribution of algorithm citation functions over diﬀerent sections.
Scholarly documents are usually composed in sections, each of which serves
a diﬀerent purpose in conveying speciﬁc context. For example, the Background
and Related Works sections discuss about past works relevant to the proposed
ones. The Experiment or Methodology sections describe the methodology and
experimental procedure in detail, etc. In this section, we analyze the distribution of
the algorithm citation functions as seen in diﬀerent standard sections in scholarly
documents. The same set of 300 manually labeled algorithm citation contexts
are further classiﬁed into 6 groups based on the sections in which they appear,
including Abstract ABS, Introduction IN T , Background and Related Works BCK,
Methodology/ Experiment/ Results and Discussions RAD, Conclusion CON , and
Other sections OT HER. Figure 5.5 illustrates the percentage distribution of each
algorithm citation function over diﬀerent sections.
Corresponding with practical intuition, a majority of previous algorithms are
ﬁrst mentioned in the INT and BCK sections. Directly used and baseline algorithms are ﬁrst mentioned in INT sections, then later explained in RAD sections.
Authors usually make arguments about the disadvantages of previous algorithms

95
in INT and BCK sections, to point out the distinction of their works.

5.2.3

Potential Future Works

The work presented here could be a building block to further interesting studies.
We list some of them that we plan to explore in the future here:
Automatic Identiﬁcation of Algorithm Citation Function. Since our
dataset would involve the whole CiteseerX repository, currently hosting roughly 2
million scholarly documents, the ability to automatically classify algorithm citation
functions should be ﬁrst investigated. Here, we plan to employ machine learning
techniques such as [74].
Algorithm Ranking. Strong algorithms tend to be built upon by successful
works, while inferior ones are always criticized and used as baselines. Our proposed classiﬁcation scheme can be used to discriminate algorithms based on the
popularity in the academic domain.
Algorithm Evolution. An algorithm inﬂuence network, where each node is
an algorithm proposing paper and each directed edge represents the citation relationship, can be generated. Such a network can give an insight into how algorithms
inﬂuence each other over time, leading to a study of algorithm evolution and trend.

5.2.4

Conclusion

Here, we present a classiﬁcation scheme for algorithm citation function in scholarly
works. The motivation comes from the fact that an algorithm is usually built upon
the existing ones; hence, the ability to identify which function a cited algorithm
serves in the citing work could shed light on multiple applications in digital library
literature. We propose a classiﬁcation scheme for algorithm citation function,
consisting of 9 classes, further divided into 3 groups based on authors’ attitude:
favorable, neutral, and critical. We study the distribution of the algorithm citation
functions by manually classifying random 300 algorithm citation contexts using
our proposed scheme. Finally, as part of our future work, we suggest potential
applications that utilize our proposed scheme.

Chapter

6

Conclusions
In this dissertation, we discussed the methodology for building a system that collects and searches algorithms in scholarly digital libraries. Algorithms are ubiquitous in computer science and related literature. The number of algorithms increases every day as researchers develop new algorithms to solve emerging problems
that have not been solved before, or algorithms that are better than the existing
ones. This ever increasing pool of algorithms calls for a system that automatically catalogs and manages them. We discussed four aspects of such a system in
this dissertation, including discovering algorithm representations, metadata
extraction, indexing and searching, and semantic analysis of algorithm
citation contexts.
We showed that a majority of algorithms in scholarly documents are represented
with pseudo-codes and algorithmic procedures. Such algorithm representations can
be automatically detected using a combination of rule-based and machine learning
based methods [79]. To the ﬁrst of our knowledge, we are the ﬁrst to explore this
problem.
Traditional search engine methodology requires that a document is represented
with textual information. In order to extract relevant textual components for
an extracted algorithm, we utilized the synopsis generation method proposed by
Bhatia and Mitra [6]. However, such a method fails to generate synopses for
some algorithms that do not have accompanied captions. Hence, we proposed
set of methods based on TF-IDF and topic modeling to transfer the annotation
from a corpus of well-annotated algorithms (i.e. algorithms with synopses) to

97
annotate those metadata records that lack synopses [85]. The proposed metadata
annotation algorithms also have been successfully used to annotate metadata for
environmental science experimental and observational data [88, 89].
We showed that it is possible to build a search engine for algorithms [8]. The
demo search engine, AlgorithmSeer, was built by indexing the textual metadata of
algorithms using Solr/Lucene. The demo algorithm search engine was well received
by multiple research communities [83, 104].
Finally, we explore the use of algorithm citation contexts to advance the search
beyond a just text-based search system. We found that algorithms that address
similar problems can be found by clustering the algorithm co-citation network [80].
Moreover, we found that algorithms are not usually composed from scratch. The
algorithm authors utilizes existing algorithm in multiple ways to create new ones.
We showed that it is possible to capture such relationship between citing and cited
algorithms using the information from algorithm citation contexts [82].

Bibliography

[1] A. Asuncion, M. Welling, P. Smyth, and Y. W. Teh. On smoothing and
inference for topic models. In Proceedings of the Twenty-Fifth Conference
on Uncertainty in Artiﬁcial Intelligence, UAI ’09, pages 27–34, Arlington,
Virginia, United States, 2009. AUAI Press.
[2] R. Atanassov, P. Bose, M. Couture, A. Maheshwari, P. Morin, M. Paquette,
M. Smid, and S. Wuhrer. Algorithms for optimal outlier removal. Journal
of Discrete Algorithms, 2009.
[3] J. B. Baker, A. P. Sexton, V. Sorge, and M. Suzuki. Comparing approaches
to mathematical document analysis from pdf. ICDAR ’11, pages 463–467,
2011.
[4] G. E. A. P. A. Batista, R. C. Prati, and M. C. Monard. A study of the
behavior of several methods for balancing machine learning training data.
SIGKDD Explor. Newsl., 6(1):20–29, 2004.
[5] S. Bhatia, S. Lahiri, and P. Mitra. Generating synopses for document-element
search. CIKM ’09, pages 2003–2006, 2009.
[6] S. Bhatia and P. Mitra. Summarizing ﬁgures, tables, and algorithms in
scientiﬁc publications to augment search results. ACM Trans. Inf. Syst.,
30(1):3:1–3:24, Mar. 2012.
[7] S. Bhatia, P. Mitra, and C. L. Giles. Finding algorithms in scientiﬁc articles. In Proceedings of the 19th international conference on World wide web,
WWW ’10, pages 1061–1062, New York, NY, USA, 2010. ACM.
[8] S. Bhatia, S. Tuarob, P. Mitra, and C. L. Giles. An algorithm search engine
for software developers. In Proceedings of the 3rd International Workshop on
Search-Driven Development: Users, Infrastructure, Tools, and Evaluation,
SUITE ’11, pages 13–16, New York, NY, USA, 2011. ACM.

99
[9] C. M. Bishop. Pattern Recognition and Machine Learning (Information Science and Statistics). Springer-Verlag New York, Inc., Secaucus, NJ, USA,
2006.
[10] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent dirichlet allocation. J. Mach.
Learn. Res., 3:993–1022, Mar. 2003.
[11] L. Breiman. Random forests. Machine Learning, 45(1):5–32, 2001.
[12] M. Bron, B. Huurnink, and M. de Rijke. Linking archives using document
enrichment and term selection. In Proceedings of the 15th international conference on Theory and practice of digital libraries: research and advanced
technology for digital libraries, TPDL’11, pages 360–371, Berlin, Heidelberg,
2011. Springer-Verlag.
[13] W. Browuer, S. Kataria, S. Das, P. Mitra, and C. L. Giles. Segregating and
extracting overlapping data points in two-dimensional plots. In Proceedings
of the 8th ACM/IEEE-CS joint conference on Digital libraries, JCDL ’08,
pages 276–279, New York, NY, USA, 2008. ACM.
[14] C. Caragea, A. Silvescu, S. Kataria, D. Caragea, and P. Mitra. Classifying
scientiﬁc publications using abstract features. In SARA, 2011.
[15] N. V. Chawla, K. W. Bowyer, L. O. Hall, and W. P. Kegelmeyer. Smote:
synthetic minority over-sampling technique. J. Artif. Int. Res., 16(1):321–
357, 2002.
[16] C. Chen, A. Liaw, and L. Breiman. Using random forest to learn imbalanced
data. University of California, Berkeley, 2004.
[17] H.-H. Chen, L. Gou, X. Zhang, and C. L. Giles. Collabseer: a search engine
for collaboration discovery. In Proceedings of the 11th annual international
ACM/IEEE joint conference on Digital libraries, JCDL ’11, pages 231–240,
New York, NY, USA, 2011. ACM.
[18] P. Chiu, F. Chen, and L. Denoue. Picture detection in document page images.
DocEng ’10, pages 211–214, 2010.
[19] S. R. Choudhury, S. Tuarob, P. Mitra, L. Rokach, A. Kirk, S. Szep, D. Pellegrino, S. Jones, and C. L. Giles. A ﬁgure search engine architecture for a
chemistry digital library. In Proceedings of the 13th ACM/IEEE-CS Joint
Conference on Digital Libraries, JCDL ’13, pages 369–370, New York, NY,
USA, 2013. ACM.

100
[20] W. W. Cohen, M. Avenue, M. Hill, C. Of, and R. Pruning. Fast Eﬀective Rule
Induction. In A. Prieditis and S. Russell, editors, Proceedings of the Twelfth
International Conference on Machine Learning, volume 3, pages 115–123.
Morgan Kaufmann, Morgan Kaufmann, 1995.
[21] T. H. Cormen, C. E. Leiserson, R. L. Rivest, and C. Stein. Introduction
to Algorithms, volume 7 of The Mit Electrical Engineering and computer
Science Series. MIT Press, 2001.
[22] J. C. Denny, A. S. III, K. B. Johnson, N. B. Peterson, J. F. Peterson, and
R. A. Miller. Evaluation of a method to identify and categorize section
headers in clinical documents. Journal of the American Medical Informatics
Association, 16(6):806 – 815, 2009.
[23] S. Dongen. Graph clustering by ﬂow simulation [Ph.D. dissertation]. Centers
for Mathematics and Computer Science University of Utrecht, 2000.
[24] G. Erkan and D. R. Radev. Lexrank: Graph-based lexical centrality as
salience in text summarization. J. Artif. Intell. Res.(JAIR), 22(1):457–479,
2004.
[25] R. Fleischer. Extraction of key sections from texts using automatic indexing
techniques, 1999. US Patent 5,960,383.
[26] G. Gallo and S. Pallottino. Shortest path algorithms. Annals of Operations
Research, 13(1):1–79, 1988.
[27] S. Guha and N. Koudas. Approximating a data stream for querying and
estimation: algorithms and performance evaluation. Proceedings 18th International Conference on Data Engineering, pages 567–576, 2002.
[28] M. Hall, H. National, E. Frank, G. Holmes, B. Pfahringer, P. Reutemann,
and I. H. Witten. The WEKA data mining software: an update. SIGKDD
Explorations, 11(1):10–18, 2009.
[29] D. Hankerson, S. Vanstone, and A. J. Menezes. Guide to elliptic curve
cryptography. Springer, 2004.
[30] T. Hassan. Object-level document analysis of pdf ﬁles. In Proceedings of the
9th ACM Symposium on Document Engineering, DocEng ’09, pages 47–55,
New York, NY, USA, 2009. ACM.
[31] M. A. Hearst. Texttiling: segmenting text into multi-paragraph subtopic
passages. Comput. Linguist., 23(1):33–64, Mar. 1997.

101
[32] M. A. Hearst, A. Divoli, H. Guturu, A. Ksikes, P. Nakov, M. A. Wooldridge,
and J. Ye. BioText Search Engine: beyond abstract search. Bioinformatics,
23(16):2196–2197, 2007.
[33] P. Heymann, D. Ramage, and H. Garcia-Molina. Social tag prediction. In
Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR ’08, pages 531–538,
New York, NY, USA, 2008. ACM.
[34] D. S. Hirschberg. A linear space algorithm for computing maximal common
subsequences. Commun. ACM, 18(6):341–343, June 1975.
[35] D. Johansson. An evaluation of shortest path algorithms on real metropolitan
area networks. 2008.
[36] G. H. John and P. Langley. Estimating continuous distributions in bayesian
classiﬁers. In Proceedings of the Eleventh conference on Uncertainty in artiﬁcial intelligence, UAI’95, pages 338–345, San Francisco, CA, USA, 1995.
Morgan Kaufmann Publishers Inc.
[37] S. Kataria, W. Browuer, P. Mitra, and C. L. Giles. Automatic extraction of
data points and text blocks from 2-dimensional plots in digital documents.
In Proceedings of the 23rd national conference on Artiﬁcial intelligence Volume 2, AAAI’08, pages 1169–1174. AAAI Press, 2008.
[38] S. Kataria, W. Browuer, P. Mitra, and C. L. Giles. Automatic extraction of
data points and text blocks from 2-dimensional plots in digital documents.
In Proceedings of the 23rd national conference on Artiﬁcial intelligence Volume 2, AAAI’08, pages 1169–1174. AAAI Press, 2008.
[39] S. Kataria, P. Mitra, and S. Bhatia. Utilizing context in generative bayesian
models for linked corpus. In AAAI’10, pages –1–1, 2010.
[40] M. Khabsa, P. Treeratpituk, and C. L. Giles. Ackseer: a repository and
search engine for automatically extracted acknowledgments from digital libraries. In Proceedings of the 12th ACM/IEEE-CS joint conference on Digital
Libraries, JCDL ’12, pages 185–194, New York, NY, USA, 2012. ACM.
[41] T. M. Khoshgoftaar, M. Golawala, and J. V. Hulse. An empirical study of
learning from imbalanced data using random forest. In Proceedings of the
19th IEEE International Conference on Tools with Artiﬁcial Intelligence,
ICTAI ’07, pages 310–317, 2007.
[42] M. E. Kipp and D. G. Campbell. Searching with tags: Do tags help users
ﬁnd things? Knowledge organization, 37(4), 2010.

102
[43] P. Kiss, U. Moon, J. Steensgaard, J. Stonick, and G. Temes. High-speed
delta; sigma; adc with error correction. Electronics Letters, 37(2):76 –77, jan
2001.
[44] P. Kiss, J. Silva, A. Wiesbauer, T. Sun, U.-K. Moon, J. Stonick, and
G. Temes. Adaptive digital correction of analog errors in mash adcs. ii.
correction using test-signal injection. Circuits and Systems II: Analog and
Digital Signal Processing, IEEE Transactions on, 47(7):629 –638, jul 2000.
[45] J. Kittler, M. Hatef, R. P. W. Duin, and J. Matas. On combining classiﬁers.
IEEE Trans. Pattern Anal. Mach. Intell., 20(3):226–239, Mar. 1998.
[46] G. W. Klau, I. Ljubić, P. Mutzel, U. Pferschy, and R. Weiskircher. The
fractional prize-collecting Steiner tree problem on trees. Springer, 2003.
[47] J. M. Kleinberg and E. Tardos. Algorithm Design, volume 30. Addison
Wesley, 2005.
[48] R. Krestel, P. Fankhauser, and W. Nejdl. Latent dirichlet allocation for
tag recommendation. In Proceedings of the third ACM conference on Recommender systems, RecSys ’09, pages 61–68, New York, NY, USA, 2009.
ACM.
[49] H. Li, I. Councill, W.-C. Lee, and C. L. Giles. Citeseerx: an architecture and
web service design for an academic document search engine. In Proceedings
of the 15th international conference on World Wide Web, WWW ’06, pages
883–884, New York, NY, USA, 2006. ACM.
[50] H. Li, I. G. Councill, L. Bolelli, D. Zhou, Y. Song, W.-C. Lee, A. Sivasubramaniam, and C. L. Giles. Citeseerx: A scalable autonomous scientiﬁc
digital library. In Proceedings of the 1st International Conference on Scalable
Information Systems, InfoScale ’06, New York, NY, USA, 2006. ACM.
[51] Y. Liu, K. Bai, P. Mitra, and C. L. Giles. Tableseer: automatic table metadata extraction and searching in digital libraries. In Proceedings of the 7th
ACM/IEEE-CS joint conference on Digital libraries, JCDL ’07, pages 91–
100, New York, NY, USA, 2007. ACM.
[52] Y. Liu, K. Bai, P. Mitra, and C. L. Giles. Improving the table boundary
detection in pdfs by ﬁxing the sequence error of the sparse lines. ICDAR ’09,
pages 1006–1010, 2009.
[53] Z. Liu, X. Chen, and M. Sun. A simple word trigger method for social
tag suggestion. In Proceedings of the Conference on Empirical Methods in
Natural Language Processing, EMNLP ’11, pages 1577–1588, Stroudsburg,
PA, USA, 2011. Association for Computational Linguistics.

103
[54] X. Lu, P. Mitra, J. Z. Wang, and C. L. Giles. Automatic categorization
of ﬁgures in scientiﬁc documents. In Proceedings of the 6th ACM/IEEE-CS
joint conference on Digital libraries, JCDL ’06, pages 129–138, New York,
NY, USA, 2006. ACM.
[55] S. Mandal, S. P. Chowdhury, A. K. Das, and B. Chanda. Automated detection and segmentation of table of contents page from document images.
ICDAR ’03, pages 398–, 2003.
[56] C. D. Manning, P. Raghavan, and H. Schütze. Introduction to Information
Retrieval. Number 4. Cambridge University Press, 2008.
[57] S. Mao and T. Kanungo. Software architecture of pset: A page segmentation evaluation toolkit. International Journal on Document Analysis and
Recognition, 4(3):205–217, 2002.
[58] O. Medelyan and I. H. Witten. Thesaurus based automatic keyphrase indexing. In Proceedings of the 6th ACM/IEEE-CS joint conference on Digital
libraries, JCDL ’06, pages 296–297, New York, NY, USA, 2006. ACM.
[59] Y. Mendoza and R. Osegueda. Theoretical explanation for the empirical
probability of detection (pod) curve: a neural network-motivated approach.
In Circuits and Systems, 1999. 42nd Midwest Symposium on, pages 435–438,
1999.
[60] G. Mishne. Autotag: a collaborative approach to automated tag assignment
for weblog posts. In Proceedings of the 15th international conference on
World Wide Web, WWW ’06, pages 953–954, New York, NY, USA, 2006.
ACM.
[61] D. Newman, K. Hagedorn, C. Chemudugunta, and P. Smyth. Subject metadata enrichment using statistical topic models. In Proceedings of the 7th
ACM/IEEE-CS joint conference on Digital libraries, JCDL ’07, pages 366–
375, New York, NY, USA, 2007. ACM.
[62] T. D. Nguyen and M.-Y. Kan. Keyphrase extraction in scientiﬁc publications.
In Proceedings of the 10th international conference on Asian digital libraries:
looking back 10 years and forging new frontiers, ICADL’07, pages 317–326,
2007.
[63] L. Page, S. Brin, R. Motwani, and T. Winograd. The pagerank citation
ranking: bringing order to the web. 1999.
[64] G. Patané and M. Russo. The enhanced lbg algorithm. Neural Networks,
14(9):1219–1237, 2001.

104
[65] B. Peritz. A classiﬁcation of citation roles for the social sciences and related
ﬁelds. Scientometrics, 5:303–312, 1983.
[66] S. Philipp-Foliguet, P. Gosselin, and M. Cord. Retin system: partial and
global feature learning imageval/tasks 4 and 5. In Workshop ImagEVAL.
Citeseer, 2006.
[67] C. Ramakrishnan, A. Patnia, E. H. Hovy, and G. A. Burns. Layout-aware
text extraction from full-text pdf of scientiﬁc articles. Source code for biology
and medicine, 7(1):7, 2012.
[68] H. SMALL. Co-citation in the Scientiﬁc Literature : A New Measure of the
Relationship Between Two Documents. Journal of the American Society for
Information Science, pages 265–269, 1973.
[69] P. Sojka and M. Lı́ška. The art of mathematics retrieval. DocEng ’11, pages
57–60, 2011.
[70] I. Spiegel-Rösing. Science studies: Bibliometric and content analysis. Social
Studies of Science, pages 97–113, 1977.
[71] J. G. Stewart and J. Callan. Genre Oriented Summarization. PhD thesis,
Carnegie Mellon University, 2009.
[72] C. Su, Y. Pan, Y. Zhen, Z. Ma, J. Yuan, H. Guo, Z. Yu, C. Ma, and Y. Wu.
Prestigerank: A new evaluation method for papers and journals. Journal of
Informetrics, 5(1):1 – 13, 2011.
[73] M. Suzuki, F. Tamari, R. Fukuda, S. Uchida, and T. Kanahori. Infty: An
integrated ocr system for mathematical documents. DocEng ’03, pages 95–
104, 2003.
[74] S. Teufel, A. Siddharthan, and D. Tidhar. An annotation scheme for citation
function. SigDIAL ’06, pages 80–87, 2006.
[75] S. Teufel, A. Siddharthan, and D. Tidhar. Automatic classiﬁcation of citation
function. In Proceedings of the 2006 Conference on Empirical Methods in
Natural Language Processing, EMNLP ’06, pages 103–110, 2006.
[76] J. Tiedemann and J. Mur. Simple is best: experiments with diﬀerent document segmentation strategies for passage retrieval. In Coling 2008: Proceedings of the 2nd workshop on Information Retrieval for Question Answering,
IRQA ’08, pages 17–25, 2008.

105
[77] D. Tkaczyk, A. Czeczko, K. Rusek, L. Bolikowski, and R. Bogacewicz. Grotoap: ground truth for open access publications. In Proceedings of the 12th
ACM/IEEE-CS joint conference on Digital Libraries, JCDL ’12, pages 381–
382, 2012.
[78] P. Treeratpituk, P. Teregowda, J. Huang, and C. L. Giles. Seerlab: A system
for extracting key phrases from scholarly documents. In Proceedings of the
5th International Workshop on Semantic Evaluation, SemEval ’10, pages
182–185, 2010.
[79] S. Tuarob, S. Bhatia, P. Mitra, and C. Giles. Automatic detection of pseudocodes in scholarly documents using machine learning. In Document Analysis and Recognition (ICDAR), 2013 12th International Conference on, pages
738–742, Aug 2013.
[80] S. Tuarob, P. Mitra, and C. L. Giles. Improving algorithm search using the
algorithm co-citation network. In Proceedings of the 12th ACM/IEEE-CS
joint conference on Digital Libraries, JCDL ’12, pages 277–280, New York,
NY, USA, 2012. ACM.
[81] S. Tuarob, P. Mitra, and C. L. Giles. Taxonomy-based query-dependent
schemes for proﬁle similarity measurement. In Proceedings of the 1st Joint
International Workshop on Entity-Oriented and Semantic Search, JIWES
’12, pages 8:1–8:6, New York, NY, USA, 2012. ACM.
[82] S. Tuarob, P. Mitra, and C. L. Giles. A classiﬁcation scheme for algorithm
citation function in scholarly works. In Proceedings of the 13th ACM/IEEECS joint conference on Digital libraries, JCDL ’13, pages 367–368, 2013.
[83] S. Tuarob, P. Mitra, and C. L. Giles. Building a search engine for algorithms.
SIGWEB Newsl., (Winter):5:1–5:9, Jan. 2014.
[84] S. Tuarob, L. Pouchard, P. Mitra, and C. Giles. A generalized topic modeling approach for automatic document annotation. International Journal on
Digital Libraries, pages 1–18, 2015.
[85] S. Tuarob, L. C. Pouchard, and C. L. Giles. Automatic tag recommendation
for metadata annotation using probabilistic topic modeling. In Proceedings
of the 13th ACM/IEEE-CS Joint Conference on Digital Libraries, JCDL ’13,
pages 239–248, New York, NY, USA, 2013. ACM.
[86] S. Tuarob, L. C. Pouchard, and L. Giles. Automatic Annotation of metadata
using probabilistic topic modeling. Jan 2013.

106
[87] S. Tuarob, L. C. Pouchard, N. Noy, J. S. Horsburgh, and G. Palanisamy.
Automatic annotation of environmental metadata using topic similarity. In
DataONE All Hands Meeting, 2012.
[88] S. Tuarob, L. C. Pouchard, N. Noy, J. S. Horsburgh, and G. Palanisamy.
Onemercury: Towards automatic annotation of earth science metadata. In
AGU Fall Meeting Abstracts, volume 1, page 1482, 2012.
[89] S. Tuarob, L. C. Pouchard, N. Noy, J. S. Horsburgh, and G. Palanisamy.
Onemercury: Towards automatic annotation of environmental science metadata. In Proceedings of the 2nd International Workshop on Linked Science,
LISC ’12, 2012.
[90] S. Tuarob and C. S. Tucker. Fad or here to stay: Predicting product market adoption and longevity using large scale, social media data. In ASME
2013 International Design Engineering Technical Conferences and Computers and Information in Engineering Conference, pages V02BT02A012–
V02BT02A012. American Society of Mechanical Engineers, 2013.
[91] S. Tuarob and C. S. Tucker. Discovering next generation product innovations by identifying lead user preferences expressed through large scale social
media data. In ASME 2014 International Design Engineering Technical Conferences and Computers and Information in Engineering Conference, pages
V01BT02A008–V01BT02A008. American Society of Mechanical Engineers,
2014.
[92] S. Tuarob and C. S. Tucker. Automated discovery of lead users and latent
product features by mining large scale social media networks. Journal of
Mechanical Engineering, 2015.
[93] S. Tuarob and C. S. Tucker. A product feature inference model for mining
implicit customer preferences within large scale social media networks. In
ASME 2015 International Design Engineering Technical Conferences and
Computers and Information in Engineering Conference. American Society
of Mechanical Engineers, 2015.
[94] S. Tuarob and C. S. Tucker. Quantifying product favorability and extracting
notable product features using large scale social media data. Journal of
Computing and Information Science in Engineering, 2015.
[95] S. Tuarob, C. S. Tucker, M. Salathe, and N. Ram. Discovering healthrelated knowledge in social media using ensembles of heterogeneous features.
In Proceedings of the 22Nd ACM International Conference on Conference on
Information & Knowledge Management, CIKM ’13, pages 1685–1690, New
York, NY, USA, 2013. ACM.

107
[96] S. Tuarob, C. S. Tucker, M. Salathe, and N. Ram. An ensemble heterogeneous
classiﬁcation methodology for discovering health-related knowledge in social
media messages. Journal of Biomedical Informatics, 49(0):255 – 268, 2014.
[97] K. van Deemter and R. Kibble. On coreferring: coreference in muc and
related annotation schemes. Comput. Linguist., 26(4):629–637, 2000.
[98] S. Vutukury and J. J. Garcia-Luna-Aceves. A simple approximation to
minimum-delay routing. SIGCOMM Comput. Commun. Rev., 29(4):227–
238, Aug. 1999.
[99] D. Walker, H. Xie, K.-K. Yan, and S. Maslov. Ranking scientiﬁc publications
using a model of network traﬃc. Journal of Statistical Mechanics: Theory
and Experiment, 2007(06):P06010, 2007.
[100] J. Wang. Mean-Variance Analysis: A New Document Ranking Theory in
Information Retrieval. pages 4–16, 2009.
[101] I. H. Witten, G. W. Paynter, E. Frank, C. Gutwin, and C. G. Nevill-Manning.
Kea: practical automatic keyphrase extraction. In Proceedings of the fourth
ACM conference on Digital libraries, DL ’99, pages 254–255, New York, NY,
USA, 1999. ACM.
[102] L. Wu, L. Yang, N. Yu, and X.-S. Hua. Learning to tag. In Proceedings
of the 18th international conference on World wide web, WWW ’09, pages
361–370, New York, NY, USA, 2009. ACM.
[103] Z. Wu, S. Das, Z. Li, P. Mitra, and C. L. Giles. Searching online book
documents and analyzing book citations. In Proceedings of the 2013 ACM
Symposium on Document Engineering, DocEng ’13, pages 81–90, New York,
NY, USA, 2013. ACM.
[104] Z. Wu, J. Wu, M. Khabsa, K. Williams, H.-H. Chen, W. Huang, S. Tuarob,
S. R. Choudhury, A. Ororbia, P. Mitra, and C. L. Giles. Towards building a
scholarly big data platform: Challenges, lessons and opportunities. In Proceedings of the 14th ACM/IEEE-CS Joint Conference on Digital Libraries,
JCDL ’14, pages 117–126, Piscataway, NJ, USA, 2014. IEEE Press.
[105] R. Zanibbi and D. Blostein. Recognition and retrieval of mathematical expressions. International Journal on Document Analysis and Recognition,
pages 1–27, 2012.

Vita
Suppawong Tuarob
Suppawong Tuarob was born in Trang Province of Thailand on December 25th , 1986. After
completing his high school at Mahidol Wittayanusorn school in Nakorn Prathom, Thailand, he
attended another year of high school at Westtown School, West Chester, PA. He completed both
his Bachelor and Master of Engineering in Computer Science and Engineering from the University
of Michigan-Ann Arbor in December 2009 and May 2010 respectively. Thereafter, he joined the
PhD program in Computer Science and Engineering at the Pennsylvania State University. He
became a member of the Intelligent Information Systems (IIS) Laboratory supervised by Dr. C.
Lee Giles and Dr. Prasenjit Mitra in 2010, and the Design Analysis Technology Advancement
(DATA) Laboratory supervised by Dr. Conrad Tucker in 2012. During his PhD, he worked as
research interns at Paciﬁc Northwest National Laboratory (PNNL) in 2011, Data Observation
Network for Earth (DataONE) in 2012, Microsoft Research in 2013, and IBM Almaden Research
Center in 2014. His research interest lies in information retrieval, applied machine learning, data
(especially text) mining, statistical topic modeling, social media, sentiment analysis, product
design informatics, and time series analysis.

Selected Representative Publications
1. Suppawong Tuarob, Sumit Bhatia, Prasenjit Mitra, and C. Lee Giles, AlgorithmSeer:
A System for Extracting and Searching for Algorithms in Scholarly Big Data (In Review ).
2. Suppawong Tuarob, Prasenjit Mitra, and C. Lee Giles, A Hybrid Approach to Discover
Semantic Hierarchical Sections in Scholarly Documents (In Review ).
3. Suppawong Tuarob, Line C. Pouchard, Prasenjit Mitra, and C. Lee Giles, A Generalized
Topic Modeling Approach for Automatic Document Annotation, International Journal on
Digital Libraries (2015).
4. Suppawong Tuarob and Conrad S Tucker. Automated Discovery of Lead Users and
Latent Product Features by Mining Large Scale Social Media Networks, Journal of Mechanical Design (2015).
5. Suppawong Tuarob and Conrad S Tucker. Quantifying Product Favorability and Extracting Notable Product Features using Large Scale Social Media Data, Journal of Computing and Information Science in Engineering (2015).
6. Suppawong Tuarob, Conrad S Tucker, Marcel Salathe and Nilam Ram. An Ensemble
Heterogeneous Classiﬁcation Methodology for Discovering Health-Related Knowledge in
Social Media Messages, Journal of Biomedical Informatics (2014).
7. Suppawong Tuarob, Sumit Bhatia, Prasenjit Mitra and C. Lee Giles. Automatic Detection of Pseudocodes in Scholarly Documents Using Machine Learning, The Twelfth
International Conference on Document Analysis and Recognition (ICDAR 2013).
8. Suppawong Tuarob, Prasenjit Mitra and C. Lee Giles. Improving Algorithm Search Using the Algorithm Co-Citation Network, ACM/IEEE Joint Conference on Digital Libraries
(JCDL 2012).

