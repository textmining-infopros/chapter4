PEER-REVIEWED REQUIRED: THE ROLE OF BIBLIOGRAPHIC
REQUIREMENTS IN THE UNDERGRADUATE
RESEARCH ASSIGNMENT

ABSTRACT
TRENTON ORMSBEE-HALE
PEER-REVIEWED REQUIRED: THE ROLE OF BIBLIOGRAPHIC
REQUIREMENTS IN THE UNDERGRADUATE RESEARCH ASSIGNMENT
AUGUST 2018
This study explored what impact requiring the use of peer-reviewed articles for an
undergraduate research assignment had on the makeup of students’ bibliographies and
how they integrated their information sources. Utilizing a nonequivalent control group
design, unit essays were collected from two sections of the same undergraduate political
science course at a public university in Texas. The experimental group was required to
use a minimum number of peer-reviewed sources while the control group was not. A
sample of 44 documents was collected. The collected essays underwent citation analysis
and content analysis to investigate potential differences in bibliographic behavior and
how students engage with their sources within the body of their essays. The citation
analysis revealed that the students who were required to include peer-reviewed sources
did cite significantly more peer-reviewed articles than the control group. They also
referred to their peer-reviewed sources more frequently in the body of their essays than
the students in the control group referred to their peer-reviewed sources. The content
analysis, however, revealed that synthesis of and engagement with outside information
sources was similarly infrequent and homogeneous in both groups, which suggests deeper
information literacy challenges faced by the students.

CHAPTER I
INTRODUCTION
Consider the phrase, “a sea of information.” The phrase alludes to a seemingly
endless abundance of information that is, now, at our fingertips — an abundant sea of
information that everyone must, at some level, learn to navigate. One would describe the
ability to navigate this sea of information as information literacy that, in 1989, the
American Library Association’s Presidential Committee on Information Literacy defined
as the ability to recognize information needs and to locate, evaluate, and use information.
This set of abilities, information literacy, is an important facet of undergraduate
education, and it is now codified as such by organizations like the Association of
American Colleges & Universities [AAC&U] (2007a), which lists information literacy as
an essential learning outcome for post-secondary students.
This study investigated the efficacy of one particular technique proposed to help
undergraduate students learn to navigate this expanse of information: explicitly requiring
undergraduate students to use peer-reviewed articles for their research. Is this an effective
method of supporting students’ information literacy? The reason for the initial, aquatic
metaphor is that it can be used to frame the differing approaches to bibliographic
requirements. The two approaches could be likened to the choice to teach someone to
swim in an ocean or a pool. Bibliographic requirements provide boundaries. They
provide a structure within the body of information that clearly indicate where the credible

1

waters of peer-reviewed sources are that students must tread to practice their research
skills. The absence of boundaries opens the learner up to waters that might prove more
treacherous than other sources, but it also provides an opportunity for learners to govern
themselves accordingly in a marketplace of information that they will most likely
encounter outside the walls of the academy. The primary question, therefore, is whether
one of these approaches is truly more effective at helping the student to navigate and use
information, but one may also venture to wonder what these two methods say about the
instructor/librarian who uses them. By assessing the written work of two groups of
students – one group that was required to use peer-reviewed articles and one that was not
– this experiment aims to revisit earlier advocates of bibliographic requirements by
retesting their conclusions and extending their questions to address deeper concerns about
students’ information literacy.
Research Questions and Hypotheses
The study is framed by two overarching research questions: one guided a citation
analysis portion of the research and the other guided a content analysis portion of the
research. The first research question asks: Does explicitly requiring the use of peerreviewed sources impact the bibliographies and citation behavior of students? The second
research question asks: How do students in both groups (those with the requirement and
those without) utilize the information sources within their essays? The answers to these
two questions are guided by six hypotheses. One through four relate to the first research
question whereas five and six relate to the second research question.

2

H1. Students who received the bibliographic requirement to use peer-reviewed
sources will cite all of their individual sources less frequently within the body of
their essays than those without the requirement.
H2. Students who received the bibliographic requirement to use peer-reviewed
sources will cite their peer-reviewed sources less frequently within the body of
their essays than those without the requirement.
H3. Students who received the bibliographic requirement to use peer-reviewed
sources will cite fewer sources overall than those without the requirement.
H4. Students who received the bibliographic requirement to use peer-reviewed
sources will cite the same percentage of peer-reviewed sources as those without
the requirement.
H5. Students who received the bibliographic requirement to use peer-reviewed
sources will directly quote their outside information sources more frequently than
those without the requirement.
H6. Students who received the bibliographic requirement to use peer-reviewed
sources will attempt to synthesize their outside information sources less
frequently than those without the requirement.
Overview
The review of literature found in Chapter II introduces precursors to this
experiment who investigated and highlighted the benefits of requiring students to use
peer-reviewed sources. This chapter gives special attention to the ways research and
assessment methods shape and situate conceptions of information literacy. More
3

specifically, the review of literature will trace how new assessment methods help to
unfurl different concerns about students’ information literacy skills that were left
unaddressed by earlier authors. Last, this chapter will present an examination of how
shifting theories and instructional approaches in information literacy accentuate the need
to revisit and update earlier tests of bibliographic requirements. These shifts in theory
also pose questions about what these differing approaches to bibliographic requirements
say about the role and priorities of the academic librarian.
Chapter III describes, in detail, the methodological approach in this study.
Informed by the literature reviewed in Chapter II, this study used a two-part process to
assess indicators of information literacy in students’ citation behaviors and the body of
their essays. In all, 44 essays were collected and analyzed from two sections of the same
undergraduate course – one section that was required to use peer-reviewed sources and
one that was not. This section will describe the development of a citation analysis
checklist which categorized sources as peer-reviewed or not and counted the frequency of
in-text citations. It will also describe the creation of a codebook that was used to code intext references for synthesis, engagement, and additional writing behaviors.
Chapter IV reports the results of six hypothesis tests and uncovers significant
differences between the two groups’ inclusion and use of peer-reviewed sources. In
addition to the two-sample hypothesis tests, numerical observations offered in this
chapter describe themes of synthesis, engagement, and writing behavior that were
strikingly similar across both groups.

4

Chapter V interprets each of the findings by situating the results in the broader
discussion examined in the literature review. This section describes instances in which
this experiment reaffirms the findings of earlier studies and, in some cases, casts doubt on
or contradicts previous research. This chapter also provides suggestions for future
research and discusses the limitations of the study.
Rationale
Mandating that students use peer-reviewed sources appears to remain a common
practice in higher education (Head & Eisenberg, 2010). Furthermore, library and
information science researchers who initially tested and advocated for these requirements
have had their claims relatively unchecked for over a decade (Davis, 2002; Davis, 2003;
Baberino, 2004; Robinson & Schlegl, 2005). In that time, scholars have offered new
conceptions of information literacy, and they have raised new concerns about students’
ability to digest and integrate information sources into their papers. The analyses
presented in this study provide a much-needed update to a practice that is still utilized.
This study’s results provide reassurance that, in this experiment, the bibliographic
requirement remained useful for affecting the bibliographies and citation behaviors of the
students, but it also casts serious doubts about the practice’s ability to address more
contemporary information literacy concerns. Last, the decision to complement traditional
citation analysis with a content analysis is an attempt at a more holistic, authentic
assessment of information literacy. This study, therefore, provides insight into the
advantages and challenges of this kind of mixed-method approach.

5

CHAPTER II
REVIEW OF LITERATURE
The following review of literature predominantly traces information literacy
research from the last two decades. First, this chapter will review early advocates for the
practice of mandating peer-reviewed articles. These authors primarily wrote at the turn of
the century, and their assessment methods and possible reasons for focusing on students’
bibliographies as an indicator of information literacy will be explored. This chapter will
then review more recent literature, with a particular focus on how new methods of
information literacy assessment raised questions about students’ synthesis and use of
information sources rather than the quality of their bibliographies. Last, this chapter will
explore the shift seen in these two areas of research, focusing not on their differences in
methodology but on underlying shifts in theory and practice. This examination of both
earlier voices and more recent voices helps to situate the two-part structure of this study.
This study builds off both groups’ findings and methods to construct research questions
and assessments that can address both concerns about students’ bibliographies and their
use and synthesis of information sources.
Recommending Bibliographic Requirements
Head and Eisenberg’s (2010) content analysis of 190 undergraduate research
assignment guidelines provides a glimpse into the parameters and expectations faculty
members place on their students and their students’ research. Beyond guiding basic

6

requirements for formatting, Head and Eisenberg observe that a noticeable percentage of
assignment guides mandate which information sources students may use. Specifically,
35% and 22% of analyzed guidelines required the use of a library’s print collection or
scholarly databases, respectively. While, in comparison, popular websites, Wikipedia,
and search engines (such as Google or Yahoo!) were among the most frequently
prohibited information sources. This dichotomy of required and prohibited information
sources that Head and Eisenberg observe in research guides points to a larger history of
privileging and mandating peer-reviewed sources within undergraduate instruction. In
this review of literature, that history will begin with research between 2002 and 2006 that
promoted the benefits of requiring peer-reviewed articles.
For some librarians and researchers writing at the turn of the century, they viewed
students’ declining use of peer-reviewed sources in their research assignments as
alarming. A citation analysis by Davis (2002) found students citing popular internet
sources, magazines, and newspapers more frequently while scholarly print materials
(journals and books) either remained the same or declined in their share of student
citations. Davis called this trend “a possible crisis in undergraduate scholarship…” and
recommends that faculty members require students to use a minimum number of peerreviewed articles (2002, p. 59). A later article by Davis (2003) reported that the mandates
they suggested were effective in improving the quality of student bibliographies. For
Davis, this improvement meant increasing the number of scholarly articles and books
while limiting the number of popular internet sources. Baberino (2004) undertook an
analysis of undergraduates’ research behaviors around the same time, and, citing a

7

concern for the apparent decline in use of “traditional” library collections, concluded with
the same recommendation to impose a minimum requirement of sanctioned, scholarly
information sources. Like Davis, it appears that Baberino’s recommendation to mandate
peer-reviewed articles, in part, stems from a desire to maintain the use and importance of
library collections while deterring the use of those popular websites that Head and
Eisenberg observed were so frequently prohibited.
Following the conclusions of Davis, Robinson and Schlegl (2004) set up an
experiment to provide even more evidence for the recommended mandate’s effectiveness.
Robinson and Schlegl (2004) analyzed bibliographies from students enrolled in an
undergraduate political science course; they compared bibliographies from students in a
control group, a group that received library instruction, and a group that received library
instruction and a mandate to use peer-reviewed sources. Not surprisingly, the
bibliographies from the group in which students were required to use scholarly sources
had the highest percentage of scholarly citations. Robinson and Schlegl (2005) go on to
interpret their findings to suggest that, while internet-research is not, itself, detrimental to
student research, imposing bibliographic restrictions sets clear guidelines for students
about what types of sources they are expected to use within their research assignments.
While Robinson and Schlegl’s study identified bibliographic mandates and library
instruction as the recipe for bibliographic success, Davis (2003) found the requirement
alone to be effective. Elsewhere in the literature, researchers have found library
instruction’s effect on improving the size or quality of bibliographies inconclusive when
not aided by the mandates recommended in these earlier studies (Howard, Nicholas,
8

Hayes, & Appelt, 2015; Rosenblatt, 2010; Conway, 2015). Furthermore, Knight’s (2006)
analysis of annotated bibliographies found that instructors simply encouraging students to
use peer-reviewed sources had no effect in curbing the use of popular internet sources.
What these studies suggest is that, bibliographic mandates have proven more effective in
altering the content of students’ bibliographies than library instruction.
Beyond sharing similar recommendations and concerns for the quality of student
bibliographies, these authors also share common assessment methods that evaluate
students’ information literacy based on their bibliographies. Walsh’s (2009) analysis of
information literacy research found that the most common form of information literacy
assessment of student work (also known as authentic assessment) was an evaluation of
bibliographies. Walsh’s findings suggested that, for many information professionals and
library scholars, student bibliographies are used as a “…proxy for skills that cover key
parts of the information literacy whole” (p. 22).
One reason for this reliance on citation analysis may be, as Hovde (2000)
suggested that it is unobtrusive, objective, and easy to execute for librarians pressured to
demonstrate quantifiable measures of their effectiveness. Furthermore, this concern and
focus on bibliographies is understandable from a library and information science
perspective. Students’ bibliographies clearly relate to outcomes set forth by the
Association of College & Research Libraries’ (ACRL) Information Literacy Competency
Standards for Higher Education (hereafter simply referred to as the Standards);
outcomes two and five define the information literate student as one who can navigate
libraries and databases to find and cite quality information and can use proper citation to
9

provide necessary attribution (2000). Even in its 2015 reiteration, the ACRL’s
Framework for Information Literacy for Higher Education (hereafter simply referred to
as the Framework), the quality of bibliographies remains a codified outcome found in the
call for students to provide proper attribution, recognize authoritative information
sources, and utilize effective search strategies to navigate library databases. In turn,
bibliographic-centered library instruction makes student bibliographies important
indicators for the effectiveness of library instruction. This fact is demonstrated in a
number of previous studies that use the quality of bibliographies as an indicator to assess
the effectiveness of library instruction (Snavley & Cooper, 1997, p. 10; Hovde, 2000,
Wang, 2006; Mery, Newby, & Peng, 2012; Clark & Chinburg, 2010; Lantz, Insua, &
Armstrong, 2016).
While authors like Davis, Baberino, and Robinson and Schlegl seemed, in part,
motivated to advocate for mandates based on a desire to maintain the pride of place of
library collections, writers have also offered other motives for the practice. French
(2004), for example, explained that this choice to require scholarly sources may be
motivated by faculty members’ desire that students use authoritative sources, utilize
online databases, or that students become accustomed to seeing authoritative and
scholarly communication to identify that which is not.
Given their focus on citation analysis, library-collection-use, and bibliographic
outcomes, it would make sense that the applications and recommendations that emerge
from the authors mentioned above would also be bibliographic in nature. Situating
oneself with a bibliographic-centered lens, in which information literacy is predominantly
10

evaluated by citation analysis and motivated by bibliographic student outcomes, allows
one to better understand why authors such as Robinson and Schlegl provided reasonable
recommendations that relate directly to conditioning and altering the content of students’
bibliographies.
Rethinking Citation Analysis
Admittedly, information literacy is an elusive construct with no one assessment
method universally used or accepted by information literacy scholars. Abdullah’s (2010)
review of research in information literacy, for example, characterized the field as divided
between perception-based approaches and authentic assessment methods. Perceptive
approaches rely on students’ self-report of behaviors or familiarity with concepts,
whereas authentic approaches evaluate students’ work or behaviors directly. Even beyond
this divide, researchers in this field use a diversity of assessment methods. Walsh (2009)
recorded at least 10 distinct methods that range from information retrieval simulations to
multiple-choice tests. While a full review of information literacy assessment methods is
outside the scope of this study, what is important is that new approaches in authentic
assessment began to shed light on aspects of information literacy that were left
unexamined in the bibliographic approach described in the previous section.
Synthesis Rubrics
In 2006, Scharf et al. developed a multivariate information literacy rubric to
analyze the content of student research portfolios. The rubric’s evaluation was based on
four information-use variables: proper citation, independent research, appropriateness of
sources, and integration of sources. What they uncovered was that, while proper citation
11

and appropriate source selection were two of the variables with the highest overall scores,
more abstract, higher-order indicators of information literacy (such as integration) were
the lowest.
This phenomenon was later observed by Rosenblatt (2010) who conducted a
citation analysis and rubricked assessment of student papers. Rosenblatt’s original study
called for a citation analysis to evaluate the effectiveness of library instruction. However,
when this approach uncovered that the library instruction had no significant impact on the
content of students’ bibliographies, they turned toward the text of the essays. They
proposed a second phase of analysis that used a rubric to evaluate students’ synthesis of
information sources within their essays. Adapting the Association of American Colleges
and Universities’ [AAC&U] (2007b) VALUE Rubric for information literacy, this
method delivered a single score for each paper’s synthesis of information sources. In the
end, Rosenblatt would conclude from both phases that, while the undergraduate students
in the study were able to locate and access the necessary information sources, they
“…provided little or no evidence that they derived any benefits from the literature they
were required to consult” (2010, p. 60). In Rosenblatt’s sample of papers, only 50%
received scores that indicated they were integrating sources into their papers.
Rosenblatt’s observation of how students in the sample struggled with synthesis
largely influenced this study’s focus on synthesis as an indicator of higher-order
information literacy dispositions. Based off Torraco’s (2005) definition, the following
working definition of synthesis was used for the purposes of this study: “Synthesis
integrates existing ideas with new ideas to create a new formulation of the topic or
12

issue…synthesis is not a data dump. It is a creative activity…” that integrates outside
information into one’s own argument or understanding to add something new to the
conversation (p. 362).
Like Rosenblatt, Carlozzi (2018) made use of a rubric to analyze students’
synthesis of sources. Carlozzi scored students based on individual in-text references to
sources as opposed to the paper as a whole. Each paper’s final score was based on the
student’s highest-scoring integration of a source. What Carlozzi uncovered was similar to
Rosenblatt’s concerns: students were able to find the necessary material but were unable
to synthesize the material. More specifically, the average synthesis score for the students
in the study was 0.57 on a 3-point scale, and 52.7% of the students scored a zero because
of the complete lack of synthesis. Given that students were graded on their best attempt at
integration, this means, for over half of the students, their best attempt was no clear
synthesis at all. Possibly more problematic, in terms of this paper’s topic, is that Carlozzi
uncovered no significant difference between the synthesis scores of scholarly sources and
non-scholarly sources, suggesting the students struggled with synthesis regardless of
format. Similarly, Luetkenhaus, Borrelli, and Johnson (2015) uncovered, through a
rubricked analysis, that students performed the best in the domains of proper citation and
source quantity while performing the worst in domains such as argument building.
Holliday et al. (2015) implemented an adapted AAC&U information literacy
VALUE rubric to assess information literacy skills from approximately 900 student
papers. Holliday et al.’s (2015) findings reaffirmed the narrative that students struggled to
synthesize information effectively. While Holiday et al.’s study did not specifically test
13

the role of bibliographic requirements, the authors claimed that their results led them to
discourage faculty members from mandating the number and type of sources students
must use for their research. Holiday et al. explained why:
We suspect that these checklists and quotas encouraged students to find
sources without thinking about their relevance. In our own instruction, we
began to emphasize the strength of the evidence provided by the
information in sources, rather than the types of ‘good’ or ‘bad’ sources,
and we encouraged instructors to do the same. (p. 182)
In-Text Citation Analysis
Even citation analysis that moves beyond the bibliography and evaluates in-text
citations began uncovering blind spots in assessments of bibliographies alone. Drawing
on the Citation Project’s (2018) collection and coding of thousands of student papers and
bibliographies, Jamieson (2013) raised concerns about the frequency with which students
refer to their sources. Jamieson’s analysis found that students rarely referred to an outside
information source more than once within their essays. Jamieson (2016) also observed
that this was especially true for peer-reviewed sources that, while adequately represented
in bibliographies, were used less frequently in the text than non-scholarly sources. This
finding is confirmed by Carlozzi (2018), who observed that students in their study rarely
used sources more than once, and they tended to use in-class readings significantly more
than their outside research. The frequency of in-text citations was not considered by
Robinson and Schlegl (2004). In fact, Robinson & Schlegl’s experiment only collected

14

bibliographies, which means their test of the bibliographic requirement was unable to
assess how often students used the sources they cited.
While Jamieson did not assess synthesis directly like Rosenblatt or Carlozzi,
indicators from their citation analysis were interpreted as signs of poor information
source integration. Citing Kennedy (1985), Jamieson suggested that low frequencies of
in-text references mean that students are not engaging with the information sources or
incorporating them into their argument; they are simply trying to “check off the box” of
including the obligatory in-text citation.
By comparing students’ in-text references to the original information source,
Jamieson’s (2013) extended citation analysis also uncovered that 94% of in-text citations
were built off only one or two sentences of text within the original source. This means
that students primarily used methods such as directly quoting, paraphrasing, or
patchwriting from a few lines rather than integrating broader portions of text or ideas
from their sources. These tendencies were seen as negative indicators of synthesis and
source integration. Howard, Serviss, and Rodrigue (2010), also contributors to the
Citation Project’s body of research, specifically analyzed students’ methods of
introducing information — direct quotations, patchwriting, paraphrasing, or
summarization. Following the same line of reasoning in Jamieson’s interpretation,
directly quoting or patchwriting from a few lines within a source was viewed as lessthan-ideal forms of source integration, and these methods were used frequently within the
papers they analyzed.

15

Content Analysis
Qualitative and content analysis approaches to assess student work have also
broadened the vision of information literacy research. Like Jamieson, Hyytinen,
Löfstrom, and Lindblom-Ylänn’s (2017) qualitative analysis identified problems in
paraphrasing (e.g., patchwriting, unincorporated references)1 as some of the most
common themes in their sample of student papers. A qualitative approach adopted by
Kanter (2006) to investigate students’ engagement with their research drew upon the field
of rhetoric to identify linguistic markers of students’ engagement. This method further
defined ways in which students interacted with their sources by reacting to, challenging,
or comparing them. What these more qualitative approaches highlight is the range of
variables potentially left unexplored when deductively looking for evidence of
information literacy via methods like rubrics. While scores from Rosenblatt (2010) or
Carlozzi’s (2018) rubrics can measure successful synthesis, they are unable to explain
how students were synthesizing or integrating their sources. Bali and Ramadan (2007)
attempted to address this shortfall of the rubric method by developing a deconstructed
rubric. In Bali and Ramadan’s (2007) study of online discussions, they measure
engagement by coding for 22 binary variables that can then be analyzed separately or in
aggregate as an index for engagement.
More recent attempts to balance and extend the breadth of information literacy
research is evident in mixed-method approaches. Gammons and Inge (2017), for

1

For these authors, patchwriting was understood as citing text while only altering a few words from the
original text, and unincorporated references were ones that students inserted without much explication or
were disguised as their own conclusion (2017).

16

example, experimented with comprehensive mixed-method assessments of information
literacy that made use of citation analysis, rubrics, and qualitative coding practices to
paint a more accurate and balanced picture of students’ information literacy. Ludovico
and Wittig (2015) similarly balanced their analysis of synthesis with an evaluation of the
accuracy of citations. It is this very mixed-method approach that is adopted in this study.
As Neuendorf (2017) pointed out, one of the advantages to content analyses is their
ability to identify layers of variables that can either be universal, adopted from past
research, emergent, or medium-specific. Unlike rubric or citation analysis approaches,
this content analysis approach provides space for a more exhaustive list of pertinent
variables which, as these varied methods discussed in this chapter demonstrate, is
necessary to provide a more complete assessment of a complex skillset like information
literacy.
The presence of these methods, and the concerns they raise, highlight how many
variables and questions are left unaddressed by the pure citation analysis employed by
Robinson and Schlegl. These new findings and extensions of authentic information
literacy assessment, therefore, reopen the case of bibliographic requirements that was
seemingly shut when citation analyses provided such promising results of the practice’s
effectiveness.
Beyond the Bibliography
Apart from previously addressed critiques of citation analyses to provide a
comprehensive assessment of students’ information literacy skills, literature in the field
of library and information science also provided critiques of the pedagogical practices
17

and theory that seem to be at the heart of the bibliographic-centered model of information
literacy. It is more than just assessment methods that have shifted; definitions of
information literacy and what the priorities of information literacy instruction are have
also evolved. A hint of this change is even seen in Rosenblatt’s (2010) study, which
concludes with the question: “Shouldn’t we, as instruction librarians, be concerned about
students’ abilities to use the information they have discovered” (p. 60)? For Rosenblatt,
this new way of thinking about the priorities of the instruction librarian directly related to
their pedagogy, and they reported spending more time in instruction sessions modeling
integration of information sources rather than typical information retrieval skills (2010).
This is a noticeable departure from the concerns of authors like Davis and Baberino who
wanted to prioritize the use of library collections and databases. Margolin and Hayden
(2015) also adopted similar instruction priorities and reported their success and method in
developing an online information literacy toolkit that balanced bibliographic instruction
with modules that included rhetorical use, integration of sources, and the process of
developing research questions. Margolin and Hayden explained the reasoning for this
approach:
We see students struggle with all aspects of research, both mechanical and
the higher-order critical thinking. However, our most important goals are
the skills students develop in the research process, regardless of the final
product, and the experience of students as both information seekers and
content creators. It is these interconnected experiences that will benefit

18

students after graduation in the workplace, as global citizens, and in terms
of “’lifelong learning.’ (pp. 610-611)
Margolin and Hayden’s call to raise up students that are “content creators” likely echoes
the evolving definition of information literacy set out in the ACRL’s successor to the
Standards (2000), the Framework (2015). One of the dispositions listed in the
Framework says that “learners who are developing their information literate
abilities…see themselves as contributors to the information marketplace rather than only
consumers of it” (p. 6).
Indeed, the 2015 publication of the Framework is a pivotal point in which the
questions of librarians like Rosenblatt — questions about whether aspects of information
literacy beyond bibliographic skills are given their rightful attention — are officially
recognized and codified in this reiteration by the ACRL. The Framework introduces an
important new concept that was absent in the earlier Standards — the concept of
metaliteracy which requires more than just the affective performance of a set of discrete
bibliographic skills. Citing Mackey and Jacobson (2014), the ACRL explains that
metaliteracy expands the definition of information literacy to include how the student
participates in the information ecosystem and how he is aware of his own thought
processes and information use behaviors. More than simply introducing metaliteracy as a
new concept, the ACRL claims “[the] Framework depends on these core ideas of
metaliteracy, with special focus on metacognition” (p. 3). The Framework goes on to
describe the new definition offered:

19

Because this Framework envisions information literacy as extending the
arc of learning throughout students’ academic careers and as converging
with other academic and social learning goals, an expanded definition of
information literacy is offered here to emphasize dynamism, flexibility,
individual growth, and community learning: Information literacy is the set
of integrated abilities encompassing the reflective discovery of
information, the understanding of how information is produced and
valued, and the use of information in creating new knowledge and
participating ethically in communities of learning. (p.3)
Authors like Rosenblatt (2010), Scharf et al. (2006), Luetkenhaus, Borrelli, and Johnson
(2015) initially raised questions about the priorities of information literacy once they
uncovered that moving beyond the bibliography revealed challenges in higher-order skills
such as synthesis. They seem to have their concerns reflected in this new interpretation of
information literacy offered in the Framework.
Admittedly, the approaches proposed by Rosenblatt (2010) and Margolin and
Hayden (2015) are not entirely new. Khohl and Wilson (1986) advocated for instruction
sessions that shifted the focus to “[begin] with the student’s research question rather than
the library tool” (p. 210). It is in more recent years, however, that this conception of
information literacy seems to manifest itself in successful instruction models that relax
the traditional bibliographic focus. For example, Deitering and Gronemyer (2011) report
on their success using blogs (as opposed to a traditional emphasis on scholarly sources) to
help teach synthesis. These shifting goals and practices are also seen in Ludovico and
20

Wittig’s (2015) report the preliminary stages of a longitudinal study of first-year
students’ essays in which both librarians and composition faculty were cooperating on the
project. For this project, Ludovico and Wittig explain that the faculty’s instructional
concern was less about bibliographic skills and more about students’ use and
understanding of their sources. Ludovico and Wittig’s study and instruction sessions,
therefore, evaluated how students were using sources as opposed to whether they were
citing the right sources correctly (p. 33). Other examples of new content and synthesisoriented instruction models can be found in McClure, Cooke, and Carlin (2011); Downs
and Wardle (2007); and Darowski, Patson, and Helder (2017).
Furthermore, thanks to Bizup’s (2008) typology of information, some libraries are
beginning to propose a different framework by which information is defined not by its
format but by the way the student uses the information. According to Bizup, information
can be classified by whether it is used for background, exhibits, argument, or methods.
The difference between exhibit sources and argument sources is of particular interest to
this study. When students use sources as exhibits, they use them as pieces of evidence to
affirm their position. On the other hand, sources used argumentatively are characterized
by an exchange between the student and information source — the student can affirm,
refute, compare, or react to the source. In this typology, information sources are
completely conceptualized by a kind of relative ontology in which information is defined
by students’ engagement with it. Libraries such as Portland State University (2017),
Indiana University, Bloomington (2018), and the University of Texas (Grace, 2015) are

21

teaching and promoting this approach, which acts against the peer-reviewed/popular
source dichotomy observed by Head and Eisenberg (2010).
The Role of the Librarian
For Ludovico and Wittig (2015), these new methods of assessment and instruction
they were experimenting with – approaches that place rhetorical use of sources at the
center – necessarily beg a much larger question about the role of academic librarians and
the nature of the profession. Ludovico and Wittig asked whose role it is to teach students
about meaningful integration of information sources into their research. Is this the role of
the classroom instructor or do librarians share some of the responsibility? Should
librarians even concern themselves about anything beyond the students’ bibliographies,
or are the studies of authors like Rosenblatt and Carlozzi simply meddling in the concerns
of English and composition faculty? Bowles-Terry and Donovan (2016) take up the
argument that librarians have a much deeper well of expertise to offer students, but that
this is rarely tapped into because of the limited amount of time librarians have to interact
with students. Is it the case, then, that librarians’ influence on anything other than
bibliographic behaviors is a practical limit, or is it also a theoretical limit imposed
(possibly self-imposed) on librarianship? These challenges of where to draw the line
between librarian and composition instructor are not new, and Fister (1993) and Norgaard
(2003), for example, contend that, if a librarian neglects to teach how to use information,
the librarian does not effectively support to goals of the composition instructor.
In exploring what bibliographic requirements reflect about the nature of
librarianship, there is one final school of thought that must be mentioned — critical
22

information literacy theory. Quoting Accardi, Drabinski, and Kumbier (2010), Downey
(2016) defined critical information literacy as
‘…a library instruction praxis that promotes critical engagement with
information sources, considers students collaborators in knowledge
production practices (and creators in their own right), recognizes the
affective dimensions of research, and (in some cases) has liberatory aims.’
In addition, it must take into account the complex power relationships that
undergird all of information, including its creation, presentation, storage,
retrieval, and accessibility. (pp. 41-42)
While still closely aligned with the ACRL Framework’s (2015) understanding of
information literacy, critical information literacy, as explained above, emphasizes a
recognition of the role power structures play in information creation, organization, and
retrieval. This additional theoretical lens is offered here, in addition to the earlier
understanding of information literacy offered by the ACRL’s Framework, to propose yet
another question beyond efficacy. While likely unable to resolve the questions here, a
critical information literacy interpretation of bibliographic mandates raises a challenge
about how the mandates relate to librarians’ own exercise of power within the
information marketplace.
The challenge is best framed by a lesson offered by librarian and critical
information literacy theorist, David Patterson. Citing an account from Casson (2002),
Patterson (2009) provided the example of catalogs excavated from the earliest known
library in the ancient city of Nippur. What is learned from this artifact is that this early
23

ancestor of our modern libraries was wedded to the functions of that community’s
temple. In short, the catalog held there was designed to collect and provide efficient
access to “sanctified” texts — those texts approved for use by the priestly class. The
challenge for modern librarians to draw from this is: how much different are our current
academic libraries from the library at Nippur? As Patterson explained, “…the tablets of
Nippur indicate librarianship’s central contradictions: sharing and holding; aiding and
hindering; furthering research and, by designating certain information as privileged,
obstructing it.” (2009, p. 350). This reflection on librarianship – the ways in which
librarians have the power to perpetuate free access to information or obstruct it – is an
important idea to keep in mind if the ACRL Framework is to be taken seriously in its call
to reflexively “…examine [our] own information privilege” (ACRL, 2015, p. 6).
As a pedagogical application of critical theory more broadly, critical information
literacy intersects with conversations surrounding bibliographic requirements because of
its ability to extend the conversation beyond whether or not these practices are effective
pedagogically to what these practices say about the profession. As Downey (2016)
suggested, these practices might be both ineffective and oppressive because they
privilege (or as Patterson might say, “sanctify”) certain forms of information thus
disallowing appropriate space for students to deeply evaluate the political nature of all
information. The motivation to uphold our own professions’ information power – the
library collection – was a clearly stated motive behind Davis and Baberino’s
recommendation for mandating peer-reviewed sources. What, then, can be said about
these bibliographic mandates if they are viewed through the theoretical lenses offered by
24

the Framework’s self-reflective metaliteracy and critical information literacy theorists’
liberative mission for librarians?
In short, what the literature demonstrates is that moving beyond the bibliography
opens up new questions about the accuracy of information literacy assessments, which
information literacy outcomes are most important, how to design information literacy
instruction, and even the role of librarians.
With these voices and questions documented in the literature, it is important to
revisit Robinson and Schlegl’s 2004 study and investigate the role that mandating
scholarly sources has in an undergraduate student’s research assignment. This
investigation, however, will be revisited with the knowledge and understanding garnered
from the review of literature above. By adding a content analysis to the traditional
citation analysis employed by Robinson and Schlegl, this study hopes to provide an
update to their previous investigation that will more adequately speak to the more recent
concerns that go beyond the bibliography. To provide a more relevant answer to the
central question of whether bibliographic requirements are an efficacious pedagogical
tactic, the assessment and point of view must be updated to reflect the more than a
decade’s worth of developments and evolutions in the body of information literacy
literature.

25

CHAPTER III
METHODOLOGY
Research Questions and Hypotheses
To investigate how requiring students to use peer-reviewed sources influenced
their written product, the study used two phases: a citation analysis and a content analysis
of students’ information-use and synthesis of outside sources. The two major research
questions each guide the two different forms of analysis utilized in this study.
First Research Question and Hypotheses
The first research question directed the citation analysis portion of the study. Its
goal is to revisit the initial questions of Robinson and Schlegl (2004) regarding the
impact that requiring the use of peer-reviewed articles has on students’ bibliographies.
Specifically, hypotheses three and four directly retest their findings (although this study
hypothesizes different outcomes than these earlier researchers). Hypotheses one and two
were not directly tested by Robinson and Schlegl, but, given the observations of Jamieson
(2013) and Carlozzi (2018) about low in-text citation frequencies, they were added as
useful extensions to their original findings.

26

The first research question asks: Does explicitly requiring the use of peerreviewed sources impact the bibliographies and citation behavior of students? This
question is further refined by the following hypotheses:
H1. Students who received the bibliographic requirement to use peer-reviewed
sources will cite all of their individual sources less frequently within the body of
their essays than those without the requirement.
H2. Students who received the bibliographic requirement to use peer-reviewed
sources will cite their peer-reviewed sources less frequently within the body of
their essays than those without the requirement.
H3. Students who received the bibliographic requirement to use peer-reviewed
sources will cite fewer sources overall than those without the requirement.
H4. Students who received the bibliographic requirement to use peer-reviewed
sources will cite the same percentage of peer-reviewed sources as those without
the requirement.
Second Research Question and Hypotheses
The second question directed the content analysis phase of the study. This
question is framed by hypotheses about two variables: the frequency of direct quotations
and students’ synthesis of individual sources, which were highlighted in Jamieson (2013)
and Carlozzi (2018), respectively. The phrasing of the question also allows for an
inductive process – described as emergent variable identification by Neuendorf (2017) –
by which additional variables may be added before coding that can help answer the
overarching question in the second phase of the study.
27

The second research question asks: How do students in both groups (those with the
requirement and those without) utilize the information sources within their essays?
H5. Students who received the bibliographic requirement to use peer-reviewed
sources will directly quote their outside information sources more frequently than
those without the requirement.
H6. Students who received the bibliographic requirement to use peer-reviewed
sources will attempt to synthesize their outside information sources less
frequently than those without the requirement.
Data Collection
Setting Description
The study examined unit essays completed by students enrolled in the same
professor’s online sections of an undergraduate political science course at a public
university in Texas. The course sections are referred to as Section 50 and Section 51
within this report.
Before the semester, the Department Chair provided approval for the altered
syllabi and assignment instructions for Sections 50 and 51. While the two sections, taught
by the same professor, would remain the same in design, structure, and pace, Section 50
had its bibliographic requirements relaxed. Outside of this experiment, it was the
instructor’s custom to require their undergraduate students to complete research
assignments with the mandate to have at least 2-3 peer-reviewed articles. For the
experiment, the instructor modified Section 50’s assignment instructions and syllabus to
remove the mandate for peer-reviewed articles. Students, instead, were provided a more
28

open-ended and vague expectation to make use of “appropriate sources.” This allowed
for Section 50 to serve as a control group to which Section 51 could be compared. The
modification can be seen in Appendix D, which contains the pertinent portion of both
sections’ syllabus.
In addition to the mandate, students in both sections received similar in-class
messages about outside research. The instructor encouraged students to make use of the
academic library and the subject librarian if they needed assistance. The instructor also
recommended JSTOR and Academic Search Complete as ideal databases for searching.
While students received similar in-class messages and recommendations about finding
information for their articles, ultimately one group of students was given the restrictive
mandate to find a certain kind of source, thus limiting their freedom to interpret and use
sources they deemed appropriate.
By the time of data collection, each section had an equal number of students
enrolled – 40 students enrolled in p 50 and 40 students enrolled in Section 51. During the
course, students were expected to complete two of four unit essays. Each unit essay had
its own set of prompts and due dates. Students were expected to decide which two essays
they would complete and then decide on a prompt for that unit. To remain as consistent
as possible, this study only analyzed papers from one unit. Essays were collected from
Unit Three. The rationale behind this decision was that units one and four had the greatest
risk of skewing results. Unit One may capture poor first attempts or early submissions of
particularly engaged students, and Unit Four may capture hurried papers at the end of the
semester. Units Two and Three, it was reasoned, would allow for a better snapshot in the
29

middle of the semester. Unit Three was selected because, next to Unit Four, it contained
the most submissions. Overall, 45 essays were submitted for Unit Three. The Unit Three
essays asked students to write about one of three topics: political socialization [Topic 1],
biases in news reporting [Topic 2], or the role of the internet in political campaigning
[Topic 3]. Excerpts of the prompts can be found in Appendix E. Despite the different
topics, the sample remained fairly homogenous, with the majority of students writing
their essays in response to Topic 3. Furthermore, while the topics were different, the
parameters and expectations about finding outside research remained the same between
the topics. A complete breakdown of essays submitted is found in Table 1.
Table 1
Total Documents Submitted
Section 50
Section 51
Topic 1
4
2
Topic 2
4
5
Topic 3
16
13
Total
24
20
*The total number of papers submitted was 45, but one student in Section 51
opted out of the study, reducing the sample size to 44.
The course targeted for this study is the first course in a required, 6-credit hour
sequence of government/political science courses within the core curriculum. While
unable to control for all the confounding variables possible in the educational setting, this
study, building off of Rosenblatt (2010) and Carlozzi (2018), relied on the control of
available instructional variables (such as professor, syllabus, and assignments) and the
assumption of similarity between students represented in two sections of a required
undergraduate course.

30

Human Subjects Protection Procedures
Confidentiality. To protect the confidentiality of students’ work, the course’s
graduate assistant was used to collect the essays and de-identify them before being
transferred to the principal investigator. The graduate assistant, along with the course
instructor, already had access to the essays, but, since the course instructor graded the
unit essays, it was important that the graduate assistant collect and de-identify the essays
on behalf of the principal investigator. This way, the principal investigator and additional
rater only had access to anonymized essays and bibliographies, and the instructor had no
way of knowing who participated in the study and who did not.
Recruitment and IRB approval. The collection procedures employed a passive
consent (or “opt out”) process that was approved and classified as non-exempt by the
Intuitional Review Board (see Appendix H). This passive consent option made it possible
for no personal information to be collected along with participants’ essays. Before
students began writing their essays, the instructor posted an IRB-approved recruitment
notice on both course sections’ Blackboard page as an announcement; an email to
student’s institutional email accounts was automatically generated in addition to the post
on their learning platform. The notice can be found as Appendix F. The notice contained
a hyperlink as well as PDF attachment of the full study description and passive consent
form (see Appendix G). Students who did not wish to volunteer their essays were
directed to follow the link where they could submit their name via a form hosted on
PsychData. This would act as notice that they did not provide their consent, and their
essays were not collected as part of this study. All other students’ consent was assumed,
31

and their essays were collected. The link remained active and available so that students
could opt-out of the study at any time.
The graduate assistant was provided the names of the students who submitted optout forms. This allowed the graduate assistant to remove the students’ essays before deidentifying and transferring the remaining volunteered essays. Of the 80 students enrolled
in the course, only two elected to opt out. Of those two, only one submitted a unit three
essay. Therefore, 44 of the total 45 Unit Three essays were collected to make up the
sample used in this study. The data collection was also delayed until all grading of the
unit three essays was complete. This allowed the principal investigator and instructor to
reassure students further that their grades and standing would not be affected by their
participation in the study. De-identified essays were then transferred from the graduate
assistant to the principal investigator by using both individuals’ cloud-based, institutional
Microsoft OneDrive accounts. A shared folder was created with access provided only to
the graduate assistant and principal investigator’s password-protected accounts. Files
were then uploaded into this file and separated into subfiles for their respective course
sections. The principal investigator then renamed each anonymous document with a
unique paper identification number for the purposes of the study.
Data Analysis
Citation Analysis
For the citation analysis, each outside information source cited in the
bibliographies was viewed as a unit of investigation. Each entry was assigned its own
unique identification number in the same way each essay was assigned a document
32

identification number. A citation analysis checklist was developed to categorize each
entry as either a peer-reviewed/scholarly source (coded as a 1) or not (coded as a 0). This
portion of the checklist adapted definitions and parameters used by Robinson and Schlegl
(2004) and Rosenblatt (2010). The second variable measured by the citation analysis
checklist was the number of times the source was referred to in the body of the essay. For
this variable, the definition for what constituted an in-text reference was adapted from a
coding glossary compiled by Jamieson and Howard (2011) and provided under an
Attribution-Noncommercial-ShareAlike Creative Commons license (CC BY-NC-SA 3.0
US). The full citation analysis checklist is included as Appendix A.
Measurements of the accuracy of citation are intentionally missing from the
citation analysis checklist. The rationale behind this decision was primarily theoretical.
While a citation analysis phase was still used for this study to test Robinson and Schlegl’s
2004 conclusions, the addition of the second phase – a non-bibliography-centered content
analysis – demonstrates a theoretical move that is consistent with the latter body of
research reviewed in Chapter II. While the count, type, and in-text citation frequency of
bibliographic entries were still included in the analysis, any further bibliographic-centric
measures were left out in an attempt to not overpower the second phase of analysis and to
permit a focus on source integration.
Before coding the bibliographies, the citation analysis checklist was normed and
checked for reliability using a second, outside rater. An outside librarian was approved by
the TWU IRB to serve as the second rater for the purposes of inter-rater reliability tests.
The complete sample of documents was randomized to collect a 10% subsample of
33

documents for the inter-rater reliability test. As Neuendorf (2017) pointed out, there is no
universal standard for the size or sampling method of reliability subsamples, but it is still
observed that 10% to 20% appears to be a commonly used range. Five randomly selected
papers (just over 10% of the total 44-document sample) were coded separately by the
principal investigator and the additional rater. Holsti’s (1969) method for calculating
agreement between two coders was used to calculate percent agreement for each variable
according to the following formula:
PAO = 2A/(nA+nB)
For the number of times a source is referenced within the text of the essay, the
first coder coded 25 instances whereas the second coder coded 24 instances. Of those,
they agreed on 21 codes. Following the above formula for crude agreement, the number
of times the coders agreed (21) is doubled then divided by the sum of codes applied by
each individual coder (the sum of 25 and 24). This yields an 85.7% agreement. For the
variable of whether a source is peer-reviewed/scholarly or not, each coder coded 15
cases. They agreed 15 times, rendering 100% agreement. Stemler’s (2004) rule of thumb
for crude or percent agreement was used as a benchmark in which the range of 75% to
100% is deemed an acceptable PAO value. With the citation analysis checklist rendering
acceptable percentages of agreement, the remaining bibliographies were coded by the
principal investigator.
In all, 142 outside information sources were coded. 25 of the 142 were coded but
then set apart from the dataset because they were required sources. The essay prompts
provide some outside information that students were already expected to use in addition
34

to their outside research. While still coded in the phase one process, they were then
removed from the dataset and no longer a part of future analysis. The rationale for this
was that the true scope of this study was to investigate what sources students were
finding and using as additional research to incorporate into their essays, not information
already given to them in the assignment or class material. Given the research questions at
hand and what is already known from Carlozzi’s (2018) findings that suggest student use
in-class readings more than outside research, it seemed reasonable to not include these 25
sources for further analysis. This decision left a remainder of 117 sources in the sample.
Content Analysis
For the second phase of the study, a quantitative content analysis was conducted
on the body of the essays. The content analysis was primarily concerned with examining
the synthesis of in-text citations and how students were integrating outside information
sources. For the content analysis, this study relied heavily upon the procedures outlined
and discussed in Neuendorf’s second edition of The Content Analysis Guidebook (2017).
The content analysis codebook was modeled off definitions and codes found in
Jamieson and Howard (2011), Rosenblatt (2010), Carlozzi (2018), the AAC&U (2007b),
and Kanter (2006). Several layers of variables were included. The medium-specific
variables available for coding were the number of in-text citations, the absence of a
bibliography page, and missing bibliographic entries. Variables identified from previous
research are those that relate specifically to hypotheses five and six – direct quotations
and synthesis. The codebook used dictionary-like entries for each variable that outlined in
detail what could be considered a direct quotation or attempted synthesis. Direct
35

quotations were coded if they appeared alongside an in-text reference already identified
in phase one. The total number of direct quotations was recorded for each paper.
Based on the narrative described in the literature review concerning the rarity of
fully-synthesized sources, synthesis was treated as a binary variable in the coding
process. Attempted synthesis was coded as a 1 and defined as an instance in which the
information from a source is used with some clear interpretation or connection by the
student. The codebook pointed to occasions when the author usually used the information
source to create something new or to add to new conclusions. A failed attempt (or no
attempt) at synthesis (coded as a 0) was defined as an instance in which there was no
clear attempt on the part of the student to synthesize or integrate the information from the
outside source. The codebook further defined it by instances in which the author simply
quotes or uses the outside source as evidence or a “stand-in” for the student’s own claims
or words. No analysis nor new conclusions were created by the author in using these
sources. These references appeared in isolation or were abrupt with little or no
discernable connection.
An example of an instance in which a reference was coded with a 1 for synthesis
(meaning synthesis was attempted) by both coders in the inter-rater reliability test was
found in document number 5104:
However, an article about online campaigning from the Netherlands state
that ‘software such as social media dashboards bring the websphere to the
fingertips of campaign managers,’ meaning that a campaign manager is
able to fine-tune their candidate’s campaign and perform damage control
36

almost instantaneously (Vergeer 14). For example, when it comes out that
a candidate was previously arrested as a teen, it is much quicker for said
candidate to come out with a statement on Twitter or Facebook than it is to
put together a big press conference.
The above segment from the sample serves as a useful example of the characteristics of
references coded as a 1 for synthesis. The references in this case were not used in
isolation, and the student made a clear attempt to connect the information to his or her
thesis. It was clear why the outside information was referenced thanks to the further
interpretation and conversation the student built around the reference in her or his own
words. On the other hand, in document 5108, both coders applied a 0 for synthesis to the
following reference that comes at the end of a paragraph:
For future elections I believe that the internet will continues to be used and
that more of the public will be reached. ‘The evidence so far indicates that
new ICTs may be increasing the divide between a largely passive
membership and new set of hyper-activist elites’ (Gibson, 186).
No further explanation is provided even though new information about the divisive nature
of information communication technology is introduced in the reference that was not
discussed anywhere else in the essay. No connection or interpretation of the new
information was attempted, and it is not entirely clear how this new information supports
the student’s claim in the previous sentence. This instance was not unlike many of the
other instances in this sample that were coded with a 0 for synthesis.

37

Last, the codebook’s final variables were constructed from an emergent process.
While Neuendorf (2017) described this method of variable identification as one that
temporarily betrays the hypothetico-deductive rigor of quantitative content analysis, it is
nonetheless a sometimes useful step by which the researcher immerses her or himself in
the raw data. This process was done to identify any remaining variables for the content
analysis. The principal investigator read all papers three times. While reading the
document, the principal investigator kept notes of themes that seemed to recur in the
sample or stand out because of their rarity. This process led to the identification of six
additional variables. Out of these six variables, two related to features of writing more
broadly – unsupported statements and opinion statements. Unsupported statements were
defined as any statement in which a fact or exhibit was presented but the author provided
no citation or reference to corroborate or support the statement. This was a theme that
emerged frequently from the inductive variable-identification process, and, given its clear
relation to information literacy, was added as a code that could be applied throughout a
students’ essay (not just at the point of in-text citations). For example, during the interrater reliability test, both coders coded the following as an unsupported statement found
in document 5108:
Recently, facebook came up with a way to categorize individuals into
political groups based on the persons facebook activity. If you are
categorized into one of these groups, facebook only shows you
information such as events or new articles for that political group.

38

While it might be common knowledge that social media sites like Facebook make use of
recommender systems to curate content, this claim about Facebook categorizing users by
political party and catering content accordingly would be well-served by a citation to
indicate where this student learned that information. The students’ use of “recently”
seems to suggest this is new behavior on the part of the social media giant that would
have been reported, and, if so, that report should have been referred to here to support the
student’s claim.
The second variable in this class was the theme of personal opinions, which also
emerged frequently in the data. This was defined as any sentence in which the student
clearly states her or his own personal opinion. These sentences had to be in the
subjunctive tense (e.g. must, should, ought) or signaled by the phrases “I think” or “I
believe.” This theme was coded because it provided an additional layer to our
understanding of engagement. While the connection to information literacy is not as
clear, this assertion of one’s opinion does demonstrate the introduction and use of an
information source – the student herself. Often, its departure from the “academic thirdperson” coupled with its frequency in a given essay is what made this variable stand out
in the inductive variable-identification process. Again, document 5007 provides two
clear examples of this code that were agreed upon by both coders in the inter-rater
reliability process: “I believe this age category is mainly concentrating in the education
and their future careers. I personally think blogging and live chat sessions can be
effective.” Kanter’s (2006) definitions for linguistic markers of engagement were referred
to for refining the definitions for the aforementioned writing behavior variables.
39

Out of the six emergent variables, four were identified as behaviors that cooccurred with synthesized references, and they helped to provide more explanatory power
to the variable of synthesis. These variables acted as categories to describe how the
source was synthesized or integrated, rather than a binary indication that it was
synthesized. The four categories were interpretation/explanation, reacting, scrutinizing
source, and acknowledging scholarly exchange. These four variables were referred to as
engagement categories and could only be coded to in-text references that received a 1 for
synthesis. Again, Kanter’s (2006) rhetorical definitions for writing engagement proved
useful in fine-tuning the definitions for these variables.
The interpreting/explaining category was coded when the student provided some
further explication of the information cited from the source. This came in the form of
explaining how the source relates to or supports the argument, explaining or breaking
down the information cited, or the student providing his or her own interpretation of the
information. An example of this kind of synthesis was found in document 5009 when the
student writes:
Socioeconomic status is linked heavily to having the access to higher
education, as well as access to the ability to afford to pay more in
taxes…If someone is unable to pay more taxes because of their income,
they have a higher tendency to avoid voting for someone that would want
to raise the tax brackets.
The students’ additional explication of the source helps to connect the information cited
to the larger point the student was attempting to make in her or his essay. In this way, the
40

information cited did not stand on its own but was situated within the essay by the
students’ own words and interpretation.
The code for reacting was applied when students provided a response to outside
information. This was seen as the students’ own response to the information such as
affirming the information, refuting it, judging it, extending it, or asking follow-up
questions of the information. It situates the student in conversation with the information
source. An example of this code can be found in document 5102 when the student writes:
In ‘Rethinking Youth Political Socialization,’ Gordon and Taft say that we
need to ‘encourage youth to participate in explicitly less critical or
dissident forms of political activity than those practiced by most of the
youth activists we study.’ While this is true, it can be much easier to
participate in things that are much more public and hear about such as
marches and protests.
In this instance, the student enters into conversation with the authors by affirming their
conclusion while providing her or his own take on the issue. The student participates by
positing the counterpoint that critical or dissident political activity might be more alluring
to young people because of its high-profile, public coverage.
The code for scrutinizing a source was applied whenever a student critically
analyzed an outside source’s currency, authority, accuracy, or purpose/bias. Both coders
in the inter-rater reliability process agreed that the following was an example of this
category of engagement in document 5104: “According to research I found, television is
‘still the most important source of campaign news’ (Vergeer 10). However, as this study
41

was done in 2012, it is evident that a lot has changed since then.” In this instance, the
author casts doubt on the information offered by the outside source because of its lessthan-ideal currency.
Last, the code for acknowledging scholarly exchange was applied to instances in
which a student writes about outside information sources in a way that recognizes
“...source authors as people” who are engaged in ongoing scholarly communication
(Kanter, 2006, p. 279). This type of engagement could be coded when the student drew
comparisons between two outside information sources or how two outside sources might
affirm or contradict one another — thus drawing the reader’s attention to the scholarly
exchange going on between outside sources. Whereas the reacting category might be seen
as the student entering into the conversation with sources, this code is applied to
instances in which the student highlights the conversation that happens in scholarship
between outside information sources. An example of this is seen in document 5002 when
the student writes:
After being encouraged by the executives of these companies, he ‘made up
his mind’ to raise two tariffs, but only after their talks (Ball). This
challenges the Post’s arguments because it demonstrates the president was
not acting without thought and without purpose.
In this instance, the student creates space within her or his essays for a debate to occur
between two outside information sources, demonstrating how one provides an opposing
viewpoint to the other.

42

The codes for each variable were annotated within the documents using Microsoft
Word, and aggregate records of each paper’s codes were recorded on a record sheet
before being entered in SPSS. The final codebook, complete with full definitions of all
variables, is included as Appendix C, and the accompanying record sheet is included as
Appendix B.
Once the variables were identified and the codebook established, an inter-rater
reliability test was conducted before coding commenced. The same outside rater from the
first phase served as the additional rater for the second phase. Another 10% subsample
was collected using a method of sampling that Neuendorf (2009) describes as a “rich
range” subsample. In this process, the subsample documents are identified by their ability
to provide ample “testing-grounds” for most (if not all) of the variables. This is an
important process for ensuring that all variables in the codebook (including the rare
variables) are given an opportunity to be tested for inter-rater reliability. Given what is
known from previous research about synthesis success rates, for example, one could
reasonably expect that a simple random subsample might deliver documents with very
little opportunities to code for synthesized texts. Therefore, the subsample was collected
purposively, and documents were selected according to their suspected ability to provide
enough opportunities for all vital variables to be coded and tested.
The results from the variables’ inter-rater reliability tests following Holsti’s
(1969) method are shown in Table 2. Again, following Stemler’s (2004) suggested
benchmark, no single variable had a less than 75% agreement. The aggregate

43

44

score of all variables is 93.24%. The weakest variable in terms of agreement was
unsupported statements (PAO = 78.95%) in which Coder B applied the code to the text
more frequently than Coder A. In all, the inter-rater agreement tests yielded promising
results concerning the reliability and objectivity of the definitions set forth in the
codebook. No second test or revisions seemed necessary given the percent agreements,
and the remaining documents in the full sample were coded by the principal investigator.
Statistical Procedures
Statistically comparing the means of dependent variables for both Section 50 and
51 was necessary to properly answer the hypotheses for both research questions. IBM’s
statistical software, SPSS, was used for the statistical analyses described in this study.
Each of the six hypotheses ask questions about a different dependent variable, and only
one of the six variables could be measured by using raw numerical data collected from
the content or citation analysis. The remaining five would require an additional
transformation (mostly in the form of an average or proportion) before analysis could
begin. This was notably the case for Hypotheses One, Two, Five, and Six which ask
about the frequency of all sources’ in-text citations, the frequency of peer-reviewed
sources’ in-text citations, the frequency of direct quotations, and the synthesis scores,
respectively. Averages were needed because these frequencies were measured at the
reference level, whereas the hypotheses ask questions at the document level. The true
subject of the study is not individual references but the individual documents/students.
Therefore, reference-based measures were averaged for each paper to provide a single
measure of these dependent variables for the paper rather than the sources. Table 3
45

describes the dependent variables used for analysis and the necessary transformations
from raw data that were made.
Table 3
Dependent Variables
Raw Data

Transformation
Made

Dependent Variable
for Analysis

Hypothesis 1

Frequency of in-text
citations per
information source

In-text citations for
each paper were
divided by paper’s
total number of
information sources

Average in-text
citation frequency per
paper

Hypothesis 2

Frequency of in-text
citations per peerreviewed source

In-text citations for
each paper’s peerreviewed sources
were divided by the
paper’s total number
of peer-reviewed
sources

Average in-text
citation frequency of
peer-reviewed
sources per paper

Hypothesis 3

Total sources cited
per bibliography

None required

Total sources cited
per bibliography

Hypothesis 4

Number of peerreviewed and nonpeer-reviewed
sources per paper

Number of peerreviewed sources for
each paper was
divided by paper’s
total number of
sources

Percentage of a
paper’s bibliography
that consists of peerreviewed sources

Hypothesis 5

Frequency of direct
quotes per paper

Number of direct
quotations was
divided by paper’s
total number of intext references

Percent of in-text
references that are
direct quotes

Hypothesis 6

Classification of
synthesis per in-text
reference

Total synthesis
count was divided
by number of in-text
references

Average synthesis
score per paper

46

Given the small sample size, normality and outliers would be the first concern for
analysis of the variables mentioned above. Not surprisingly, none of the variables
demonstrated a normal distribution. To confirm this, a Kolmogrov-Smirnov test for
normality was run on each of the 6 variables, with each reporting a statistically
significant result (p < .05) which means that the null hypothesis of normality was
rejected. Furthermore, analysis of graphical representations such as histograms and boxplots revealed little normality and multiple outliers, respectively. Therefore, it was
decided that the most conservative approach was to analyze these variables nonparametrically using the Mann Whitney U two-sample test. With the Mann Whitney U,
however, it should be noted that some information is lost in that the ranks of the variable
results are what are tested as opposed to the raw data. Even still, in the face of abnormal
data from a limited sample size, the Mann Whitney U still allowed the hypotheses to be
answered by indicating the presence of significant difference between the two groups’
distributions.
For additional data collected in the content analysis – the variables beyond those
questioned by the deductive hypotheses – simple descriptive statistics were used to
provide numerical observations about themes that emerge from the data and inform the
second research question. Questions about correlations were also analyzed. For instance,
was there any correlation between a paper’s average synthesis score and its percentage of
scholarly sources? All these correlative questions would have been appropriately
investigated using a Spearman’s rank-order correlation that is best suited for abnormally
distributed data. However, this test was not employed because graphical representations
47

of the correlations (scatter plots) as well as measures of co-occurrences did not display
the prerequisite monotonic pattern. In all, six additional relationships were observed
using scatter plots, co-occurrence counts, and measures of overlap using Jaccard’s Index
to investigate possible correlations: average synthesis score and number of sources,
average synthesis score and percentage of scholarly sources, average synthesis score and
number of unsupported statements, average synthesis score and number of opinion
statements, average synthesis score and percent of direct quotations, and number of
unsupported statements and opinion statements. None of the six displayed a monotonic
relationship, and, therefore, the Spearman’s rank-order correlation was not warranted nor
viable for this study. All additional numerical observations remained at the descriptive
level, which would allow the results to provide helpful descriptions about writing or
information-use behaviors while stopping short of correlative or inferential statistics.

48

CHAPTER IV
RESULTS
Phase One: Citation Analysis
Descriptives
From the 44 essays collected, 142 outside information sources were coded. As
explained in Chapter III, however, only 117 of those outside sources were selected for
analysis because the 25 set aside were already given by the assignment and therefore not
a part of the additional sources students were expected to research. The mean, median,
and standard deviation for the number of sources found in each group are given below:
Table 4
Descriptives: Total Number of Sources
N
Mean

Median

Standard Deviation

Section 50

24

2.79

2.50

1.285

Section 51

20

2.50

2.00

1.539

What is immediately telling about the descriptives for the count of sources is how closely
the sampled documents conformed to the minimum requirement of 2-3 sources. A
strikingly small portion of the sample seemed to venture past three sources. This is seen
well in the histograms of the total number of sources for each section in Figure 1 and
Figure 2.

49

Figure 1: Histogram of total number of sources for Section 50

Figure 2: Histogram of total number of sources for Section 51

50

The second variable of interest in the citation analysis was the category a source
fell into: peer-reviewed or not. In all, 76 of the 117 information sources were marked as
peer-reviewed with 39 from Section 50 and 37 from Section 51. As described in Chapter
III these counts were then divided by the total number of sources in each paper to provide
a percentage that describes what share of a paper’s bibliography is made up of peerreviewed sources. For Section 50, the mean percentage of scholarly sources per paper
was 56% and the mean for Section 51 was 75%.
The third variable of interest in the citation analysis was the frequency with which
a source was cited. There were, in all, 129 in-text references – 60 in Section 50 and 69 in
Section 51. Since a paper would have multiple information sources, an average frequency
of in-text citation was found by dividing the paper’s total in-text citations by the total
number of information sources. This average would more accurately describe the
frequency with which a student tended to cite an individual information source. For
Section 50, the mean of all its papers’ in-text frequency averages was 0.91. For Section
51, the mean of all its papers’ in-text frequency averages was 1.22. At first glance, what
both means point to is the fact that the students in the sample tended to refer to an
individual source only once within the text. This tendency can be seen graphically in the
histograms for the average in-text citation frequencies of both sections shown in Figure 3
and Figure 4. This variable was further subdivided by peer-reviewed and not peerreviewed. The same frequency was rendered for peer-reviewed articles only. The mean
frequency with which papers in Section 50 referred to their peer-reviewed sources only
was 0.75 while it was 1.31 for Section 51.
51

Figure 3: Histogram of average in-text reference frequency for Section 50

Figure 4: Histogram of average in-text reference frequency for Section 51
52

Hypothesis Testing
There were four hypotheses associated with the citation analysis. The rank
distributions of each dependent variable were analyzed for statistical difference (i.e., a
two-tail test) using the Mann Whitney U test which provides an alternative to two-sample
t-tests in the face of small, abnormally distributed samples. Below are four sections
restating the hypothesis and reporting the results of the respective Mann Whitney U test.
Hypothesis 1. Students who received the bibliographic requirement to use peerreviewed sources will cite their individual sources less frequently within the body of their
essays than those without the requirement.
Using SPSS, a Mann Whitney U test was run to test the null hypothesis that the
distribution of a paper’s average in-text citation frequency is the same across the two
course sections. Here, it should be noted that the null hypothesis does not match the
original hypothesis. The Mann Whitney U is a two-tailed test so the restated alternative
hypothesis is more accurately worded: Students who received the bibliographic
requirement to use peer-reviewed sources and students without the requirement will
differ in the frequency with which they cite their individual sources in their essays.
The total sample size differed for this test. One case had to be excluded because
the paper lacked a bibliography therefore the full citation analysis was not able to proceed
for that case. The test was run at the 95% confidence interval, and the results are reported
below in Table 5.

53

Table 5
Hypothesis 1 Mann Whitney U Results
Section 50
Section 51
Section 50
N
N
Mean Rank
24

19

20.04

Section 51
Mean Rank

Asymptotic
Test Statistic Significance

24.47

275.00

0.217

The resulting asymptotic significance from the Mann Whitney U was greater than the set
significance level (α = 0.05), so the decision is to retain the null hypothesis and to
conclude that there is no statistical difference between the two groups’ distribution for
average in-text citation frequency.
Hypothesis 2. Students who received the bibliographic requirement to use peerreviewed sources will cite their peer-reviewed sources less frequently within the body of
their essays than those without the requirement.
As with the first hypothesis, this hypothesis will again need to be restated to
correspond to the null hypothesis tested in the Mann Whitney U. For the test procedures
used, the alternative hypothesis is more accurately stated: Students who received the
bibliographic requirement to use peer-reviewed sources and those without the
requirement will differ in the frequency with which they cite peer-reviewed sources in
their essays.
The sample size tested for this dependent variable will again differ from the total
sample size. The question here is about the frequency with which students cited their
peer-reviewed sources; this necessarily means that we were not concerned with the intext citation frequencies of those students who did not use any peer-reviewed sources. In
fact, citation analysis revealed that there were five (or 20.83%) papers that included zero
54

peer-reviewed sources from Section 50 and 2 (or 10%) papers that included zero peerreviewed sources from Section 51. The test was run at the 95% confidence interval, and
the results are reported below in Table 6.
Table 6
Hypothesis 2 Mann Whitney U Results
Section 50
Section 51
Section 50
N
N
Mean Rank
19

18

15.00

Section 51
Mean Rank

Test
Statistic

Asymptotic
Significance

23.22

247.00

0.013

The resulting asymptotic significance for this dependent variable (p = 0.013) was less
than the significance level (α = 0.05) which means that the decision is to reject the null
hypothesis and to accept the alternative hypothesis that there is a difference in the two
groups’ distributions for the average in-text citation frequency of peer-reviewed sources.
While the nature of the Mann Whitney U is two-tailed, when presented with a significant
difference, the mean ranks can reveal the direction of the difference. With a mean rank of
15 for Section 50 and a mean rank of 23.33 for Section 51, the difference is that Section
51 had higher-ranking results and tended to cite their peer-reviewed sources more
frequently. The ranked distributions of these two groups, and their difference, can be seen
graphically in Figure 5.

55

Figure 5: Distributions and rank of average peer-reviewed in-text citations

The originally-stated hypothesis expected that students who received the bibliographic
requirement would use their peer-reviewed sources less frequently than their peers who
did not receive the requirement. The results from this test conclude the opposite –
students in Section 51, who received the requirement to use peer-reviewed sources,
referred to their peer-reviewed sources in their essays more than their peers in Section 50.
Hypothesis 3. Students who received the bibliographic requirement to use peerreviewed sources will cite fewer sources overall than those without the requirement.
The Mann Whitney U test for this hypothesis was run at the 95% confidence
interval to test the null hypothesis that both groups’ distributions are the same for the
total number of sources cited by each essay. In all, 43 documents were included in this
test with one case excluded because of the absence of a bibliography. The results are
displayed in Table 7.

56

Table 7
Hypothesis 3 Mann Whitney U Results
Section 50
Section 51
Section 50
N
N
Mean Rank
24

19

23.08

Section 51
Mean Rank

Test
Statistic

Asymptotic
Significance

20.63

202.00

0.492

With an asymptotic significance less than the set significance level (α = 0.05), the
decision is to retain the null hypothesis that the distribution of the total number of sources
is the same across both groups.
Hypothesis 4. Students who received the bibliographic requirement to use peerreviewed sources will cite the same percentage of peer-reviewed sources as those without
the requirement.
For this hypothesis, its original wording adopts the stance of the null hypothesis
that there is no statistical difference between the two group’s inclusions of peer-reviewed
sources. This hypothesis is what is directly tested by the Mann Whitney U. As described
earlier in this chapter, the mean percentage of scholarly sources per paper did differ
between the two groups with an average 56% of Section 50 papers made up of peerreviewed sources versus an average of 75% of Section 51 papers made up of peerreviewed sources. The Mann Whitney U test, however will clarify whether these two
groups’ overall distributions are statistically different from one another. Again, the case
with a missing bibliography was excluded, leaving a total sample of 43 documents across
both groups. The results of the test are represented in Table 8.

57

Table 8
Hypothesis 4 Mann Whitney U Results
Section 50
Section 51
Section 50
N
N
Mean Rank
24

19

18.60

Section 51
Mean Rank

Test
Statistic

Asymptotic
Significance

26.29

309.50

0.038

The results of this test reveal that there is a statistically significant difference in the
distributions, and the decision is to reject the null hypothesis that the distributions of the
percentage of scholarly sources per paper are the same across both groups. These
differing distributions can be seen in Figure 6.

Figure 6: Distributions and rank for percent scholarly

As for the direction of the difference, with a mean rank of 18.60 for Section 50 and a
mean rank of 26.29 in Section 51, the difference is that papers collected in Section 51
contained a greater share of peer-reviewed sources than those in Section 50. The
graphical representation in Figure 6 illustrates well the high frequency of papers in
Section 51 (the section which received the bibliographic requirement) that contained
58

bibliographies consisting of 100% peer-reviewed articles. Section 50’s second-most
prominent modal class, by comparison, fell at 0% peer-reviewed. This test confirms that
the average percentages for groups 50 and 51 – 56% peer-reviewed and 75% peerreviewed, respectively – does represent a significant difference between the two groups.
Phase Two: Content Analysis
Descriptives
The primary job of the content analysis was to measure successful synthesis of
students’ references to the outside sources they found. Of the 44 essays collected, 122 intext references to outside sources were coded. Since some information sources were
referred to in the essays more than once and others not at all, the sum of a source’s
synthesis scores for each of its in-text references was averaged to provide each of the
outside information sources with an average synthesis success rate (ranging from 0 to 1).
For those sources that were not referenced at all, they were excluded from the record of
average synthesis score. The descriptives for both group’s average synthesis score per
information source is listed in Table 9.
Table 9
Average Synthesis Descriptives
Missing N

Valid N

Mean

Median

Standard Deviation

Section 50

8

52

0.32

0.00

0.45

Section 51

18

44

0.36

0.00

0.45

Total

26

96

59

At first glance, the descriptives for this variable tell a story of the rarity of outside
information sources that were successfully integrated or synthesized within the text. For
both groups, the means reveal that less than half of the references coded per paper were
synthesized within the essays. The counts for this variable are best described using a
crosstab table (see Table 10).
Table 10
Crosstab of Synthesis
No Synthesis (0)

Attempted Synthesis (1)

Total

Section 50

41

19

60

Section 51

40

22

62

Total

81

41

122

While a formal non-parametric test for significant difference between the two groups will
be carried out, initial analysis from the descriptives reveal striking homogeneity between
the two groups. For the most part, the theme appears to be that, for every attempt at
synthesizing an outside information source, there are two instances in which there was no
discernable attempt.
To test the variable of synthesis at the appropriate subject level – the individual
papers – the variable above was averaged for each paper to deliver a single indicator of a
papers’ synthesis. For Section 50, the average score per paper was 0.30, and for Section
51 the mean was similar at 0.29.
The second dependent variable that was initially identified based on past research
was the frequency of direct quotations. Of the 122 references, directly quoting a source
was used as a method to refer to a source 60 times across both groups – 26 times in
60

papers from Section 50 and 34 times in papers from Section 51. Like most of the other
variables in this study, a transformation was required to move this variable from the level
of individual reference to the level of interest – the individual essay that is the true
subject of the study. Therefore, the dependent variable in question is the percentage of a
paper’s in-text references that were also direct quotations. This acts as a proportion,
which, at the essay level, allows one to observe the degree to which a student relies on
direct quotation as a method to introduce outside information. For papers in Section 50,
the mean was 13.56% and for students in Section 51, the mean was 10.29%.
The following sections will report on the Mann Whitney U tests of both of these
dependent variables across Sections 50 and 51. Additional descriptive statistics and
numerical observations from the inductive portion of the content analysis will conclude
this chapter.
Hypothesis Testing
Hypothesis 5. Students who received the bibliographic requirement to use peerreviewed sources will directly quote their outside information sources more frequently
than those without the requirement.
The Mann Whitney U test, which is reported below in Table 11, tested the null
hypothesis that there is no difference in the distribution of papers’ direct quotation
percentage across Sections 50 and 51. Three cases were excluded from Section 50
because of the absence of any in-text references.

61

Table 11
Hypothesis 5 Mann Whitney U Results
Section 50
Section 51
Section 50
N
N
Mean Rank
21

20

20.29

Section 51
Mean Rank

Test
Statistic

Asymptotic
Significance

21.75

225.00

0.682

With the asymptotic significance greater than the significance level (α = 0.05), the results
were not statistically significant.
Hypothesis 6. Students who received the bibliographic requirement to use peerreviewed sources will attempt to synthesize their outside information sources less
frequently than those without the requirement.
Restated appropriately for the Mann Whitney U, the following results test whether
there is any difference in the distribution of papers’ average synthesis scores across
Sections 50 and 51. Four cases were excluded from this test because of their lack of any
in-text references or, in one case, the lack of a bibliography. The results are reported in
Table 12.
Table 12
Hypothesis 6 Mann Whitney U Results
Section 50
Section 51
Section 50
N
N
Mean Rank
21

19

19.74

Section 51
Mean Rank

Test
Statistic

Asymptotic
Significance

21.34

215.00

0.647

The reported asymptotic significance was greater than the set significance level (α =
0.05), therefore, the decision is to retain the null hypothesis and reject the alternative that
there is a difference between the two groups’ distributions for this variable.

62

While not an originally-stated hypothesis, to be thorough in answering this line of
questions about synthesis, it seemed necessary to also test any possible difference that
may exist between the two groups’ synthesis of peer-reviewed sources alone. To do so,
both samples’ information sources were then subdivided into peer-reviewed and nonpeer-reviewed. Then, only the synthesis scores of a paper’s peer-reviewed sources were
used to provide each paper with the average synthesis score of peer-reviewed sources
alone. Following the same Mann Whitney U procedures, the resulting asymptotic
significance level was 0.176 indicating that there was also no difference between the two
groups’ synthesis of peer-reviewed sources. The mean synthesis score of peer-reviewed
sources alone in Section 50 was 0.20, 0.10 points lower than the mean synthesis when all
sources were included. For Section 51 the synthesis score of peer-reviewed sources alone
was 0.30 which was only slightly less the mean of 0.33 when all sources were considered.
The non-parametric test of difference in distribution was also run on the average
synthesis scores for all in-text references and compared between peer-reviewed and nonpeer-reviewed sources. The Mann Whitney U returned an asymptotic significance (α =
0.180) which indicated no significant difference between students’ synthesis of peerreviewed sources and non-peer-reviewed sources.
Additional Numerical Observations
While the content analysis proposed two deductive hypotheses based on previous
research, the question was framed intentionally to allow for a more inductive approach
that could draw observations from the data. This kind of analysis is observational rather
than inferential or correlational. What follows is a report of numerical observations
63

collected from the remaining portions of the content analysis that were coded based on an
inductive variable identification process.
Engagement categories. In addition to marking whether synthesis was attempted
or not on a given in-text reference, the essays were coded to indicate how students were
synthesizing and integrating these sources. This additional information provides more
explanatory power by describing how the students interacted with sources when, in those
roughly 30% of instances, synthesis was attempted. There were 41 instances (across both
groups) in which an outside information source was coded as a 1 for synthesis. The
crosstab in Table 13 reports the frequency with which a certain engagement category was
coded for a synthesized information source. The four categories were
interpretation/explanation (INTR), reacting (REAC), scrutinizing source (SSCU), and
acknowledging scholarly exchange (SCEX). While these categories were not mutually
exclusive, only two instances were coded with more than one category of engagement.
One case was coded with both the INTR code and the SCEX code, and the other was
coded with both the INTR code and the SSCU code.
Table 13
Crosstab of Codes Applied for Engagement Categories
INTR

REAC

SSCU

SCEX

Total

Section 50

14

3

0

3

20

Section 51

16

4

2

1

23

Total
30
7
2
4
43*
*This total represents the total of codes applied, not the total number of references
coded, which would be 41. The discrepancy is due to two cases which received two codes.

64

In short, the categories for engagement labelled REAC, SSCU, and SCEX were
relatively rare, INTR was the most frequent method by which students engaged with their
information sources. There also appeared to be no discernable co-occurrence between
these categories and whether a referenced source was peer-reviewed or not. The
following crosstab demonstrates this:
Table 14
Crosstab of Engagement Categories by Section and by Source Type
INTR

REAC

SSCU

SCEX

Total

Section 50
Peer-Reviewed

6

1

0

1

8

Section 51
Peer-Reviewed

12

3

1

1

17

Section 50
Not Peer-Reviewed

8

2

0

1

11

Section 51
Not Peer-Reviewed

4

1

1

1

7

Total

30

7

2

4

43

As demonstrated in Table 14, both course sections’ instances of engagement seemed to
remain relatively proportional, and there was no significant co-occurrence that seemed to
stick out from the data. One may notice in Table 14 that more than 100% more codes
were applied to Section 51 peer-reviewed sources than Section 51 non-peer-reviewed
sources. On the other hand, Section 50 non-peer-reviewed sources had only 3 more codes
applied than Section 50 peer-reviewed sources. While this difference was noted, it was
not interpreted as a significant co-occurrence seeing as how it still aligns with what is

65

known about the proportion of peer-reviewed sources in these sections. Section 51’s
average paper cited 75% peer-reviewed sources, whereas Section 50’s proportion was
roughly 50%. The difference in frequencies of codes applied in Table 14, then, are what
should be expected given the proportions. The overarching story is the homogenous
approach to engagement that seemed to be taken by students who did attempt to
synthesize and integrate their sources. For both sections, non-interpretation/explanation
categories were postures rarely adopted by students to engage their outside information
sources.
Writing behaviors. Two other variables were identified in the emergent variableidentification process – unsupported statements (USUP) and personal opinion statements
(OPIN). Unlike the engagement categories, synthesis variable, or count of direct
quotations, these codes were not bound to in-text references but could be coded
throughout the paper. A third, medium-specific variable is also included in this section –
no bibliography entry (NENT). This third variable was coded for instances in which
students provided an in-text reference for a source that was not included in their
bibliography. The following crosstab reports the frequency of these codes across all
papers for both Sections 50 and 51 (see Table 15).
Table 15
Crosstab of Writing Behavior Codes
USUP

OPIN

NENT

Total

Section 50

37

21

13

71

Section 51

37

20

2

59

Total

74

41

25

130

66

Noticeably, the instances of unsupported statements and opinion statements appeared
with relatively similar frequency in both course sections. The code that did not appear as
similarly across both groups was NENT (no bibliography entry). However, in comparing
the percentage of papers from each section that received these codes at least once, the
share for the code NENT becomes slightly more similar, suggesting that some papers in
Section 50 received this code multiple times, thus skewing the count. In fact, the
percentage of papers that received the NENT code at least once was 25% for Section 50,
only slightly higher than Section 51’s 11%. This comparison of percentages is displayed
in Figure 7.

Figure 7: Percentage of papers that received additional codes

67

An examination of Figure 7 reveals the common occurrence of unsupported statements
within students’ essays for both sections. A majority 66% of students in both sections had
an unsupported statement at least once in their essay, with a maximum of seven instances
in a single paper reported for both sections.
As described in Chapter III, co-occurrence counts, scatter plots, and Jaccard’s
Index were used to discern any possible monotonic relationship between variables so as
to indicate whether further correlational tests were warranted or viable. In all, there was
no such discernable relationship. The most related co-occurrences were between direct
quotations and synthesis, unsupported statements and opinion statements, and
unsupported statements and direct quotes. Direct quotations and synthesis co-occurred 23
times, which represents a 22% overlap. Unsupported statements and opinion statements
co-occurred 29 times, which represents a 25% overlap. Unsupported statements and
direct quotes co-occurred 30 times, which represents a 23% overlap. These observations
did not provide strong enough evidence of a monotonic relationship that would be
required for formal tests of correlations.
Summary of Results
This study called for a two-part analysis of students’ citations and the content of
their papers. The study analyzed 44 documents in all, 24 from the control group, course
Section 50, and 20 from the experimental group, course Section 51, which received the
bibliographic requirement to use peer-reviewed sources. Descriptive statistics found that,
in all, students cited 117 outside information sources. The average number of sources per
paper was 2.79 for Section 50 and 2.50 for Section 51. Of those 117 sources, 76 were
68

peer-reviewed. In Section 50, the average bibliography was made up of 56% peerreviewed articles whereas the average Section 51 bibliography was 75% peer-reviewed
sources. 129 in-text references to outside information sources were counted in the papers.
60 of those references were found in Section 50 and 69 in Section 51. On average,
Section 50 students referred to an outside information source 0.91 times, and Section 51
students referred to an outside source 1.22 times.
The content analysis also coded these references for synthesis. Overall, only 41
in-text references to outside information sources were coded as having attempted
synthesis. The average synthesis score for Section 50 papers was 0.32 and, for Section
51, the average score was 0.36. The number of times students relied on direct quotations
was also counted. Of all the in-text references, direct quotation of outside material was
used 60 times – 26 times in Section 50 papers and 34 times in Section 51 papers.
The citation analysis revealed statistically significant differences between
Sections 50 and 51 regarding the percentage of peer-reviewed articles in their
bibliographies and the frequency with which they referred to those peer-reviewed articles
in the text of their essays. In both cases, students from Section 51 cited a greater
proportion of peer-reviewed sources in their bibliographies, and they referred to those
peer-reviewed sources more frequently in their essays than their peers in Section 50. The
analysis, however, revealed no significant difference in the total number of sources cited
nor the frequency of in-text references for all sources (peer-reviewed and not).
The content analysis revealed no significant difference between the two sections’
synthesis of outside information sources nor their reliance on direct quotations.
69

Furthermore, additional coding revealed common themes that describe how the students
in the study were writing about and using their information sources. Of the 41 instances
that synthesis was attempted, the preferred method of integrating or engaging with the
outside information source was to interpret or explain the information. Other ways of
engaging with a source – such as reacting to it, scrutinizing it, or placing it within the
scholarly exchange – were noticeably less common methods. Other writing behaviors,
such as unsupported statements and opinion statements, were also coded. The presence of
unsupported statements was noticeably common across both sections. For both Sections
50 and 51, this code appeared in 66% of papers. Opinion statements were the next most
frequently occurring with it appearing in 38% of Section 50 papers and 40% of Section
51 papers. Instances in which students referred to a source but failed to provide an entry
in their bibliography occurred 13 times in Section 50 papers while only occurring 2 times
in Section 51 papers. Last, among these variables from the content analysis, examination
of co-occurrence revealed no discernable relationship among them that would merit
further correlational analysis.

70

CHAPTER V
DISCUSSION
This study arose as an attempt to revisit earlier research that recommended
mandating peer-reviewed sources to improve students’ bibliographies and written
research (Davis, 2002; Davis, 2003; Robinson & Schlegl, 2005; Baberino, 2004). The
study’s two-part design aimed to, in the first phase; replicate these citation analyses to
test again whether these mandates truly impact students’ citation behaviors. The second
phase aimed to extend these earlier studies by incorporating the theories and methods of
more contemporary information literacy scholars who found that, regardless of
bibliographic skill, students struggle to synthesize and use their sources in a meaningful
way (Rosenblatt, 2010; Carlozzi, 2018; Jamieson, 2010; Hyytinen, H., Löfström, &
Lindblom-Ylänne, 2017). To both replicate and extend earlier research questions about
the impact of mandating peer-reviewed articles for undergraduate students, this study set
out to answer the following questions: Does explicitly requiring the use of peer-reviewed
sources impact the bibliographies and citation behavior of students? How do students in
both groups (those with the requirement and those without) utilize the information
sources within their essays?
Findings
The results of this two-part citation and content analysis for the students enrolled
in the two undergraduate courses studied, describes what sources these two groups of

71

students included and how they integrated them into their essays when a mandate for
peer-reviewed articles was either imposed or loosened. The study provides useful
replication of research that is over a decade old, and it updates the conclusions of
previous research with new information about how the students in the study are using
their sources within their essays. Six initial hypotheses were proposed before data
collection:
H1. Students who received the bibliographic requirement to use peer-reviewed
sources will cite all of their individual sources less frequently within the body of
their essays than those without the requirement.
H2. Students who received the bibliographic requirement to use peer-reviewed
sources will cite their peer-reviewed sources less frequently within the body of
their essays than those without the requirement.
H3. Students who received the bibliographic requirement to use peer-reviewed
sources will cite fewer sources overall than those without the requirement.
H4. Students who received the bibliographic requirement to use peer-reviewed
sources will cite the same percentage of peer-reviewed sources as those without
the requirement.
H5. Students who received the bibliographic requirement to use peer-reviewed
sources will directly quote their outside information sources more frequently than
those without the requirement.

72

H6. Students who received the bibliographic requirement to use peer-reviewed
sources will attempt to synthesize their outside information sources less
frequently than those without the requirement.
In addition to testing these six hypotheses, the inductive variable identification
process utilized for the content analysis allowed for a richer description of how students
integrated outside information sources by categorizing engagement into four categories –
interpreting/explaining, reacting, scrutinizing, and acknowledging scholarly exchange.
Students’ writing was further described by coding for unsupported statements, opinion
statements, and when students failed to provide a bibliographic entry for a source they
cited. The citation analysis reaffirmed the findings and suggestions of Robinson and
Schlegl (2004, 2005) by uncovering a significant difference in the citation behavior of
students who received the bibliographic requirement versus those that did not. The
content analysis, however, revealed relatively homogeneous results that cast doubt on the
bibliographic requirements’ ability to serve as a relevant tool to support renewed,
contemporary information literacy goals.
In-Text Citation Frequency
The findings in this study echo Jamieson (2013) and Carlozzi’s (2018) concern
about the low frequency with which the students referred to the sources in their essays.
The average frequency was around once – 0.9 and 1.2 for Sections 50 and 51,
respectively. When only measuring frequency of in-text references for peer-reviewed
sources, however, the mandate showed some promise. Students in Section 51 (the course
section required to use peer-reviewed sources) referred to their peer-reviewed sources
73

significantly more than their peers in Section 50 referred to their peer-reviewed sources.
The average peer-reviewed in-text citation frequency was 0.15 less than the combined
average. The average peer-reviewed in-text frequency for Section 51, on the other hand,
was a 0.11 increase over its combined average.
What this suggests is that the bibliographic requirement was successful in
improving the experimental group’s use of peer-reviewed sources while students without
the requirement seemed to become even more lackadaisical in their use of peer-reviewed
sources than they were with all their sources combined. Both means still provide little
comfort in that they still suggest that, on average, the students used their sources (peerreviewed or otherwise) only once. While there was a significant difference in both
groups’ in-text citation frequency of peer-reviewed sources, the improvement found in
Section 51 does not dismiss the concerning infrequency with which students utilize and
integrate outside information into their research.
Number and Type of Sources
Hypotheses three and four were directed at retesting the suggestions that the
mandate to use peer-reviewed sources will increase the number of sources cited and will
improve the bibliographies by increasing the proportion of peer-reviewed sources cited.
There was no significant difference in the number of sources that either group cited, but it
was notable that both groups’ means (2.79 for Section 50 and 2.50 for Section 51)
remained closely aligned with the minimum of two to three sources set by the instructor.
These results also reaffirm Robinson and Schlegl’s (2004) findings that the bibliographic
requirement was not effective at improving the overall number of citations.
74

Contrary to the initial hypothesis in this study, the experiment corroborates the
conclusions of Davis (2003) and Robinson and Schlegl (2004) in that students who
received the requirement to use peer-reviewed sources did cite a significantly greater
proportion of peer-reviewed sources. While the difference was significant, it is worth
noting that, even without the explicit mandate, the average bibliography for students in
Section 50 was made up of 56% peer-reviewed articles, compared to the 75% in Section
51. What the percentage from Section 50 demonstrates is that, most of the time, students
not given explicit mandates to do so still deemed peer-reviewed sources to be the most
appropriate format for their information needs. The control group’s 56% composition of
peer-reviewed sources might reflect messages that students have already received about
the expectations of college-level research outside of this experiment in other classes, high
school, or even from library instruction. This is a question left unanswered from this
study’s results. On closer examination, these numbers are not far from the results of
Robinson and Schlegl (2005), who reported a 14% difference between the control group
and the group with the bibliographic requirement. With a 19% difference between the
two groups’ means, this experiment received arguably more promising results for those
who promote the use of bibliographic requirements to increase the use of peer-reviewed
articles. This promise is extended by the fact that students in this experiment not only
cited more peer-reviewed sources in their bibliographies when they were required to do
so, but they also used them more in their essays than their peers without the requirement.
In this instance, the benefit of the requirement was two-fold, and not detrimental to the
bibliographic outcomes as initially hypothesized.
75

Direct Quotations
While this study found the usage of direct quotes to be a common occurrence, the
data do not seem to warrant much alarm. 50% of Section 50 papers used direct quotes at
least once, and 65% of papers in Section 51 used the method at least once. When
analyzing the proportional use of this method (the number of times it was used in a paper
divided by the number of in-text references), the numbers begin to look less drastic. On
average, only 10.29% of in-text references in Section 50 were introduced using direct
quotations. This was found to be similar to the results in Section 51, which had a mean of
13.56%. If direct quotations are considered an indicator of low-level compositional skills
or synthesis, then the bibliographic requirement, on this front, appeared to affect little to
no difference on students’ integration of their sources. While not the intended outcome of
the inclusion of this variable, its presence in this experiment did cast doubt on the
variable’s validity as an indicator of synthesis. Notably, when variables from the content
analysis were analyzed for co-occurrence, there were no significant monotonic
relationships — this includes the relationship between number of direct quotations and
synthesis score. If the use of direct quotes were a valid, negative indicator of students’
higher-order rhetorical and information-use skills, one might expect a co-occurrence in
this sample to emerge between direct quotations and scores of 0 for synthesis. This was
not the case, and, in fact, a number of synthesized references were directly quoted
themselves. So, while this variable was included because of its previous use as an
indicator of compositional skill, it neither proved to differ significantly among the tested
groups nor did it prove to be a very useful or valid indicator.
76

Synthesis
First and foremost, comparisons of the two groups’ distributions of average
synthesis score revealed no significant difference between the two groups. What this
suggests is that, while the bibliographic requirement did alter aspects of students’
bibliographic behavior, its influence stopped there. The results do contradict what was
originally hypothesized that the bibliographic requirement would be detrimental to
students’ synthesis. This was also not the case. The results corroborate the findings of
Rosenblatt (2010) and Carlozzi (2018), who were alarmed by the rarity with which
students synthesized or integrated their sources. The results from this experiment were
very near Carlozzi’s results, which found that over half of the sources were not
synthesized at all. The results from this study were slightly more severe with 68.3% of
Section 50 references and 64.5% of Section 51 references showing no signs of synthesis.
Ultimately, this experiment adds to Rosenblatt and Carlozzi’s observations of
their students. This affirms that the problem of synthesis was not unique to their students,
but it is now documented as a problem for the students involved in this study. Without,
yet, any widespread analysis of students’ writing and synthesis skills, localized studies
like this one add to a collection of librarians, instructors, and researchers’ voices that are
raising concerns about their college students’ abilities to properly integrate and
synthesize outside information. In short, this study observed that both groups of students
struggled to synthesize and use their sources in a meaningful way. Yes, the students with
the bibliographic requirement did use their peer-reviewed sources more, but, when it

77

came to them synthesizing and incorporating those sources into their research, they were
no better off than their counterparts.
Authors like Davis and Robinson and Schlegl contained their analysis to student
bibliographies, and they appeared pleased with the effectiveness of the bibliographic
mandate’s ability to maintain library collections’ pride of place in student research. So,
while this study’s citation analysis results certainly support their findings, the results
concerning synthesis call into question the practice’s overall relevance in supporting
contemporary information literacy goals. A recognition of more recent conceptions of
information literacy and librarianship should force one to question whether these results
still demonstrate much value in mandating peer-reviewed sources.
Is poorly-synthesized research dressed-up with academic-looking bibliographies a
pleasing enough result for the practice to stick around? With a broad and rich definition
of information literacy offered by the Framework, it would be hard to suggest that it is
good enough that students’ research look like academic material without having any of
the substance. While bibliographic skills are still essential elements to information
literacy, the alarming lack of synthesis and diversity of engagement are much more
pressing concerns if librarians and instructors are to rise to the ACRL’s challenge to train
active “…contributors to the information marketplace” (2015, p. 6).
Furthermore, students in Section 50, who used significantly less peer-reviewed
sources, still struggled to synthesize their arguably more accessible information sources.
In fact, the findings revealed that, when testing the synthesis of peer-reviewed references
versus non-peer-reviewed synthesis, there was no significant difference. This means that
78

students in both groups were no more successful synthesizing non-scholarly sources than
they were synthesizing academic, peer-reviewed sources. This fact echoes French (2004)
and MacMillan and Rosenblatt’s (2015) concern that the challenges in synthesis may
indicate more foundational issues in students’ reading level regardless of format. If this is
the case, then imposing peer-reviewed mandates may be a foolhardy move that ignores
the need for more foundational information literacy and developmental reading
intervention. If this is true, it may suggest that librarians and instructors need to meet
students where they are; working from non-academic, popular sources to address more
basal concerns. These results concerning synthesis should challenge practitioners to
question whether bibliographic mandates are simply a distraction from the deeper
difficulties students are facing as they conduct research and write with outside
information sources.
If altering the composition of students’ bibliographies has no discernable effect on
the written research that students produce, advocates of this practice should be asked why
they are defending these mandates. Patterson (2009), for example, may suggest that
advocating for these kinds of bibliographic mandates in the face of little evidence that
they improve overall student scholarship may be yet another example of how academic
librarians risk mirroring their ancient and ritualistic ancestors at the library of Nippur.
Like the ancient library which collected and sanctified texts, in the face of these results,
to suggest that the mandate is still worthwhile could be interpreted as doing so for the
sake of maintaining the power of the collection rather than the benefit of the student –

79

ensuring the use of library-approved sources rather than focusing on the deeper academic
needs of students.
Additional Observations
Engagement categories. With the inclusion of engagement categories, the
infrequent instances of synthesis were classified to better describe how students
integrated their sources. This step revealed a homogenous approach. Out of the four
categories, interpreting/explaining a source was the most frequent. Less frequently did
students react to information, scrutinize it, or write about a scholarly exchange happening
between information sources. Here, Bizup’s (2008) BEAM typology might prove useful
in interpreting the results. Students frequently integrated outside information and facts by
explaining how these sources fit their conclusions or served as evidence. This type of
engagement is characteristic of Bizup’s exhibit category by which information is merely
integrated as a piece of evidence. The remaining three types of engagement – in which
students face the source directly to insert themselves in dialog with it – were less
common and more closely aligned with the argumentative approach described by Bizup.
As with Bizup’s BEAM, this is not a value judgement on which approach is better, but, if
one follows the lead of the ACRL’s Framework (2015), then the information literate
student should be an active participant, inserting herself in the ongoing scholarly
communication. The observations from this study, therefore, suggest that, while there
was some success in properly synthesizing sources, deeper analysis of the ways these
students wrote about their sources reveals a concerning lack of variety and dialog in those
instances of engagement.
80

Writing behaviors. This study’s inclusion of additional, emergent variables
related to writing behavior provided another useful insight into how students were
approaching their research. Unsupported statements appeared frequently in the data —
occurring at least once in 66% of papers in both Sections 50 and 51. The presence of an
unsupported statement could be understood as a failure to recognize an information need
or when the inclusion of citations is necessary, both of which are essential skills for
information literacy (ACRL, 2015). While this study was primarily focused on synthesis
as an indicator of information literacy, this emergent variable illuminated another facet of
information literacy that may have been deficient in the sampled papers and not fully
assessed by the deductive measurement methods. In a way, this acted as a diagnostics test
of the assessment method’s ability to properly assess the many components of
information literacy, and this adds to the realization of just how challenging authentic
assessment can be when dealing with such a broad, multilayered skillset like information
literacy.
The inductive method, however, was able to highlight the frequent occurrence of
unsupported statements, and this added to the study’s understanding of another way,
beyond synthesis, that students in the sample displayed an alarming information literacy
deficiency. Like the infrequency of synthesis, the frequency of unsupported statements
indicated a failure on the part of the student to know when and how to incorporate outside
information into her/his essay. This observation was troublesome because it suggests that,
while synthesis was the primary aim of this study, these students’ information-use
challenges did not stop there. With over 60% of documents in this sample containing an
81

unsupported statement, students’ identification of information-needs and when to support
claims with reputable evidence may be just as pressing as their inability to synthesize
sources. Just as Rosenblatt’s 2010 observations called attention to their students’
difficulty in synthesis, this observation suggests a possible need for more investigations
into students’ identification of information-needs.
The same could be said of the presence of in-text citations that lacked
bibliographic entries. While less frequent than unsupported statements, they did occur
and, again, indicate a clear failure to practice the proper attribution skills outlined in the
Framework. While the citation analysis in this study did not include measures of proper
citation format and application, the presence of bibliographic missteps (like missing
bibliographic entries for in-text citations) might suggest that this was a mistake and that
the breadth of the assessment limited the ability to assess particular information literacy
facets with depth.
Last, opinion statements emerged from the content analysis with some frequency
— occurring in 38% and 40% of papers in Sections 50 and 51, respectively. While this
linguistic marker did not appear to differ between the two groups, its lack of cooccurrence with synthesis does cast some doubt on Kanter’s (2006) use of the variable as
a valid indicator for engagement. It is possible that this linguistic marker indicated the
students’ engagement with their research topics as Kanter suggested, but it did not prove
to be an effective indicator or the students’ engagement with their information sources in
this study.

82

The observed homogeneity across both groups’ writing behavior variables
provides an insight into the reach that the bibliographic requirement had in this
experiment. It seems as though, once analysis passed from the first phase (citation
analysis) to the second phase (content analysis), the differences between the two groups
ceased. In fact, the overarching observation from the content analysis was the striking
similarity between the two groups on all fronts. What this ultimately suggests is that
bibliographic requirements had very little power to penetrate and positively affect the
sampled students’ writing and information-use as a whole. Instead, the picture painted by
this study’s results is one in which the practice is a tool confined to its ability to increase
students’ inclusion of 19% more peer-reviewed articles and to increase the frequency
with which the students referred to those sources in their essays. On all other fronts —
synthesis, engagement, and writing behaviors — the bibliographic requirement seemed
unable to affect any improvement (or cause any harm) within the sample of student
essays. Furthermore, the additional findings regarding synthesis and the presence of
unsupported statements reaffirms the concerns of Rosenblatt (2010) and Carlozzi (2018)
that there are more pressing challenges in students’ ability to properly use sources and
develop strong written research.
Significance
The purpose of this study was to replicate and extend earlier quasi-experimental
research that analyzed differences in student outcomes between groups that received
bibliographic requirements to use peer-reviewed research versus those that did not. It
replicated previous research in that it revisited the same questions about the effectiveness
83

of the requirement to significantly increase students’ citation and use of peer-reviewed
articles. It extended previous research by adding a new component – content analysis –
that examined the essays themselves to determine if that difference carries over into in
the way students integrate sources and engage in their written research. The significance
of the study is three-fold:
•

Much of the experimental research that has analyzed the implementation of
these bibliographic requirements is now over a decade old. Replication is an
important part of our scientific process and testing these questions again (in a
new era with a new sample of undergraduate students) helps to determine
whether the initial conclusions remain true in this instance.

•

The initial recommendations for peer-reviewed mandates stopped short at the
bibliographies. This study extends the vision of earlier experiments by
analyzing how students in this sample synthesized the sources they found.
This extension adds to a body of literature in information literacy research that
focuses on students’ ability to synthesize sources rather than concerns about
them finding or citing the right type and number of sources. This new iteration
of the earlier experiments, therefore, updates the initial conclusions to address
whether the bibliographic requirement had any impact on addressing newer
priorities about the sampled students’ information source synthesis and
integration.

•

A deeper analysis of the content of the students’ essays adds to localized
observations of how some students integrate outside information sources into
84

their written work. These observations serve as a kind of diagnostics test,
providing observational data about other challenges the students in this sample
faced in their written research assignments and that may merit closer
examination in future research.
Limitations
Although the quasi-experimental design of this study limits its ability to
generalize its conclusions outside the population of students enrolled in the courses
targeted for this study, its results do affirm the findings of previous studies which,
employing a similar quasi-experimental and purposive sampling design, were also unable
to generalize beyond their given population (Robinson & Schlegl, 2004; Rosenblatt,
2010; Carlozzi, 2018). While unable to transfer the conclusions directly to other
populations, there is some reassurance in the fact that this study rendered similar
observations about the effect of the bibliographic requirement and the students’ synthesis
and writing behaviors as studies, which took place in noticeably different locations,
contexts, and times. While still limited, this growing body of localized, quasiexperimental results seems to point to common themes in undergraduate research and
writing that are not necessarily specific to one university or even one particular decade.
The other limitation to this study, which was already alluded to in the last
section’s discussion of additional writing behaviors, is that the two forms of assessment
are, by no means, a perfect assessment of information literacy. As discussed in the review
of literature, information literacy is a complex, multifaceted concept with a history of
diverse assessment methods. So, while limited in its ability to possibly address every skill
85

and disposition associated with information literacy, this two-part assessment model was
able to balance a temptation to either focus assessment entirely on bibliographies and
citation behavior or entirely on information source integration. In so doing, the study was
able to provide results about both the students’ bibliographic behaviors and their use of
information sources — both essential aspects of information literacy in the Framework
(2015). The tradeoff, however, is that some aspects of bibliographic skills (such as proper
citation format) and information-use (such as hearing directly from the students about
how they decided to use certain sources) were too far beyond the time and scope
limitations of this assessment. In this assessment model that aimed for breadth, it seems
as though depth was naturally sacrificed in the process.
Finally, like its predecessors, Robinson and Schlegl (2004), Rosenblatt (2010),
and Carlozzi (2018), this study relied on the assumption of general homogeneity among
students enrolled in different sections of the same course. The educational setting, unlike
a laboratory, does not lend itself to complete control of confounding variables. With more
resources and time, additional obtrusive measures of data collection, such as
comprehensive surveys, could attempt to collect background information on the students
– information about past experiences with research or the library and data on their
educational achievement and history. This would have allowed for statistical control of
confounding variables. These obtrusive data collection measures, however, were not
possible, and this does leave some serious questions as to whether another variable
besides the bibliographic requirement was to blame for the between-group differences
observed.
86

Future Research
The previous section closes with an allusion to the first recommendation for
future research — a replication of this experiment with the addition of a comprehensive
survey to collect and control for potential confounding variables. This, coupled with
larger sampling, would allow for more definitive conclusions about the effectiveness of
bibliographic requirements.
The observations and results from the content analysis also left several questions
looming about students’ synthesis and integration of their sources. While the content
analysis revealed that the students in the sample struggled to synthesize the sources and
that they rarely ventured to use their information sources in more argumentative ways, it
does not answer why students were struggling with synthesis. Maybe it is, as French
(2004) and MacMillian and Rosenblatt (2015) suggested, that students simply struggle to
read college-level sources and, therefore, struggle to synthesize it. Whatever the case may
be, the observations from this study seem to affirm the concerns of other researchers and
librarians referenced in this study, and it simply adds to the red flags about undergraduate
students’ poor integration of outside information sources. The next, most productive
question may not be whether students struggle with synthesis, but how librarians can
support students and instruction faculty to improve students’ synthesis of sources.
Last, what was learned from this study is that the papers analyzed did increase
their use of peer-reviewed sources when required to do so, but no additional, significant
differences emerged. While French (2004) offers some reasons why faculty members
87

might impose these mandates, additional research would be helpful to learn from faculty
members themselves why they impose (or do not impose) mandates for peer-reviewed
articles. For academic librarians, knowing the reason why faculty members include such
parameters might help them to inform their decisions about recommending the practice.
What differences are instructors hoping to see from imposing these kinds of mandates?
For example, concerned about the requirements being a distraction from the more
important skills of synthesis, Holliday et al. (2015) reported that they began
recommending to instruction faculty that they suspend such mandates. While not an
outrageous assumption, this nonetheless assumes that instructors have the same
information literacy priorities in mind when imposing these mandates. Knowing what
motivates instructors’ imposition of these requirements may help librarians to enter more
productive dialogs with instructors about realistic expectations of the practice’s
capabilities to support their instructional goals.
Last, the discovery of the how often students included unsupported statements in
their essays adds an additional concern to the already-stated worries about synthesis. 66%
of students in this sample included an unsupported statement at least once in their essay.
These statements reflect a failure to recognize an information need and when it is
necessary to support ones’ research with outside information sources and reliable
evidence. While not the focus of this study, this observation warrants additional
investigations. It may be productive to investigate how students understand their
information needs and how they decide what warrants outside evidence. While the issue
of how students synthesize information is still pressing, the presence of unsupported
88

statements in this sample seems to suggest that students’ information-use challenges go
even deeper than synthesis.
Final Reflection: Learning to Swim in a Sea of Information
Reconsider the allegory of learning to swim. How did the 44 undergraduate
students included in this study perform in the waters of their research? 24 of them were
thrown into the sea with few clear boundaries. They waded into the surf with whatever
instruction they received previously and some suggestions from their instructor, but,
ultimately, it was up to them to decide where they went, and which parts of the water
were ideal. The other 20 were set in a pool with clearly-defined boundaries. They knew
where to go to get into the water, and the pool’s boundaries ensured that, at the very least,
they would swim in reliable waters — not out in the open ocean of the internet where
authority and reliability are not so clear. The walls of the proprietary library databases
kept them safe as they practiced their swimming.
In this experiment, however, what we learned was that not even the confines of
the peer-reviewed mandate were able to help students keep their heads above the water of
information. While students with the mandate to use peer-reviewed sources were finding
information where we expected them to, it did not help them use their information any
better than their peers without the requirement. If the goal was to teach the students to
swim, that is, to effectively and meaningfully use information, they struggled to do so
irrespective of the requirements imposed on them about where they may seek
information.

89

With this in mind, the primary question that lingers is, “Why bother?” Certainly,
in this case, to say that the bibliographic requirement improved the overall quality of
students’ written research would be an outlandish claim given the results. If anything, all
the requirement seemed to do was dress-up students’ poorly-synthesized research into
more scholarly-looking research with bibliographies chalked full of peer-reviewed
sources. Why force students to find and cite sources if they do not engage and use the
information in a meaningful way? Is it, as Patterson (2009) suggested in the comparison
to the ancient library at Nippur, that it is a mechanism of academia and librarianship’s
own information power structure? Is it a practice that simply privileges products of
academia’s own labor – ritualistically molding students’ bibliographies after our own
image?
If, however, the substance of students’ research is a priority, bibliographic
requirements might just be an ineffective distraction from more pressing pedagogical
concerns – fairly limited vestiges of a former citation obsession. In keeping with the
allegory, this may mean that librarians and instructors need to suspend their own
expectations about what parts of the expanse of information students should be allowed
to tread in, so that they can jump in and save students that are simply trying to stay afloat
in a sea of information. The central questions posed by this allegory and worth taking to
heart are: Do these mandates reflect an instructor’s fixation about where students are
allowed to swim, and do they risk clouding the more alarming fact that the students
cannot swim?