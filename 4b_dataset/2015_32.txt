ENHANCING USER SEARCH EXPERIENCE IN DIGITAL LIBRARIES
WITH ROTATED LATENT SEMANTIC INDEXING
Serhiy Polyakov, Dipl.Eng., M.S.

Dissertation Prepared for the Degree of
DOCTOR OF PHILOSOPHY

UNIVERSITY OF NORTH TEXAS
August 2015

APPROVED:
Nicholas Evangelopoulos, Committee Co-Chair
William E. Moen, Committee Co-Chair
Jiangping Chen, Committee Member
Suliman Hawamdeh, Chair of the Department of
LIS, Director of the Interdisciplinary
Information Science PhD Program
Herman Totten, Dean of the College of Information
Costas Tsatsoulis, Dean of the Toulouse Graduate
School

ProQuest Number: 10034369

All rights reserved
INFORMATION TO ALL USERS
The quality of this reproduction is dependent upon the quality of the copy submitted.
In the unlikely event that the author did not send a complete manuscript
and there are missing pages, these will be noted. Also, if material had to be removed,
a note will indicate the deletion.

ProQuest 10034369
Published by ProQuest LLC (2016). Copyright of the Dissertation is held by the Author.
All rights reserved.
This work is protected against unauthorized copying under Title 17, United States Code
Microform Edition © ProQuest LLC.
ProQuest LLC.
789 East Eisenhower Parkway
P.O. Box 1346
Ann Arbor, MI 48106 - 1346

Polyakov, Serhiy. Enhancing User Search Experience in Digital Libraries with
Rotated Latent Semantic Indexing. Doctor of Philosophy (Information Science),
August 2015, 132 pp., 31 tables, 31 figures, references, 94 titles.
This study investigates a semi-automatic method for creation of topical labels
representing the topical concepts in information objects. The method is called rotated
latent semantic indexing (rLSI). rLSI has found application in text mining but has not
been used for topical labels generation in digital libraries (DLs). The present study
proposes a theoretical model and an evaluation framework which are based on the LSA
theory of meaning and investigates rLSI in a DL environment. The proposed evaluation
framework for rLSI topical labels is focused on human-information search behavior and
satisfaction measures. The experimental systems that utilize those topical labels were
built for the purposes of evaluating user satisfaction with the search process. A new
instrument was developed for this study and the experiment showed high reliability of
the measurement scales and confirmed the construct validity. Data was collected
through the information search tasks performed by 122 participants using two
experimental systems. A quantitative method of analysis, partial least squares structural
equation modeling (PLS-SEM), was used to test a set of research hypotheses and to
answer research questions. The results showed a not significant, indirect effect of
topical label type on both guidance and satisfaction. The conclusion of the study is that
topical labels generated using rLSI provide the same levels of alignment, guidance, and
satisfaction with the search process as topical labels created by the professional
indexers using best practices.

Copyright 2015
by
Serhiy Polyakov

ii

ACKNOWLEDGMENTS
I would like to express my deep appreciation to my major professors and mentors
Dr. William E. Moen and Dr. Nicholas Evangelopoulos for their time, guidance,
invaluable feedback, and encouragement. I also thank to my dissertation committee
member, Dr. Jiangping Chen, for her ideas that helped to improve my dissertation. I am
grateful to the faculty and students from the College of Information at the University of
North Texas. They made this research possible through their participation. I would also
like to gratefully acknowledge Eugene Garfield Doctoral Dissertation Fellowship from
Beta Phi Mu which has assisted my work on the dissertation. My special thanks go to
my family and my friends. Without their intellectual and emotional supports in every way
possible, I could not come this far.

iii

TABLE OF CONTENTS
Page
ACKNOWLEDGMENTS .................................................................................................. iii
LIST OF TABLES .......................................................................................................... viii
LIST OF FIGURES .......................................................................................................... x
Chapters
1. INTRODUCTION ................................................................................................. 1
1.1 General Background ................................................................................ 1
1.1.1 Digital Libraries and Representations of Information Objects ..... 1
1.1.2 Vector Space Model, Latent Semantic Analysis, and Rotated
Latent Semantic Indexing .................................................................... 4
1.1.3 User Experience and Domain Knowledge .................................. 8
1.2 Statement of the Research Problem ........................................................ 9
1.3 Study Goals............................................................................................ 10
1.4 Research Questions and Hypotheses .................................................... 11
1.4.1 Research Questions ................................................................. 11
1.4.2 Research Hypotheses .............................................................. 11
1.5 Purpose of the Study .............................................................................. 12
1.5.1 Theoretical Perspective ............................................................ 13
1.5.2 Proposed Generic Search Guidance (GS) Model ..................... 13
1.5.3 Proposed Topical Labels Guidance (TLG) Model ..................... 17
1.5.4 Variables .................................................................................. 19
1.6 Justification and Significance of the Study ............................................. 21
1.7 Overview of the Research Design .......................................................... 22
1.8 Scope and Limitations ............................................................................ 23

iv

1.9 Summary ................................................................................................ 23
2. REVIEW OF THE LITERATURE ....................................................................... 24
2.1 Introduction ............................................................................................ 24
2.2 Latent Semantic Analysis ....................................................................... 24
2.2.1 Overview .................................................................................. 24
2.2.2 LSA Theory of Meaning ............................................................ 26
2.3 Human-Centered Information Retrieval Areas of Research ................... 27
2.3.1 Human-computer Information Retrieval .................................... 27
2.3.2 Interactive Information Retrieval ............................................... 29
2.4 Information Retrieval Evaluation ............................................................ 31
2.4.1 Measures in IIR Research Evaluation ...................................... 31
2.4.2 Methodological Approaches to Evaluate IR Systems ............... 33
2.5 Models in Information Retrieval and Information Seeking Research ...... 34
2.5.1 Ingwersen’s Model.................................................................... 35
2.5.2 Evaluation Using Technology Acceptance Model ..................... 36
2.5.3 Usability Evaluation .................................................................. 37
2.6 Summary ................................................................................................ 38
3. RESEARCH DESIGN ........................................................................................ 39
3.1 Introduction ............................................................................................ 39
3.2 Research Design Framework ................................................................. 39
3.3 Collection of Documents for Experimental Systems ............................... 40
3.4 Participants ............................................................................................ 43
3.5 Experimental Design .............................................................................. 45
3.6 Experimental Systems ........................................................................... 47
3.7 Instrument .............................................................................................. 48

v

3.8 Experimental Procedures ....................................................................... 49
3.9 Reliability, Validity, and Generalizability ................................................. 52
3.10 Data Analysis Procedures .................................................................... 53
3.11 Pilot Study ............................................................................................ 55
3.12 Summary .............................................................................................. 55
4. DATA COLLECTION AND ANALYSIS .............................................................. 57
4.1 Introduction ............................................................................................ 57
4.2 Experimental Systems ........................................................................... 57
4.3 Collection and Metadata ........................................................................ 61
4.4 Generation of rLSI Topical Labels for the Proposed System ................. 63
4.5 Participants, Experiment, and Data Collection ....................................... 70
4.6 Descriptive Statistics .............................................................................. 72
4.7 Reliability of Measurement Scales ......................................................... 79
4.8 Construct Validity ................................................................................... 79
4.9 Partial Least Squares Structural Equation Modeling (PLS-SEM) ........... 81
4.9.1 Modeling without Control Variables .......................................... 81
4.9.2 Modeling with Control Variables ............................................... 84
4.10 Hypotheses Tests and Answers to the Research Questions................ 86
4.11 Summary .............................................................................................. 88
5. DISCUSSIONS AND CONCLUSIONS .............................................................. 89
5.1 Introduction ............................................................................................ 89
5.2 Summary of the Findings ....................................................................... 89
5.3 Limitations of the Study .......................................................................... 91
5.4 Implications and Recommendations for the Future Research ................ 91
5.5 Contribution of the Study ........................................................................ 93

vi

5.6 Summary ................................................................................................ 93
Appendices
A. EXAMPLE OF COMPUTING VSM, LSA AND RLSI .......................................... 94
B. INSTITUTIONAL REVIEW BOARD APPROVAL LETTER AND INFORMED
CONSENT NOTICES ...................................................................................... 105
C. INSTRUMENT ................................................................................................. 111
D. TOPICAL LABELS FROM EBSCOHOST ........................................................ 120
E. SAMPLE ABSTRACT OF THE ARTICLE FROM THE DOCUMENT
COLLECTION.................................................................................................. 122
REFERENCES ............................................................................................................ 124

vii

LIST OF TABLES
Page
1

Proposed Generic Search Guidance (SG) Model Constructs Names................... 16

2

Proposed Topical Labels Guidance (TGL) Model Constructs Names................... 18

3

Correspondence between SG and TLG Model Constructs ................................... 19

4

Variables and Hypotheses .................................................................................... 20

5

Experimental Design............................................................................................. 45

6

Top Loading Terms for Factor F25.8 .................................................................... 66

7

Top Loading Documents for Factor F25.8 ............................................................ 66

8

Topical Labels Generated Using rLSI and Standardized with ASIS&T
Thesaurus............................................................................................................. 68

9

Experimental Groups Workflow and Number of Participants ................................ 71

10

Sets of Responses................................................................................................ 74

11

Descriptive Statistics for the IEMA, TLG, and SSP Variables; Query 1:
“Examples of Social Tagging” ............................................................................... 75

12

Descriptive Statistics for the IEMA, TLG, and SSP Variables; Query 2: “Types
of Access to Electronic Resources” ...................................................................... 76

13

Descriptive Statistics for the Domain Knowledge (DK) Variable for all Types of
Labels and Tasks.................................................................................................. 77

14

Reliability of the Measurement Scales .................................................................. 79

15

Construct Validity .................................................................................................. 80

16

Standardized Regression Coefficients β of the PLS-SEM .................................... 84

17

Coefficients of Determination R2 of the PLS-SEM ................................................ 84

18

Standardized Regression Coefficients β of the PLS-SEM: Control Variables ....... 86

19

Summary of Hypotheses Testing .......................................................................... 87

A.1 Sample Collection of Documents and a Query ..................................................... 95
A.2 Normalized TF-IDF2 Terms Weights and Documents Query Similarities ............. 96
viii

A.3 Normalized TF-IDF2 Terms Weights Processed with Two-dimensional LSA
and Documents Query Similarities........................................................................ 98
A.4 Pseudo Documents Terms Counts ..................................................................... 100
A.5 Normalized TF-IDF2 Weights for the Pseudo Documents .................................. 100
A.6 Loadings of Terms on Un-rotated and Rotated Factors ...................................... 102
A.7 Loadings of Documents on Un-rotated and Rotated Factors .............................. 102
C.1 Q1–Q5: Alignment between User’s Mental (Internal) Map and External Map
Displayed by the System (IEMA) ........................................................................ 115
C.2 Q6–Q10: Topical labels guidance (TLG) ............................................................ 116
C.3 Q11–Q14: Satisfaction with the Search Process (SSP) ...................................... 117
C.4 Q15–Q18: Domain Knowledge (DK) ................................................................... 118
D.1 First 30 Most Frequent Topical Labels from EBSCOhost (266 Articles Set) ....... 121

ix

LIST OF FIGURES
Page
1.

The general model of information retrieval. ............................................................ 2

2.

Vector space model (VSM). .................................................................................... 5

3.

Vector space model (VSM) term space (a) and latent semantic analysis (LSA)
abstract dimensions space with rotation of dimensions (b). .................................... 6

4.

Proposed generic search guidance (SG) model. .................................................. 14

5.

Proposed topical labels guidance (TLG) model. ................................................... 17

6.

Sub-disciplines of the human-computer information retrieval area of research. ... 28

7.

Areas of research related to interactive information retrieval. ............................... 29

8.

Research continuum for conceptualizing IIR research.......................................... 30

9.

The constructs (classes) and measures traditionally used in IIR, IR, and HCI.
Based on the review by Kelly (2009). ................................................................... 32

10.

Interactive information seeking, retrieval and behavioral processes.
Generalized model of any participating cognitive actor(s) in context. ................... 35

11.

Technology acceptance model (TAM). ................................................................. 36

12.

Prototype of the search user interface of the experimental system. ..................... 48

13.

Fragment of the partial least squares structural equation modeling (PLS-SEM)
results. .................................................................................................................. 55

14.

Functional diagram of the software components for the experimental IR
systems. ............................................................................................................... 59

15.

Implementation of LSA search with the use of ancillary metadata field. ............... 60

16.

Example of metadata for one article at the EBSCOhost database. ...................... 62

17.

The workflow of semi-automatic rLSI topical label generation. ............................. 65

18.

Proposed experimental system with LSA search engine and topical labels
generated using rLSI. ........................................................................................... 69

19.

Distribution of time taken by the participants to complete the experiment. ........... 70

20.

Geographic distribution of the participants. ........................................................... 72

x

21.

Means and confidence intervals for the IEMA, TLG, and SSP variables;
Query 1: “Examples of social tagging”. ................................................................. 75

22.

Means and confidence intervals for the IEMA, TLG, and SSP variables;
Query 2: “Types of access to electronic resources”. ............................................. 76

23.

Means and confidence intervals for the domain knowledge (DK) variable for all
types of labels and tasks. ..................................................................................... 77

24.

Demographics of the participants. ........................................................................ 78

25.

Partial least squares structural equation modeling (PLS-SEM) visualization. ....... 83

26.

Partial least squares structural equation modeling (PLS-SEM) visualization
with control variables. ........................................................................................... 85

27.

Proposed topical labels guidance (TLG) model: Data analysis. ............................ 88

A.1. Illustration of singular value decomposition (SVD)................................................ 97
A.2. Illustration of the reduced singular value decomposition model. ........................... 98
A.3. Correlation between X̂k and X̂k-pseudo matrixes. ............................................... 101
A.4. Representation of term and of document loadings in un-rotated and rotated
two-dimensional space. ...................................................................................... 104

xi

CHAPTER 1
INTRODUCTION
1.1 General Background
This section introduces relevant terms and concepts and presents background
and overview of the problem. The overview includes a brief introduction to the areas on
which this study is built.

1.1.1 Digital Libraries and Representations of Information Objects
A digital library (DL) is a “collection of digital objects (text, audio, video), along
with methods for access and retrieval, as well as methods for selection, organization,
and maintenance” (Witten, Bainbridge, & Nichols, 2010, p. 590). Digital objects are
information objects comprising information content of DLs. “One of the primary
responsibilities of libraries is to develop and implement tools that help connect users
with appropriate information to resolve users’ information needs” (Moen, 2009, p. 1). An
information need “is often considered to be the motivating force behind a user’s action
to seek information” (Naumer & Fisher, 2009, p. 2453). One of the assumed underlying
principles of information retrieval (IR) systems, including DLs, is that “users can express
their information needs by formulating queries which represent their topics” (Wang,
2011, p. 27). A topic generally represents what an information object or information
content is about. IR systems are designed to facilitate selection of “relevant material
from large collections of information in response to user queries” (Larson, 2012, p. 29).
The general model of IR shown in Figure 1 is based on the concept of
representation where the IR system represents information objects, users represent

1

their information needs, and a matching process returns information objects which may
help the user in resolving an information need (Lancaster & Warner, 1993).

Conceptual
Analysis

Collection of Information Objects

Human
Indexer

Translation into Representation
Human Indexing

Translation into Representation
and Preparation of Search
Strategies

Conceptual Analysis
Representations
of Information
Objects

Translation into Representation
Automatic Indexing

Matching Process

IR System

Conceptual Analysis
Information Need

User

Figure 1. The general model of information retrieval.
Adapted from Information Retrieval Today (p. 8), by F. W. Lancaster and A. J. Warner,
1993, Arlington, VA: Information Resources Press.

Translation into representation on both the user side and on the system side of
this model includes conceptual analysis. Conceptual analysis is an “examination of the
intellectual or creative contents of an information resource to understand what the item
is about and what the item is” (Taylor & Joudrey, 2009, p. 305). A conceptual analysis of
the user’s information need is taking place in the user’s mind. Similarly, the information
objects in an IR system undergo conceptual analysis, other processing, and receive
representations in a system. This process is called indexing. The indexing can be done
by humans, as when people (indexers) perform the process, or it can be automatic, as
in the case of full text indexing in IR systems or it can be a combination of both (semiautomatic).

2

An information object can consist of different types of concepts such as topics,
names, places, or time periods. All these concepts belong to a group of concepts called
subject concepts or simply the subject of an information object. This study is focused on
the topical concepts that comprise one type of subject concept.
Most people think of topical terms when asked to identify the subject of an
item. A topic can represent a principle object of attention or a theme
running through an information resource. Topics can be concrete or
abstract. Ideas, objects, phenomena, activity, process, structures, groups,
substances, and more can be the topics of works [e.g., information
objects]. (Taylor & Joudrey, 2009, p. 321)
Translation of the concepts into representations is achieved with the terms used
by an indexer or IR system. Indexers normally assign these terms to information objects
to create their structured representations. IR systems normally extract terms from the
content of the information objects to create a full text index. An optional component of
the assignment process is a controlled vocabulary. A controlled vocabulary is a list of
standardized terms that can be used to name the concepts. Controlled vocabularies
may be used to translate the terms initially used by an indexer (uncontrolled or free text
terms) into standardized (controlled) terms for the same concepts. Both uncontrolled
and controlled terms may or may not occur in the information object. There are different
types of controlled vocabularies such as subject heading lists, thesauri, and ontologies.
Also, there are different controlled vocabularies specific to the subject areas used by
different communities. “The primary purpose of vocabulary control is to achieve
consistency in the description of content objects and to facilitate retrieval” (Zeng & Qin,
2008, p. 105).
Searching and browsing are two major types of interactions between the users
and IRs (Wilson, 2000) and are the “most used search strategies” (Xie, 2008, p 1369).

3

The most common and useful type of browsing in DLs is by subject and this browsing is
provided through metadata (Miller, 2011, p. 99). “Metadata is broadly defined as
structured data about data” (Duval, Hodgins, Sutton, & Weibel, 2002, para. 56). For
example, metadata fields in a DL such as subject or topic may contain values which are
representations of topical concepts (one of the subject concepts) of an information
object and these fields are used for browsing or searching in IR systems.
In this study, representations of the topical concepts of the information objects
were called topical labels. The process of generating topical labels using a semiautomatic indexing method as performed in this study included two major steps: The
first step was automatic processing of the information objects resulting in intermediate
representations of the topical concepts. The second step was assignment of the topical
labels by a human indexer with the use of a controlled vocabulary.

1.1.2 Vector Space Model, Latent Semantic Analysis, and Rotated Latent Semantic
Indexing
The vector space model (VSM) is an algebraic model introduced by Gerard
Salton and his associates (Salton & Buckley, 1988; Salton, Wong, & Yang, 1975) for
representing text documents. A text document is a type of information object. In this
model, each document of a document collection is represented as a vector in a term
space. Each term selected for use (i.e., for indexing) is a dimension in this space and a
set of these terms is a vocabulary of the model. Terms in this model are normally all
words that occur in the documents, with the exception of stop words. They are
automatically selected and used to create representation of each document in the IR
system. The stop words are words with little conceptual load such as articles, pronouns,

4

conjunctions, etc. The components of a document vector are weights of the
corresponding terms in that document (see Appendix A for details on mathematical
foundations of the VSM and examples).
Figure 2 shows a two-dimensional example of two documents (Document A and
Document B) and a query, where only two terms (Term X and Term Y) are selected for
indexing. Similarities between documents (queries may be also treated as documents)
are represented as angles between the document vectors in the term space. Similarities
computed using a VSM model are used for relevance ranking in modern IR systems
and DLs, for example, DSpace and Fedora Commons (Apache Software Foundation,
2014). The number of dimensions in VSM equals the number of terms in the collection
selected for indexing which is the size of the collection’s vocabulary.
Term Y, weight
Document B
Query

similarity (Document A, Query)

θ

Document A

Term X, weight

Figure 2. Vector space model (VSM).

Another approach for representation of information objects, particularly text
documents, is latent semantic analysis (LSA) introduced by Deerwester et al. (1990).
5

This approach is built on VSM and provides a space for representing both terms and
documents. This space is introduced in addition to the VSM space and, unlike the VSM
terms space, uses abstract dimensions (see Figure 3-b).
Term Y, weight

Dimension 2

Document B

Rotated
Dimension 2

Rotated
Dimension 1

Document B
Term Y

Dimension 1
Concept L
(meanings)

Document A

Concept K
Term X
Document A

Term X, weight
(a)

(b)

Figure 3. Vector space model (VSM) term space (a) and latent semantic analysis (LSA)
abstract dimensions space with rotation of dimensions (b).

The maximum number of LSA dimensions is the lesser of two numbers: the
number of documents in the collection and number or terms selected for indexing (see
Appendix A for details on mathematical foundations of the LSA and examples). When
all LSA dimensions are used, the LSA model incorporates the original non-reduced
term-document weights matrix from the VSM model. So relationships between
documents and terms are represented in two types of spaces: One is the terms space
(see Figure 3-a) and another is the abstract dimensions space (see Figure 3-b).
The number of LSA dimension can be reduced and the reduced LSA model can
be used to approximate the relationships between terms and documents in the abstract

6

dimensions space. The reduced LSA model can be also used to compute alternative
weights for the new VSM term-document matrix which represents documents in a term
space (see Figure 3-a).
The advantage of this approximation is to describe the major associative
conceptual patterns and ignore the less important ones (noise) in the documents. The
relations between documents and terms in the reduced model depend on indirect
associations in the whole collection and allow representing underlying meaning of a
single term or document. The relations described by the reduced model also allow for
solving the synonymy problem and partially alleviating the polysemy problem
(Deerwester et al., 1990).
It was believed that “LSA dimensions as extracted are fundamentally
uninterpretable” (Landauer, Laham, & Derr, 2004, p. 5219). This conclusion suggests
that it is not possible to interpret a dimension by its association with a distinct concept
through the documents and terms because the documents and terms containing the
same concept may not be co-located in the vicinity of a given LSA dimension. For
example, in Figure 3-b, the angles between the Document A vector and the
Dimension 1 axis, and the Document B vector and the same axis are approximately
equal. The angles between these documents’ vectors and Dimension 2 are also of the
same magnitude.
Assuming that Document A and Document B contain two distinct unrelated
predominant concepts or, specifically, topical concepts, there is no predominant topical
concept or meaning that can be associated with Dimension 1 or Dimension 2. However,
Sidorova et al. (2008) demonstrated that applying post-LSA dimensions rotation can

7

solve the problem of interpretability of dimensions. This post-LSA rotation of dimensions
and a set of procedures for determining concepts associated with each rotated
dimension were referred to here as rotated latent semantic indexing (rLSI). Figure 3-b
shows rotated dimensions (also called factors) and concepts areas. The assumption
was made that Document A and Term X represent the predominant Concept K, while
Document B and Term Y represent the predominant Concept L. The new Rotated
Dimensions 1 and 2 may be associated with the Concepts K and L correspondingly.

1.1.3 User Experience and Domain Knowledge
There are many user-centered approaches for studying and evaluating IR
systems. O’Brien and Lebow (2013) proposed user experience framework as a “broader
conceptualization of information interaction and its evaluation” (p. 1543). User
experience is “a person’s perceptions and responses that result from the use or
anticipated use of a product, system or service” (International Organization for
Standardization, 2010, clause 2.15). The user experience is measurable and may
include such metrics as “some aspect of effectiveness (being able to complete a task),
efficiency (the amount of effort required to complete the task), or satisfaction (the
degree to which the user was happy with his or her experience while performing the
task)” (Tullis & Albert, 2013, p. 5). Robertson and Beaulieu (1997) suggested that
evaluation models in IR “may have a theoretical component (based on, for example, a
cognitive view of the search process)” (p. 56). The present study was focused on user
satisfaction with the search process as one of the central measures of evaluation of the
proposed indexing method.

8

The important factor that influences user experience and satisfaction with the
search process is domain knowledge which is the expertise of a user in the area or
discipline represented by the information objects. Marchionini, Dwiggins, Katz, and Lin
(1993) stated that domain knowledge helps “users to understand the problems and
have expectations of the possible answers” (p. 33).

1.2 Statement of the Research Problem
The body of information content in DLs worldwide is growing dramatically and is
becoming one of the key sources of knowledge for students, scholars, and the general
public. Accompanying this growth, new topics and trends are emerging rapidly in this
information content. In these conditions, representation of the information objects in DLs
is an important issue (Larson, 2012; Smucker, 2011).
One of the most important concepts for users in their interaction with DLs is the
topic (Miller, 2011). The challenge of the conceptual analysis on the user side is to
capture the key concepts describing an information need and relationships between
these concepts. Furnas, Gomez, Dumais, and Landauer (1983) found that random pairs
of people used the same word to describe an object only 10 to 20% of the time. This
problem is partially attributed to synonymy when a single word can have more than one
meaning. Further, the same concepts can be represented differently by the users and in
IR system.
The research problem that was addressed in this study is the lack of efficient
methods to create the representations of the topical concepts of information objects that
are aligned with the users’ mental model of the objects’ topical domain (e.g.,

9

engineering, medicine, history, etc.) or are adequate to the users’ expectations about
these representations (Chen & Ke, 2014; Tang, 2007; Tunkelang, 2009; Warner, 2010).

1.3 Study Goals
Research work in psychology and cognitive sciences over the past two decades
found a similarity between how humans and LSA establish relations between words and
concepts (Anderson & Pérez-Carballo, 2001; Kintsch, 2007; Landauer, 2007).
Consequently LSA has been used and studied as one of the IR models for full text
indexing (Kumar, Gupta, Batool, & Trehan, 2006; Manning, Raghavan, & Schütze,
2008). The rLSI is an LSA-based method that was used for semi-automatic topical
conceptual analysis of collections of documents, leading to topic extraction (Sidorova et
al., 2008). A number of studies have been done to show that rLSI is a promising method
for creating topical labels for collections of documents for the purpose of discovering
themes and trends or science mapping (Evangelopoulos, Zhang, & Prybutok, 2012;
Kulkarni, Apte, & Evangelopoulos, 2014; Winson-Geideman & Evangelopoulos, 2013).
However, there have been no studies investigating in detail the role of rLSI in the
IR or DLs environment. There have been no cognitive models proposed or studies done
linking rLSI-based representation of topical concepts to user satisfaction in an IR
environment. In addition, there are no studies where rLSI topical labels are part of the
IR process explicitly presented to users. Filling this literature gap was particularly
beneficial to the developers of IR systems and DLs. The present study focused on this
area of research. More specifically, the goal of the present study was to make a
contribution in the quest for efficient indexing and representation of information objects

10

in a way that matches user mental models so that the IR system can provide an
enhanced user experience.

1.4 Research Questions and Hypotheses
1.4.1 Research Questions
This study used the proposed topical labels guidance (TLG) model (see Figure 5)
as a general framework for the study and attempted to answer the following research
questions:
RQ1: Which type of topical labels (i.e., assigned by human indexers using
current best practices in DLs vs. generated with rLSI) results in higher user
satisfaction with the search process?
RQ2: How does domain knowledge influence the relationship between topical
labels type and user satisfaction with the search process?

1.4.2 Research Hypotheses
The following set of hypotheses was tested in order to answer the proposed
research questions and establish the relationships between the constructs of the
proposed TLG model (see Figure 5) and corresponding variables (see Table 4). The
hypotheses are:
H1: There is a direct relationship between topical label type and alignment: Given
a fixed level of domain knowledge, topical label type corresponding to rLSI
topical labels results in better alignment when comparing with other topical
label type.

11

H2: Domain knowledge moderates the effect of topical label type on alignment: A
higher level of domain knowledge results in a better alignment when topical
label type corresponds to rLSI topical labels when comparing with other
topical label type.
H3: There is a direct relationship between alignment and guidance: Higher
alignment is associated with higher guidance.
H4: Task type moderates the effect of alignment on guidance: Different task types
result in different levels of guidance for the same level of alignment.
H5: There is a direct relationship between guidance and satisfaction with the
search process: Higher guidance is associated with higher satisfaction with
the search process.

1.5 Purpose of the Study
This study explored whether rLSI topical labels can represent topical concepts of
the information objects in DLs and improve users’ satisfaction with the search process.
The study proposed a TLG model as a framework for investigation. The TLG model
allowed for examination of the cognitive aspects of user interaction with the DL in
relation to the use of rLSI topical labels.
The purpose of this study was to test a set of hypotheses related to the proposed
TLG model to understand the relationships among the variables. The outcomes of the
tested hypotheses provided the answers to the research questions of the study.

12

1.5.1 Theoretical Perspective
A theory can be briefly defined as a scientific prediction or explanation. Kerlinger
(1979) defined theory as “a set of interrelated constructs (variables), definitions and
propositions that presents a systematic view of phenomena by specifying relations
among variables, with the purpose of explaining natural phenomena” (p. 64). Theories
develop when the researchers test a prediction over and over in different settings.
The LSA theory of meaning was used in this study. Deerwester et al., (1990)
developed the foundations of LSA, and Landauer (2007) expanded these foundations
into the LSA theory of meaning. Besides IR and topic extraction applications, LSA was
has been applied in cognitive research (Kintsch, 2007), in education (Streeter,
Lochbaum, LaVoie, & Psotka, 2007), and in other areas of research.
This study proposed the generic search guidance (SG) model (see Figure 4) and
its special case, the TLG model. These models are based on the LSA theory of
meaning. The hypotheses of this research study formulated in the previous sections of
this chapter are based on the proposed TLG model.

1.5.2 Proposed Generic Search Guidance (GS) Model
Building on the generalized model of participating cognitive actor(s) in context
(Ingwersen & Järvelin, 2005), the technology acceptance model (Davis, Bagozzi, &
Warshaw, 1989; Davis, 1989), the information systems success model (DeLone &
McLean, 1992), and the LSA theory of meaning (Landauer, 2007), this study proposed
a generic SG model. The SG theorizes that an individual’s satisfaction with the search
process is determined by: presentation type, alignment between user’s mental (internal)
map and the external map displayed by the system, and presentation guidance (see

13

Figure 4). Using the generic SG model one can compare the effect of different
presentations on satisfaction in any search environment. Presentation types can refer to
the visual, textual, auditory types, or to the types that reflect semantics or ontologies
adopted in a given domain where a search system is used.
For example, an imaginary search system provides access to auto paints.
Presentation type may refer to the color samples, code list or color names, as well as to
the language of the color names. Different user types could be an auto technician, a
general customer, or a customer who speaks a language that is different from the
language of the system. These users have different initial alignments with different
presentation types. Once a user of the system establishes some acceptable level of
alignment he or she receives some level of guidance. Particularly, using presentations
provided by a search system a user is able (or not able) to find something he or she had
in mind. For example, by selecting of filtering by colors for a particular auto model a
user receives a presentation of only paints suitable for that model for further exploration.
Finally, this process can be measured as a satisfaction with the search process.

User type
(UT)

Presentation
type
(PT)

Alignment
between user’s
mental (internal)
map and
external map
displayed by the
system
(IEMA)

Presentation
guidance
(PG)

Satisfaction
with the search
process
(SSP)

User characteristics (UC)

Figure 4. Proposed generic search guidance (SG) model.

14

The presentation type that best matches the users’ mental (internal) map
provides more guidance; therefore it produces more satisfaction. However, different
users may have different internal maps, so one presentation type may not offer a
universal advantage. To address this issue, the generic SG model becomes complete
with the addition of user type. This construct serves as a moderator of the effect of
presentation type on alignment. Controlling for user characteristics (shown with dashed
line on Figure 4) allows rulings out the effects that are unrelated to the presentation
type, alignment, guidance, and user type.
The following propositions use the generic SG model to describe the user
experience when a search system interface displays elements that symbolically
represent the topical domain of the information content of the search system.
Proposition 1: Every person has a mental representation of any domain, albeit in
an incomplete form. In the extreme case of a completely unfamiliar
domain, humans create improvised representations with the help
of hypothetical thinking.
Proposition 2: As the level of domain knowledge increases, domain
representation in an individual’s mind approaches the
representation of a socially-constructed standard reference.
Proposition 3: Alignment of an individual’s mental representation of a domain
with an external symbolic representation of the same domain leads
to a state of cognitive convenience (i.e., a state of reduced
cognitive effort).

15

Proposition 4: A state of cognitive convenience leads to a sense of satisfaction
with the search process.
The generic SG model is composed of a set of theoretical constructs. The
constructs are summarized in Table 1. The constructs are assigned long names, short
names, and acronyms. Primarily the short names are used through the text of this study
and in the definitions of constructs that follow.
Presentation type: Type of objects’ presentations stored and displayed in a
search system. Overview of the presentations provides a user with system’s side map
or initial search cues.
User type: Type of the user by user’s knowledge about the objects stored in the
system and their presentations, and the user’s ability to read search cues.
Table 1
Proposed Generic Search Guidance (SG) Model Constructs Names
Construct long name
Presentation type
Alignment between user’s mental (internal) map and
external map displayed by the system
Presentation guidance
Satisfaction with the search process
User type
User characteristics

Construct short name
Presentation
Alignment

Acronym
PT
IEMA

Guidance
Satisfaction
User type
User characteristics

PG
SSP
UT
UC

Alignment: The degree of correspondence between the map of objects’
presentations displayed by the system and users’ internal mental map.
Guidance: The ability of characteristics of a system and objects’ presentations to
provide cues that lead the user to the next step.
Satisfaction: Satisfaction with the search process.
User characteristics: General characteristics of users such as age, gender, etc.
16

1.5.3 Proposed Topical Labels Guidance (TLG) Model
The proposed TLG model (see Figure 5) is a special case of the generic SG
model. Using the TLG model this study compared different types of topical labels in the
text IR system (i.e., DL) and hypothesized that domain experts develop a mental map
that is modeled by rLSI topical labels. In the TLG model, the constructs used in the
generic SG model become specific to the case of a text IR system. The TLG model is
composed of a set of theoretical constructs. Table 2 presents the TLG model constructs
long names, short names, and acronyms. Primarily the short names were used
throughout the text of this study and in the definitions of the constructs that follow.

Domain
knowledge
(DK)
H2
Topical label
type
(TLT)

H1

Task type
(TT)
H4
Alignment
between user’s
mental (internal)
map and
external map
displayed by the
system
(IEMA)

H3

Topical labels
guidance
H5
(TLG)

Satisfaction
with the search
process
(SSP)

Control
Demographics (DM): Gender, Age group

Figure 5. Proposed topical labels guidance (TLG) model.

Topical label type: Type of topical labels (generated with rLSI or with other
method) used for presentations of the objects stored in the IR. An overview or list of the
topical labels provides system’s side map or initial search cues.
Domain knowledge: A user’s knowledge about topical domain of the information
objects and their presentations stored in the system, and ability to read search cues.
17

Table 2
Proposed Topical Labels Guidance (TGL) Model Constructs Names
Construct long name
Topical label type
Alignment between user’s mental (internal) map and
external map displayed by the system
Topical labels guidance
Satisfaction with the search process
Domain knowledge
Task type
Demographics

Construct short name
Topical label type
Alignment

Acronym
TLT
IEMA

Guidance
Satisfaction
Domain knowledge
Task type
Demographics

TLG
SSP
DK
TT
DM

Alignment: The degree of correspondence between the map of objects’
presentations displayed by the system and users’ internal mental map.
Guidance: The ability of characteristics of a system and objects’ presentations to
provide cues that lead the user to the next step.
Satisfaction: Satisfaction with the search process.
Demographics: General characteristics of users such as gender, age, etc.
Task type: Type of the task or query that user performs, for example, searching
for more specific or more general topical concepts.
Table 3 provides correspondence between the constructs of the SG and the TLG
models. The TLG model explains how a topical label type provides user guidance
during a search.
The TLG model posits that the alignment of a latent semantic structure implied by
the topical labels and a latent semantic structure that exists in the user’s mind increases
the topical labels’ potential to provide guidance during the search. In turn, when the
topical labels provide more guidance, the user gets a higher level of satisfaction with the
search process.

18

Table 3
Correspondence between SG and TLG Model Constructs
Construct in generic SG model
Presentation type (PT)
Alignment between user’s mental (internal) map and
external map displayed by the system (IEMA)
Presentation guidance (PG)
Satisfaction with the search process (SSP)
User type (UT)
N/A
User characteristics (UC)

Construct in TLG model
Topical label type (TLT)
[the same as in GS model]
Topical labels guidance (TLG)
[the same as in GS model]
Domain knowledge (DK)
Task type (TT)
Demographics (DM)

1.5.4 Variables
A variable is a characteristic or attribute that varies and is subject to
measurement and observation. A change in one variable may cause other variable to
change (i.e., vary). Causation may be expressed as a probability, especially in studies
with humans, because in such studies it is not possible to absolutely prove cause end
effect (Creswell, 2013, p. 50).
Independent variables, also called treatments or exogenous variables, are those
that may be the probable cause of an outcome. In this study the independent variable is
topical label type. This variable was physically controlled through the use of two
identical systems (except that each system used different topical label type).
Dependent variables, also called outcome, effect, or endogenous variables, are
those that change based on the effect of the independent variables. The dependent
variable in this study is satisfaction with the search process.
Mediator variables stand “between the independent and dependent variables,
and they mediate the effect of the independent variables on the dependent variables”
(Creswell, 2013, p. 50). Mediator variables in this study are alignment and guidance.
19

Moderator variables qualify the relationships between variables by influencing the
casual process (Brewer, 2000, p. 5). The moderator variables in this study are domain
knowledge and task type.
Control variables potentially influence the dependent variables and are controlled
in the research design so that true influence of the independent variable on the
dependent can be determined. Control variables are potentially confounding variables
that need to be accounted for but they do not drive a model. The two control variables
from the demographics group were used in the TLG model: gender of the participants
and age group of the participants. The assumption of the model is that regardless of the
gender or age group, alignment has significant effect on guidance and guidance has
significant effect on satisfaction. Other demographic characteristics of the participants
collected in the experiment were degree level, major, and proficiency in English. These
characteristics were used to describe the sample but did not serve as variables in the
TLG model.
Table 4 shows how variables are related to the hypotheses of the study.
Table 4
Variables and Hypotheses
Variable name
Topical label type
Alignment
Domain knowledge
Guidance
Task type
Satisfaction
Demographics: Gender
Demographics: Age group

Variable type
Independent (exogenous)
Mediator (endogenous)
Moderator
Mediator (endogenous)
Moderator
Dependent (endogenous)
Control
Control

20

Variable scale
Dichotomous
Likert scale (assumption: interval)
Likert scale (assumption: interval)
Likert scale (assumption: interval)
Dichotomous
Likert scale (assumption: interval)
Dichotomous
Nominal

Hypothesis
H1
H1; H3
H2
H3; H5
H4
H5

1.6 Justification and Significance of the Study
Topical labels (i.e., subject terms) assigned to the information objects in DLs
systems are utilized by users during their information seeking process to browse and
find relevant information. Topical labels result from indexing. During the process of
indexing in DLs, the representations of topical concepts of information objects are
translated into topical labels and then into metadata values that can be browsed by
users.
Topical labels may be determined manually (by a human indexer who examines
each information object) or by using automatic indexing methods. Recently, semiautomatic methods are receiving more attention in DL contexts because they combine
the benefits of both manual and automatic methods. Park and Lu, (2009, p. 230)
showed that “…practical applications of semi-automatic metadata generation in libraries
seem to be at an incipient stage.” In another study, Greenberg, Spurgin, and Crystal
(2006) found that “Metadata experts are in favour of using automatic techniques,
although they are generally not in favour of eliminating human evaluation or production
for the more intellectually demanding metadata” (p. 3). These studies suggest that more
research is needed on the development and evaluation of semi-automatic methods of
subject metadata creation.
Evidence about whether the rLSI semi-automatic method of topical labels
generation, which has previously not been used in IR system applications, can improve
user satisfaction with the search process is important for both researchers and
developers. I believe that this study adds to the scholarly research by providing such
evidence and has the potential to improve practice.

21

1.7 Overview of the Research Design
This study used a quantitative research design framework to examine the
relationship among variables using the proposed TLG model.
The natural collection of the documents, which are articles from a single journal,
was used for the experiment. The topical labels for this collection were generated using
rLSI. This generation included automatic determination of several candidate terms and
documents for each rLSI dimension (i.e., factor). These terms and documents were
determined by their higher weight (i.e., load) on the corresponding dimension. Then
topical labels were assigned to each rLSI dimension representing a topical concept. All
documents that are related to a given rLSI dimension were assigned the corresponding
topical labels.
A baseline and the proposed experimental systems were built and they
processed the collection of documents. The rLSI topical labels were assigned to the
documents and presented to users in the form of browsable metadata in the proposed
experimental system. Other standard topical labels were used in the baseline
experimental system.
Participants were recruited for this study using convenience sampling. The
instrument for measurement of the constructs was designed. This instrument included a
questionnaire. An experiment was conducted by asking the participants to perform two
search tasks given to the participants in the form of queries.
Data were collected and analyzed. With the results emerging from the analysis,
the study tested the hypotheses allowing me to draw conclusions about relationships
among variables expressed in the model. The statistical procedure used for the study is
partial least squares structural equation modeling (PLS-SEM). The analysis presented
22

an integrated model for the entire TLG model. The results of the data analysis were
presented, interpreted and conclusions were drawn.

1.8 Scope and Limitations
The results of this study are not generalizable for the entire population because
non-probability sampling method (convenience sample) was used. When conducting
studies with the use of non-probability sampling “researchers should exercise caution
when generalizing from their data and be explicit about sampling limitations in their
reports” (Kelly, 2009, p. 67). The other limitation of the study is related to the search
task: The process of transformation of information-seekers’ information needs into a
query is outside of the model proposed in this study. Research on task types is also
outside the scope of this study.

1.9 Summary
This chapter presents background and overview of the areas related to the
problem. The overview includes explanation and definitions of the concepts and
processes that are important for this study. Based on the background and overview, the
research problem, research questions, and hypotheses are stated. The TLG model is
proposed and associated constructs and variables are defined. Significance of the
study, overview of the research design, and scope and limitations are stated.

23

CHAPTER 2
REVIEW OF THE LITERATURE
2.1 Introduction
This chapter presents review of the literature on LSA and LSA theory of meaning.
Overview of new human-centered IR areas of research and measures commonly used
in IR evaluation are discussed. Methodological approaches and traditional models used
in IR evaluation are described.

2.2 Latent Semantic Analysis
2.2.1 Overview
LSA, also called latent semantic indexing (LSI) in IR applications, was first
introduced in (Furnas et al., 1988) and (Deerwester et al., 1990) as a new method for
automatic indexing and retrieval. The authors assume that “there is some underlying
latent semantic structure in the data that is partially obscured by the randomness of
word choice with respect to retrieval” (Deerwester et al., 1990, p. 391). The goal is to
design a model of relationship between terms and documents that might predict what
the term-document associations relay should have been. For example, this model would
predict that “a given term should be associated with a document, even though, because
of variability in word use, no such association was observed” (Deerwester et al., 1990,
p. 391).
With this method, observed term-document associations (i.e., term occurrences)
are replaced by indicators computed using proposed statistical techniques. This
technique estimates and takes into account latent structure, and get rid of the “noise.”
Afterwards, computed indicators of document-term associations are used for retrieval.

24

“The approach is to take advantage of implicit higher-order structure in the association
of terms with documents (‘semantic structure’) in order to improve the detection of
relevant documents on the basis of terms found in queries” (Deerwester et al., 1990, p.
391).
Proposed statistical technique is based on the singular value decomposition
(SVD). See Appendix A for details on mathematics of LSA. SVD represents both terms
and documents as vectors in a space of chosen dimensionality, and the dot product or
cosine between points in the space gives their similarity. In contrast with traditional
vector-space model where dimensions are terms, here dimensions are orthogonal
factors—artificial concepts representing conceptual content of the collection. This
method allows for finding major patterns in the term-document associations and
diminishing less important patterns. Information about these major patterns is used to
discover latent term-document associations. For example, some terms may be not
found in a document but may still be considered related to the document if patterns of
these terms across all documents are pointing at that relation.
“LSA constitutes a fundamental computational theory of the acquisition and
representation of knowledge” (Landauer, Foltz, & Laham, 1998, p. 262). It is a method
for extracting and representing the contextual-usage meaning of words by statistical
computations applied to a large corpus of text. Similarity estimates derived by LSA are
not simple proximity frequencies, co-occurrence counts, or correlations in usage. These
similarities infer much deeper relations (latent semantic) obtained with powerful
mathematical analysis in which LSA performs inference or induction (Landauer et al.,
1998).

25

LSA benefits depend of the chosen dimensionality. Given an n x p termdocument weights matrix of rank r, LSA projects n documents and p terms into the kdimensional space, where k is number of singular values or dimensions. Dimensionality
reduction removes overfitted information and noise form the similarity model. In most
applications of LSA choice of number of dimensions is informed by ad hoc criteria.
However, there are methods for determining dimensionality based on strong theoretical
ground. One of them, amended parallel analysis (Efron, 2005), provides estimation of
optimal dimensionality that led to the very good results in terms if IR systems precision
and average search lengths.

2.2.2 LSA Theory of Meaning
There are two approaches to the representation of meaning: mental lexicon
approaches that list word meanings like a huge dictionary in the mind, and generative
approaches that construct meaning out of certain elements (e.g., semantic markers or
LSA vectors) via some sort of composition rules (Kintsch, 2007).
“LSA theory addresses the problem of exactly how word and passage meaning
can be constructed from experience with language, that is, by what mechanisms—
instinctive, learned, or both—this can be accomplished” (Landauer, 2007, p. 3).
Successful simulation of human word and passage meaning can depend strongly on
giving it a sufficiently large and representative text corpus to learn from, just as humans
need vast experience to learn their languages.
LSA can be used to align conceptual spaces, for example different languages.
Littman and Jiang (1998) applied the technique to retrieval from three languages when
only partial alignments were available. They showed how LSA-based analysis could be

26

used for English-Spanish retrieval, even when only French-English and French-Spanish
corpora were available. In one analysis, separate LSA spaces were developed for
French-English and French-Spanish. French terms that are common to the two spaces
can be used for alignment.
LSA can be used as a computational model to learn word meanings from vast
amounts of exposure to text, just as humans do, so that it could tell what an inquiring
person’s words meant. What LSA can do has pointed to a new path in the study and
engineering of meaning (Landauer, 2007).

2.3 Human-Centered Information Retrieval Areas of Research
2.3.1 Human-computer Information Retrieval
Most modern search engines are designed for scenarios where users formulate
their information needs in form of the queries and search engines provide the result so
that users can select the desired ones. In these scenarios the search engines serve as
means of providing access to information. “However, this type of support is insufﬁcient
for tasks requiring more involved information interaction… and situations where
information behavior encompasses more than just information seeking” (White et al.,
2013, p. 1053). People are interested in more active engagement in the search process
that would include learning and investigation. Expert searchers, for example, can
formulate analytical queries using traditional search systems but they also require
“browsing services that also provide annotations and result manipulation tools”
(Marchionini, 2006a, p. 43).

27

To answer these challenges “search systems need to form symbiotic relationship
with their users” (White et al., 2013, p. 1053). This relationship between human and
machine can be built using various strategies including cognitive models.
There are several areas where researchers are working on solving these
problems. Marchionini (2006b, p.20) coined the term human-computer information
retrieval (HCIR) that he defined as a study of methods that “combine the lessons from
designing highly interactive user interfaces with the lessons from human information
behavior to create new kinds of search systems that depend on continuous human
control of the search process.” According to White et al., (2013), the sub-disciplines of
HCIR area of research are interactive information retrieval (IIR), information retrieval in
context (IRiX), and exploratory search (see Figure 6).
human-computer
information retrieval
(HCIR)

interactive
information
retrieval (IIR)

information retrieval
in context (IRiX)

exploratory search

Figure 6. Sub-disciplines of the human-computer information retrieval area of research.

One of the goals of the HCIR conception is building the search systems that get
people closer to the meaning of the information they need. Such systems have to
deliver not only list of relevant objects, but also provide a way for making meaning with
those objects (Marchionini, 2006b, p. 21).

28

2.3.2 Interactive Information Retrieval
Kelly and Sugimoto (2013, p. 745) state that research in the area of IIR combines
research from IR, human-computer interaction (HCI), and information behavior (see
Figure 7).

information
retrieval (IR)
information
behavior

human-computer
interaction (HCI)
interactive
information
retrieval (IIR)

Figure 7. Areas of research related to interactive information retrieval.

IIR is focused on studying of how people manage their information problems, on
people’s information search behaviors, and their interaction with information systems
using interfaces and search features. Unlike classic IR research, IIR does not have
prescribed experimental methods and uses a range of methods and measures from
different fields because IIR requires simultaneous evaluation of user behavior and
system interfaces (Kelly & Sugimoto, 2013, p. 746).
One of the problems of the IIR research is an increased need for valid and
reliable measures that can be used for different interactive search models and
experiments. Many of the measures used for IIR evaluation are derivative of the
precision and recall measures developed to evaluate systems in batch-mode. For
example, the measures of relative relevance (RR), ranked half-life (RHL) (Borlund &
Ingwersen, 1998), cumulated gain (CG) and cumulated gain with discount (DCG)
(Järvelin & Kekäläinen, 2000). General usability measures such as efficiency and

29

satisfaction can be used in IIR evaluation. However, they are widely variable and
usually created by researchers for specific experiments and are often of questionable
validity and reliability (Kelly & Sugimoto, 2013, p. 746).
The Interactive Track at Text REtrieval Conference (TREC) developed a series of
experimental designs and methods that were used to study both the search process
and the final search outcome. These methods evolved between 1994 and 2002 and
have been the closest of a standard for IIR evaluations (Kelly & Sugimoto, 2013, p.
746). For example, TREC-6 concentrated on a new experimental design comparing an
experimental system to a control system. The goal was to develop a measure to
indirectly compare different experimental systems. However, in general, many of the
results of the Interactive Track studies “have been inconclusive, illustrating the extreme
difficulty with user testing of systems (as opposed to usability testing)” (Harman &
Voorhees, 2006, p. 133).
Kelly (2009) identified continuum that relates different types of studies in IIR (see
Figure 8) and middle of this continuum constitutes classic IIR studies. These studies are
evaluations of IIR systems and interfaces conducted as experiments that used
questionnaires.
Human Focus

Archetypical IIR Study

System Focus

Information-Seeking Behavior in Context
Information-Seeking Behavior with IR Systems
Experimental Information Behavior
TREC Interactive Studies
Log Analysis
Filtering and SDI
“Users” make relevance assessments
TREC-style Studies

Figure 8. Research continuum for conceptualizing IIR research.
Adapted from “Methods for Evaluating Interactive Information Retrieval Systems with
Users,” by D. Kelly, 2009, Foundations and Trends® in Information Retrieval, 3(1–2),
p. 10.
30

2.4 Information Retrieval Evaluation
2.4.1 Measures in IIR Research Evaluation
Evaluation was always a major part of IR research. Büttcher, Clarke, and
Cormack (2010) stated that the “purpose of evaluation is to measure how well IR
methods achieve their intendant purpose” (p. 406). These methods can refer to indexing
and ranking algorithms, user interfaces, and other features of the systems. Even though
larger range of methods is available in user-centered research in comparison to systemcentered research, relatively few user-centered studies are reported in the context of
IIR.
Kelly and Sugimoto (2013) and Kelly (2009) categorized the measures that are
normally used in IIR research into four conceptual classes (see Figure 9). These are
contextual measures, performance measures, process and interaction measures, and
usability in IIR measures. Performance measures characterize how successful a
participant is in a specific task (e.g., recall, precision, and accuracy). Process or
interaction measures describe the interaction between that subject and the system (e.g.,
number of clicks, number of queries, number of document viewed, and time taken to
complete the search).
Usability measures characterize the subjects’ perceptions of and attitudes about
the system and their experiences with the system. Usability measures are normally
obtained from users using questionnaire such as questionnaire for user interaction
satisfaction (QUIS) or custom questionnaires. Often IIR researchers utilize usability
measures such as general satisfaction, satisfaction with the search results, ease of use,
usefulness, understandability, user-friendliness, and tailor these measures to speciﬁc
features of the system.
31

IIR research constructs

contextual

measures

process or
interaction

performance

number of clicks,
number of
queries, number
of document
viewed, time
taken to complete
the search;
(some come from
search logs)

recall,
precision,
accuracy

effectiveness
in IIR, IR, or
HCI

efﬁciency in
IIR, IR, or
HCI

IR research
constructs

usability in
IIR
general
satisfaction,
satisfaction with
the search
results, ease of
use, usefulness,
understandability,
user-friendliness
(attitudes and
preferences)
satisfaction
usability
in HCI

HCI research
constructs

Figure 9. The constructs (classes) and measures traditionally used in IIR, IR, and HCI.
Based on the review by Kelly (2009).

Usability as a measure has been traditionally deﬁned in ISO and general HCI
literature and comprises effectiveness, efﬁciency, and satisfaction (International
Organization for Standardization, 1998).
The classes, constructs, and measures traditionally used in IIR, IR, and HCI
areas of research are related (see Figure 9). The dashed lines show approximate
correspondence when semantics of the constructs and measures have some
differences that depend of the area of research. For example, in IIR research usability

32

construct or class is separated from performance and process constructs (in IR the
related constructs that share similar measures are effectiveness and efficiency).
However, in HCI its central construct usability comprises of effectiveness, efficiency and
satisfaction.

2.4.2 Methodological Approaches to Evaluate IR Systems
Evaluation studies of IR systems “can provide strategic guidance for the design
and deployment of future systems, can assist in determining whether systems address
the appropriate social, cultural, and economic problems, and whether they are as
maintainable as possible” (Borgman, Sølvberg, & Kovács, 2002, p. 7).
The traditional and most widely used performance measures for IR are recall and
precision, in which recall measures the ability of the system to retrieve all relevant
materials, and precision measures the ability to retrieve only relevant materials. “A
classical retrieval experiment uses a laboratory environment in which many variables
are controlled: the document collection is static, the queries are provided in a standard
form, and the documents that are relevant to the query are known a priori” (Rasmussen,
2005, p 99).
One of the classical examples of utilizing precision and recall measures for
evaluating IR systems is study of various term-weighting approaches by Salton and
Buckley (1988). The authors performed a number of term-weighting experiments. A
term-weight is a combination (product) of several components: term frequency,
collection frequency, and length normalization component. Each component may be
defined using different approach and formula. Researchers used six document
collections of different sizes and domains: biomedical, computer engineering, National

33

Physical Lab, and others. National Physical Lab collection was in indexed form only (not
in original natural language form). Collection of user queries was used for retrieval.
Average performance over these queries was calculated as well as average precision
(for different recalls) and rank.

2.5 Models in Information Retrieval and Information Seeking Research
The task of IR system is to retrieve documents with information content that is
relevant to a user’s information need. IR includes two activities: indexing and searching.
Indexing refers to the way items and requests are represented for retrieval purposes.
Searching refers to the way the items are examined and taken as related to search
query. How these activities are distributed between the time when items added to the
system and when searching is done, and how the indexing and searching process are
carried out, can vary enormously. “The two activities of indexing and searching have
formed the focus of most of the research that has been carried out by the IR community.
However, there is now increasing interest in studies of the ways that people use IR
systems and how user-system interactions should be organized to facilitate effective
retrieval” (Jones & Willett, 1997, p. 1). These studies are becoming more important as
most IR systems are moving to Web environment. While indexing and searching are
central activities of IR, modern IR systems can support different forms of retrieval, like
browsing and sophisticated visual representation.

34

2.5.1 Ingwersen’s Model
Another model is suggested by Ingwersen and Järvelin (2005) where authors
seek to establish conceptual framework for research and on information seeking and
retrieval phenomena, founded on the holistic cognitive viewpoint.
The model (Figure 10) emphasizes the information processes that are executed
during information seeking and retrieval in context over time. Numbers on the model
deal with process of interaction. First, process of social interaction (1) is found between
the actors and their past and present socio-cultural or organizational context.

Org.

Information
objects

Cognitive
Actors(s)
(team)

Interface

IT: Engines
Logics
Algorithms

Social
Context

Cultural
Cognitive transformation and influence
Interactive communication of cognitive structure

Figure 10. Interactive information seeking, retrieval and behavioral processes.
Generalized model of any participating cognitive actor(s) in context.
Adapted from The Turn: Integration of Information Seeking and Retrieval in Context
(p. 261), by P. Ingwersen and K. Järvelin, 2005, Dordrecht, Netherlands: Springer.

Second, Information interaction also takes place between the cognitive actors
and the cognitive manifestation embedded in the IT and the existing information objects
via interfaces (2/3). “The latter two components interact vertically (4) and constitute the
core of an information system. Third, cognitive and emotional transformations and
35

generation of potential information may occur as required by individual actor (5/7) as
well as from the social, cultural or organizational context towards the IT and information
object components (6/8) over time” (Ingwersen & Järvelin, 2005, p. 262).

2.5.2 Evaluation Using Technology Acceptance Model
Another approach for evaluation information systems including IR systems is
using technology acceptance model (TAM). TAM was developed by Fred Davis and
Richard Bagozzi (Davis et al., 1989; Davis, 1989). TAM integrated diverse theoretical
perspectives and built on social psychology research and constitutes a model of
adoption and use (Figure 11). The purpose of the author’s study was to find better
measures for predicting and explaining use.

Perceived
Usefulness
(U)
Attitude
Toward
Using (A)

External
Variables

Behavioral
Intention
to Use (BI)

Actual
System
Use

Perceived
Ease of
Use (E)

Figure 11. Technology acceptance model (TAM).
Adapted from “User Acceptance of Computer Technology: A Comparison of Two
Theoretical Models,” by F. D. Davis, R. P. Bagozzi, and P. R. Warshaw, 1989,
Management Science, 35(8), p. 985.

The study focused on two theoretical constructs, perceived usefulness and
perceives ease of use, which are considered as fundamental determinants of system
use. The author defines perceived usefulness as “the degree to which a person believes

36

that using a particular system would enhance his or her job performance”, perceived
ease of use is defined as “the degree to which a person believes that using a particular
system would be free of effort” (Davis, 1989, p. 320). The idea was that users’ opinion
about usefulness of the system and ease of use determines future use which directly
indicate how good particular system is. A custom process was used to develop multiitem scales for perceived usefulness and perceived ease of use having high reliability
and validity. These scales were used to examine the empirical relationship between
these two measurements and self-reported indicators of system use. For example, one
of the items from the scale for perceived usefulness was formulated like “My job would
be difficult to perform without electronic mail”. The example item from the scale for
perceived ease of use: “I often become confused when I use the electronic mail system”
(Davis, 1989 p. 324).
Indicators of system use were self-reported current use and self-predicted future
use. These indicators were measured by asking participants to self-report their degree
of use on six-position categorical scales: “Don't use at all”, “Use less than once each
week”, “Use about one each week”, “Use several times a week”, “Use about once each
day”, “Use several times each day”.

2.5.3 Usability Evaluation
The definition of usability in the ISO 9241 standard is: “The extent to which a
product can be used by specified users to achieve specified goals with effectiveness,
efficiency, and satisfaction in a specified context of use” (International Organization for
Standardization, 1998, p. 4). Usability, which is synonym with overall user satisfaction,
is influenced by many factors. Major goals for usability evaluation include: predicting

37

user satisfaction; understanding appearance, behavior, and user interaction
approaches, problem determination and resolution, criteria validation, competitive
assessment (Torres, 2002).
There are several techniques of usability evaluation: reviews, walk-throughs, labbased tests, in house tests, field tests, etc. Performing an evaluation of a product
without a structured technique or predetermined criteria, guidelines, and methods is an
ad hoc review. The review is performed using documentation, prototype, or actual
product. Each evaluator identifies perceived problems in an informal manner. A heuristic
review evaluated a system via documentation, prototypes, or during actual use. One or
more individuals perform the evaluation using common heuristics and procedures.
Techniques that inspect a product by stepping through a user interface with enduser scenarios are a walk-through. One or more evaluators perform a walk-through
using documentation, prototypes, or products. A walk-through can also be conducted
with end users who provide feedback (Torres, 2002).
Conventional and formal evaluations of product usability are conducted in a
laboratory environment with cameras, recorders, one-way mirrors, etc. However,
informal usability evaluations of an equally effective manner are conducted with a user
and test conductor sitting side-by side in an office or conference room.

2.6 Summary
Evaluation of the system is essential part of IR system development. Traditional
approaches of evaluation based on Cranfield evaluation model do not explicitly consider
the user. Various researchers have expressed a demand for alternative approaches to
the performance evaluation of IR systems.

38

CHAPTER 3
RESEARCH DESIGN
3.1 Introduction
This chapter presents the study’s design for investigating the research problem.
The research problem of this study is lack of efficient methods for creation of the
information objects’ topical concepts representations that are aligned with the users’
mental model of the topical domain or adequate to the users’ expectations about these
representations.

3.2 Research Design Framework
Decisions about research design framework (e.g., quantitative, qualitative or
mixed) are defined by the basic philosophical assumptions of the study, the types of
strategies of inquiry, also called research strategies, and by specific research methods
used in conducting these strategies (Creswell, 2013, pp. 3–4). The research design,
however, is primarily influenced by the nature of the research problem and research
questions or hypotheses.
This study investigates applied aspects of the LSA theory of meaning (Landauer,
2007) and examines the relationship among variables using the proposed TLG model.
These key elements suggest that the use of a quantitative research design is
appropriate for this study. The following sections discuss the details of the research
design framework.
This study is based on the postpositivist scientific method, a “deterministic
philosophy in which causes probably determine effect or outcomes” (Creswell, 2013,
p. 7). According to this scientific method, also called the philosophical worldview, the

39

theories that describe the world need to be tested and refined so that we can better
understand the world. These problems can be studied by identifying and assessing the
causes that influence the outcomes. The problems or ideas can be expressed as
variables and research questions and can be measured, for example, in experiments
(Creswell, 2013, p. 7).
The present study began with the existing LSA theory of meaning and the
proposed TLG model. The relationships among variables of the model were reflected in
the hypotheses.
The choice of a strategy of inquiry is a part of the research design. This study
employed an experimental research strategy. The experiment was conducted with the
participants performing search tasks using baseline and proposed experimental
systems. A questionnaire collected quantitative descriptions of attitudes and the
opinions of the participants regarding the experience with the experimental systems.
Data were collected during the experiment using the instrument and measures
completed by the participants, and these data were analyzed. The study tested the
hypotheses, drew conclusions about relationships among variables expressed in the
proposed model, and interpreted the results. However, the postpositivist worldview
assumes that absolute truth can never be found and evidence found in research is
always imperfect (Creswell, 2013, p. 7). Therefore issues of validity and reliability were
also discussed.

3.3 Collection of Documents for Experimental Systems
This study used a natural collection of documents (research articles from a single
journal) as the content processed by the two experimental search systems. The domain

40

of the collection is information science because it matches the domain knowledge of the
researcher and is close to the domain knowledge of the participants in the experiment.
Using a collection of information science articles also closely modeled real life academic
situations wherein students or scholars search for literature on a given topic in a given
domain, database, or within a selected journal.
The fact that this study did not use the standard collections (such as TREC,
CLEF, etc.) lessened the replicability of the study. However, the use of a natural
collection of documents implied that the materials of the study approximated the realworld that is being examined, and thus improved the ecological validity of the study.
Ecological validity in an experimental study characterizes how tasks, materials, and task
contexts represent or approximate the real world which is being examined (Brunswik,
1955; Payne & Howes, 2013, p. 80). “Ecological validity is too restrictive a
conceptualization of generalizability for research that is designed to test causal
hypotheses. Ecological validity is, however, crucial for research that is undertaken for
descriptive or demonstration purposes” (Brewer, 2000, p. 12).
In the experiment conducted in this study, the participants could search the
documents only using one available search box called Title and Abstract. This search
box corresponds to the ancillary title-abstract metadata field that combines and
represents the values from two other metadata fields: title and abstract. This ancillary
field was indexed and processed to enable LSA searching using titles and abstracts of
the documents as semantic sources. Other metadata fields such as author, year,
separate title and abstract fields, etc., were also stored in the experimental systems and
their values were used in the search results view. The participants saw the search

41

results list including basic metadata for each document in the list and were able to
explore full text of the relevant documents. This workflow required providing the
participants with the URL links to full texts of the documents openly available on the
Internet or storing full text of the documents in the experimental systems. The
advantage of storing the full text of the documents in the experimental systems was an
ability to display query-matching snippets of text in the search results which also
included matches against the full text (even though the full text was not actually
searched for the purposes of retrieval and ranking in this experiment). Therefore the
option of storing the full text of the documents in the systems was used. However,
according to the licensing agreement of the University of North Texas (UNT) with the
publishing companies that provide the content to UNT users by subscription, users
cannot create a secondary database for the storage of the licensed electronic products.
This requirement limited the choice of the collection to open access journals.
Another limitation for the choice of the documents for the collection was the
availability of topical labels assigned by publisher, citation indexing service, or full text
bibliographic database. The topical labels assignment by these services is normally
done by the human indexers using best practices. These topical labels, which are
actually subject terms, were used in the baseline experimental system as one of the
topical label types (control). The other topical labels type (treatment) were topical labels
generated with rLSI and used in the proposed experimental system.
According to the ISI Web of KnowledgeSM Journal Citation Reports® the most
highly cited open access journal in the category of information science and library
science is Information Research (http://www.informationr.net/ir/). The articles stored on

42

this journal web site do not have topical labels. However, this journal is indexed by the
EBSCOhost full text database and topical labels (i.e., subject terms) were assigned by
professional indexers for each article published from 2005 onwards. The Information
Research journal was chosen to create a collection of the documents for the
experiment. The full text of 266 articles from this journal was loaded to the experimental
systems. The collection included all articles published between 2005 and 2013. The
metadata for the articles was imported to the experimental systems from the
EBSCOhost database. The number of document in this collection is comparable to the
number of articles in other typical electronic journal collections covering time periods
between 2–3 years (e.g., Journal of the Association for Information Science and
Technology, Information Processing & Management) and 8–10 years (e.g., D-Lib
Magazine, Annual Review of Information Science and Technology).

3.4 Participants
The implied population of the study is all people who engage in online
information search in DLs. The nature of this study (and the majority of the experimental
studies in IR that involve people) makes it impossible to meet all of the criteria of
probability sampling of participants. Therefore, this study used a non-probability
sampling method, a convenience sample where participants are chosen based on their
convenience and availability. The details about the impact of the chosen sampling
method on the generalizability (also called external validity in the quantitative research)
are provided in the sections that follow.
The recruitment was carried out by contacting students in classes in the College
of Information and the College of Business at UNT. Individuals were randomly assigned

43

to groups at the time when they were staring participation. Each participant performed a
search task with two experimental systems: baseline and proposed. Participants
engaged with the experimental systems in two different orders: Half the participants
worked with the baseline system first, then with the proposed system; the other half of
the participants reversed this order. The order in which the participants would engage
with the systems was determined at random. There were also two search tasks in this
experiment, and each of these tasks was randomly assigned to approximately half of
the participants requiring a total of four groups.
The minimum number of participants in the experimental sample depends of the
type of the method used for the data analysis such as detecting the difference between
groups (ANOVA), examining relationships (regression analysis), factor analysis, etc.
The minimum number also depends of the desired effect size and statistical power. In
this study relationships between the variables were investigated. Green (1991)
describes the procedures used to determine regression sample sizes. He suggests the
following formula for the sample size: N > 50 + 8m for testing the multiple correlation
and N > 104 + m for testing individual predictors, where m is number of independent
variables. The total number of independent, mediator, moderator, and control variables
in this study is 7. In order to meet these recommendations the total minimum number of
participants must be greater than 111. This number is slightly larger than the number of
participants that are normally used in IR user studies because those studies primarily
employ methods of detecting the differences between groups. For example, Kelly and
Sugimoto (2013) analyzed 127 IIR experimental studies completed between 1967 and
2006 and reported that mean number of participants in the studies was 37.07.

44

3.5 Experimental Design
The design of this study incorporates a within-groups design with respect to the
topical labels type, in which each participant tested each of the two experimental
systems: the baseline and proposed system. This type of design normally provides
greater power with fewer subjects compared to the between-group design. Also, withingroups design allows for subjects to make comparisons of different systems and
indicate preferences. Between-group design is more prevalent in studies of search
behavior than system evaluation because exposure to one condition may bias
performance in another condition (Kelly & Sugimoto, 2013, p. 758). However, in this
study between-group design was used with respect to the task type. Four groups of
participants were used in the experiment (Table 5).
Table 5
Experimental Design
Between-group design with respect to a task type
Task (query) 1
Task (query) 2
Within-group design
Within-group design
with respect to a topical label type:
with respect to a topical label type:
Alternative order of control and treatment
Alternative order of control and treatment
Group 1
Group 2
Group 3
Group 4
EBSCOhost labels
rLSI labels
EBSCOhost labels
rLSI labels
Measure:
Measure:
Measure:
Measure:
alignment,
alignment,
alignment,
alignment,
guidance,
guidance,
guidance,
guidance,
satisfaction
satisfaction
satisfaction
satisfaction
rLSI labels
Measure:
alignment,
guidance,
satisfaction

EBSCOhost labels
Measure:
alignment,
guidance,
satisfaction

rLSI labels
Measure:
alignment,
guidance,
satisfaction

EBSCOhost labels
Measure:
alignment,
guidance,
satisfaction

Measure:
domain,
demographics

Measure:
domain,
demographics

Measure:
domain,
demographics

Measure:
domain,
demographics

45

This study was a true experiment. A true experiment does not require random
sampling of selecting participants for the study. However, random assignment to groups
is necessary for a true experiment (Griffin, 2011). This study used a convenience
sample for selecting participants and random assignment to groups.
A task was represented by a type of search query given to the participants. A
total of two tasks were used in the experiment but each participant worked with only one
task. Two groups received the first type of task and other two groups received the
second task.
Each participant worked with two experimental systems (baseline and proposed)
during the experiment. The baseline system provided the environment to work with the
topical labels imported from the EBSCOhost and represented control. The proposed
system provided the environment to work with the topical labels generated with rLSI and
represented treatment. First, each participant worked on a task with one system, and
completed the part of the questionnaire that measured alignment, guidance, and
satisfaction. The participant then worked on the same task using the other system, and
then completed the second part of the questionnaire, again measuring alignment,
guidance, and satisfaction. Finally, each participant completed a portion of the
questionnaire that collected domain knowledge assessment and demographics data.
Use of two groups with the alternative order of control and treatment (topical label type)
per each task controlled for order effect.
The research study recruitment materials, the instrument (including the
questionnaire), and the informed consent notices were submitted and approved by the
Institutional Review Board (IRB) at UNT before conducting the main experiment (see

46

Appendix B). The researcher asked the instructors who teach courses in the College of
Business and College of Information for their help with recruiting participants. These
courses are taught in both face-to-face and online format using the Blackboard Learn
learning management system. The invitations were sent to the prospective student
participants via regular e-mail, learning management system messages and
announcements tools, and announced in class for the face-to-face courses. The details
of the experiment, voluntary participation, and compensation for the participation were
described in the invitation and in the informed consent notice approved by the IRB.

3.6 Experimental Systems
Two systems were used in the experiment: one was a baseline system and the
other is a proposed system. In both systems the LSA search engine for query matching
and ranking of the results was implemented. The titles and abstracts of the documents
were two semantic sources that were used by the LSA search engine in the baseline
and proposed systems and were also used for generation of rLSI topical labels in the
proposed system. The collection of articles processed by the two systems and their
search user interfaces were identical except for the actual topical labels used to
describe the documents, which were different. Figure 12 shows the prototype of the
search user interface.
The participants in the experiment did not have knowledge of whether they were
working with the baseline or with the proposed system. In both systems they saw the
topical labels even though these terms had a different origin. The set of topical labels
used for the baseline system was imported to the system from the EBSCOhost

47

database that indexes the Information Research journal. The set of the topical labels
used for the proposed system was generated using rLSI.
Search system
Subject:
topical label
topical label
topical label
topical label
topical label
topical label
topical label
topical label
topical label
topical label

Help

Results:
1
2
3
4
5
6
7
8
9
10

Search:

(40)
(35)
(25)
(15)
(10)
(9)
(7)
(5)
(3)
(1)

[+] [-]
[+] [-]
[+] [-]
[+] [-]
[+] [-]
[+] [-]
[+] [-]
[+] [-]
[+] [-]
[+] [-]

Title: Title of the article X
Subject: topical label 2; topical label 5
Snippet of the full text: …Lorem ipsum dolor sit amet,
consectetur adipiscing elit, sed do eiusmod tempor
incididunt…Lorem ipsum dolor sit amet, consectetur adipiscing
elit, sed do eiusmod tempor incididunt…
Title: Title of the article Y
Subject: topical label 3; topical label 8
Snippet of the full text: …Consectetur adipiscing elit, lorem
ipsum dolor sit amet, sed do eiusmod tempor incididunt…

Lorem ipsum

Search

Title: Title of the article Z
Subject: topical label 1; topical label 10; topical label 3
Snippet of the full text: …Consectetur adipiscing elit, sed do
eiusmod tempor incididunt, lorem ipsum dolor sit amet…
…

Figure 12. Prototype of the search user interface of the experimental system.

The experimental systems were hosted on a server at UNT and its interface was
designed for use on traditional types of devices such as desktop or laptop computers
via the Internet.

3.7 Instrument
Since the study introduced a new model for the evaluation of the proposed IR
system, a new measuring instrument had to be developed. The instrument development
process followed similar steps to those described in the development of a questionnaire
for user interface satisfaction (QUIS) that was designed to assess satisfaction with
specific aspects of the human-computer interface (Chin, Diehl, & Norman, 1988).
48

The present study designed an instrument based on the proposed TLG model,
the model’s constructs, and the variables operationalizing these constructs. Each
variable was measured using this instrument. The research hypotheses were tested
using data collected with this instrument and analyzed using methods described in this
chapter.
A questionnaire is an appropriate type of data collection instrument for this study
because it allows for collecting participants’ opinions during and immediately after the
experiment. The online questionnaire was designed to measure the following variables:
alignment, guidance, satisfaction, domain knowledge, and demographics (see Figure 5
for the reference to TLG model). Questionnaire responses were captured using a 7point Likert scale, where 1 is strongly disagree, and 7 is strongly agree on a ordinal
scale. The questionnaire was implemented using Qualtrics TM, an online survey
application available to UNT students and employees. See Appendix C for the
instrument which includes complete set of instructions for participants and a
questionnaire.

3.8 Experimental Procedures
The prospective participants received an email with the link to the initial Web
page of the study which explained the details of the experiment. The participants were
free to perform the tasks at any time from any computer within a two-week period. After
individuals arrived at the initial Web page of the study, read the consent notice, and
expressed their agreement to participate by clicking on the URL link to start the
experimental activity, they were randomly assigned to one of four groups (Group 1,
Group 2, Group 3, or Group 4). This procedure assured that the number of individuals in

49

each group was approximately even no matter how many individuals agreed to
participate. This procedure also assured elimination of systematic bias and systematic
differences among characteristics of the participants that could affect the outcomes.
This means that any differences in outcomes were attributed to the experimental
treatment (Keppel & Wickens, 2004). The instructions and questionnaires for the
participants assigned to each of the four groups were identical except for the task (i.e.,
query) type and order in which the two systems would be used.
Generally, IIR studies may include experiments where users only formulate
queries without viewing the results and also studies where users only make relevance
judgments about objects. The searching task in the present study included an explicit
initial query, which is the expression of an information need.
At the beginning of the experiment, after a participant read the instructions and
was ready to engage with the system, she or he was provided with a query. Each
participant worked with one query during the course of the experiment but a total of two
different queries were used in this experiment, each with the two different groups (see
Table 5). The queries represented different task types which is a moderator variable in
this study. The queries were: (a) “Examples of social tagging,” and (b) “Types of access
to electronic resources.” One of the queries is more specific and involved participants
finding examples. The other query is more abstract and involved participants finding
categories or concepts. This difference was sufficient to account for the moderation
effect of the task type in the proposed TLG model.
The participants ran the provided query against the title-abstract metadata field
(the only option available in the search user interface) and saw the list of documents

50

ranked by relevance with the LSA search engine. The system also showed titles and
topical labels for each document, and up to three query matching snippets of text per
any metadata field or full text of the documents. Figure 12 shows an example of two
matching snippets around the search phrase “Lorem ipsum” in the full text for the first
relevant object. Other relevant objects in this example had only one instance of the
search phrase in the full text. The participants were able to explore the result list as well
as full text of the documents by opening them in a separate window.
The instrument instructed participants to explore search results and perform
some manipulations using topical labels presented as facets in the search user interface
(see Appendix C, P2.4). The instrument also suggested that participants could
reformulate queries if they wanted to. These instructions allowed the researcher to
assume that participants engaged with more than one exchange with the system where
one exchange with the system was submitting a query and browsing only the first result
set without manipulation with the topical labels. The experimental procedure also
instructed participants to provide brief answers to the query in the form of one or two
sentences copied from the full text of the relevant documents and inserted into the input
box of the online questionnaire. The documentation of the findings after working with
each of the two systems assured that participants did engage in the searching activity.
See Appendix C for the complete set of instructions.
Kelly and Sugimoto (2013) analyzed 127 studies in IIR that included experiments
and reported that the average time needed to complete the task in those studies was
17.1 minutes. For the experiment in the present study, the expected time for completion
of a task with each of the two systems was about 10 minutes and the time required to

51

answer the questionnaire was estimated at 10 minutes. So participants were asked to
allot about 40 minutes for their participation.

3.9 Reliability, Validity, and Generalizability
The instrument of the study included a questionnaire. According to Nunnally
(1967), the important measures of the questionnaire quality are reliability and validity.
The reliability of a questionnaire is normally assessed with coefficient alpha (also known
as Cronbach’s alpha), a measurement of internal consistency across different
questionnaire items attempting to measure the same construct (Cortina, 1993).
Coefficient alpha can range from 0 (no reliability) to 1 (perfect reliability). Reliability
coefficients in the range of 0.7 to 0.8 are customary required (Landauer, 1997, p. 219).
The reliability test of the questionnaire items related to the constructs was performed.
The validity of a questionnaire informs the researcher about the extent to which
that questionnaire measures what it claims to measure. One of the approaches to
establish validity is to assess content validity through the use of factor analysis which
helps to discover or confirm clusters of related items (Sauro & Lewis, 2012, p. 187).
Content validity examines loading values of items on their respective constructs.
Validation of the instrument was performed to avoid construct validity threat.
Internal validity threats were reduced by using the appropriate measures and
appropriate construction of the experiment and the instrument. The adequate statistical
power, right assumptions, and adequate definitions and measures of variables were
used to minimize statistical conclusion validity threat.
Maturation validity threat (Creswell, 2013, p. 163) was minimal because the
information seeking and domain knowledge of the participants was similar during the

52

experiment. Thus the maturity of the participants was changing at the same rate, having
minimal impact on the results. Selection validity threat was avoided by the random
nature of the assignment of the participant to the groups. Mortality validity threat is
present when participants drop out during the experiment due to any number of possible
reasons. This threat was alleviated by recruiting a number of participants that was 5–
10% larger than the number informed by the power analysis. Diffusion of treatment did
not constitute a validity threat because the groups of participants and each individual
participant worked separately during the experiment. Other internal validity threats such
as history, regression, compensatory rivalry, and instrumentation are not pertinent to
this study.
Order effect refers to the order in which treatment is administered and can be
one of the external validity threats. Multiple groups and counterbalancing order of
treatments were used to control for this threat.
External validity refers to the generalizability of a study to other persons or other
settings (Creswell, 2013, p. 162). In this study the inferences and claims drawn from the
experiment were restricted to the individuals that have characteristics similar to the
characteristics of the participants of the experiment. The generalizations were also
restricted to the individuals in the settings that are similar to the settings of the
experiment in this study.

3.10 Data Analysis Procedures
Actual statistics on the responders and non-responders was summarized.
Analysis of the data for the variables was performed and descriptive statistics such as
mean, standard deviations, and range of scores was provided.

53

The statistical procedure used for the study was PLS-SEM, a variance-based
structural modeling method. “PLS is a latent structural equation modeling technique that
utilizes a component-based approach and has many advantages over traditional
structural equation model” (Huang, Lin, & Chan, 2012). The rationale for adopting this
procedure was that structural equation modeling analysis is used “to test theories about
hypothesized casual links between variables that are correlated” (Gall et al., 2007,
p. 339). PLS-SEM has several advantages over covariance-based structural modeling
methods implemented, for example, in the software packages such as LISREL
(Schumacker & Lomax, 2010). One of the advantages is that PLS does not require that
variables included in the model be normally distributed (Gefen, Straub, & Boudreau,
2000). A similar procedure was used in several studies on user engagement in search
(O’Brien & Toms, 2010, 2013), user technology acceptance (Sun & Zhang, 2008;
Venkatesh, Morris, Davis, & Davis, 2003), information behavior (Sin, 2011), and user
acceptance of semantic social tagging (Huang et al., 2012).
The analysis presented an integrated PLS-SEM for the proposed TLG model
(see Figure 5). The results of the data analysis were presented and interpreted. The
values of the following parameters were estimated for each construct and
corresponding variable and relationship: standardized regression coefficient or the
effect, β, coefficient of determination, R2 which is percent of variance explained by the
independent variables, and t-statistic, which is an indication of significance at a given
confidence level. Figure 13 shows generic example of these parameters for a set of
sample constructs, X1, X2, and Y.

54

Conclusions were drawn from the results and tests of hypotheses were
presented. These include whether the results of the statistical tests were statistically
significant or not, whether the results supported the hypotheses, the explanation of the
results as they relate to the theory, and the discussion of implications of the results for
practice and future research.
Construct X1
Independent variable

H1

Construct Y
Dependent variable
R2

β1
(t1)
Construct X2
Independent variable

H2
β2
(t2)

Figure 13. Fragment of the partial least squares structural equation modeling (PLSSEM) results.

3.11 Pilot Study
Pilot studies are important to test experimental IR systems, in order to establish
the reliability and validity of the instrument, and to improve questions, format, and
scales of the instrument. The researcher carried out a pilot study and the results of that
informed the additional details about the design for the research study. No revision of
the systems and instrument was necessary because the experimental system worked
as it was defined in the specification. The questionnaire demonstrated high reliability
and construct validity.

3.12 Summary
This chapter discusses the research design of the study and proposes a
quantitative research design framework. The experimental design and procedures with

55

participants selected as a convenience sample is outlined. The baseline and proposed
experimental systems and collection of the documents are described. Data analyses
procedures and methods are defined. An instrument that includes a questionnaire is
described, and its reliability and validity are established.

56

CHAPTER 4
DATA COLLECTION AND ANALYSIS
4.1 Introduction
This chapter presents the data collection process and results of the investigation
of a semi-automatic method for creation of topical labels representing the topical
concepts in information objects (Sidorova et al., 2008). The method is referred to here
as rLSI. The evaluation framework which is based on the LSA theory of meaning
(Landauer, 2007) utilized the TLG model introduced in previous chapters. The
experimental systems that utilize those topical labels were built for the purposes of
evaluating user satisfaction with the search process. Data were collected through the
information search tasks performed by 122 participants using two experimental
systems. A quantitative method of analysis (PLS-SEM) was used to test a set of
research hypotheses and to answer research questions.

4.2 Experimental Systems
The baseline and proposed experimental systems were built according to the
specifications described in the research design chapter. Because the only difference
between the two IR systems was the type of topical labels, the baseline system was
built on a virtual machine, then it was cloned to another virtual machine, and
modifications on the clone were made to transform it into the proposed system. The
Fedora Commons open source repository platform was chosen as a backend
component of the IR system. “Fedora is a modular architecture built on the principle that
interoperability and extensibility is best achieved by the clean separation of data,
interfaces, and mechanisms” (Payette & Lagoze, 2000). These principles provided the

57

researcher with great flexibility in terms of customization of the search engine, indexing
system, and implementation of faceted browsing in the user interface. The groundwork
on developing the IR system was completed in earlier projects and presented at
national and international conferences (Polyakov & Moen, 2012, 2013; Polyakov, 2012).
In addition to Fedora Commons, a number of other open source software
components (see Figure 14) were installed and customized to design the experimental
IR systems: (a) repository framework Islandora 12.1; (b) repository system Fedora
Commons 3.4.2; (c) content management system Drupal 6.31; (d) search server
Solr/Lucene 3.5; (e) search toolkit Apache Tika 1.2; (f) search service Fedora Generic
Search 2.5 (g) triplestore (semantic store) Mulgara 2.1; (h) database server MySQL 5.5;
(i) Java web server Apache Tomcat 6; (g) HTTP web server Apache 2.4; (k) operating
system Ubuntu Linux 14.04 LTS.
The Solr/Lucene is the indexing and search results ranking software component
of the IR systems and it uses VSM with a TF-IDF weighting scheme as its algorithm
(Apache Software Foundation, 2014). However, the specification for the experimental
system requires that baseline and proposed systems provide an LSA indexing and
ranking algorithm. This requirement is applicable only to the title-abstract metadata field
because only this field was provided for use by the participants to perform the search
tasks. The titles and abstracts were also used for generation of rLSI topical labels in the
proposed system so the same semantic sources were used for searching and for topical
label browsing mechanisms built into the system.

58

Figure 14. Functional diagram of the software components for the experimental IR systems.
59

All techniques and methods discussed in the section are illustrated in
Appendix A, which provides a complete example for processing a small collection of
documents and computing the terms weights, VSM similarities, LSA, LSA approximation
for the purposes of retrieval in the experimental systems, and using rLSI for generation
of the topical labels.
Implementation of the complete LSA search engine is outside the scope of this
study. For the purposes of this study, the LSA algorithm for searching the title-abstract
field was implemented in the IR system with the use of an ancillary title-abstract
metadata field. The data for this ancillary metadata field was pre-computed outside of
the systems, and the values were constructed according to rules that allowed for closely
approximate LSA search and ranking results even though the actual algorithm used by
the systems during the online processing was VSM (see Figure 15).

Title
Abstract
---------------------------------------------------

copy
values

rLSI offline

rLSI
topical labels
metadata field

metadata
fields:
title, abstract

Index

X

VSM
match and
rank

LSA offline X̂k

ancillary
metadata
field
title-abstract

Solr/Lucene
online
processing

index
index
X̂kpseudo

index
string

match and
rank ~= LSA

rLSI
topical labels

Figure 15. Implementation of LSA search with the use of ancillary metadata field.

60

Query

The dashed lines in the figure show a direct LSA processing workflow that was
not used due to the study limitations. Solid lines show the processing workflow that
approximates LSA search and which was used in the experimental systems.
The degree of this approximation was measured using the correlation coefficient
between the true reduced rank k term-document weights matrix X̂k and the
approximation of this matrix, X̂k-pseudo, actually used by the experimental systems for
the query-document ranking. The matrix X̂k was computed outside of the systems using
LSA. This same matrix was also used as an input to generate rLSI topical labels. The
matrix X̂k-pseudo was computed by the search engine with its VSM algorithm from the
values of the ancillary metadata field.
Appendix A describes the procedure for constructing the values for the ancillary
metadata field using a sample collection of 12 documents. The value of the correlation
coefficient between matrix X̂k and matrix X̂k-pseudo for the sample collection is 0.966
(see Figure A.3). The value of the correlation coefficient for the collection of 266
documents that was used in this study is 0.887. This approximation allowed for
achieving the query-document matching and ranking results that are very similar to the
results that would have been achieved with the direct LSA implementation at the search
engine level.

4.3 Collection and Metadata
The full texts of 266 research articles from the open access journal Information
Research were downloaded from the journal’s web site (http://www.informationr.net/ir/)
in the form of PDF files. Metadata for these articles was exported from the EBSCOhost

61

database in a form of an XML file and prepared for import into the experimental IR
system. Figure 16 shows an example of one record at the EBSCOhost database.

Figure 16. Example of metadata for one article at the EBSCOhost database.

The IR system used the Metadata Object Description Schema (MODS) for
storing documents’ metadata. The full text of the articles in the form of PDF files and
associated metadata records was imported to the experimental systems using a batch
import facility. The baseline and proposed experimental systems were identical except
for the topical labels assigned to each document. The baseline system used metadata
exported from the Subjects field of the EBSCOhost database in order to populate the
values of the topical labels field in the system. Statistics and samples of the topical

62

labels assigned to the 266 articles by the EBSCOhost database are presented in the
Appendix D.

4.4 Generation of rLSI Topical Labels for the Proposed System
The proposed system used topical labels generated using rLSI. One of the first
steps in this process was to decide on the number of dimensions which later resulted in
the rotated factors. The number of dimensions can be decided by expert opinion, or this
number can be computed. Researchers typically use between 5 and 100 dimensions.
“Ideally we want enough dimensions to capture all the real structure in the termdocument matrix, but not too many, or we may start modeling noise or irrelevant detail
in the data” (Deerwester et al., 1990, p. 402). Each document can be subsequently
loaded (i.e., weighted) high on more than one dimension, this is called cross-loading.
The decision was made to use 25 dimensions. The titles and abstracts of the articles
were used as semantic sources for the analysis. An example of the title and abstract
from one of the articles in the collection is shown in Appendix E. The abstracts of the
articles in Information Research are structured and include the following sections:
Introduction, Method, Analysis, Results, and Conclusion. However, this structure was
not taken into account and abstracts together with titles, were treated as a bag-ofwords.
The full text of the articles was not used because titles and abstracts are good
representations the aboutness of the articles, while the full text may contain sections
such as literature review that may include content not directly representative of the main
topical concepts of the article. This assumption is supported by a study on effectiveness
of semantic sources for the purposes of subject indexing (Chung, Miksa, & Hastings,

63

2010). The authors of that study found that, for example, titles were as effective for the
subject indexing as the full text.
The titles and abstracts of each of the 266 articles were merged and resulting
documents were processed using the following software: (a) custom software package
written in Java that is an implementation of LSA/rLSI (Sidorova et al., 2008);
(b) Microsoft Access®; (c) Microsoft Excel®; (d) Minitab® statistical software. The
following steps were used to process the collection (see Appendix A for illustration of
Steps 2, 4, and 5 on a small sample collection):
1. Stop words were removed. The stop words list was based on the English stop
word list commonly used in the natural language processing
(http://www.ranks.nl/stopwords) with modifications that yielded 513 stop words.
Unique terms were eliminated and only terms that appear at least twice in the
entire collection were kept. Words with British English spelling were substituted
with words using American English spelling. Porter stemming was applied. This
step yielded 1,651 terms.
2. TF-IDF terms weights were computed for each term in each document.
3. The first round of LSA was computed for 100 dimensions and terms were ranked
according to the variance explained by these terms based on the term loadings
(weights) on each dimension. The top 1,169 terms that cumulatively explain 95%
of the variance were kept and other terms were eliminated in addition to the
terms eliminated in Step 1.
4. LSA for 25 dimensions was computed. It yielded terms and document loadings
on each dimension.

64

5. The rLSI procedure was performed. It included varimax rotation of dimensions
which simplified relationships between terms, documents, and dimensions and
yielded rotated factors. The terms and documents (articles’ titles and abstracts)
which had the highest loadings on each of the rotated factors were examined by
a human indexer to determine predominant topical concepts associated with
each factor and assign topical labels to the factors. This step is illustrated in
Figure 17.

Figure 17. The workflow of semi-automatic rLSI topical label generation.

Example of top five loading terms for Factor F25.8 is presented in Table 6.
Example of top five loading documents (titles and fragments of abstracts) for
Factor F25.8 is presented in Table 7.

65

Table 6
Top Loading Terms for Factor F25.8
#

Loading
0.68
0.34
0.26
0.22
0.22
…
0.06

1
2
3
4
5
…
131

Term
inform
index
term
retriev
languag
…
us

Table 7
Top Loading Documents for Factor F25.8
#
1

Loading
0.39

2

0.37

3

0.33

4

0.32

5

0.31

…
18

…
0.19

Title
Investigation into the existence of the
indexer effect in key phrase extraction.
Term based comparison metrics for
controlled and uncontrolled indexing
languages.
Evaluation of controlled vocabularies
by inter-indexer consistency.
An associative index model for the
results list based on Vannevar Bush's
selection concept.
Extracting variant forms of chemical
names for information retrieval.
…
The impact of time in link-based Web
ranking.

Abstract fragment
The indexer effect has been studied in several
research studies in the field of information...
We define a collection of metrics for describing
and comparing sets of terms in controlled…
Several controlled vocabularies are used for
indexing three journal articles to check if
better…
We define the results list problem in
information search and suggest the associative
index…
Chemical substance names are long, complex
and prone to variation. This study
investigates…
…
The strong dynamic nature of the Web is a
well-known reality. Nonetheless, research on
Web…

Both terms and documents can be loaded high on more than one factor and
subsequently assigned more than one topical label with each topical label describing
one factor. rLSI topical labels which are the final result of these procedures may not be
identical to the top loading terms for each rotated factor. The top loading terms (as well
as top loading documents titles or abstracts) only provide clues for a human indexer to

66

make final decisions regarding the assignment of the topical labels. After examining top
loading terms and documents the human indexer (i.e., researcher) determined that the
main topic of Factor F25.8 could be described by the topical label indexing and
information retrieval.
Generally, the topical labels assigned to each factor to represent its topic may
include terms selected from actual top loading terms and terms selected from the top
loading documents (titles and abstracts). They may include terms assigned by the
human indexer based on the indexer’s expertise, or they may include terms taken by
the indexer from the standardized vocabularies or thesauri. To generate the topical
labels that utilize the standardized language of the information science field, the online
version of the ASIS&T Thesaurus of Information Science, Technology, and
Librarianship (http://thesview.accessinn.com/asistThes) was used. This version is based
on its previous printed edition (Redmond-Neal & Hlava, 2005) and stays current on the
terminology of the field with regular updates.
Generating standardized topical labels for the rotated factors included matching
of the top loading terms (or topical labels of the topics initially determined by the human
indexer) against the terms in the ASIS&T Thesaurus and redirecting the non-preferred
terms to the preferred. For example, the topical label for Factor F25.8 initially
determined by the indexer was indexing and information retrieval. This label was
changed into the ASIS&T Thesaurus preferred term information retrieval indexes (see
Table 8). Another example is Factor F25.16. The indexer initially determined a
candidate label social tagging and then it was changed to the preferred label
collaborative indexing.

67

Table 8
Topical Labels Generated Using rLSI and Standardized with ASIS&T Thesaurus
Factor
F25.1
F25.2
F25.3
F25.4
F25.5
F25.6
F25.7
F25.8
F25.9
F25.10
F25.11
F25.12
F25.13
F25.14
F25.15
F25.16
F25.17
F25.18
F25.19
F25.20
F25.21
F25.22
F25.23
F25.24
F25.25

rLSI Topical label
information seeking -- biomedical information
mobile communications -- information sharing
information resources -- credibility
information theory -- activity theory
environmental scanning
school -- learning -- information literacy
open access publications -- electronic journals
information retrieval indexes
information professionals
information seeking -- learning
corporate reporting -- knowledge acquisition
digital repositories
information seeking -- serendipity
web sites -- search engines
news media -- social web
image indexing -- collaborative indexing
bibliometrics -- citation networks
information seeking -- electronic publications
business intelligence
question answering systems -- credibility
virtual communities
cultural heritage -- portals
social networking -- blogs -- disclosure
reference services -- cognitive perception
information theory -- knowledge management

Number of Articles
101
82
68
62
13
22
13
19
9
9
8
5
4
13
9
9
11
12
9
13
5
10
8
10
8

The ASIS&T Thesaurus consists of seven levels of hierarchy, related terms, and
scope notes. The process of determining the topical labels also included examining the
broader and narrower terms with respect to the candidate terms. The final decisions
about the terms from the ASIS&T Thesaurus for the topical labels of the rotated factors
at the appropriate level of specificity was determined by the number of factors, among
other aspects. The thesaurus includes many pre-coordinated terms (n-grams) for
representing complex concepts. For some of the factors, several terms combined with a

68

dash separator were also used. The use of several different topical labels for one factor
increased the exhaustivity of the topical labels by describing a larger number of topical
concepts per factor. Assigning topical labels to one factor with this method took
between 3 and 8 minutes.
Figure 18 shows a screenshot of the proposed system (available at http://txcdkv19.unt.edu/d6) with the topical labels generated using the rLSI method. The baseline
system has an identical interface but topical labels for the baseline system were taken
from the EBSCOhost database (the baseline system is available at http://txcdkv18.unt.edu/d6/).

Figure 18. Proposed experimental system with LSA search engine and topical labels
generated using rLSI.

69

The purpose of creating topical labels was to interpret the predominant topical
concepts of the factors and to describe the collection rather than individual documents.
The ASIS&T Thesaurus was an appropriate instrument for this task because while it is
commonly used for assigning topical labels to documents, it is also designed as a
“…descriptive outline of information science and technology…” (Redmond-Neal &
Hlava, 2005, p. v).

4.5 Participants, Experiment, and Data Collection
Students from the College of Information and College of Business at UNT were
recruited for the study. A total of 140 people participated in the experiment during a
three week time period. The responses were downloaded from the Qualtrics TM system,
cleaned up and analyzed (see instrument in the Appendix C). The distribution of time
taken by the participants to complete the experiment is presented in Figure 19.
25

Number of Participants

20

15

10

5

0

Time, min

Figure 19. Distribution of time taken by the participants to complete the experiment.

70

Responses which took less than 10 minutes and more than 1 day to complete
were removed. The final data set included 122 responses. The median time for these
responses was 29.30 minutes and the average time was 38.01 minutes. The
participants were randomly placed into four experimental groups. The number of
participants by experimental group is presented in Table 9. Also, see Table 5 for the
experimental design.
Table 9
Experimental Groups Workflow and Number of Participants
Group, query (task type), and order of systems (topical labels types)
Group 1: Query 1 => EBSCOhost topical labels => rLSI topical labels
Group 2: Query 1 => rLSI topical labels => EBSCOhost topical labels
Group 3: Query 2 => EBSCOhost topical labels => rLSI topical labels
Group 4: Query 2 => rLSI topical labels => EBSCOhost topical labels

Number of participants
32
30
30
30

The Qualtrics TM system recorded the geographical locations of the participants at
the time of the experiment (without recording their identity). The geographical

71

distribution of the participants is shown in Figure 20.

Figure 20. Geographic distribution of the participants.
The map was generated using Google Fusion Tables TM service. Students taking
both the online and face-to-face courses at UNT took part in the experiment. Some of
the students were taking online courses at UNT and lived in different U.S. states; others
participated during the between-semester break and that was the reason for being
geographically away from UNT.

4.6 Descriptive Statistics
Alignment, guidance, satisfaction, and domain knowledge variables (representing
constructs and called latent variables in SEM) were measured with four to five
alternative items (called manifest variables in SEM) described in the instrument of the
study (see Appendix C). Questionnaire responses were captured using a 7-point Likert

72

scale, where 1 is strongly disagree, and 7 is strongly agree on a ordinal scale. However,
the assumption of the study is that the intervals between each of these seven values
are the same. This assumption allowed for using various statistics on these variables.
Topical label type and task type are dichotomous variables that take on one of only two
possible values, 0 and 1. Demographics construct included several variables on a
nominal scale what allowed for representing them as frequencies.
The opinions of each participant related to the three constructs: alignment,
guidance, and satisfaction were captured twice during the experiment. One time,
participants gave their opinions after working with the system that uses the EBSCOhost
topical labels, and the other time after working with the system that used the rLSI topical
labels. The responses data representing alignment, guidance, and satisfaction were
arranged by the task (i.e., query) type and by the topical label type. This resulted in four
sets of responses (see Table 10). Each set is a union of responses representing
opinions related to the same query and the same topical label type.

73

Table 10
Sets of Responses
Task (query) 1:
“Examples of social tagging”
32 participants:
30 participants:
Group 1
Group 2

Task (query) 2:
“Types of access to electronic resources”
30 participants:
30 participants:
Group 3
Group 4

(Set 1)
EBSCOhost labels
Measure:
alignment
guidance,
satisfaction

(Set 2)
rLSI labels
Measure:
alignment
guidance,
satisfaction

(Set 3)
EBSCOhost labels
Measure:
alignment
guidance,
satisfaction

(Set 4)
rLSI labels
Measure:
alignment
guidance,
satisfaction

(Set 2)
rLSI labels
Measure:
alignment
guidance,
satisfaction

(Set 1)
EBSCOhost labels
Measure:
alignment
guidance,
satisfaction

(Set 4)
rLSI labels
Measure:
alignment
guidance,
satisfaction

(Set 3)
EBSCOhost labels
Measure:
alignment
guidance,
satisfaction

Measure:
domain,
demographics

Measure:
domain,
demographics

Measure:
domain,
demographics

Measure:
domain,
demographics

Mean, standard deviation, and margin of error at 95% confidence level were
computed for the alignment, guidance, and satisfaction variables. The data for the query
“Examples of social tagging” for both EBSCOhost and rLSI topical label types (Set 1
and Set 2) are presented in Table 11 and in Figure 21. The data for the query “Types of
access to electronic resources” for both EBSCOhost and rLSI topical label types (Set 3
and Set 4) are presented in Table 12 and in Figure 22.

74

Table 11
Descriptive Statistics for the IEMA, TLG, and SSP Variables; Query 1: “Examples of
Social Tagging”
Variable
(manifest variable)
Alignment (IEMA-1)
Alignment (IEMA-2)
Alignment (IEMA-3)
Alignment (IEMA-4)
Alignment (IEMA-5)
Guidance (TLG-1)
Guidance (TLG-2)
Guidance (TLG-3)
Guidance (TLG-4)
Guidance (TLG-5)
Satisfaction (SSP-1)
Satisfaction (SSP-2)
Satisfaction (SSP-3)
Satisfaction (SSP-4)

EBSCOhost topical labels
Mean
Standard Margin of
deviation
error
5.52
5.18
5.48
5.11
4.68
4.98
4.90
4.63
4.69
4.58
4.89
4.97
4.98
4.84

1.13
1.44
1.39
1.57
1.54
1.68
1.64
1.71
1.70
1.74
1.67
1.68
1.63
1.68

0.28
0.36
0.35
0.39
0.38
0.42
0.41
0.43
0.42
0.43
0.42
0.42
0.41
0.42

Mean

rLSI topical labels
Standard Margin of
deviation
error

5.02
4.94
5.21
5.13
4.61
5.03
4.98
4.71
4.84
4.76
4.87
5.00
4.98
4.97

1.47
1.38
1.45
1.41
1.60
1.51
1.58
1.77
1.67
1.68
1.65
1.55
1.59
1.55

0.36
0.34
0.36
0.35
0.40
0.38
0.39
0.44
0.42
0.42
0.41
0.39
0.40
0.39

7.00

6.00
5.00

4.00
EBSCOhost Topical Labels

3.00

rLSI Topical Labels

2.00
1.00

0.00

Figure 21. Means and confidence intervals for the IEMA, TLG, and SSP variables;
Query 1: “Examples of social tagging”.

75

Table 12
Descriptive Statistics for the IEMA, TLG, and SSP Variables; Query 2: “Types of Access
to Electronic Resources”
Variable
(manifest variable)
Alignment (IEMA-1)
Alignment (IEMA-2)
Alignment (IEMA-3)
Alignment (IEMA-4)
Alignment (IEMA-5)
Guidance (TLG-1)
Guidance (TLG-2)
Guidance (TLG-3)
Guidance (TLG-4)
Guidance (TLG-5)
Satisfaction (SSP-1)
Satisfaction (SSP-2)
Satisfaction (SSP-3)
Satisfaction (SSP-4)

EBSCOhost topical labels
Mean
Standard Margin of
deviation
error
5.27
4.93
5.23
5.07
4.77
4.88
5.02
4.58
4.93
4.88
4.78
4.73
4.63
4.62

1.16
1.42
1.38
1.46
1.47
1.51
1.59
1.58
1.49
1.49
1.58
1.66
1.70
1.69

0.29
0.36
0.35
0.37
0.37
0.38
0.40
0.40
0.38
0.38
0.40
0.42
0.43
0.43

Mean

rLSI topical labels
Standard Margin of
deviation
error

4.95
4.73
5.15
4.80
4.60
4.65
4.87
4.55
4.65
4.35
4.40
4.45
4.33
4.47

1.49
1.53
1.61
1.61
1.72
1.89
1.81
1.92
1.90
1.99
1.84
1.84
1.78
1.80

0.38
0.39
0.41
0.41
0.44
0.48
0.46
0.49
0.48
0.50
0.47
0.46
0.45
0.46

7.00

6.00
5.00

4.00
EBSCOhost Topical Labels

3.00

rLSI Topical Labels

2.00
1.00

0.00

Figure 22. Means and confidence intervals for the IEMA, TLG, and SSP variables;
Query 2: “Types of access to electronic resources”.

76

The domain knowledge variable was measured one time for each participant of
the experiment using a Likert scale. Mean, standard deviation and margin of error at
95% confidence level were computed for the domain knowledge variable and are
presented in Table 13 and in Figure 23 for a set of all 122 participants.
Table 13
Descriptive Statistics for the Domain Knowledge (DK) Variable for all Types of Labels
and Tasks
Variable
(manifest variable)
Domain knowledge (DK-1)
Domain knowledge (DK-2)
Domain knowledge (DK-3)
Domain knowledge (DK-4)

Mean
5.16
4.90
5.00
4.96

Standard
deviation
1.49
1.58
1.57
1.56

Margin of
error
0.26
0.28
0.28
0.28

7.00
6.00
5.00
4.00
3.00
All types of labels and tasks
2.00
1.00
0.00
DK-1

DK-2

DK-3

DK-4

Figure 23. Means and confidence intervals for the domain knowledge (DK) variable for
all types of labels and tasks.

77

The descriptive statistics for the demographics of the participants are presented
in Figure 24.
Number of participants
GENDER: ————————————————
Female

45

Male

77

AGE CATEGORY: —————————————
18–20

0

21–25

26

26–30

33

31–40

33

41–50

23

51+

7

DEGREE LEVEL CURRENTLY PURSUING: —
Undergraduate, non-degree seeking

1

Undergraduate degree, e.g. BA/BS
Graduate non-degree seeking

22
1

Master’s, e.g. MA, MS, MBA, MFA

69

Doctoral, e.g. Ph.D.

29

MAJOR: —————————————————
Information Science (College of Information-COI)

31

Library Science (College of Information-COI)

36

Other in COI, non-Info. Science

9

IT/BCIS (College of Business-COB)

27

Other in COB, non-IT/BCIS

14

Other, non-COI and non-COB
Undeclared major

5
0

IS ENGLISH YOUR FIRST LANGUAGE?: ——
Yes

93

No (but I am fluent)
No (I am still learning)

23
6

Figure 24. Demographics of the participants.

The descriptive statistics served as an overview of the collected data and did not
serve for drawing conclusions about the relationships among the variables of the

78

proposed TLG model or about the hypotheses of the study. The PLS-SEM computed in
the following sections was used for these purposes.

4.7 Reliability of Measurement Scales
Reliability scores for the variables of the instrument measured with the Likert
scale were established using data collected in the experiment. The calculation was
done with the Item Analysis tool in the Minitab® statistical software. The measurement
scales of the questionnaire demonstrated high reliability, with Cronbach’s alpha values
ranging between 0.94 and 0.97 (see Table 14). Alignment, guidance, and satisfaction
were captured twice for each participant during the experiment. Therefore, the number
of measurements for these constructs is twice the number of participants. Domain
knowledge was measured one time for each participant at the end of the experiment.
Table 14
Reliability of the Measurement Scales
Construct short name and code
Alignment (IEMA)
Guidance (TLG)
Satisfaction (SSP)
Domain knowledge (DK)

Number of measurements
244
244
244
122

Cronbach’s alpha
0.94
0.96
0.97
0.97

4.8 Construct Validity
To establish validity scores for the instrument, exploratory factor analysis with
varimax rotation was performed. The calculation was done with the Factor Analysis tool
in the Minitab® statistical software. The number of factors was forced to four by the
number of the constructs measured with the Likert scale which were: alignment,
guidance, satisfaction, and domain knowledge. The analysis showed that all items

79

represented by questions in the questionnaire (i.e., manifest variables) that had been
created to describe the same construct (i.e., latent variable) had high loadings on one of
the four factors (see Table 15). All items unrelated to a given construct had low loadings
on that same factor. The conclusion of the analysis was that items of the questionnaire
adequately represent the constructs. The questionnaire demonstrated high reliability
and construct validity.
Table 15
Construct Validity
Variable
Alignment (IEMA-1)
Alignment (IEMA-2)
Alignment (IEMA-3)
Alignment (IEMA-4)
Alignment (IEMA-5)
Guidance (TLG-1)
Guidance (TLG-2)
Guidance (TLG-3)
Guidance (TLG-4)
Guidance (TLG-5)
Satisfaction (SSP-1
Satisfaction (SSP-2)
Satisfaction (SSP-3)
Satisfaction (SSP-4)
Domain knowledge (DK-1)
Domain knowledge (DK-2)
Domain knowledge (DK-3)
Domain knowledge (DK-4)
Variance
% Var

Factor 1
0.31
0.37
0.26
0.32
0.42
0.66
0.72
0.84
0.81
0.74
0.38
0.34
0.40
0.37
0.16
0.16
0.07
0.11

Factor 2
0.26
0.27
0.31
0.48
0.46
0.42
0.38
0.29
0.38
0.44
0.78
0.83
0.78
0.81
0.11
0.09
0.12
0.12

Factor 3
-0.77
-0.78
-0.78
-0.70
-0.63
-0.43
-0.43
-0.30
-0.33
-0.33
-0.39
-0.36
-0.32
-0.31
-0.15
-0.10
-0.16
-0.14

Factor 4
0.24
0.13
0.22
0.17
0.12
0.25
0.08
0.21
0.19
0.12
0.12
0.08
0.20
0.20
0.92
0.93
0.94
0.94

Communality
0.81
0.83
0.83
0.85
0.80
0.87
0.85
0.92
0.93
0.87
0.92
0.93
0.91
0.93
0.91
0.90
0.93
0.92

4.05
0.23

4.03
0.22

3.92
0.22

3.90
0.22

15.91
0.88

80

4.9 Partial Least Squares Structural Equation Modeling (PLS-SEM)
4.9.1 Modeling without Control Variables
PLS-SEM makes a regression model from X to Y so that in the future, a
researcher would only need X and could than predict Y from X. PLS-SEM takes into
account the structure of both the independent and dependent variables. The matrix form
of the model is: Y = Xβ + ε. For one dependent variable it takes the form: 𝑦 =
∑𝑘𝑖=1 𝛽𝑖 𝑥𝑖 + 𝜀, where xi are the k independent variables and y is the dependent variable.
PLS-SEM is similar to multiple linear regression, however the way the βi are found is
different. The matrixes X and Y are decomposed into latent structures in an iterative
process. The latent structure corresponding to the most variation of Y is extracted and
explained by a latent structure X that describes this variation the best. The PLS-SEM
algorithm is essentially a sequence of regressions. This iterative procedure is repeated
until convergence is obtained or the maximum number of iterations is reached (Ringle,
Wende, & Becker, 2015). The advantages of PLS-SEM is that it deals with
multicollinearity (when independent variables are highly correlated), allows a researcher
to take into account the data structure, and provides visual results that help with
interpretation.
Each variable of the proposed TLG model (see Figure 5) is represented as a
variable in a set of regression equations. The moderator variables are represented as
interaction terms by multiplying corresponding variables. There are also control
variables in the TLG model: gender and age group. However, adding control variables
may decrease the effects of independent variables even though it may increase
coefficients of determination, R2. That is way the TLG model was tested without the

81

control variables first, and then control variables were added. The set of regression
equations that describes the proposed model is:
IEMA = β0’ + β1 TLT + β2 DK + β3 TLT DK + ε’

(1)

TLG = β0’’ + β4 IEMA + β5 TT + β6 TT IEMA + ε’’

(2)

SSP = β0’’’ + β7 TLG + ε’’’

(3)

PLS-SEM is designed, in part, to test these equations in a single analysis instead
of testing separate regression analyses. The data were imported to and analyzed with
SmartPLS software (Ringle et al., 2015). The TLG model was visualized as a PLS-SEM
in Figure 25 where each latent variable is represented as a circle. Each latent variable is
linked to the manifest (i.e., measurable) variables, represented as rectangles.
Independent variable TLT is linked to mediator variable IEMA. The IEMA variable is
linked to another mediator variable, TLG and TLG is linked to the dependent variable
SSP. Moderator variables DK and TT were modelled as interaction terms represented
with thick border circles.
The criteria of the PLS-SEM computation were the following: 300 maximum
iterations, 10-7 stop criterion, path weighting scheme. The values of the following
parameters were estimated for each mediator and dependent variable and for each
relationship. The standardized regression coefficient, or the effect, β, is shown as a
number on the lines connecting the constructs. Percent of variance explained by
combination of independent and mediator variables R2 is shown inside the circles
representing the variables. The t-statistic at 95% confidence level, which is an indication
of significance, was computed using a bootstrapping algorithm. In bootstrapping,

82

subsamples are created with observations randomly drawn from the original set of data
(with replacements).

Figure 25. Partial least squares structural equation modeling (PLS-SEM) visualization.

The number of subsamples was set to 5,000 to ensure stability of results. The tstatistic is shown in parenthesis on the lines connecting the variables. The standardized
regression coefficients which are significant at 95% confidence level are marked with
asterisks in Figure 25. Table 16 and Table 17 provide more details about the
parameters of the PSL-SEM computed for the proposed TLG model.

83

Table 16
Standardized Regression Coefficients β of the PLS-SEM
Variable and relationship
Direct effect (path coefficient):
TLT -> IEMA (β1)
DK -> IEMA (β2)
Moderating Effect of DK -> IEMA (β3)
IEMA -> TLG (β4)
TT -> TLG (β5)
Moderating Effect or TT -> TLG (β6)
TLG -> SSP (β7)
Indirect effect:
TLT -> TLG
TLT -> SSP
IEMA -> SSP
DK -> TLG
DK -> SSP
Moderating Effect of DK -> TLG
Moderating Effect of DK -> SSP
TT -> SSP
Moderating Effect or TT -> SSP

β

Significance at 95%
(* - p <0.05)

t-statistic

p value

-0.082
0.402
0.065
0.817
0.019
-0.033
0.819

not significant
*
not significant
*
not significant
not significant
*

1.415
6.067
0.926
29.279
0.522
0.851
28.660

0.157
0.000
0.354
0.000
0.602
0.395
0.000

-0.067
-0.055
0.670
0.329
0.269
0.053
0.044
0.016
-0.027

not significant
not significant
*
*
*
not significant
not significant
not significant
not significant

1.413
1.405
17.157
5.764
5.555
0.929
0.933
0.524
0.859

0.158
0.160
0.000
0.000
0.000
0.353
0.351
0.601
0.390

Table 17
Coefficients of Determination R2 of the PLS-SEM
Variable
IEMA
TLG
SSP

2

R

0.173
0.665
0.671

Significance at 95%
(* - p < 0.05)
*
*
*

t-statistic
3.084
14.533
14.388

p value
0.002
0.000
0.000

4.9.2 Modeling with Control Variables
It was also important to evaluate the impact of control variables on alignment,
guidance, and satisfaction. This was done to eliminate other possible effects that are
unrelated to the effects from the independent and mediator variables. The following
equations describe the potential effect of control variables:
IEMA = βg0’ + β8 Gender + εg’

(4)

IEMA = βa0’ + β9 Age + εa’

(5)
84

TLG = βg0’’ + β10 Gender + εg’’

(6)

TLG = βa0’’ + β11 Age + εa’’

(7)

SSP = βg0’’’ + β12 Gender + εg’’’

(8)

SSP = βa0’’’ + β13 Age + εa’’’

(9)

The PLS-SEM with added control variables is presented in Figure 26. The effect,
t-statistics and significance (market with the asterisks) at 95% confidence level are
shown for each relationship. The effects and t-statistics for the relationships
corresponding to measurable variables (i.e., indicators or items of the instrument) are
not shown in this figure for the purposes of visualization clarity. Table 18 shows more
details about the parameters describing the effect of control variables. The parameters
describing the independent, moderator, and mediator variables had not changed
significantly with the addition of the control variables to PLS-SEM and are not shown in
the table.

Figure 26. Partial least squares structural equation modeling (PLS-SEM) visualization
with control variables.
85

Table 18
Standardized Regression Coefficients β of the PLS-SEM: Control Variables
Control variable and relationship
Direct effect (path coefficient):
Gender -> IEMA (β8)
Age -> IEMA (β9)
Gender -> TLG (β10)
Age -> TLG (β11)
Gender -> SSP (β12)
Age -> SSP (β13)

β
-0.085
0.052
-0.011
-0.065
0.000
-0.015

Significance at 95%
(* - p < 0.05)
not significant
not significant
not significant
not significant
not significant
not significant

t-statistic
1.283
0.830
0.295
1.646
0.003
0.338

p value
0.200
0.406
0.768
0.100
0.997
0.735

The control variables gender and age group turned out to have not significant
effect on alignment, guidance, and satisfaction.

4.10 Hypotheses Tests and Answers to the Research Questions
The proposed TLG model explained 67.1% of the variability in the satisfaction
with the search process. The three standardized regression coefficients were significant
at 95% confidence level: β2 = 0.402 (DK -> IEMA), β4 = 0.817 (IEMA -> TLG), and
β7 = 0.819 (TLG -> SSP). The other standardized regression coefficients were not
significant: β1 (TLT -> IEMA), β3 (DK·TLT -> IEMA, β5 (TT -> TLG), and β6 (TT·IEMA > TLG). This data allowed for testing the hypotheses of the study. The results of the
testing are summarized in Table 19.
The research questions of the study were answered based on the tests of the
hypotheses.
RQ1: Which type of topical labels (i.e., assigned by human indexers using
current best practices in DLs vs. generated with rLSI) results in higher user satisfaction
with the search process?

86

Table 19
Summary of Hypotheses Testing
Hypothesis
H1 (not
supported)
H2 (not
supported) 1
H3 (supported)
H4 (not
supported)
H5 (supported)

Statement
There is a direct relationship between topical label type and alignment: Given a fixed
level of domain knowledge, topical label type corresponding to rLSI topical labels results
in better alignment when comparing with other topical label type.
Domain knowledge moderates the effect of topical label type on alignment: A higher
level of domain knowledge results in a better alignment when topical label type
corresponds to rLSI topical labels when comparing with other topical label type.
There is a direct relationship between alignment and guidance: Higher alignment is
associated with higher guidance.
Task type moderates the effect of alignment on guidance: Different task types result in
different levels of guidance for the same level of alignment.
There is a direct relationship between guidance and satisfaction with the search
process: Higher guidance is associated with higher satisfaction with the search process.

1

Note: Direct effect of domain knowledge (DK) on alignment (IEMA) was significant even though it was
not initially hypothesized in the TLG model.

According to this study, there is insufficient evidence at 95% confidence level to
support the claim that there is a relationship between type of topical labels and user
satisfaction with the search process as hypothesized by the TLG model. The effect as
well as indirect effect of the topical label type on the satisfaction with the search process
is shown in Table16. The answer to the RQ1: There is insufficient evidence to claim that
either type of the topical labels, exported from EBSCOhost or generated with rLSI, has
an advantage over the other type in terms of improving the user satisfaction with the
search process.
RQ2: How does domain knowledge influence the relationship between topical
labels type and user satisfaction with the search process?
The conclusion was made that there is insufficient evidence to support the claim
that domain knowledge influences the relationship between topical label type and user
satisfaction with the search process because its moderation effect on this relationship is
not significant at 95% confidence level (see Table 16). However, PLS-SEM revealed

87

that domain knowledge has a direct significant effect on alignment: β2 = 0.402 (DK > IEMA), p < 0.05. Higher domain knowledge results in better alignment between user’s
mental (internal) map and the external map displayed by the system. This conclusion
holds for both types of topical labels used in this study.
The data analysis results are presented on the updated diagram of the proposed
TLG model in Figure 27 (see Figure 5 for the original diagram).
Domain knowledge
(DK)
H2 (NS)
Topical label
type
(TLT)

H1
(NS)

Task type
(TT)

Not hypothesized:
β2 = 0.402 *

Alignment
between user’s
mental (internal)
map and
external map
displayed by the
system
(IEMA)

H4 (NS)

H3

β4 =
0.817 *

Topical
labels
guidance
(TLG)

R2 = 0.665

H5

Satisfaction
with the search
process
(SSP)

β7 =
0.819 *

R2 = 0.671

R2 = 0.173

Control (not significant):
Demographics (DM): Gender, Age group

Figure 27. Proposed topical labels guidance (TLG) model: Data analysis.

4.11 Summary
This chapter describes the following: how the experimental systems were built for
the study, the collection of documents and their metadata, the process of generation of
rLSI topical labels, the participants, and experimental procedures. The chapter
concludes with data analysis. Research hypotheses were tested, relationships among
variables of the proposed TLG model were described and research questions were
answered.
88

CHAPTER 5
DISCUSSIONS AND CONCLUSIONS
5.1 Introduction
The problem motivating this study was a lack of efficient methods to create the
representations of the topical concepts of information objects that are aligned with the
users’ mental model of the objects’ topical domain or are adequate to the users’
expectations about these representations. This study used rLSI, an LSA-based method
for semi-automatic topical conceptual analysis of collections of documents, leading to
topic extraction. rLSI was previously used for discovering themes and trends in
collections of documents (Evangelopoulos et al., 2012; Kulkarni et al., 2014; WinsonGeideman & Evangelopoulos, 2013). However, there have been no studies
investigating the potential use of rLSI in DLs for the purposes of documents’ topical
concepts representation and use of these representations as a faceted search tool in
the user interface.
The study proposed a cognitive TLG model to evaluate the rLSI topical labels
using satisfaction with the search process as a primary construct. A set of other
constructs were used in TLG model to investigate in detail the cognitive aspects of
users’ behavior while they were conducting search tasks. To evaluate the rLSI in the
DLs context two experimental systems were built, collection of the documents was
processed, and experiment involving users was conducted.

5.2 Summary of the Findings
The study tested the hypotheses and answered the research questions using a
proposed TLG model and PLS-SEM. As mentioned in the previous chapter (see

89

Figure 27), there is insufficient evidence that topical label type has significant direct
effect on alignment (TLT -> IEMA). The results also showed (see Table 16) a not
significant indirect effect of topical label type on guidance and on satisfaction (TLT ->
TLG, TLT -> SSP). One of the findings of the present study is that topical labels
generated using rLSI provide the same levels of alignment, guidance, and satisfaction
with the search process as EBSCOhost topical labels created by the professional
indexers using best practices. The implication of this finding is that employment of
the―much more efficient―rLSI method of subject term generation is not worse, in
terms of user satisfaction with the search process, than existing manual indexing
methods that constitute best practices in the DL industry.
There is insufficient evidence that domain knowledge has a significant direct
effect on the relationship between topical label type and alignment (TLT -> IEMA).
However, the PLS-SEM analysis revealed that domain knowledge has a direct
significant effect (p < 0.05) on the alignment, a relationship that was not hypothesized in
the TLG model. This finding shows that user’s knowledge about topical domain of the
information objects has a positive effect on the degree of correspondence between the
map of objects’ presentations (topical labels) displayed by the system and users’
internal mental map. Users with higher information science domain knowledge can
better recognize a set of topical labels that represent collection of information objects.
Different task types were represented in this study by two queries: “Examples of
social tagging” and “Types of access to electronic resources”. According to the present
study, task type does not have a significant effect on the relationship between alignment
and guidance (TT -> TLG). The conclusion is that alignment between user’s mental

90

(internal) map and external map displayed by the system has a significant positive effect
on the topical label guidance without regard to the task type.
The effects of the control variables, gender and age, were also tested, together
with the hypotheses. 45 females and 77 males participated in this study and they belong
to the age groups, from 21 to 51+ years (see Figure 24). None of the control variables
exhibit significant effect on alignment, guidance, and satisfaction.

5.3 Limitations of the Study
The results of this study are not generalizable for the entire population which is
all people who engage in online information search in DLs. The reason for this limitation
is the non-probability sampling method used in this study. This study used a natural
collection of documents which makes comparison with other studies which use a
standard collection difficult. The advantage of the natural collection is that it
approximated the real world that is being examined, and thus improved the ecological
validity of the study. This study measured participants’ perceptions about topical label
types using the constructs of the proposed TLG model. However, logs about users’
behavior describing the choices and query reformulations during the search process
were not used. This approach limited the depths of interpretation of the answers to the
research questions.

5.4 Implications and Recommendations for the Future Research
The present study shows that satisfaction with search process is not significantly
affected by the topical label types used in the experiment. One of the topical label types
used in the study is EBSCOhost subject terms generated by the professional indexers

91

using best practices. Another type is rLSI topical labels. The implication of the study is
that rLSI can be used as a method of creation of topical concepts representations. This
method is more efficient than traditional manual methods because it does not require
examination of each information object. This method is also scalable because objects
added to the collection can be automatically associated with topical labels assigned to
the objects which already exist in the collection.
The TLG model proposed in the present study can be used in other types of
research. For example, different population, such as non-academic users, can be used
in the future studies with TLG model. The TLG model can be tailored to study search
systems that use different types of objects and properties. For example, music scores,
paintings, different types of consumer products, etc. The TLG model can also be refined
based on the outcomes of the present study which shows that there are significant
relationships between alignment, topical labels guidance, and satisfaction. These
constructs can be used as core constructs and, at the same time, more antecedents of
the alignment construct can be introduced to the model. Another variation of the future
study can be addition of a third system (the control system) to the experiment. This third
system would use topical labels randomly generated from the vocabulary of the
collection of documents. Addition of a third system would add one more topical label
type and introduce more variation into this independent variable. Log analysis can be
performed in future research, which could help in understanding the aspects of user
behavior associated with the search process.

92

5.5 Contribution of the Study
The TLG model was introduced and validated in the present study. The TLG
model has the potential to be used for purposes of evaluation of user satisfaction with
search systems future studies. The present study shows that rLSI topical labels are not
worse than labels created by EBSCOhost in terms of user satisfaction with the search
process and are efficient alternative in organizing digital collections of documents for
search purposes. The two experimental systems were built and can be used in future
experiments in metadata and IR.

5.6 Summary
This chapter presents a short summary of the research findings. The research
findings of the study are explained using the proposed TLG model. The hypothesis
about the effect of alignment between user’s mental map and external displayed by the
system (IEMA) on topical labels guidance (TLG) is supported by this study. The findings
also support the hypothesis about the effect of topical labels guidance (TLG) on
satisfaction with the search process (SSP). There was insufficient evidence to support
the other hypotheses posed in this study. The limitations of the study, implications, and
recommendations for the future research are also presented in this chapter. The
limitations of the study are related to the generalizability and search task types.
Recommendations for the future research are discussed in terms of extending the study
to the analysis of the search logs recorded by the system. This would allow for
answering questions about differences in user behavior in relation to the topical label
types.

93

APPENDIX A
EXAMPLE OF COMPUTING VSM, LSA AND RLSI

94

A.1 Introduction
This appendix provides an example of computing of terms weights and VSM similarities
for a small sample collection of short documents and a query. Then, LSA and rLSI
techniques are illustrated.

A.2 Terms Weighting for VSM, LSA, and rLSI
The sample collection consists of five documents labeled d1–d5 and query labeled q
(Table A.1). Words occurring in more than one document were italicized and selected
for indexing, term stemming was not applied.
Table A.1
Sample Collection of Documents and a Query
Doc ID
d1
d2
d3
d4
d5
q

Document and query text
Higher education opens new opportunities.
Many universities offer information science or library studies programs.
Information science and library studies are related disciplines.
Higher education in information science gives theoretical knowledge.
Higher education in library studies provides practical preparation.
Information science degree

The general approach for weighting the terms was developed by Gerard Salton and his
associates (Salton & Buckley, 1988; Salton et al., 1975) and can be described as:
𝑤𝑖𝑗 = 𝑡𝑓𝑖𝑗 ∙ 𝑖𝑑𝑓𝑖𝑗 ,
where:
wij
tfij
idfij

(A.1)

weight of term i in document j,
term frequency of term i in document j,
inverse document frequency showing how rare is term i in the entire collection
of documents.

In this example the following version of the TF-IDF document-term weights formula
(also called TF-IDF2) is used:
𝑡𝑓𝑖𝑗 = 𝑓𝑖𝑗 ,

(A.2)
𝑑

𝑖𝑑𝑓𝑖𝑗 = (log2 (𝑑 ) + 1),

(A.3)

𝑖

where:
fij

raw term i frequency in document j,
95

d
di

number of documents in the collection,
number of documents in the entire collection where term i appears.

The TF-IDF2 weighting formula takes the following form:
𝑑

𝑤𝑖𝑗 = 𝑓𝑖𝑗 ∙ (log 2 (𝑑 ) + 1).

(A.4)

𝑖

A document collection can be represented as a set of document vectors d consisting of
the term weights or as a term-document weights matrix X. A query can be represented
as a vector q (or one-column matrix) of terms weights. Each weight can be normalized
by dividing it either by the length of the corresponding document or by the query vector.
The matrix X of the normalized TF-IDF2 term weights and query vector q are presented
in Table A.2.

A.3 VSM Similarities
The measures of VSM similarity between each document and a query, or query results,
can be computed as product of matrixes, r = qTX. Two terms from the query,
information and science, are present in the documents, the term degree is not present
in the documents so this term is does not affect the computations. The last row of
Table A.2 includes query-document similarity values.
Table A.2
Normalized TF-IDF2 Terms Weights and Documents Query Similarities
Document, X

Term
education
higher
information
library
science
studies

Query, q

d1

d2

d3

d4

d5

0.71
0.71
0.00
0.00
0.00
0.00

0.00
0.00
0.50
0.50
0.50
0.50

0.00
0.00
0.50
0.50
0.50
0.50

0.50
0.50
0.50
0.00
0.50
0.00

0.50
0.50
0.00
0.50
0.00
0.50

0.00
0.00
0.71
0.00
0.71
0.00

Query-document VSM similarity, r
0.00
0.71
0.71
0.71
0.00

A.4 Computing VSM Similarities for LSA
Matrix X of terms and documents with rank(X) = r ≤ min(t, d), where t is number of terms
in the collection and d is number of documents in the collection, can be decomposed
into the product of three other matrices using SVD model, X = TSDT (Deerwester et al.,
96

1990, p. 397). T and D are the matrices of left and right singular vectors and S is the
diagonal matrix of singular values. Figure A.1 illustrates SVD model.
dimensions

Ttxr

.

Srxr

.

dimensions

=

dimensions

Xtxd

documents

dimensions
terms

terms

documents

DTrxd

Figure A.1. Illustration of singular value decomposition (SVD).
Equitation X = TSDT for the sample collection of documents used in this example is
 0.71
 0.71

0.00

0.00
0.00

0.00

0.00 0.00 0.50 0.50
0.00 0.00 0.50 0.50

0.50 0.50 0.50 0.00

0.50 0.50 0.00 0.50
0.50 0.50 0.50 0.00

0.50 0.50 0.00 0.50
0.41 - 0.58 0.00 0.00 0.00
0
0
0   0.33
0.41 - 0.58 0.00 0.00 0.00 1.73 0

  0 1.22
0
0
0  - 0.67
 
0.41 0.29 0.50 0.00 0.00 

0
0.71 0
0    0.00
 0
 
0.41 0.29 - 0.50 0.00 0.00  0
0
0
0.00
0   0.00
0.41 0.29 0.50 0.00 0.00 
0
0
0
0.00  0.00

  0
0.41 0.29 - 0.50 0.00 0.00

(A.5)
0.47 0.47 
0.47 0.47 - 0.24 - 0.24

0.00 0.00 0.71 - 0.71.

0.00 0.00 0.00 0.00 
0.00 0.00 0.00 0.00 
0.47 0.47

The beauty of an SVD, however, is that it allows a simple strategy for optimal
approximate fit using smaller matrices. If the singular values in S, are ordered by
size, the first k largest may be kept and the remaining smaller ones set to zero.
The product of the resulting matrices is a matrix X̂k which is only approximately
equal to X, and is of rank k. It can be shown that the new matrix X̂k is the matrix
of rank k, which is closest in the least squares sense to X. (Deerwester et al.,
1990, p. 398)
The reduced k-rank or truncated model X ≈ X̂k = TkSkDkT can be used to approximate
data. The number of dimensions k < r can be chosen depending of the goals of the
approximation. The reduced model is illustrated in Figure A.2. The advantage of this
approximation and the use of a reduced model lies in representing relationships
between terms and documents at a higher level of semantic abstraction. The family of
methods based on this method is called LSA. LSA allows solving a number of problems
in IR such as representing indirect semantic relationships between terms and

97

documents, reducing the negative effect of synonymy and polysemy on IR results
(Deerwester et al., 1990; Landauer & Dumais, 1997; Landauer et al., 1998).
documents

dimensions

documents

.

Skxk

.

dimensions

Ttxk

dimensions

=

X̂k

terms

terms

dimensions

DTkxd

Figure A.2. Illustration of the reduced singular value decomposition model.

Reduced model (k = 2) equitation X̂k = TkSkDkT for the sample collection of documents:
 0.71
 0.71

0.00

0.00
0.00

0.00

0.00 0.00 0.50 0.50 0.41 - 0.58
0.00 0.00 0.50 0.50 0.41 - 0.58
 

0.50 0.50 0.25 0.25 0.41 0.29  1.73 0   0.33 0.47 0.47 0.47 0.47  (A.6)



0.50 0.50 0.25 0.25 0.41 0.29   0 1.22 - 0.67 0.47 0.47 - 0.24 - 0.24
0.50 0.50 0.25 0.25 0.41 0.29 
 

0.50 0.50 0.25 0.25 0.41 0.29 

Matrix X̂k, which is an approximation of terms weights matrix X using LSA, is also
presented in Table A.3.
Table A.3
Normalized TF-IDF2 Terms Weights Processed with Two-dimensional LSA and
Documents Query Similarities
Document, X̂k

Term
education
higher
information
library
science
studies

d1
0.71
0.71
0.00
0.00
0.00
0.00

d2
0.00
0.00
0.50
0.50
0.50
0.50

d3
0.00
0.00
0.50
0.50
0.50
0.50

Query, q
d4
0.50
0.50
0.25
0.25
0.25
0.25

d5
0.50
0.50
0.25
0.25
0.25
0.25

Query-document LSA-VSM similarity, rk
0.00
0.71
0.71
0.35
0.35

98

0.00
0.00
0.71
0.00
0.71
0.00

Measures of VSM similarities between each document and a query using TF-IDF2
weights processed with LSA can be computed as rk = qTX̂k. These similarities illustrate
the relationships between the documents and query that are different from those
relationships with the use of only TF-IDF2 model (see Table A.2). Particularly, similarity
of the query information science degree and document d4 decreased and similarity of
the document d5 (includes library studies) changed from 0 to 0.35 even though query
and document d5 do not have common terms. This positive value of similarity can be
explained by the fact that query and document d5 have latent relationships through the
terms in other documents.

A.5 Implementation of LSA Indexing and Ranking in Experimental IR System
One way to implement LSA processing in an IR system (which uses TF-IDF2 weighting
and VSM similarity engine) without redesigning its search engine, is creating an
ancillary metadata field that contains pseudo documents. The values for this field can
be computed outside of the system in the offline mode. This implementation provides an
approximation of LSA processing and is adequate for experimental studies.
The construction of the pseudo documents starts with the conversion of the X̂k matrix
into a matrix of integer values in order to model LSA pattern with terms frequencies.
This can be done using the following linear transformation: X-integer = round(aX̂k + b),
where:
𝑎 = (𝑤

(𝑚−𝑛)

,

(A.7)

𝑏 = 𝑛 − 𝑎 ∙ 𝑤𝑚𝑖𝑛 ,

(A.8)

𝑚𝑎𝑥 −𝑤𝑚𝑖𝑛 )

where:
m
is maximum target frequency for any term, e.g., 10, 50, 100, etc.,
n
is minimum target frequency for any term, e.g., 0 or 1,
This procedure provides the following transformation: [wmin, wmax] → [n, m]. The
following values were used in this example: n = 0, m = 10 (see Table A.4).
Other non-linear transformations (e.g., log), maximum and minimum numbers as well as
thresholds can be used. The pattern in Table A.4 was used to construct pseudo
documents by repeating the corresponding terms. For example, in pseudo document
d4-pseudo term education is repeated 7 times while term information is repeated 4
times, etc. The resulting pseudo document is:
d4-pseudo = “education education education education education education education
higher higher higher higher higher higher higher information information
information information library library library library science science
science science studies studies studies studies”

99

Table A.4
Pseudo Documents Terms Counts
Term
education
higher
information
library
science
studies

Number of terms proportional to X̂k weights
d1
10
10
0
0
0
0

d2

d3
0
0
7
7
7
7

d4
0
0
7
7
7
7

d5
7
7
4
4
4
4

7
7
4
4
4
4

The structure of pseudo document approximates the patterns discovered by LSA during
the offline processing and prepares the pseudo documents for processing by the IR
system with the regular built-in TF-IDF2 algorithm.
To estimate the quality of this approximation the collection of pseudo documents was
processed using TF-IDF2 weighting (the same procedure used by the IR system on the
pseudo documents during online processing) and matrix of the terms weights with
respect to the pseudo documents obtained. This matrix is an approximation of the
matrix X̂k corresponding to the original documents: X̂k-pseudo ≈ X̂k (see Table A.5).
Table A.5
Normalized TF-IDF2 Weights for the Pseudo Documents
Pseudo document, X̂k-pseudo

Term
education
higher
information
library
science
studies

d1

d2

d3

d4

d5

0.71
0.71
0.00
0.00
0.00
0.00

0.00
0.00
0.50
0.50
0.50
0.50

0.00
0.00
0.50
0.50
0.50
0.50

0.60
0.60
0.26
0.26
0.26
0.26

0.60
0.60
0.26
0.26
0.26
0.26

Comparison of the values in X̂k (Table A.3) and in X̂k-pseudo (Table A.5) suggested that
the pattern of values is similar. To measure the closeness of these matrixes in least
square sense the correlation coefficient was computed as cor(X̂k, X̂k-pseudo) = 0.966.
The correlation is also visualized in Figure A3. The conclusion is that X̂k-pseudo is a
very good approximation of X̂k and this method can be used for approximation of LSA
processing in an experimental IR system.

100

0.80
0.70
0.60
0.50
0.40
0.30
0.20
0.10
0.00
0.00

0.20

0.40

0.60

0.80

Figure A.3. Correlation between X̂k and X̂k-pseudo matrixes.

A.6 Rotation of LSA Dimensions with rLSI
Figure A.2 and Equation 6 illustrate the reduced rank, k = 2, model which can be used
to approximate the term-document weights matrix X ≈ X̂k = TkSkDkT and compute
similarities between query and documents. To find the degree of relatedness of a term
or a document to each of k dimensions the terms and document loadings (weights) on
each of k dimensions can be computed: LT = TkSk and LD = DkSk. Matrixes of loadings
LT and LD for two dimensions (k = 2) are presented in Table A.6 and Table A.7.

101

Table A.6
Loadings of Terms on Un-rotated and Rotated Factors
LT

Term
education
higher
information
library
science
studies

F1
0.71
0.71
0.71
0.71
0.71
0.71

L*T (rotated)
F2*
-0.71
-0.71
0.35
0.35
0.35
0.35

F1
0.16
0.16
0.78
0.78
0.78
0.78

F2*
-0.99
-0.99
-0.13
-0.13
-0.13
-0.13

Table A.7
Loadings of Documents on Un-rotated and Rotated Factors
Doc ID

LD

L*D (rotated)

d1

F1
0.58

F2*
-0.82

F1
-0.01

F2*
-1.00

d2

0.82

0.58

1.00

-0.01

d3

0.82

0.58

1.00

-0.01

d4

0.82

-0.29

0.49

-0.71

d5

0.82

-0.29

0.49

-0.71

Document text
Higher education opens new opportunities.
Many universities offer information science or library
studies programs.
Information science and library studies are related
disciplines.
Higher education in information science gives
theoretical knowledge.
Higher education in library studies provides practical
preparation.

One of the fundamental problems of LSA is that it is not always possible to understand
the meaning of its dimensions because “there are no explicit interpretations for the
[LSA] dimensions” (Hu, Cai, Wiemer-Hastings, Graesser, & McNamara, 2007).
However, there are studies that address the issue of interpreting the latent semantic
dimensions. Sidorova et al., (2008) empirically demonstrated that such interpretation
can, in fact, be attained by rotating the dimensions. The varimax rotation algorithm
which simplifies the structure of the relationships between terms, documents and
dimensions, was used in order to extract rotated factors (new dimensions). The rotation
is done by multiplying term loadings by a rotation matrix L*T = LTM. Rotation matrix M is
obtained according to a varimax terms structure simplification criterion. Then reciprocal
rotation is applied to the document loadings L*D = LDM. The formula of SVD with
rotation takes the form X = (TSM)(DM)T. Terms weights matrix X remain unaffected with
this rotation due to the orthonormality property of the rotation matrix MMT = I. The
rotation also has no effect on the queries because X remains unchanged and so the
query results r = qTX. The rotation rules hold for the reduced model X̂k.

102

Table A.6 presents rotated term loadings L*T = TkSkMk and Table A.7 represents rotated
document loadings L*D = DkSkMk in comparison with un-rotated loadings.

A.7 Interpretation of Factors with rLSI
Figure A.4 shows and compares terms and document loadings in two-dimensional
spaces of un-rotated and rotated factors. The axes of rotated factors show that the
relationships between terms and factors, and documents and factors are simplified
compared to those relationships in un-rotated factor space. The terms and documents
which are mostly about information science and library and studies are located close to
the axis F1*, the terms and documents that are mostly about higher education are
located close to axis F2*.
While the graph in Figure A.4 illustrates the mechanics of the factor rotation, the
interpretation of factors can be done by examining each factor’s several highest loaded
terms and documents in formats presented in Table A.6 and Table A.7. This procedure
of interpretation is also efficient for large collections with a large number of terms.
Loadings in Table B.5 show that all terms were equally loaded on the un-rotated
Factor F1 what did not allow determining the topical concepts of this factor. After
rotation, loadings of terms information, science, library, and studies on F1* increased
and loadings of terms higher and education decreased. Also, loadings of the documents
on the rotated Factor F1* have changed. These documents received higher loadings
and provided additional clues about this factor’s topical concepts. The rotated
Factor F1* is about information science and library studies. Similarly, it was concluded
that Factor F2* is about higher education.

103

0.70

LT, LD
(F1* rotated)

LT, LD (F2)

d2, d3
0.50

0.30

information,
science,
library,
studies

F1* =
information
science and
library studies

0.10
LT, LD (F1)
0.00
-0.10

-0.30

0.20

0.40

0.60

0.80

d4, d5

F2* = higher
education

-0.50

higher,
education

-0.70
d1
-0.90

o - documents
LT, LD
x - terms
(F2* rotated)

Figure A.4. Representation of term and of document loadings in un-rotated and rotated
two-dimensional space.

104

APPENDIX B
INSTITUTIONAL REVIEW BOARD APPROVAL LETTER
AND INFORMED CONSENT NOTICES

105

106

107

108

109

110

APPENDIX C
INSTRUMENT

111

The following questionnaire instrument is organized by sections and pages numbered
using the P#.# format which replicates the format in the Qualtrics TM online survey
system. The term survey (instead of the term questionnaire) is used in the text of the
questionnaire because this term is familiar to the participants from the branding of the
Qualtrics TM as “the online survey software” (http://www.qualtrics.com/). Each page was
implemented as a separate screen in a Web browser.

P1.1
Introduction
You are being asked to help us evaluate two online search systems. The search
systems will give you access to a digital library of journal articles. First you will try
System 1 and then System 2. The systems will appear in separate browser windows or
tabs after you follow provided links. While working inside a system, do not close this
main survey window or tab, so you can go back to it.
With each system, you will need to follow three steps:
STEP 1: Browse the contents of the digital library using the “BROWSE ALL ITEMS”
menu.
STEP 2: Execute the search query that is provided to you on the next pages of this
survey.
STEP 3: Focus on the subject terms list displayed on the left panel of the digital library
interface. The survey will later ask you to evaluate those subject terms.
The survey system will guide you through these steps.
P2.1
Before you advance to the next page of this survey, follow your search system URL link
provided below. This is System 1. The system will open in a new browser window or
tab. Browse the contents of the digital library using “BROWSE ALL ITEMS” menu.
Without closing the system's browser window or tab, return to this page and advance to
the next screen.
Your search system URL link is: [URL Link for System 1 will be provided here]
P2.2
Now you are asked to find information on a given topic. Use your search query (listed
below) and perform the search using the system’s search box. Without closing the
system’s browser window or tab, return to this page and advance to the next screen.

112

Your search query is:
[Search query text was provided here] 1
P2.3
After processing the query, the system will display a list of journal articles. You will be
able to open their full text and see if there are any articles relevant to your query. The
system is interactive, so you will be able to modify your search.
Please spend some time doing that. Without closing the system’s window or tab, return
to this page and advance to the next screen.
P2.4
To limit your search results, please select or unselect appropriate subject terms on the
left side of the interface. You will also be able to reformulate the query in the search box
and resubmit, but please focus on using the subject terms on the left.
Please spend some time doing that. Without closing the system’s window or tab, return
to this page and advance to the next screen.
P2.5
As you discover relevant findings, come back to this survey and compose your findings
paragraph in the box provided below. Compose a findings paragraph that reports:
(1) One or more sentences from one of the papers in the system’s collection that
provides an answer or solution to the search query. Copy/paste the sentences directly
from articles found using the system. Add quotation marks to indicate direct quotation.
(2) Add the authors' last names and the publication year in parentheses, to indicate
attribution.
(3) If necessary, repeat with a second set of sentences taken from a second source.
Example search query: Measures of relevance in information science
Example findings paragraph:
“An important measure of relevance is recall” (Smith, King, & Doe, 1999). “Not all
variants of recall are consistent with human perception” (Bride & Jones, 2003).
Remember that your search query is:
[The same search query text will be repeated here – see footnote 1]
1

Examples of search query texts include “Examples of social tagging” and “Types of access to
electronic resources”.

113

Your findings paragraph (enter your findings paragraph in the box below and then
advance to the next screen):

P2.6
You have finished the task involved in evaluation of digital library System 1. Please
close the system’s browser window or tab. Then advance to the next screen of this
survey to answer several questions about your experience with this system.

114

P3.1
Please indicate your agreement or disagreement below using 1 for strongly disagree
and 7 for strongly agree. 2
Table C.1
Q1–Q5: Alignment between User’s Mental (Internal) Map and External Map Displayed
by the System (IEMA)

Questions
The subject terms
presented on the left
panel described well
my understanding of
information science.
The subject terms
presented on the left
panel matched what I
would expect to see.
The subject terms
presented on the left
panel made sense to
me.
The subject terms
presented on the left
panel were a good
choice.
The subject terms
presented on the left
panel were just right.

Strongly
Disagree
(1)

Disagree
(2)

Somewhat
Disagree
(3)

Neither
Agree nor
Disagree
(4)

Somewhat
Agree
(5)

Agree
(6)

Strongly
Agree
(7)

o

o

o

o

o

o

o

o

o

o

o

o

o

o

o

o

o

o

o

o

o

o

o

o

o

o

o

o

o

o

o

o

o

o

o

2

Numbers and titles of the tables will not appear on the actual questionnaire. They are provided
here for the investigators’ reference.

115

P3.2
Please indicate your agreement or disagreement below using 1 for strongly disagree
and 7 for strongly agree.
Table C.2
Q6–Q10: Topical labels guidance (TLG)

Questions
The subject terms
presented on the left
panel provided good
guidance during my
search.
The subject terms
presented on the left
panel were helpful
during my search.
The subject terms
presented on the left
panel were helpful in
suggesting what to do
next.
The subject terms
presented on the left
panel helped me find
my way as my search
progressed.
The subject terms
presented on the left
panel provided a
roadmap that helped
my search succeed.

Strongly
Disagree
(1)

Disagree
(2)

Somewhat
Disagree
(3)

Neither
Agree nor
Disagree
(4)

Somewhat
Agree
(5)

Agree
(6)

Strongly
Agree
(7)

o

o

o

o

o

o

o

o

o

o

o

o

o

o

o

o

o

o

o

o

o

o

o

o

o

o

o

o

o

o

o

o

o

o

o

116

P3.3
Please indicate your agreement or disagreement below using 1 for strongly disagree
and 7 for strongly agree.
Table C.3
Q11–Q14: Satisfaction with the Search Process (SSP)

Questions
I was happy with the
search.
I liked the outcome of
this search.
I am satisfied with my
experience during this
search.
I feel positively about
this search.

Strongly
Disagree
(1)

Disagree
(2)

Somewhat
Disagree
(3)

Neither
Agree nor
Disagree
(4)

Somewhat
Agree
(5)

Agree
(6)

Strongly
Agree
(7)

o

o

o

o

o

o

o

o

o

o

o

o

o

o

o

o

o

o

o

o

o

o

o

o

o

o

o

o

P4.1
Please repeat the same task with the second system. Make sure that the browser
window or tab with System 1 is closed. The next several pages will provide a different
URL link for System 2 and will include similar instructions and questions.

NOTE: The instructions P2.1 through P2.6 will be repeated here to evaluate System 2
(different URL link will be provided). Then, questions Q1-Q14 (from P3.1 to P3.3) will be
repeated here.
P6.1
You have completed the task involved in the evaluation of two digital library systems.
Please answer the last several questions that follow.

117

Please indicate your agreement or disagreement below using 1 for strongly disagree
and 7 for strongly agree.
Table C.4
Q15–Q18: Domain Knowledge (DK)

Questions
I feel very familiar with
the domain of
information science.
I believe I have
considerable
experience with the
domain of information
science.
I understand the
domain of information
science pretty well.
I feel competent in the
area of information
science.

Strongly
Disagree
(1)

Disagree
(2)

Somewhat
Disagree
(3)

Neither
Agree nor
Disagree
(4)

Somewhat
Agree
(5)

Agree
(6)

Strongly
Agree
(7)

o

o

o

o

o

o

o

o

o

o

o

o

o

o

o

o

o

o

o

o

o

o

o

o

o

o

o

o

Q19–Q23: Demographics
Q19: What is your gender?
o Female
o Male
Q20: What is your age category?
o 18–20
o 21–25
o 26–30
o 31–40
o 41–50
o 51+
Q21: What degree level are you currently pursuing?
o Undergraduate, non-degree seeking
o Undergraduate degree, e.g. BA/BS
o Graduate non-degree seeking
o Master’s, e.g. MA, MS, MBA, MFA
o Doctoral, e.g. Ph.D.

118

Q22: What is your major?
o Information Science (College of Information-COI)
o Library Science (College of Information-COI)
o Other in COI, non-Info. Science
o IT/BCIS (College of Business-COB)
o Other in COB, non-IT/BCIS
o Other, non-COI and non-COB
o Undeclared major
Q23: Is English your first language?
o Yes
o No (but I am fluent)
o No (I am still learning)

P6.8
This is the last page on the main survey.
Please advance forward to save your responses. A separate exit survey will open after
this. There you may leave your name for the purposes of receiving compensation for
your participation. This survey is stored separately from the one you have just
completed, therefore your names cannot be associated with the results of the main
survey.
Thank you!

Exit questionnaire
Please submit your name for purposes of receiving compensation for your participation.
Thank you!
First Name
Last Name
Course Number
Instructor’s Name

___________
___________
___________
___________

119

APPENDIX D
TOPICAL LABELS FROM EBSCOHOST

120

The EBSCOhost topical labels for the 266 article set used in the experiment are
presented in Table C.1. The topical labels per document were: minimum = 4,
average = 7.5, maximum = 19. Documents per topical label were: minimum = 1,
average = 1.9, maximum = 48. Number of unique labels used: 1031.
Table D.1
First 30 Most Frequent Topical Labels from EBSCOhost (266 Articles Set)
#
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
…
1031

Topical label

Number of Times Used

Internet Publishing and Broadcasting and Web Search Portals
Information-seeking behavior
Information science
Information-seeking behavior -- Research
Information resources
Internet searching
Information resources management
Information-seeking strategies -- Research
Wired Telecommunications Carriers
Information science -- Research
Information retrieval
All Other Information Services
Library science
Information services
Research
Information sharing
Information-seeking strategies
Information technology
Human behavior
Interviews
Internet users
Websites
Information literacy
Libraries and Archives
Surveys
Internet
Quantitative research
Research and Development in the Social Sciences and Humanities
Open access publishing
Internet research
…
Self-presentation

121

48
42
40
31
29
23
21
17
15
14
14
13
12
12
12
11
11
11
11
10
10
10
10
9
9
9
8
8
8
7
…
1

APPENDIX E
SAMPLE ABSTRACT OF THE ARTICLE FROM THE DOCUMENT COLLECTION

122

(Abrahamson & Goodman-Delahunty, 2013)

123

REFERENCES

Abrahamson, D. E., & Goodman-Delahunty, J. (2013). The impact of organizational
information culture on information use outcomes in policing: An exploratory study.
Information Research, 18(4). Retrieved from http://www.informationr.net/ir/184/paper598.html
Anderson, J. D., & Pérez-Carballo, J. (2001). The nature of indexing: How humans and
machines analyze messages and texts for retrieval. Part II: Machine indexing, and
the allocation of human versus machine effort. Information Processing &
Management, 37(2), 255–277. doi:10.1016/s0306-4573(00)00046-7
Apache Software Foundation. (2014). TFIDFSimilarity: Lucene API. Retrieved from
https://lucene.apache.org/core/4_3_0/core/org/apache/lucene/search/similarities/TF
IDFSimilarity.html
Borgman, C., Sølvberg, I., & Kovács, L. (Eds.). (2002). Introduction. In Evaluation of
digital libraries: Testbeds, measurements, and metrics: Proceedings of the DELOS
Workshops. Budapest, Hungary: Hungarian Academy of Sciences.
Borlund, P., & Ingwersen, P. (1998). Measures of relative relevance and ranked half-life.
In Proceedings of the 21st Annual International ACM SIGIR Conference on
Research and Development in Information Retrieval (SIGIR) (pp. 324–331). New
York, NY: ACM Press. doi:10.1145/290941.291019
Brewer, M. B. (2000). Research design and issues of validity. In H. T. Reis & C. M. Judd
(Eds.), Handbook of research methods in social and personality psychology (pp. 3–
16). Cambridge, U.K.: Cambridge University Press.
Brunswik, E. Representative design and probabilistic theory in a functional psychology. ,
62 Psychological Review 193–217 (1955). doi:10.1037/h0047470
Büttcher, S., Clarke, C. L. A., & Cormack, G. V. (2010). Information retrieval:
Implementing and evaluating search engines. Cambridge, MA: MIT Press.
Chen, Y.-N., & Ke, H.-R. (2014). A study on mental models of taggers and experts for
article indexing based on analysis of keyword usage. Journal of the Association for
Information Science and Technology, 65(8), 1675–1694. doi:10.1002/asi.23077
Chin, J. P., Diehl, V. A., & Norman, L. K. (1988). Development of an instrument
measuring user satisfaction of the human-computer interface. In Proceedings of the
SIGCHI Conference on Human Factors in Computing Systems (CHI) (pp. 213–
218). New York, NY: ACM Press. doi:10.1145/57167.57203

124

Chung, E., Miksa, S. D., & Hastings, S. K. (2010). A framework of automatic subject
term assignment for text categorization: An indexing conception-based approach.
Journal of the American Society for Information Science and Technology, 61(4),
688–699. doi:10.1002/asi.21272
Cortina, J. M. (1993). What is coefficient alpha? An examination of theory and
applications. Journal of Applied Psychology, 78(1), 98–104. doi:10.1037/00219010.78.1.98
Creswell, J. W. (2013). Research design: Qualitative, quantitative, and mixed methods
approaches (4th ed.). Thousand Oaks, CA: SAGE.
Davis, F. D. (1989). Perceived usefulness, perceived ease of use, and user acceptance
of information technology. MIS Quarterly, 13(3), 319. doi:10.2307/249008
Davis, F. D., Bagozzi, R. P., & Warshaw, P. R. (1989). User acceptance of computer
technology: A comparison of two theoretical models. Management Science, 35(8),
982–1003.
Deerwester, S., Dumais, S. T., Furnas, G. W., Landauer, T. K., & Harshman, R. A.
(1990). Indexing by latent semantic analysis. Journal of the American Society for
Information Science, 41(6), 391–407. doi:10.1002/(SICI)10974571(199009)41:6<391::AID-ASI1>3.0.CO;2-9
DeLone, W. H., & McLean, E. R. (1992). Information systems success: The quest for
the dependent variable. Information Systems Research, 3(1), 60–95.
doi:10.1287/isre.3.1.60
Duval, E., Hodgins, W., Sutton, S., & Weibel, S. L. (2002). Metadata principles and
practicalities. D-Lib Magazine, 8(4). doi:10.1045/april2002-weibel
Efron, M. J. (2005). Eigenvalue-based model selection during latent semantic indexing.
Journal of the American Society for Information Science and Technology, 56(9),
969–988. doi:10.1002/asi.20188
Evangelopoulos, N. E., Zhang, X., & Prybutok, V. R. (2012). Latent semantic analysis:
Five methodological recommendations. European Journal of Information Systems,
21(1), 70–86. doi:10.1057/ejis.2010.61
Furnas, G. W., Deerwester, S., Dumais, S. T., Landauer, T. K., Harshman, R. A.,
Streeter, L. A., & Lochbaum, K. E. (1988). Information retrieval using a singular
value decomposition model of latent semantic structure. In Proceedings of the 11th
Annual International ACM SIGIR Conference on Research and Development in
Information Retrieval (pp. 465–480). New York, NY: ACM Press.
doi:10.1145/62437.62487

125

Furnas, G. W., Gomez, L. M., Dumais, S. T., & Landauer, T. K. (1983). Human factors
and behavioral science: Statistical semantics: Analysis of the potential performance
of key-word information systems. Bell System Technical Journal, 62(6), 1753–
1806. doi:10.1002/j.1538-7305.1983.tb03513.x
Gall, M. D., Gall, J. P., & Borg, W. R. (2007). Educational research: An introduction (8th
ed.). Boston, MA: Pearson/Allyn & Bacon.
Gefen, D., Straub, D., & Boudreau, M.-C. (2000). Structural equation modeling and
regression: Guidelines for research practice. Communications of the Association for
Information Systems, 4(7), 1–76.
Green, S. B. (1991). How Many Subjects Does It Take To Do A Regression Analysis.
Multivariate Behavioral Research, 26(3), 499–510.
doi:10.1207/s15327906mbr2603_7
Greenberg, J., Spurgin, K., & Crystal, A. (2006). Functionalities for automatic metadata
generation applications: A survey of metadata experts’ opinions. International
Journal of Metadata, Semantics and Ontologies, 1(1), 3–20.
doi:10.1504/IJMSO.2006.008766
Griffin, B. (2011). Lecture: Quantitative research matrix. Retrieved from
http://www.bwgriffin.com/gsu/courses/edur7130/content/quantitative_research_matr
ix.htm
Harman, D. K., & Voorhees, E. M. (2006). TREC: An overview. Annual Review of
Information Science and Technology, 40(1), 113–155.
doi:10.1002/aris.1440400111
Hu, X., Cai, Z., Wiemer-Hastings, P., Graesser, A. C., & McNamara, D. S. (2007).
Strengths, limitations, and extensions of LSA. In T. K. Landauer, D. S. McNamara,
S. Dennis, & W. Kintsch (Eds.), Handbook of latent semantic analysis (pp. 401–
426). Mahwah, NJ: Lawrence Erlbaum Associates.
Huang, S.-L., Lin, S.-C., & Chan, Y.-C. (2012). Investigating effectiveness and user
acceptance of semantic social tagging for knowledge sharing. Information
Processing & Management, 48(4), 599–617. doi:10.1016/j.ipm.2011.07.004
Ingwersen, P., & Järvelin, K. (2005). The turn: Integration of information seeking and
retrieval in context. Dordrecht, Netherlands: Springer. doi:10.1007/1-4020-3851-8
International Organization for Standardization. (1998). ISO 9241-11:1998: Ergonomic
requirements for office work with visual display terminals—Part 11: Guidance on
usability. Geneva, Switzerland: International Organization for Standardization.

126

International Organization for Standardization. (2010). ISO 9241-210:2010: Ergonomics
of human-system interaction—Part 210: Human-centred design for interactive
systems. Geneva, Switzerland: International Organization for Standardization.
Järvelin, K., & Kekäläinen, J. (2000). IR evaluation methods for retrieving highly relevant
documents. In Proceedings of the 23rd Annual International ACM SIGIR
Conference on Research and Development in Information Retrieval (SIGIR) (pp.
41–48). New York, NY: ACM Press. doi:10.1145/345508.345545
Jones, K. S., & Willett, Peter (Eds.). (1997). Readings in information retrieval. San
Francisco, CA: Morgan Kaufmann.
Kelly, D. (2009). Methods for evaluating interactive information retrieval systems with
users. Foundations and Trends® in Information Retrieval, 3(1–2), 1–224.
doi:10.1561/1500000012
Kelly, D., & Sugimoto, C. R. (2013). A systematic review of interactive information
retrieval evaluation studies, 1967–2006. Journal of the American Society for
Information Science and Technology, 64(4), 745–770. doi:10.1002/asi.22799
Keppel, G., & Wickens, T. D. (2004). Design and analysis: A researcher’s handbook
(4th ed.). Upper Saddle River, NJ: Pearson Prentice Hall.
Kerlinger, F. N. (1979). Behavioral research: A conceptual approach. New York, NY:
Holt, Rinehart and Winston.
Kintsch, W. (2007). Meaning in context. In T. K. Landauer, D. S. McNamara, S. Dennis,
& W. Kintsch (Eds.), Handbook of latent semantic analysis (pp. 89–106). Mahwah,
NJ: Lawrence Erlbaum Associates.
Kulkarni, S. S., Apte, U. M., & Evangelopoulos, N. E. (2014). The use of latent semantic
analysis in operations management research. Decision Sciences, 45(5), 971–994.
doi:10.1111/deci.12095
Kumar, A., Gupta, A., Batool, M., & Trehan, S. (2006). Latent semantic indexing based
intelligent information retrieval system for digital libraries. Journal of Computing and
Information Technology, 14(3), 191–196. doi:10.2498/cit.2006.03.02
Lancaster, F. W., & Warner, A. J. (1993). Information retrieval today. Arlington, VA:
Information Resources Press.
Landauer, T. K. (1997). Behavioral research methods in human-computer interaction. In
M. G. Helander, T. K. Landauer, & P. V. Prabhu (Eds.), Handbook of humancomputer interaction (2nd ed., pp. 203–227). Amsterdam, Netherlands: Elsevier.

127

Landauer, T. K. (2007). LSA as a theory of meaning. In T. K. Landauer, D. S.
McNamara, S. Dennis, & W. Kintsch (Eds.), Handbook of latent semantic analysis
(pp. 3–34). Mahwah, NJ: Lawrence Erlbaum Associates.
Landauer, T. K., & Dumais, S. T. (1997). A solution to Plato’s problem: The latent
semantic analysis theory of acquisition, induction, and representation of knowledge.
Psychological Review, 104(2), 211–240. doi:10.1037/0033-295X.104.2.211
Landauer, T. K., Foltz, P. W., & Laham, R. D. (1998). An introduction to latent semantic
analysis. Discourse Processes, 25(2-3), 259–284.
doi:10.1080/01638539809545028
Landauer, T. K., Laham, D., & Derr, M. (2004). From paragraph to graph: Latent
semantic analysis for information visualization. Proceedings of the National
Academy of Sciences of the United States of America, 101 Suppl, 5214–5219.
doi:10.1073/pnas.0400341101
Larson, R. R. (2012). Information retrieval systems. In M. J. Bates (Ed.), Understanding
information retrieval systems: Management, types, and standards (pp. 15–30).
Boca Raton, FL: CRC Press. doi:10.1201/b11499-4
Littman, M. L., & Jiang, F. (1998). A comparison of two corpus-based methods for
translingual information retrieval (Technical Report No. CS-1998-11). Durham, NC:
Duke University.
Manning, C. D., Raghavan, P., & Schütze, H. (2008). Introduction to information
retrieval. New York, NY: Cambridge University Press.
Marchionini, G. (2006a). Exploratory search: From finding to understanding.
Communications of the ACM, 49(4), 41. doi:10.1145/1121949.1121979
Marchionini, G. (2006b). Toward human-computer information retrieval. Bulletin of the
American Society for Information Science and Technology, 32(5), 20–22.
doi:10.1002/bult.2006.1720320508
Marchionini, G., Dwiggins, S., Katz, A., & Lin., X. (1993). Information seeking in full-text
end-user-oriented search systems: The roles of domain and search expertise.
Library and Information Science Research, 15(1), 35–69.
Miller, S. J. (2011). Metadata for digital collections: A how-to-do-it manual. New York,
NY: Neal-Schuman Publishers.
Moen, W. E. (2009). Unit 3: Metadata and information retrieval. Metadata and
networked information organization and retrieval: Learning modules. Denton, TX:
University of North Texas.

128

Naumer, C. M., & Fisher, K. E. (2009). Information needs. In M. Bates & M. N. Maack
(Eds.), Encyclopedia of library and information sciences (3rd ed., pp. 2452–2458).
New York, NY: CRC Press.
Nunnally, J. C. (1967). Psychometric theory. New York, NY: McGraw-Hill.
O’Brien, H. L., & Lebow, M. (2013). Mixed-methods approach to measuring user
experience in online news interactions. Journal of the American Society for
Information Science and Technology, 64(8), 1543–1556. doi:10.1002/asi.22871
O’Brien, H. L., & Toms, E. G. (2010). The development and evaluation of a survey to
measure user engagement. Journal of the American Society for Information
Science and Technology, 61(1), 50–69. doi:10.1002/asi.21229
O’Brien, H. L., & Toms, E. G. (2013). Examining the generalizability of the User
Engagement Scale (UES) in exploratory search. Information Processing &
Management, 49(5), 1092–1107. doi:10.1016/j.ipm.2012.08.005
Park, J., & Lu, C. (2009). Application of semi-automatic metadata generation in libraries:
Types, tools, and techniques. Library & Information Science Research, 31(4), 225–
231. doi:10.1016/j.lisr.2009.05.002
Payette, S., & Lagoze, C. (2000). Policy-carrying, policy-enforcing digital objects. In J.
Borbinha & T. Baker (Eds.), Research and Advanced Technology for Digital
Libraries (Vol. 1923, pp. 144–157). Berlin, Heidelberg: Springer. doi:10.1007/3-54045268-0_14
Payne, S. J., & Howes, A. (2013). Adaptive interaction: A utility maximization approach
to understanding human interaction with technology. San Rafael, CA: Morgan &
Claypool. doi:10.2200/S00479ED1V01Y201302HCI016
Polyakov, S. (2012, May). Enhancing a digital repository with objects’ embedded
metadata. Poster session presented at the Texas Conference on Digital Libraries
(TCDL 2012), Austin, TX.
Polyakov, S., & Moen, W. E. (2012, August). Using Islandora to manage heterogeneous
research documents. Presentation and poster session at the conference Islandora
Camp 2012, Charlottetown, Canada.
Polyakov, S., & Moen, W. E. (2013, July). Expanding metadata reuse with an Islandora
metadata extraction utility. Paper presented at the Fedora User Group of the 8th
International Conference on Open Repositories (OR 2013), Charlottetown, Canada.
Rasmussen, E. M. (2005). Indexing and retrieval for the Web. Annual Review of
Information Science and Technology, 37(1), 91–124. doi:10.1002/aris.1440370104

129

Redmond-Neal, A., & Hlava, M. M. K. (Eds.). (2005). ASIS&T thesaurus of information
science, technology, and librarianship (3rd ed.). Medford, NJ: Information Today.
Ringle, C. M., Wende, S., & Becker, J.-M. (2015). SmartPLS (v. 3). Boenningstedt,
Germany: SmartPLS GmbH. Retrieved from www.smartpls.com
Robertson, S. E., & Beaulieu, M. (1997). Research and evaluation in information
retrieval. Journal of Documentation, 53(1), 51–57.
doi:10.1108/EUM0000000007190
Salton, G., & Buckley, C. (1988). Term-weighting approaches in automatic text retrieval.
Information Processing & Management, 24(5), 513–523. doi:10.1016/03064573(88)90021-0
Salton, G., Wong, A., & Yang, C. S. (1975). A vector space model for automatic
indexing. Communications of the ACM, 18(11), 613–620.
doi:10.1145/361219.361220
Sauro, J., & Lewis, J. R. (2012). Quantifying the user experience: Practical statistics for
user research. Waltham, MA: Morgan Kaufmann.
Schumacker, R. E., & Lomax, R. G. (2010). A beginner’s guide to structural equation
modeling (3rd ed.). Mahwah, NJ: Lawrence Erlbaum Associates.
Shannon, C. E., & Weaver, W. (1949). The mathematical theory of communication.
Urbana, IL: University of Illinois Press.
Sidorova, A., Evangelopoulos, N. E., Valacich, J. S., & Ramakrishnan, T. (2008).
Uncovering the intellectual core of the information systems discipline. MIS
Quarterly, 32(3), 467–482.
Sin, S.-C. J. (2011). Towards agency-structure integration: A Person-in-Environment
(PIE) framework for modelling individual-level information behaviours and
outcomes. In A. Spink & J. Heinström (Eds.), New directions in information
behaviour (pp. 181–209). Bingley, U.K.: Emerald.
Smucker, M. D. (2011). Information representation. In I. Ruthven & D. Kelly (Eds.),
Interactive information seeking, behaviour and retrieval (pp. 77–94). London, U.K.:
Facet.
Streeter, L. A., Lochbaum, K. E., LaVoie, N., & Psotka, J. E. (2007). Automated tools for
collaborative learning environments. In T. K. Landauer, D. S. McNamara, S.
Dennis, & W. Kintsch (Eds.), Handbook of latent semantic analysis (pp. 279–290).
Mahwah, NJ: Lawrence Erlbaum Associates.

130

Sun, H., & Zhang, P. (2008). An exploration of affect factors and their role in user
technology acceptance: Mediation and causality. Journal of the American Society
for Information Science and Technology, 59(8), 1252–1263. doi:10.1002/asi.20819
Tang, M.-C. (2007). Browsing and searching in a faceted information space: A
naturalistic study of PubMed users’ interaction with a display tool. Journal of the
American Society for Information Science and Technology, 58(1), 1998–2006.
doi:10.1002/asi.20689
Taylor, A. G., & Joudrey, D. N. (2009). The organization of information (3rd ed.).
Englewood, CO: Libraries Unlimited.
Torres, R. J. (2002). Practitioner’s handbook for user interface design and development.
Upper Saddle River, NJ: Prentice Hall PTR.
Tullis, T., & Albert, B. (2013). Measuring the user experience: Collecting, analyzing, and
presenting usability metrics (2nd ed.). Amsterdam, Netherlands: Elsevier.
Tunkelang, D. (2009). Faceted search. San Rafael, CA: Morgan & Claypool.
doi:10.2200/S00190ED1V01Y200904ICR005
Venkatesh, V., Morris, M. G., Davis, G. B., & Davis, F. D. (2003). User acceptance of
information technology: Toward a unified view. MIS Quarterly, 27(3), 425–478.
doi:10.2307/30036540
Wang, P. (2011). Information behavior and seeking. In I. Ruthven & D. Kelly (Eds.),
Interactive information seeking, behaviour and retrieval (pp. 15–42). London, U.K.:
Facet.
Warner, J. (2010). Human information retrieval. Cambridge, MA: MIT Press.
White, R. W., Capra, R., Golovchinsky, G., Kules, B., Smith, C., & Tunkelang, D. (2013).
Introduction to special issue on human-computer information retrieval. Information
Processing & Management, 49(5), 1053–1057. doi:10.1016/j.ipm.2013.02.002
Winson-Geideman, K., & Evangelopoulos, N. E. (2013). Topics in real estate research:
1973–2010: A latent semantic analysis. Journal of Real Estate Literature, 21(1),
59–76.
Witten, I. H., Bainbridge, D., & Nichols, D. M. (2010). How to build a digital library (2nd
ed.). Burlington, MA: Morgan Kaufmann.
Xie, H. I. (2008). Users’ evaluation of digital libraries (DLs): Their uses, their criteria, and
their assessment. Information Processing & Management, 44(3), 1346–1373.
doi:10.1016/j.ipm.2007.10.003

131

Zeng, M. L., & Qin, J. (2008). Metadata. New York, NY: Neal-Schuman Publishers.

132

