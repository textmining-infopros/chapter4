NO LITERACY LEFT BEHIND:
ADDRESSING INFORMATION ILLITERACY IN THE INFORMATION AGE

A disquisition presented to the faculty of the Graduate School of
Western Carolina University in partial fulfillment of the
Requirements for the degree of Doctor of Education.

By

Kevin Martin Matney Washburn

Director: Dr. Brandi Hinnant-Crawford
Assistant Professor
Department of Human Services
Committee Members:
Dr. Ann Allen, Human Services
Dr. Beth McDonough, Hunter Library
Dr. Maureen Furr, Charlotte-Mecklenburg Schools

March 2016

© 2016 by Kevin Martin Matney Washburn

ProQuest Number: 10103074

All rights reserved
INFORMATION TO ALL USERS
The quality of this reproduction is dependent upon the quality of the copy submitted.
In the unlikely event that the author did not send a complete manuscript
and there are missing pages, these will be noted. Also, if material had to be removed,
a note will indicate the deletion.

ProQuest 10103074
Published by ProQuest LLC (2016). Copyright of the Dissertation is held by the Author.
All rights reserved.
This work is protected against unauthorized copying under Title 17, United States Code
Microform Edition © ProQuest LLC.
ProQuest LLC.
789 East Eisenhower Parkway
P.O. Box 1346
Ann Arbor, MI 48106 - 1346

ACKNOWLEDGEMENTS

The advancement of this study was made possible through the support of my
committee members and director. I would like to thank Dr. Ann Allen, Dr. Maureen Furr,
and Dr. Beth McDonough for their engagement. A special thanks and recognition to Dr.
Brandi Hinnant-Crawford for her dedication, guidance, mentorship, and encouragement
throughout this venture as my committee director.
I would like to recognize and thank the following people who have dedicated
numerous hours of thoughtful discussion, inspiration, and patient kindness that enabled
me to overcome challenges and obstacles that occurred along the way: Dr. Michael
Eisenberg, Dr. Alison Head, Dr. Robert Crow, Dr. Anna Wells Bloomer, Cathy DuPre,
Dana Harper, Rickie Welch, Christine Efird, Traci Anderson, Debbie Wooden, Chuck
Gordon, the TRAILS Project Team at Kent State University, and all the members of my
EdD Cohort group at Western Carolina University.
I especially want to recognize all the students, parents, staff, teachers, and
personnel at Charlotte-Mecklenburg Schools who worked with me for this research study.
Special thanks to Robert Folk, my principal at Alexander Graham Middle School, for
providing coaching and leadership support for this project. I also greatly appreciate the
support of my lead collaborative teachers, school library media specialists, district media
curriculum specialist and technology administrator: Laura Mathers, Rhonda Small,
Mariam Lackey, Robin Williams, Janet Jones, and Jacob Standish.

DEDICATION

I dedicate this work to my husband, Vinson W. Washburn, Jr.

TABLE OF CONTENTS

Page
List of Tables ...................................................................................................................... 6
List of Figures ..................................................................................................................... 7
List of Abbreviations .......................................................................................................... 9
Abstract ............................................................................................................................. 10
Introduction ....................................................................................................................... 12
Key Terms and Definitions ................................................................................... 15
Information Literacy as an Essential Dimension of Multiliteracies for
Secondary Education Curriculum ......................................................................... 16
Multiliteracy Framework .......................................................................... 18
Information Literacy in CMS.................................................................... 20
Literature Review Information Illiteracy in the Information Age .................................... 22
Addressing Information Illiteracy: Contributing Factors ..................................... 22
Contributing Factor: Haphazardly Applied Research Models .................. 22
Big Six (Big6™) or Super Three (Super 3) .................................. 23
FLIP-IT ......................................................................................... 23
Follett’s Information Skills Model................................................ 24
Jamie McKenzie’s Research Cycle ............................................... 24
Contributing Factor: Standards or Lack of Applied Standards ................. 24
Contributing Factor: Access to Valid Assessment Tools ......................... 26
Prevalence of Information Literacy and Illiteracy ................................................ 28
Instructional Models ............................................................................................. 29
Conclusion ............................................................................................................ 31
Methods and Procedures ................................................................................................... 33
Methods................................................................................................................. 33
Context .................................................................................................................. 33
Establishing a Baseline ......................................................................................... 35
Improvement Project Assessment Tools ................................................... 37
Improvement Project Intervention—Instructional Research Model ......... 38
Graduation Project .................................................................................... 39
Improvement Project Intervention—Creating Online Resources ............. 40
Improvement Project Timeframe .............................................................. 42
Project Challenges .................................................................................... 43
Project Calendar ........................................................................................ 44
Pre-intervention Activities ........................................................................ 45
Intervention Activities .............................................................................. 48
Post-test Assessment ................................................................................. 51
Results ............................................................................................................................... 52
Introduction ........................................................................................................... 52
Results ................................................................................................................... 53
Baseline of Students’ Ability to Seek, Use, Disseminate, and
Communicate Information ........................................................................ 53

Analysis of Instructional Strategies (Including Self-Paced
Resources, Collaborative Direct Instruction, and Assessments) .............. 58
Total gain for all strands as compared against different
intervention groups ....................................................................... 59
“Developing a Topic” strands as compared against
different intervention groups......................................................... 60
“Identifying Sources” strands as compared against
different intervention groups......................................................... 60
“Utilizing Search Strategies” strands as compared against
different intervention groups......................................................... 60
“Evaluating Resources and Information” strands as
compared against different intervention groups ........................... 60
“Use of Information Responsibly and Ethically” strands as
compared against different intervention groups ........................... 60
Impact of Interventions Compared by Domain ........................................ 61
Impact of the Use of Canvas LMS ............................................................ 62
Students who completed the Canvas course ................................. 62
Student and Stakeholder Perceptions of IL ............................................... 63
Analysis of Coded Key Stakeholder Interviews ................................................... 64
Relevant Feedback Responses from the Stakeholder Interviews ............. 66
Student Survey .......................................................................................... 67
Discussion ............................................................................................................. 68
Report and Recommendations .......................................................................................... 70
Discussion ............................................................................................................. 71
Limitations ............................................................................................................ 74
Recommendations ................................................................................................. 75
Conclusion ............................................................................................................ 77
References ......................................................................................................................... 79
Appendices ........................................................................................................................ 84
Appendix A: Project Information Literacy Infograph .......................................... 85
Appendix B: Framework for 21st Century Learning Infographic ........................ 86
Appendix C: Pre-Assessment Sample Score Report ............................................ 87
Appendix D: CMSINFOLIT Canvas LMS Modules Screenshots ........................ 88
Appendix E: CMSINFOLIT Canvas Modules Content Outline ......................... 107
Appendix F: CMSINFOLIT Course Module Reference List ............................. 111
Appendix G: CMSINFOLIT Weebly Webpage ................................................. 116
Appendix H: Student Perception Survey Results ............................................... 117

6
LIST OF TABLES

Table
Page
1. CMS Demographic Data for Participating High Schools (CMS
Accountability, 2015) ............................................................................................. 35
2. Information Literacy Standards, Strands, and Research Model Comparison ........ 38
3. Initial CMS IL Intervention Project Calendar ........................................................ 44
4. Overview of Modules ............................................................................................. 47
5. Comparison of Mean Students Who Completed Canvas LMS and NonCompleters by IL Strand and Total Gain ............................................................... 63
6. Stakeholder Interview Question Coding and Subcoding ....................................... 65
7. CMSINFOLIT Canvas Module Course Content .................................................. 107
8. Student Perception Survey Questions and Results ............................................... 117

7
LIST OF FIGURES

Figure
Page
1. Conceptual framework for K-12 information literacy development ...................... 14
2. Comic strip created using MakeBeliefsComix.com ............................................... 48
3. CMS Information Literacy Study participant information ..................................... 53
4. TRAILS baseline data comparison for strand 1 by schools ................................... 54
5. TRAILS baseline data comparison for strand 2 by schools ................................... 54
6. TRAILS baseline data comparison for strand 3 by schools ................................... 55
7. TRAILS baseline data comparison for strand 4 by schools ................................... 56
8. TRAILS baseline data comparison for strand 5 by schools ................................... 56
9. TRAILS baseline data comparison total average by schools ................................. 57
10. Results comparison by instruction level for honors and standards students .......... 59
11. Intervention type comparison by domain ............................................................... 61
12. Student Perception Survey overview ..................................................................... 67
13. Project Information Literacy infographic (2016) ................................................... 85
14. P21 Framework for 21st Century Learning ............................................................ 86
15. TRAILS pre-assessment score report sample ........................................................ 87
16. CMSINFOLIT Canvas course screenshot main webpage ...................................... 88
17. CMSINFOLIT Canvas course screenshot overview .............................................. 89
18. CMSINFOLIT Canvas course screenshot #1 ......................................................... 89
19. CMSINFOLIT Canvas course screenshot #2 ......................................................... 90
20. CMSINFOLIT Canvas course screenshot #3 ......................................................... 90
21. CMSINFOLIT Canvas course screenshot #4 ......................................................... 91
22. CMSINFOLIT Canvas course screenshot #5 ......................................................... 91
23. CMSINFOLIT Canvas course screenshot #6 ......................................................... 92
24. CMSINFOLIT Canvas course screenshot #7 ......................................................... 92
25. CMSINFOLIT Canvas course screenshot #8 ......................................................... 93
26. CMSINFOLIT Canvas course screenshot #9 ......................................................... 93
27. CMSINFOLIT Canvas course screenshot #10 ....................................................... 94
28. CMSINFOLIT Canvas course screenshot #11 ....................................................... 94
29. CMSINFOLIT Canvas course screenshot #12 ....................................................... 95
30. CMSINFOLIT Canvas course screenshot #13 ....................................................... 95
31. CMSINFOLIT Canvas course screenshot #14 ....................................................... 96
32. CMSINFOLIT Canvas course screenshot #15 ....................................................... 96
33. CMSINFOLIT Canvas course screenshot #16 ....................................................... 97
34. CMSINFOLIT Canvas course screenshot #17 ....................................................... 97
35. CMSINFOLIT Canvas course screenshot #18 ....................................................... 98
36. CMSINFOLIT Canvas course screenshot #19 ....................................................... 98
37. CMSINFOLIT Canvas course screenshot #20 ....................................................... 99
38. CMSINFOLIT Canvas course screenshot #21 ....................................................... 99
39. CMSINFOLIT Canvas course screenshot #22 ..................................................... 100
40. CMSINFOLIT Canvas course screenshot #23 ..................................................... 100
41. CMSINFOLIT Canvas course screenshot #24 ..................................................... 101
42. CMSINFOLIT Canvas course screenshot #25 ..................................................... 101

8
43.
44.
45.
46.
47.
48.
49.
50.
51.
52.
53.

CMSINFOLIT Canvas course screenshot #26 ..................................................... 102
CMSINFOLIT Canvas course screenshot #27 ..................................................... 102
CMSINFOLIT Canvas course screenshot #28 ..................................................... 103
CMSINFOLIT Canvas course screenshot #29 ..................................................... 103
CMSINFOLIT Canvas course screenshot #30 ..................................................... 104
CMSINFOLIT Canvas course screenshot #31 ..................................................... 104
CMSINFOLIT Canvas course screenshot #32 ..................................................... 105
CMSINFOLIT Canvas course screenshot #33 ..................................................... 105
CMSINFOLIT Canvas course screenshot #34 ..................................................... 106
CMSINFOLIT Canvas course screenshot #35 ..................................................... 106
CMSINFOLIT Home Weebly Webpage .............................................................. 116

9
LIST OF ABBREVIATIONS

AASL
ALA
AP
Big6™
CMS
DPI
HS
IL
ITES
LMS
MPHS
NC
NCLB
NWSOA
PDSA
PIL
POB
SMHS
SLMS

American Association of School Librarians
American Library Association
Advanced Placement
Big Six Research Process Model
Charlotte-Mecklenburg Schools
Department of Public Instruction
High School
Information Literacy
Information and Technology Essential Standards
Learning Management System
Myers Park High School
North Carolina
No Child Left Behind
Northwest School of the Arts High School
Plan, Do, Study, and Act cycle for improvement activities
Project Information Literacy
Phillip O. Berry Academy of Technology High School
South Mecklenburg High School
School Library Media Specialist

10
ABSTRACT

NO LITERACY LEFT BEHIND: ADDRESSING INFORMATION ILLITERACY IN
THE INFORMATION AGE

Kevin M. Washburn, Ed.D.
Western Carolina University (March 2016)
Director: Dr. Brandi Hinnant-Crawford

There is a growing concern in scholarly literature indicating that college students
struggle with conducting research and using information effectively (Head, 2013;
Lawrence, 2013; Head & Eisenberg, 2011). This research study examines the elements
and causes of information illiteracy from a secondary education perspective. The methods
used assess the information literacy skill levels of high school juniors, intervene with
information literacy instruction, and evaluate the merit of the intervention. The project
purpose is threefold: (1) establish a baseline of students’ ability to seek, use, disseminate,
and communicate information; (2) determine what instructional strategies (including selfpaced resources, collaborative instruction, and assessments) will increase student
capacity to locate and integrate information; and (3) measure student and teacher
perceptions of the importance of information literacy.
Currently, the Common Core State Standards Initiative (2012) treats information
literacy as a skill component to be integrated into the curriculum. Since there is not a
state-sanctioned assessment examining information literacy within North Carolina, this
study will serve as a springboard for capturing data related to information literacy. When

11
implemented at the district level, this intervention method will use existing standards
(specifically curriculum) to increase student readiness for career and college-level course
work.

12
CHAPTER ONE: INTRODUCTION

Far too many of America’s high school graduates are information illiterate. This
statement is substantiated by the work of Project Information Literacy (PIL), which has
presented studies that college students struggle to conduct research as well as transfer
these skills to employment and careers. Considering the multiple literacies that exist,
information literacy (IL) is not treated with equal importance as other components
considered critical for students’ overall development. In North Carolina, information
standards exist but are not directly embedded or assessed within the required content
areas. With these perplexing factors in place, how can educators determine if students
have mastered IL skills? As a school library media specialist, I have spent the last 14
years working with secondary students. In this role, I have seen firsthand the challenges
that students face in completing research projects or using information effectively to
investigate issues, think critically, and solve problems. These observations have led me in
the past to seek answers specific to students’ performances in the IL area and how
students successfully apply these skills.
The American Library Association (ALA) defines IL as the capacity “to
recognize when information is needed and have the ability to locate, evaluate, and use [it]
effectively” (ALA, 1989). Baseline assessment data is necessary in the area of IL to
evaluate North Carolina’s progress toward its “guiding mission” for 21st century learning
that prepares students for college and career readiness (North Carolina Department of
Public Instruction [NCDPI], 2007). IL is essential for college and career success in the
21st century. While North Carolina has acknowledged the importance of IL by strongly
recommending a high school graduation project, there is not a clear mandate for teaching

13
these skills. Some local school districts have eliminated the graduation project as a
requirement, thus reducing the opportunity for students to develop critical thinking and
problem-solving skills that are rooted in IL. NCDPI uses a variety of assessments to
determine student achievement, none of which include IL. These assessment tools are
described in the North Carolina Testing Program Overview (NCDPI, 2014a), which
focuses on End-of-Grade or End-of-Course Assessment. No strategy exists to collects IL
data as part of an overall paradigm.
Research conducted by Project Information Literacy (2016) from Washington
State University validated academic librarian and instructor concerns that college
students are ill-prepared to successfully integrate information, conduct research, and use
IL strategies to solve problems (see Figure 13: PIL infographic in Appendix A). Head
(2013) reported results from a large-scale national study highlighting students’ struggles
initiating research. While IL has a well-defined research base to draw from within the
post-secondary arena, less research is being conducted on IL development in the K-12
educational setting (Head, 2013; Head & Eisenberg, 2011; Lawrence, 2013). Pinto,
Cordon, and Diaz (2010) analyzed professional research and literature produced from
1977 to 2007 related to library science and found 2,580 relevant articles. From this index
and database review, 0.01% referred to the combination terms “library skills” as
compared with 64% for “information literacy.” These findings support the premise that
research focusing on the acquisition of basic library skills, which scaffolds IL, has merit
(see Figure 1). Carey (1998) notes that library skills are the “knowledge and tool building
blocks of problem-solving,” whereas information literacy is the “cognitive strategies
component of problem-solving” (p. 10), therefore establishing that basic library skills

14
comprise the foundation for the initial instruction students receive while developing IL
skills.

Figure 1. Conceptual framework for K-12 information literacy development.

This disquisition examines the growth of IL in secondary students, particularly
high school juniors preparing to complete their graduation research project, as
recommended by the state of NC and as required by the Charlotte-Mecklenburg School
(CMS) District (NCDPI, 2014). With this focus in mind, the purpose of this project is
threefold: (1) to establish a baseline of students’ ability to seek, use, disseminate, and
communicate information; (2) to determine what instructional strategies (including selfpaced resources, collaborative instruction, and assessments) will increase student
capacity to locate and integrate information; and (3) to measure student and teacher
perceptions of the importance of IL.

15
Using an Improvement Science approach, I developed an online instructional
resource as part of an intervention to support students attempting to improve their IL
knowledge (Carnegie Foundation for the Advancement of Teaching, 2016). Improvement
Science is a pragmatic methodology that aims to “support improvement efforts by
contributing to the re-conceptualization of problems and the conditions that create them;
prototyping possible processes, tools, or specific practices to address these problems; and
testing them to gauge their potential efficacy” (Park & Takashi, 2013, p. 6). This
approach is integral in addressing this problem in a short time frame (90-day project
cycle), developing a design and implementation team, as well as using interventions,
evaluating, and reporting results. Now, at the project’s culmination, I can make specific
recommendations for secondary schools’ IL curricular strategies, ensuring that students
are information literate and ready to contribute to the journey of lifelong learning.
Key Terms and Definitions
Improvement Science: seeks to solve problems that directly impact institutions,
stakeholders, and direct users. The improvement process focuses on quickly accessing
and addressing issues that can be replicated, improved upon, and expanded to the next
potential level of a larger problem. Improvement Science is “explicitly designed to
accelerate learning-by-doing. It is a more user-centered and problem-centered approach
to improving teaching and learning” (Carnegie Foundation for the Advancement of
Teaching, 2016, para. 2).
Information Literacy: The American Library Association (ALA, 1989) states “to
be information literate, a person must be able to recognize when information is needed
and have the ability to locate, evaluate” (p. 1) and effectively use said information. The

16
instructional method used to develop this skill includes teaching based on a research
model, integration of standards, and practice based on information-seeking and problemsolving activities.
Literacy: is the process used to acquire and apply “knowledge and skills to deal
successfully with novel information and situations” (Farmer & Henri, 2008, p. 2).
Literacy occurs when reading, writing, and problem solving are developed as a skill by a
learner.
Research Model: is a component of the information literacy skill set. A strategy,
guide, or resource which, when used by a researcher, serves as a method to conduct
research that addresses questions and curiosities or sparks knowledge in the area of
inquiry (Loertscher & Woolls, 1997).
School Library Media Specialist (SLMS): is the education professional who
provides instruction and program management for the school library media center. This
individual may also be referred to as a school librarian, media coordinator, or teacher
librarian (Information Power, 1998).
Information Literacy as an Essential Dimension of Multiliteracies for Secondary
Education Curriculum
Both information literacy and illiteracy can be understood within the larger frame
of multiliteracies. Conscious of the changing demands in the professional, public, and
personal lives of humans, multiliteracy theory contends that schools must evolve to
instruct students in literacies falling outside the traditional canon. This instructional shift
will prepare students for the “post-Fordism” era, in which employees are required to be
“‘multiskilled,’ well rounded workers . . . flexible enough to be able to do complex and

17
integrated work,” all of which comprise the essential traits of modern workers (Cazden et
al., 1996, p. 66). Creating a workforce equipped with these skills requires a keen
examination of how we teach these skills and associated literacies, and is critical to
creating an equitable society. The New London Group contends that “some have argued
that educational research should become as design science, studying how different
curricular, pedagogical, and classroom designs motivate and achieve different sorts of
learning” (Cazden et al., 1996, p. 73), which is precisely what my improvement study
was designed to do. Critically examining the “pedagogical tension between immersion
and explicit models of teaching” (p. 62) provides a framework for understanding the need
to determine whether students learn best by having IL skills embedded in the curriculum,
or if librarian collaborative instruction provides a more effective means for students to
acquire these skills (Cazden et al., 1996). My improvement study findings suggest the
best process to develop IL among high school juniors is direct instruction.
Students are assumed to acquire information-seeking skills and strategies as part
of the general knowledge and literacies currently available through the curriculum. In a
recent study by Kovalik, Yutzey, and Piazza (2013), a team of educators explored the
“Information and Literacy Skills of High School Seniors” and found more than 50% of
the participants had challenges finding and selecting research materials, and more than
86% stated that the research process required more time than they anticipated. It is clear
students are not acquiring IL skills in secondary curricula because college students face
challenges conducting research beyond basic textual analysis. Head (2013) reported “a
majority of first-term college freshmen faced challenges in both locating and then
searching through research information systems” (p. 3). Factors revealed by Head’s

18
(2013) research included students using a limited range of skills and resources for
addressing problem-solving activities (p. 4), as well as rarely “used the full range of
library resources and/or services” (p. 4) while struggling to complete assignments.
Head’s work represents a recent collaborative project spanning 30 high schools and six
college/university libraries. Head (2012) states, through a series of focus groups with 33
recent college graduates, that many "struggled to make their transition to a workplace
where their information-seeking was driven by an urgent pace” (p. 24). Minimal
expectations for jobs and career paths include students with skills enabling them to easily
adapt to different technological situations and to adopt IL strategies as “competent
researchers” (Head, 2012, p. 25).
Multiliteracy Framework
When considering the multiple literacies that exist, educators must question
whether or not IL is privileged within secondary curricula because of instructional
difference relative to course level (Standard, Honors, or Advanced Placement, for
example); I argue that it is not. Within the Common Core State Standards Initiative
(2012) and the North Carolina Information and Technology Essential Standards (NCDPI,
2011), there is an expectation that IL skills will be integrated into instruction as part of
the general English and Language Arts discourse. The curriculum policy as presented by
the NCDPI in 2009 states that the ITES standards are designed for use in all curriculum
areas with the specific expectation that teachers would collaborate with school library
professionals in the delivery, integration, and assessment of the instructional context of
these skills. The issue implicit in this approach is that no data exists from state-required
testing to support that students have acquired or mastered IL skills within any established

19
curricular area. This issue was confirmed by Dr. Tammy Howard, director of
Accountability Services for NCDPI (personal communication, December 11, 2015).
Other literacies are treated as integral to the associated curriculum in NC and validated
through assessment: Computer, Financial Literacy through Career and Technical
Education, Cultural and Global Literacy via Social Studies curriculum, and Reading
Literacy through the English Language Arts curriculum, which are all relevant
assessments (NCDPI, 2015d).
Applying the multiliteracy framework is appropriate since information standards
exist in NC but are not specifically embedded within required subject content areas.
Establishing a baseline for student performance in this discipline is relevant because
students within my school district (and others across the state) are expected to complete a
research project as part of their overall graduation requirement. The most significant
outcome for IL instruction is that failing to teach IL strategies has real consequences for
the students who acquire the skills and for those left illiterate—consequences for college
and careers. With increased demands for critical-thinking and problem-solving skills in
the workplace, we must teach students these skills as a monitored objective with key
concepts systematically constructed and assessed. Ideally, the curriculum for English at
the junior and senior level of high school will require an instructional unit on IL, as well
as a benchmark evaluation or test before students begin mapping steps for completing
their HS graduation project.
In kindergarten through 12th grade, high-stakes standardized testing has created a
focus on teaching core subjects almost exclusively, instead of promoting a broad
spectrum of knowledge; this narrowed curriculum has led to instruction privileging

20
objectives that are tested, but unfortunately, IL is not assessed. Lawrence (2013) argues,
“High school curriculums remain out of step” (p. 2) with abilities students require for the
future. Instead of discerning the best solution, students often accept the quickest answer,
blocking learning opportunities (Head, 2013). Based upon my observations and
experience as a teacher and information specialist, secondary students struggle to use
critical-thinking strategies to locate valid, reliable reference sources. Students unable to
conduct research, identify valid, reliable information resources, and evaluate data and
draw conclusions will be inherently challenged in their quest to thrive in the informationage workforce. A strong learning culture with collaboratively-developed strategies will
align instructional goals, link standards, and provide opportunities for students to learn
and practice inquiry as part of IL development.
Information Literacy in CMS
In a focus group investigating the merits of this work, eight high school library
media professionals who serve CMS expressed their primary obstacle to facilitate student
IL dexterity is the lack of instructional resources and tools. A fundamental guide
outlining the minimal knowledge students should acquire before HS graduation should be
readily accessible to all students. The discussions this focus group generated also
revealed that each school develops its own method of instruction, leading to a lack of
district continuity. Lesson plans and content which school library media professionals
and English teachers could use to provide instruction, support, and training would greatly
benefit students’ acquisition of IL aptitude.
I collected interviews and other preliminary data, revealing one overarching
theme consistently noted by English teachers, librarians (SLMS), and administrators: No

21
assessment tool existed to capture information related to students’ specific competencies
with IL, which limits the ability to measure the impact of current practices, or the
likelihood of creating strategies for future improvements. A review of the literature
provides additional insight into recent work developed in the area of IL research, the
associated solutions that have been applied, and the impact of instructional standards on
addressing these literacy deficiencies.

22
CHAPTER TWO: LITERATURE REVIEW
INFORMATION ILLITERACY IN THE INFORMATION AGE

Information exists in almost every dimension of society as a driver for student
learning. The National Forum on Information Literacy expands on this definition by
establishing a vision for IL skills that states the purpose of IL education is “to produce
independent, self-sufficient lifelong learners who can successfully navigate the
competitive challenges of post-secondary educational and/or workplace opportunities”
(NFIL, 1989, para. 1). Stemming from the purpose of this study, this literature review
details factors that contribute to information illiteracy, scholarly literature on the
prevalence of IL and illiteracy, and current instructional models for teaching IL to US
and international students.
Addressing Information Illiteracy: Contributing Factors
A variety of factors that contribute to information illiteracy appear in the
literature; three of the most prevalent factors are various, haphazardly applied research
models; standards or lack of applied standards for IL; and access to valid assessment
tools.
Contributing Factor: Haphazardly Applied Research Models
There are many research models that have been developed by educational
practitioners in the school library media profession to aid in providing instruction to
students for developing IL skills. Using a model as part of the instructional component
for developing IL skills provides a pathway to align with standards and knowledge
relevant for today’s students. IMPACT: Guidelines for North Carolina Media and
Technology Programs (NCDPI, 2006), the official state guidebook for school library

23
media programs, provides recommendations for various types of research models to be
included in the curriculum (NCDPI, 2006, p. 21). These models include the Big Six
(Big6™) or Super Three (Super 3), Flip-IT, Follett’s Information Skills Model, and
Jamie McKenzie’s Research Cycle. Each of these models has merit but also contributes
confusion and weak results by students attempting to follow or adopt the model.
Big Six (Big6™) or Super Three (Super 3). Created by Eisenberg and
Berkowitz in 1990, Big Six (Big6™) is a popular and widely used research model. Core
principles of this model enable the researcher to follow six steps to successfully solve
information problems. Super Three (Super 3) is a version of this model that can be used
as a learning resource for younger students in grades K-3. Wolf, Brush, and Saye (2003)
advocated for use of the Big6™ model because of the “complex nature of the information
search process coupled with the influence of metacognitive skills” (p. 6). James-Maxie
(2007) argues that because the Big6™ works to introduce all the IL “skills at once . . .
(SLMS) should teach and reinforce the skills in stages” (p. 25).
FLIP-IT. Developed by Alice Yucht in 1988, FLIP-IT works as a four-step
research approach to problem-solving and IL. McCarthy (2003) states that FLIP-IT is a
“nonlinear information literacy research process that, rather than insisting on a lock-step
approach to research, allows flexibility at each stage” (p. 22). FLIP-IT leverages prior
knowledge as a means to build toward “the IT of Intelligent Thinking” (p. 22). Access to
resources and information related to this model and the developer is limited to the
original print manuscript. After Yucht’s retirement, the website promoting the model was
discontinued.

24
Follett’s Information Skills Model. Also referred to as Pathways to Knowledge
Information Skills Model, Follett’s Information Skills Model was developed by Marjorie
Pappas and Ann Tepe in 1997 with support from the Follett Software Company
(Loertscher & Woolls, 1997, p. 5). Originally presented as a free online learning
resource, this website tool is no longer available.
Jamie McKenzie’s Research Cycle. Introduced in 2000 as a whole school
approach to address gaps in IL instruction, the McKenzie Research Cycle provides a
practical approach for teaching the research process (Milam, 2002, p. 2). Created as a
seven-step process that builds on students developing questions related to their topic, the
Research Cycle guides students to refine and clarify the information needed as part of a
decision making and problem solving process
Of research models reviewed, only the Big Six (Big6™) and McKenzie’s
Research Cycle provide the structure needed to address components of IL instruction for
today’s digital learners. These two models engage students in the research process,
enabling them to identify what information is needed based upon a clear understanding of
the initial research questions to be answered.
Contributing Factor: Standards or Lack of Applied Standards
There are many examples of educational standards that were developed by
educators and librarians that form a promising foundation for student learning.
Unfortunately, these standards have not bridged the connection between theories to direct
application for instruction. Many of these standards form the framework for NC
Information and Technology Essential Standards, which are not assessed as part of the
state accountability (NCDPI, 2011). A major factor for this disconnect is the lack of

25
specifically embedded curriculum strategies drawn across all curriculum assessment
areas.
The history of information literacy standards relates basic principles and
expectations for student learning. Standards for Student Learning is an essential
document developed as part of a national focus on IL (ALA, 1998). These standards
appeared in the groundbreaking publication Information Power: Building Partnerships
for Learning in 1998. The American Association of School Librarians updated these
standards with the release of the Standards for the 21st-Century Learner in 2007 with an
associated application guidebook, Standards for the 21st-Century Learner in Action
(2009). Supporting IL at the college and university level, the Association of College and
Research Libraries (ACRL) released Information Literacy Competency Standards for
Higher Education in 2000. Also of merit is the School Library Guidelines from
IFLA/UNESCO (2002), which provides a model for IL programs.
In 2007, North Carolina directly aligned the state’s vision for literacy to the
Partnership for 21st Century Learning Skills (see Appendix B, Figure 14). This
framework specifically identified IL skills within the “Thinking and Learning Skills”
umbrella. In the document An Overview of 21st Century Skills in North Carolina, the
high school graduation project is seen as a defining element for IL assessment:
The North Carolina Graduation Project provides students the opportunity to
demonstrate their ability to apply what they learn in a 21st century context. All
North Carolina public school students currently in ninth grade will be required to
produce a four-part assessment that showcases their 21st century content
knowledge and skills. The project, to be completed in the final year of high

26
school, will include a paper, a reflective portfolio, a product, and a presentation.
(NCDPI, 2007, p. 2)
In 2010, North Carolina adopted the Common Core State Standards Initiative
(2012). This shift in educational standards also heralds a renewed focus for graduating
students to demonstrate college and career readiness. The Common Core State Standards
Initiative (2012) incorporates IL skills within the English Language Arts (ELA) and
Writing standards. When Common Core standards are reviewed with an eye toward IL, it
is difficult to find a direct reference, but several components are embedded. Within the
Common Core State Standards Initiative (2012), three College and Career Readiness
Anchor Standards (CCRA) speak directly to IL and the component of research. These
standards include CCRA-W7, which recognizes the value of short or sustained research
projects; CCRA-W8, which emphasizes the importance of acquiring and using
information ethically; and CCRA-W9, which supports the process of evaluating evidence,
drawing conclusions, and reflection (Eubanks, 2014, p. 27).
Current North Carolina standards lack specific guidance on the process for
embedding IL standards across all curricular areas. With a limited focus on the English
Language Arts curriculum, this limited focus also remains a weak alignment to specific
IL strands. With a lack of opportunities to assess student skill in this area, recommending
standards for IL serve a valid purpose but provide no avenue for policy and
administrative consideration.
Contributing Factor: Access to Valid Assessment Tools
A lack of accepted standards around IL coupled with a lack of assessment in this
area leads to instruction of IL as a low-stakes endeavor. Though not assessed in NC,

27
assessments for IL are available. Other states use the technology proficiency requirement
of No Child Left Behind (NCLB) to include information literacy as a criterion for
evaluation. New Jersey offered recommendations starting in 2006 that required all school
districts to determine the level of computer and information literacy for all students (State
of New Jersey Department of Education, 2014). This assessment is based upon a group of
recommendations that include crosswalks, checklists, and rubrics. This research study
used the Tool for Real-time Assessment of Information Literacy Skills (TRAILS)
instrument as a method to assess student knowledge in this area. This assessment
instrument has been aligned to the American Association of School Librarians (AASL)
and Common Core State Standards. The instrument is available for educational use at no
cost, is available online, is easy to administer, provides quick access to results, and has
the potential for collaboration with the leadership for TRAILS at Kent State University to
address future needs that may arise during the design stage of the implementation phase
of the research project. This instrument was evaluated by Salem (2014) in a study that
determined that questions included in the resource were valid and reliable based upon
comparison to other reliable models and that other conditional variables (e.g., reading
ability) had no impact on the reliability of the assessment instrument. Other IL
assessment tools considered include iSkills Assessment from Educational Testing Service
and NoodleTools, as well as creating a rubric based on product demonstration. Several
instruments also exist to assess IL at the college entrance level. The most common of
these tools include the Standardized Assessment of Information Literacy Skills (SAILS,
also from Kent State University), the Information Literacy Test from James Madison

28
University, and the Research Readiness Self-Assessment from Central Michigan
University (2016).
Prevalence of Information Literacy and Illiteracy
Scholarly literature specific to IL and illiteracy establishes that the challenges that
college-bound students have conducting research and applying the information-seeking
and problem-solving strategies are rooted in primary and secondary education. Evidence
in the literature shows IL skills are lacking among high school students. Kovalik et al.
(2013) found that among 289 high school seniors, 44.3% indicated that information
located from an initial search was confusing, 63.2% indicated that they were seldom able
to find sources of information needed in the library catalog, 56.3% indicated information
needed was in an unexpected place, and 50.9% stated they found it difficult to find
specific information on a research topic. Students (78%) also reported they rarely asked
for assistance from the library professional in the school.
Similarly, Gross and Latham (2007) examined the IL skills of college freshmen.
Researchers examined experiences and habits of college students and the challenge of
conducting research. Utilizing the Information Literacy Test from James Madison
University, the researchers sought to determine if secondary education preparation and
academic success served as an indicator for IL. The results showed that 45.1% of
participating students, nearly half, scored at a level identified as information illiterate
(p. 343). Fifty-three percent were ranked as proficient and only 2% were ranked as
advanced (p. 343). Researchers also found that a student’s prior academic success did not
have a significant impact toward competency of IL. This study illustrates that even star
students in high school may be far below proficient when it comes to IL.

29
In a case study presented by Chu, Yeung, and Chu (2012), research was
conducted at a school in Hong Kong to examine IL skills for 176 students aged 12. The
study used the Hong Kong Information Literacy Framework, an identified information
search process, and access to the TRAILS online assessment tools. Results indicated that
students’ skill level for IL was measured on average to be one to two grade levels below
expectation.
Though research studies exist specific to IL skills as directly applied to secondary
education and the experiences of first-year college students, additional insight and
knowledge can be gained in this area with further examination. The purpose of this
improvement project is to provide new and relevant knowledge toward this research area.
Instructional Models
In order for IL to be developed as a skill by students, educators must define the
curriculum method used to teach these skills. Carey (1998) provides a clear approach to
developing instruction for IL as related to solving problems, identifying strategies,
providing instruction, and incorporating cognitive learning behaviors of students.
Specifically, the author advocates for a constructivist approach to teaching IL which
moves beyond basic library skills for finding information and toward information
problem-solving. Loertscher and Woolls (1997) add to Carey’s approach by advocating
for the inclusion of IL as a co-curricular instructional process across all content areas.
Their report examined popular instructional models and outlined the field of research and
study that existed from the late 1980s to 1997 within and related to IL. Loertscher and
Woolls (1997) contributed to the potential for this research study by providing strategies

30
for teaching problem-solving and critical thinking skills as a function of IL with
curriculum content integration.
Addison and Meyers (2013) expand on potential best practices for IL instruction
by proposing three perspectives: “Acquisition of information age skills, habits of mind,
and engagement as a form of Social Practice” (p. 4). “Acquisition of information age
skills” speaks directly to students’ ability to access knowledge “as a behavior in
information environment, such as libraries, and the emphasis is on how users gain and
employ such skills, as measured by assessment” (p. 5). This attribute is relevant when
students need to demonstrate information-seeking skills to find informational text and
resources. The “habits of mind” perspective addresses the realm of problem-solving for
IL (p. 8). The authors cite the Big6™ Model from Eisenberg and Berkowitz (1990) as a
model for information problem-solving. “Social practice” stems from a “set of practices
involving tools and media that are deeply embedded in a particular context or activity”
(p. 11). The authors state that the social practice of IL is connected to multiliteracies as
defined by the New London Group (Cazden et al., 1996). All of these factors merit
consideration during the design stage for this project, which is essential for creating an
effective learning system with the goal of increasing students’ IL skills.
Educators and educational leaders must be willing to make IL a priority for
student outcomes and seek instructional and curricular solutions to aid in the
development of IL in secondary students. With standards in place, national resources
available through the Common Core State Standards Initiative (2012), and the
Partnership for 21st Century Skills, now is the time to implement tools to identify, track,
and evaluate what instructional practices are enabling students to be college and career

31
ready. As with all content, a one-size-fits-all approach to IL will be insufficient; teachers
must be given opportunities to differentiate instruction when working with IL skill
development. Professional development may address this need, but access to mentors,
team leaders, and online instructional experts will also support this process. Leadership
development in the area of creating and developing best practices for today’s learning
environment specific to IL curriculum integration should also be encouraged.
Instructional resources identified to be included as part of the online intervention
tools include S.O.S for Information Literacy and the online “EMPOWER: Information
Literacy” practice activities from Wichita State University Libraries (2014). S.O.S. for
Information Literacy is a web-based resource hosted by Syracuse University. This
website provides access to teaching ideas, lesson plans, and instructional units in the area
of IL. Content collected within these resources aligns with IL standards from AASL and
is screened through a review process before becoming available for public use. The
EMPOWER: Information Literacy website from Wichita State University Libraries is an
online tool used during the design stage of the project for including resources for
instructional content and practice examples. With the ability to customize units, tools,
and content, application of this online tool has potential to increase relevant student
practice and learning that will occur as part of the intervention for this project.
Conclusion
The primary function of this literature review was to provide insight into factors
that contribute to inhibiting students’ development of skills for information seeking,
utilization, dissemination, and communication; to review the scholarly literature
examining IL and illiteracy; and to explore instructional strategies that can aid in

32
increasing students’ information skill level. The literature provides a strong background
for the value of information literacy related to standards, but little is known about its
impact on secondary students in the area of assessment. There is also little evidence
showing the benefit of instruction toward the development of IL skills. The literature
supports the use of a research model for developing a curricular approach to IL.
If students can master skills needed to apply information learned and produce
evidence of this learning, student achievement will rise to a new level. Ideally, students
will demonstrate their knowledge through investigation, collaboration, production, and
ultimately engage with a teacher or an online coach in a process of communication
extending beyond a linear pathway of education. The challenge to this development will
be if students can leverage their skills toward IL and the research task as applied to the
production of research.

33
CHAPTER THREE: METHODS AND PROCEDURES

Methods
This study evaluated three approaches to aid in the development of IL among high
school students and investigated perceptions of IL among stakeholders. Students were
assigned to three course levels based on prior academic achievement before the study
commenced. These course level designations were Standard, Honors, and Advanced
Placement. All three course designations were used in establishing baseline data; only
Honors and Standard courses received instructional interventions. The assessment tool
used for this investigation was the TRAILS online resource. After assessing baseline
levels of IL, three interventions were used to improve the content knowledge of
participants. These interventions included appraisal of information literacy skills
(baseline assessment data), a self-guided, web-based IL course, and direct instruction.
These activities and tools provided instruction and guidance for the development of
information literacy skills among 11th graders.
Context
The development of IL was investigated at four Charlotte-Mecklenburg School
District locations: Myers Park HS, Northwest School of the Arts HS, Phillip O. Berry
Academy HS, and South Mecklenburg HS. These schools represent diverse student
subgroups as designated by NCDPI as part of the state’s Annual Measurable Objectives.
Student subgroups are reported by “gender, ethnicity, language proficiency, disability,
and economic conditions” (NCDPI, 2015d). For this study, the primary site was South
Mecklenburg High School (SMHS), which included four of the ten 11th-grade English
classes involved in the study. There were two Standard and two Honors classes involved

34
from SMHS. Named as one of the best high schools in North Carolina, SMHS has a
diverse overall student population, with more than 657 11th-grade students.
Three additional high schools were added to the study by request of the CMS
Office of Accountability. Myers Park High School (MPHS) had three English classes
participate, two Honors and one Advanced Placement. MPHS also has a diverse student
population with more than 666 11th-grade students. MPHS offers students the option to
participate in the International Baccalaureate program, which is seen as an asset for
college admission. Northwest School of the Arts High School (NWSOA) had one
Standard English class participate. NWSOA is the district’s only dedicated fine arts
school accepting students through audition and a recommendation process. The smallest
school participating in the study, NWSOA has a balance of students represented among
the population, with 148 11th-grade students. As the last school participating in the
study, Phillip O. Berry Academy of Technology High School (POB) represents the
district’s only lottery-based high school magnet program dedicated to technology. One
AP English class from POB was involved in the study. POB has a balanced
representation of students in most categorical areas as compared to other school
populations, with 361 11th-grade students represented. Table 1 presents specific
demographic data for 11th-graders at all four participating CMS high schools grouped by
gender, ethnicity, and academic status.

35
Table 1
CMS Demographic Data for Participating High Schools (CMS Accountability, 2015)
High School 11th-grade Enrollment
Demographic
Characteristic
Gender
Female
Male
Ethnicity
Asian
African-American
Hispanic
Multi-Racial
Native American
White
Academic Status
Academically Gifted
Exceptional Children
Limited English Proficiency

MPHS
(N = 666)

NWSOA
(N = 148)

POBHS
(N = 361)

SMHS
(N = 657)

50.9%
49.1%

73.6%
26.4%

50.1%
49.9%

53%
47%

4.4%
23.3%
8.6%
2.1%
0.3%
61.4%

2.7%
39.2%
4.7%
4.1%
0.7%
48.6%

4.7%
75.6%
15.8%
1.4%
0.6%
1.9%

3.8%
30.4%
23.4%
4.0%
0.5%
37.9%

26.1%
6.9%
3.2%

20.3%
3.4%
0.0%

11.1%
3.6%
1.1%

15.2%
6.7%
3.0%

Establishing a Baseline
While most Improvement Science projects have a clear baseline, part of this
project is establishing the baseline of IL knowledge across participating schools in the
district and across different but similar high schools. Improvement Science seeks not only
to understand what works, how it works, and in what context it works, but also to
implement sustainable improvements. Because of this, stakeholder engagement for this
project is different from the approach that would typically be used for a traditional
research study. The procedure for this improvement initiative focused on engaging
multiple stakeholders, illustrated by creation of a design and implementation team. The
design team served as a knowledge base for collecting ideas, content, and strategies
incorporated into intervention resources and assessment tools. The implementation team

36
ensured successful navigation of the project at the local site, which included access to
materials, facilities, equipment, teachers, students, parents, and any other related
resource.
One of the primary stakeholder constituencies engaged in this improvement
project was the design team, which reviewed the improvement project plan, made
recommendations for identifying sample student populations in which to conduct
assessment and intervention activities, and provided access to internal procedures and
practices relevant to the school. The design team also served as experts for identifying
content and activities included in the online learning activities delivered during the
implementation stage. Two 11th-grade teachers (Mr. Chris Folk, AP English Teacher,
and Ms. Rhonda Small, Standard and Honor English Teacher from SMHS) and the
SLMS (Mariam Lackey) served in the initial collaborative role for both design and
implementation. Scott Smith later joined the SMHS Implementation Team as a
collaborative teacher with a Standard English class.
At each additional high school location, one 11th-grade English teacher and a
school contact were identified to support the project (MPHS with lead English Teacher,
Laura Mathers, and Media Specialist, Robin Williams; NWSOA with lead English
Teacher, Sarah Strahan, and Media Specialist, Elizabeth Slater; and POB with Lead
English Teacher, Tiemi Halverson, and Media Specialist, Rosalind Moore). Members of
the CMS Technology Department also supported the project in a consultant capacity as
well as the district curriculum specialist for media. CMS Technology Department liaison,
Jake Standish (Technology Project Manager), provided recommendations and training for
online intervention tools. The district media specialist, Janet Jones, reviewed the

37
curriculum plan and provided recommendations for scope and clarity. The district media
and technology specialists supported the process as team members by serving in an
advisory role at a higher level, reflecting on the impact of IL across the curriculum and
the district.
Similar to the design team, the improvement project also employed an
implementation team, which served as key partners in the research study process. In this
case, the identified English teachers provided class time for collaborative instruction as
well as for pre- and post-assessments. The school library media specialist assisted as a
partner to make recommendations for conducting assessments and locating additional
resources available within the school. The implementation team focused on resources at
the school site, whereas the design team included members of the larger school district
community. The implementation team also included school leadership (principal or
designee) who supported the overall project.
Improvement Project Assessment Tools
The Tool for Real-time Assessment of Information Literacy Skills, commonly
referred to as TRAILS, was selected as the assessment instrument because it was
developed by a reputable agency (Kent State University) and is geared toward assessment
in the area of IL specifically within a K-12 setting. The assessment tool was first released
in 2004 for use by educators and has been administered to over “817,000 students in over
44,500 sessions; more than 15,500 registered users” from 2004 to 2012 (Kent State
University Libraries, 2014, p. 1). This assessment instrument also provided specific data
points based on the five curriculum strands (see Table 2).

38
Table 2
Information Literacy Standards, Strands, and Research Model Comparison
Standards for 21st Century
Learner from the American
Association of School
Librarians
Standard 1: Inquire, think
critically, and gain
knowledge.
Standard 2: Draw
conclusions, make informed
decisions, apply knowledge
to new situations, and create
new knowledge.
Standard 3: Share
knowledge and participate
ethically and productively as
members of our democratic
society.
Standard 4: Pursue
personal and aesthetic
growth.

TRAILS Information
Literacy Stands

Big Six Skills (Big6™)
Research Model

Strand 1: Developing a topic


Step 1: Task Definition


Strand 2: Locating valid
source information

Step 2: Information
Seeking strategies 

Strand 3: Utilizing
successful search strategies


Step 3: Location and
Access 

Strand 4: Strategies for
selecting the best sources for
information 
Strand 5: Responsible,
ethical, and legal use of
information 

Step 4: Use of
Information 
Step 5: Synthesis 
Step 6: Evaluation 

Improvement Project Intervention—Instructional Research Model
For the purpose of this research project, the Big Six Skills process model
(Eisenberg & Berkowitz, 1990) was utilized. Selection of this model was primarily due to
the CMS adoption of Big6™ as a research instructional method as part of the district’s
2014 Strategic Plan (Charlotte-Mecklenburg Schools, 2009). Unfortunately, this research
model has been weakly implemented and inconsistently applied across schools (as
documented by the original research proposal focus group for this project). The Big6™
Skills by Eisenberg and Berkowitz (1990) is described as a model for information
problem-solving. Big6™ uses six steps that seek to allow users to enter the model at any

39
stage and enable users to find an answer or solution relevant to the information needed.
The steps include task definition, information-seeking strategies, location and access, use
of information, synthesis, and evaluation. I have used this model with more than 2,000
students and have found it to be an effective means of teaching research strategies that
led to students successfully and ethically completing research assignments. The Big6™
research model is also relevant to this study because it connects successfully to the basic
library skills needed by students while connecting to the information-seeking, problemsolving, and critical thinking attributes as defined by educational standards (see Table 1
for additional comparisons to standards found in the Methods sections).
The primary reason for using a research model is to provide an instructional
strategy that creates a path for students to successfully secure and use the information
needed; however, with overlapping steps, confusing vernacular terms, and use by
different grade levels, there is not a model that can easily work in all settings. The
strategy identified in the Big6™ research model is relevant when connected to IL and the
specific standards, which speaks directly to seeking, accessing, using, evaluating, and
analyzing information specific to learning. Based on the information collected for this
project, there appears to be a direct and relevant relationship between implementing and
teaching a research model and the potential for students to develop IL skills.
Graduation Project
One catalyst for creating the intervention tool used in this project was that the
state recommended and CMS required a high school graduation research project. In
North Carolina, some school districts have adopted an accountability model that includes
a graduation requirement for students that incorporates research, writing, and presentation

40
skills as a structured process that occurs during the junior and senior years of high school.
For CMS, students complete the graduation project as a graded portion of their English
course work. Sometimes referred to as the Senior Exit Project, the intervention resources
created for this project were designed to help students successfully complete their
graduation project and use 21st century skills. The intervention tool for this project
addresses the skills students need as users of IL.
The intervention design team was challenged to help create a tool that:
1. was web-based and accessible within a learning management system,
2. could guide students to completion of tasks in a variety of domains
3. offered practice assessments, and
4. provided feedback with supporting resource materials.
An additional component required for this intervention tool was a clear navigation system
that enabled students to see all the lessons, resources, and activities in one location. All
activities, practices, and assessments were required to be aligned with the Standards for
21st-Century Learner from the American Association of School Librarians.
Improvement Project Intervention—Creating Online Resources
Two instructional resources were created for this project: an online instructional
unit delivered via a learning management system and a dedicated IL website. On March
27, 2015, NCDPI announced that they had entered into a state-wide contract with
Instructure for their Canvas LMS system (NCDPI, 2015c). As this information became
available across the state, I consulted with Jake Standish, the district technology manager,
about the status of this system and was advised that CMS was fully vested to utilize this
resource. Part of the rollout plan for Canvas was a series of trainings that would be

41
available during the summer of 2015. With the district committed to using Canvas LMS
and an examination of functions based upon similar products (Blackboard, Google
Classroom, and Edmodo), I consulted with the design team and updated the project plan
to include adapting the IL instructional unit to this interface.
In creating the instructional unit in Canvas, I focused on the four curriculum
standards represented in the IL content area (as defined by the Standards for 21stCentury Learner from the American Association of School Librarians). I also assembled
learning activities into online modules that could be accessed by the student participants
in the research study. These specific learning activities aligned to the five IL strands
identified within the TRAILS online assessment that also incorporates the Common Core
State Standards Initiative (2012) and the Big Six Skills Research Model (Eisenberg &
Berkowitz, 1990; see Table 1 for standards, strands, and research model comparison).
Content incorporated from these online tools came from existing IL sources and direct
instruction content created by this researcher. The framework for instructional modules
also became the base of the resources listed in the dedicated online IL website.
The design team reviewed the instructional unit, consisting of six modules. The
design of this IL unit provided opportunities for assessment after instruction based on
completing the project within a 90-day period. As lessons and activities to be included in
each of the modules were developed, members of the design team reviewed and provided
feedback before I finalized them to the online unit. The design team provided expertise in
the identification, application, and quality of content used with other online resources as
part of the direct instruction intervention. I worked with this team by sharing documents
and online resources via bi-weekly email and telephone conversations when appropriate.

42
Feedback and changes were confirmed by the team with a final reporting on August 30,
2016, which confirmed resources available for the implementation phase of the study.
In collaboration with the English teachers, an instructional unit timeline that
showed the progression for the instructional component of the project after the pre-test
assessment was created. This timeline specifically outlines the instruction and practice
that occurred with face-to-face instruction during the first 40 minutes of class over a twoto three-week period. The structure for the lessons supported introducing an IL strand
during day one and a practice section for the same strand for day two. This process
repeated until all five strands and the research model were reviewed with students and a
practice had been completed. The timeframe for assembling the design team,
implementation team, creating intervention resources, and online tools occurred from
May 2015 through August 2015.
Improvement Project Timeframe
The principles of Improvement Science were foundational throughout this
improvement project. With the expectation that decisions, strategies, or resources that
were developed for this project would have positive outcomes, establishing a research
approach that would address the identified problem was critical. The project timeline
enabled me to manage each task identified during the planning stage and make
adjustments as needed when issues occurred as well as document adjustments that were
made along the way.
After the project had been approved by the disquisition committee, I facilitated
the formation of the design and implementation team at the initial school-based site. The
goal was to have the improvement project kickoff during the fall semester of the school

43
year before students began to develop ideas and conduct research for their high school
senior year graduation project. The first activity for this project was the development of a
“PDSA Cycle.” PDSA is an acronym for “Plan, Do, Study, and Act” that establishes a
strategy for navigating a plan to address issues that are problem-oriented and merit study.
The PDSA approach was introduced by Deming in 1960 and has become a cornerstone
principle for Improvement Science research (The Deming Institute, 2014). A benefit that
came from using the PDSA during this first cycle of the study was the establishment of
team expectations; addressing questions, issues, or concerns; and documenting potential
issues or concerns as well as establishing structure to complete the study within a 90-day
time period.
Project Challenges
One of the first identified challenges for the project was recruiting teachers to
participate who represented diverse English course designations. I had several teachers
offer to participate from AP or Honors with only a few willing to participate from the
Standard level courses. A second challenge that occurred later in the project after the
CMS IRB approval was the navigation and overlap of activities across four locations.
There were several occasions where I needed to be at different schools on the same day. I
had to coordinate driving across the city to arrive at the scheduled time for the
intervention activities, and then manage to return to my home school to complete my
workday. An additional challenge was presented by the collaborative teachers with a
request that all students receive the intervention instruction and related activities. Because
the IL instruction was already included in the English teachers’ curriculum plan, there
was a concern about excluding students and how students would be managed if not

44
participating during the class time. After additional discussion, an agreement was made
that all students could participate in the study for the identified classes and I would
provide the IL instruction and conduct the pre- and post-test assessments as well as
provide results back to students, but only include students who returned consent forms
within the study results.
Project Calendar
During the planning phase, I worked with the collaborating teachers to understand
how research skills were taught as part of the standard course of study and to determine
the best methods, resources, timetable, and scaffolding to use as the study moved forward
(see Table 3).

Table 3
Initial CMS IL Intervention Project Calendar
Month

Activity

Notes

February 2015

 Initial Design Team Contacts and
Participation Agreements
 Complete WCU IRB Process

SMHS and District
Contacts

March 2015

 Initial Implementation Team Contacts

SMHS and District
Contacts

April 2015

 Evaluate LMS Tools

NC and CMS select
Canvas LMS

May 2015

 Begin CMS IRB Process
 Begin training on Canvas LMS Tools

District Contact

June 2015 –
August 2015

 Finalize CMS IRB Process
 Complete training on Canvas LMS Tools
 Finalize only unit of instruction and web
resources

District Contacts

September 2015

 Project Kick-off meeting

SMHS

October 2015

 Assessment Test TRAILS
 Begin Intervention

SMHS

November 2015

 Complete Intervention

SMHS

December 2015

 Analyze Data

All Intervention Schools

45
This planning stage resulted in the creation of a detailed plan with potential dates
that were used to conduct the assessments at each high school location using the TRAILS
online tools. This initial TRAILS assessment served as a pre-test for the selected student
population.
Pre-intervention Activities
Starting in March 2015, I began working with the SMHS implementation team to
create a formal schedule for the delivery of the IL assessment and interventions. An
initial timeframe was selected with a plan to conduct the pre-assessment in TRAILS on
September 15, 2016. One week was set aside for me to evaluate the test results and create
reports that contained student results. Direct instruction using the Canvas LMS system
was scheduled to take place from October 1st through October 15th with the postassessment taking place on October 16th. After the implementation plan was approved by
the team, I collected stakeholder agreement forms and stored them with the IRB
documentation in the designated project location.
After a review of the available options for providing online instruction was
started, the design team learned that a new program called Canvas would become
available from the North Carolina Department of Public Instruction. Based upon the state
and school districts’ support for the new learning management system (LMS) Canvas, the
available functions for integrating grades and alignment to our existing student
management system (PowerSchools), I developed a recommendation for using this
resource for direct instruction, which was accepted by the members of the design team.
From May through August 2015, I completed the CMS IRB process and was
instructed to include additional schools in the study. Therefore, I was required to recruit

46
potential collaborative teachers with whom I could work with that represented similar
diverse student populations. Once the three additional schools were on board, I created a
new timetable that enabled me to complete the study within a 90-day period. Summer
months provided me an opportunity to attend training on the new Canvas LMS system,
create the instructional model with activities, and create a TRAILS online account with
scheduled assessments for each of the participating school sites and intervention groups.
The Canvas IL instruction unit was titled CMSINFOLIT and consisted of 36 wikistyle webpages (see Appendix D, Figures 16–52). The progression of the modules was
scaffolded in a way that enabled students to follow along with instruction during the faceto-face meetings and practice strategies independently. The initial design was 12
modules, with modules paired to match each of the standards and strands encompassed in
this project. The initial timeframe for instruction was slated for 400 minutes (ten sessions
at 40 minutes each). This was outside of the time needed to conduct the pre- and post-test
assessment. After collecting feedback from the English teachers participating with
Standard students that they could only provide half the time requested, I restructured the
modules from 12 to six units (see Table 4). In the previous version, the IL instructional
unit reflected two paired modules that would allow for instruction in the first module and
practice in the second module. With the revised design, the practice was included in each
dedicated instructional module (see Appendix E for course content outline and Appendix
F for course reference list).

47
Table 4
Overview of Modules
Title

Standards Covered

Content

Activities

Module 1:

Standard 1
Strand 1
Big6™ Step 1

Introduction
Task Definition
Big 6

Assignment (Answer
three questions about IL)

Module 2:

Standard 1
Strand 2
Big6™ Step 2

Inquiry
Selecting and
refining a topic

Assignment (Refine topic
from broad to narrow),
Quiz

Module 3:

Standard 2
Strand 2 and 4
Big6™ Step 3

Locating
informational
sources

Assignment (Identify
sources and how to
access), Quiz

Module 4:

Standard 2
Strand 3
Big6™ Step 4

Collecting and
evaluating
information

Assignment (Search
database and report
results), Quiz

Module 5:

Standard 3
Strand 5
Step 5

Synthesis
Copyright
Plagiarism

Assignment (Discussion
on citation tools), Quiz

Module 6:

Standard 3 and 4
Strand 5
Step 6

Evaluate Work
Graduation Project

Assignment (Define
copyright and its impact),
Quiz

As an enhancement to the instructional unit, I used an online comic strip resource
from MakeBeliefsComix.com to create a graphic illustration introducing students about
the functioning of the online LMS “Using Informational Resources” module, and as part
of the review section for the last module (see Figure 2).
At the start of the 2016–2017 school year, I contacted all members of the design
team to have them preview the new Canvas instructional unit to collect feedback and
make changes. After this design review process was completed, I contacted the
implementation team at SMHS to schedule the intervention plan. Before conducting the
pre-assessment, I gave an orientation of the project to all students, and the first TRAILS

48
pre-assessment took place on September 29th at SMHS in the school media center
computer area for each class. Including all the other intervention classes at other school
sites, the TRAILS pre-assessment was delivered to 220 high school juniors (eight English
classes). Pre-test assessment data established a baseline report for IL skill level for each
student participating in the improvement project.

Figure 2. Comic strip created using MakeBeliefsComix.com.

Intervention Activities
Before working with a group of students for the direct instruction intervention, I
created the website-only intervention resource. This website was developed using the free
hosting and design resources from Weebly. Once completed, the website was shared with
students as part of their pre-assessment score report details for this select group of
students. The final product was named CMSINFOLIT and is available online at
http://www.cmsinfolit.weebly.com.
As pre-assessments were completed and score reports prepared for each class, I
scheduled follow-up reviews with participating classes to review the results of the
assessment and provide individual score documents to each student (see Appendix C,
Figure 14 for a sample report). AP students were only provided their scores, whereas

49
other groups were provided scores and intervention. AP students participated in the initial
assessment solely to determine baseline data. Interventions were assigned randomly to
class groups, with the first assessed class to have an intervention for score reports, the
second class to have access to an IL website that I had created, and the third class group
would have access to direct instruction support with LMS. Additional classes added to
the study were paired by class type to intervention (this is why there are two Honors
groups at MPHS and SMHS). All student participants were provided their individual
score reports from the pre-assessment. Classes assigned to the website intervention were
directed to the CMSINFOLIT website and encouraged to use this online resource in
conjunction with their score report to improve their IL skills. The direct instruction class
group was also provided their individual score reports, as well as a link to self-enroll into
the online Canvas LMS course. This unit was built and listed in the CMS Canvas system
as a course labeled CMSINFOLIT. All intervention class groups were advised that a
follow-up assessment would take place and that their English teacher would announce the
post-test assessment date.
For the direct instruction group, a new challenge presented itself in the form of
needing to provide instruction to students on how to gain access to the new state single
sign-on system that served as a gateway to Canvas LMS. Introduced to instructional staff
during the summer of 2015, NCEdCloud IAM Service (NCDPI, 2015b) provides access
to many of the programs, tools, and online resources available as part of the engagement,
instruction, and support for students, staff, and parents. By instructing students on the
process of creating their account in the NCEdCloud IAM Service and demonstrating how
to navigate the new system, I was able to have all participating students successfully log

50
on and gain access to the first module in the IL unit in Canvas. The direct instruction
students were then provided an overview of the course, expectations and goals, a list of
activities, resources, and contact information to submit questions.
The first group of students to receive the direct instruction was the Honors level,
which took place over 10 days with an introduction to content on the first day and
application of content on the following day; this cycle repeated until all modules within
the instructional unit were completed. The total amount of time for direct instruction for
the Honors intervention group was 400 minutes. The instructional method used was to
present the information related to the standard, strand, or Big6™ process step that was
being reviewed; engage students through discussion; provide clarifications; and model
how to access additional information and resources on the topic. The practice sessions
were designed to use resources available within Canvas to engage students in discussion,
submit assignments, or complete practice quizzes. At the end of the modules, students
were given a review and summary practice quiz that they could take repeatedly until
answering all questions correctly to progress to the next module. Students were also
provided a “Student Trained Researcher CMSINFOLIT” badge that could be displayed
on their profile page within the Canvas system. This badge tool is a standard feature and
available to any student who either participates through direct instruction or self-paced
enrollment in the online modules. This feature was added as a progress monitoring
component because, at the end of the study, all participating students were provided
access to the course and given directions for using the system to improve IL as a selfpaced learner.

51
The second group of students participating in the direct instruction was the
Standard level students. Because I was already aware that I would have a significant
reduction in the time allocated to work with these students, the instructional method that I
used with the previous group was modified to fit the allocated 200 minutes of time. This
change also meant that I could only work with students for five days as part of the
instructional component of the study. This change resulted in combining the instructional
portion of the IL unit with the practice session within each content day. Though I was
able to successfully accomplish both tasks in the time allotted, there were instances where
I needed to encourage students to use out of class time to complete practice activities.
Post-test Assessment
A post-test assessment was conducted at the end of the librarian-led training for
the identified face-to-face instruction class. This post-test assessment and perception
survey were given to all participants. The goal of the perception survey was to collect
data on students’ understanding, practice, and appreciation for IL. With an eye toward
continuous improvement, reflective interviews were conducted at the culmination of each
collaborative instructional session. The purpose was to ascertain from instructional
partners what worked well, what could be improved, and what is different from the usual
methods of teaching research. While collaborative English teachers are an important part
of the design and implementation teams, this research design recognized their feedback
and real-time evaluation of the intervention as an invaluable data source for
understanding how to create sustainable improvements.

52
CHAPTER FOUR: RESULTS

Introduction
The goals of this study were to identify the baseline skill level in IL for a diverse
group of HS students within CMS, apply and evaluate the impact of three types of
interventions, and gain insight into students’ and key stakeholders’ perceptions of this
topic. Data collected from this study attempt to illustrate students’ IL skill level, before
and after interventions, as well as capture their perceptions of the importance of IL.
Understanding what circumstances contribute to student growth in IL skill was the
primary objective of this inquiry. After comparing pre- and post-test results, direct
instruction was shown to have a significant positive impact on students’ acquisition of IL
skills when compared with other conditions in the study.
The total sample consisted of 222 11th-grade students. From this population, 161
returned the parental consent form. From this remaining group of students, 135 were
classified as “participants” and 26 were identified as “incomplete.” For the purpose of
this study, incompleters were determined as students who did not take either the pre- or
post-test. Outliers were identified during preliminary data cleaning and removed from
analysis. A student was considered an outlier (with unreliable data) if he or she received a
grade of zero on 2 or more sub-strands of the post-test assessment (indicating no attempt
was made in more than one section of the assessment). Figure 3 shows the breakdown of
participants for this study arranged by school name.

53

222
135
26
61

Figure 3. CMS Information Literacy Study participant information.

Results
Baseline of Students’ Ability to Seek, Use, Disseminate, and Communicate
Information
From the pre-test conducted at the four school locations included in this study,
descriptive statistics were compiled on the pre-assessment data to establish a baseline of
IL in the sample. From the data presented in Figure 4 for the first strand on the
assessment related to how to develop a topic, the Advanced Placement (AP) students
scored higher than all other categories of students with a mean score of 55.44% (across
all groups), followed by Standards with a mean score of 44.92% (across all groups), and
Honors with a mean score of 43.32% (across all groups). Though the gap between
Standards and Honors students in this strand is small, it is still significant given that
Honors students have a higher academic performance record. In a comparison of schools
for the TRAILS pre-assessment test for the second strand related to identifying sources,
Figure 5 shows AP students scored higher than all other categories of students with a

54
mean score of 66.68% (across all groups), followed by Honors with a mean score of
46.91% (across all groups), and Standard students with a mean score of 39.33% (across
all groups).

Figure 4. TRAILS baseline data comparison for strand 1 by schools.

Figure 5. TRAILS baseline data comparison for strand 2 by schools.

55
Strand three represents the skill that students use to conduct search strategies,
which was the lowest total performance area of all topics. Figure 6 shows that AP
students scored higher than all other categories of students with a mean score of 51.36%
(across all groups), followed by Honors with a mean score of 48.68% (across all groups),
and Standard students with a mean score of 44.89% (across all groups).

Figure 6. TRAILS baseline data comparison for strand 3 by schools.

The fourth strand assessed focused on how students evaluated resources. Figure 7
shows AP students scored higher than all other categories of students with a mean score
of 67.28% (across all groups), which was the highest overall average score of all strands,
followed by Honors with a mean score of 51.57% (across all groups), and Standard
students with a mean score of 39.81% (across all groups).
The ability to use information responsibly and ethically was the fifth strand skill
evaluated in the assessment. Figure 8 shows that AP students scored higher than all other
categories of students with a mean score of 67.40% (across all groups), followed by

56
Honors with a mean score of 57.79% (across all groups), and Standard students with a
mean score of 55.14% (across all groups).

Figure 7. TRAILS baseline data comparison for strand 4 by schools.

Figure 8. TRAILS baseline data comparison for strand 5 by schools.

In comparison of the total scores across all schools and groups, AP students
performed higher with a mean average of 61.68%, with Honors at 49.60%, and Standard

57
at 44.78% (see Figure 9). A review of the data based on IL strands shows most students
found the process of “Developing a Topic” the most challenging, with an exception for
the Standard class of students who struggled significantly with the process for “Identify
Resources.” This exception may be due in part to the general challenges that Standard
students have academically with the process for building on learning methods that require
a scaffolding approach. In this case, having mastered the process for “Developing a
Topic” required exposure to a list of ideas or the opportunity to develop a list of ideas for
a topic, whereas the process for identifying sources would occur from developing a
familiarity with sources, as well as navigating to the potential locations where sources
exist. The second area with which students struggled is the process for selecting “Search
Strategies.” With the total average score across the assessment within a five-point margin
between the Standard and Honors students, this justifies a direct comparison of these two
groups of students specific to the applied interventions.

Figure 9. TRAILS baseline data comparison total average by schools.

58
Analysis of Instructional Strategies (Including Self-Paced Resources, Collaborative
Direct Instruction, and Assessments)
The intervention applied during this study included the process for
communicating score results, providing access to online website resources, and direct
instruction utilizing a six-module online Canvas LMS course. In a comparison of the preand post-test data from the TRAILS online assessment used in this study, only the
Standards and Honors level course were used as part of the intervention groups. The
Standard students received 200 minutes of instructional time compared to Honors
students, who received 400 minutes of instructional time.
To understand the impact of the interventions used during this study, I measured
the pre- and post-test assessments within each TRAILS strand across the class type,
intervention type, instructional time allocated, and the total gain. I completed a one-way
ANOVA (analysis of variance) on gain scores for the overall gain as well as gains by
domain (Warner, 2013, pp. 960–973). The gain score is the difference between post-test
and pre-test raw scores. There were significant differences between groups on Total
Gains (F(3, 105)=5.411, p=.002), and the specific IL skill areas for Strand 1: Topic
Development (F(3, 105)=4.898, p=.003), Strand 3: Utilizing Search Strategies (F(3,
105)=4.707, p=.004), and Strand 5: Responsibility and Ethics (F(3, 105)=5.003, p=.003).
The data show there was no significant difference among intervention groups when
comparing scores against the skill areas for Identifying and Evaluating Sources. Post hoc
tests show the differences between the groups often lie between 400 minutes of
instruction and self-paced website instruction or scores only. I used Tukey’s statistic test
(Warner, 2013) as part of a post hoc review to compare multiple elements within

59
interventions (scores, website, 200 minutes of instruction, and 400 minutes of instruction)
(p. 247). Figure 10 provides an illustration of these comparisons.

Figure 10. Results comparison by instruction level for honors and standards students.

Total gain for all strands as compared against different intervention groups.
There was a statistically significant difference between groups as determined by one-way
ANOVA (F(3,105) = 5.411, p=.002). A Tukey post hoc test revealed that students who
participated in the website intervention had significantly lower gains on the TRAILS
assessment (-4.27 ± 2.3, p = .001) as compared to the 400 minutes of the instruction
group (.40 ± 4.6).

60
“Developing a Topic” strands as compared against different intervention
groups. There was a statistically significant difference between groups in gains in Topic
Development as well as determined by one-way ANOVA (F(3,105) = 4.898, p=.003). A
Tukey post hoc test revealed that students who participated in the website intervention
had less gains on the TRAILS assessment (-.60 ± 1.7, p=.014), as did the 200 minutes of
instruction group (-.38 ± 1.5 points, p=.022) as compared to the 400 minutes of
instruction group (.72 ± 1.5 points).
“Identifying Sources” strands as compared against different intervention
groups. No statistically significant gain was revealed between the domains.
“Utilizing Search Strategies” strands as compared against different
intervention groups. There was a statistically significant difference between groups as
determined by one-way ANOVA (F(3,105) = 4.707, p=.004). A Tukey post hoc test
revealed that students who participated in the website intervention scored statistically
significantly lower on the TRAILS assessment (-1.20 ± 1.1, p=.027) as compared to the
scores of the 200 minutes of instruction group (.31 ± 1.7) and the 400 minutes of
instruction group (.16 ± 1.3).
“Evaluating Resources and Information” strands as compared against
different intervention groups. No statistically significant gain was revealed between the
domains.
“Use of Information Responsibly and Ethically” strands as compared against
different intervention groups. There was a statistically significant difference between
groups as determined by one-way ANOVA (F(3,105) = 5.003, p=.003). A Tukey post
hoc test revealed that students who participated in the scores intervention scored

61
statistically significantly lower on the TRAILS assessment (-1.44 ± 1.6 points, p=.018),
as did the website intervention (-1.40 ± .9 points, p=.018) as compared to the 400 minutes
of instruction group (-.14 ± 1.8 points).
Impact of Interventions Compared by Domain
A comparison of the data specific to the average changes found in the pre- and
post-test by domain showed that all students struggled in the area of “Identify Sources”
and “Use of Information Responsibly and Ethically” (see Figure 11). For the strand areas
for “Developing A Topic” and “Utilizing Search Strategies,” the scores and 400-minute
direct instruction intervention saw improvement. The domain area for “Evaluating
Resources and Information” found that all intervention groups reported improved scores.
Comparison by domain based upon intervention shows that students struggled in several
areas which indicates that additional instruction may have been needed.

Figure 11. Intervention type comparison by domain.

62
Impact of the Use of Canvas LMS
One of the capstones that tracked students’ completion of the direct instruction
LMS was the use of the Canvas badge system. Students earned a badge by completing all
assignments, quizzes, and navigating through all of the module sections. Of the 78
students participating in the instruction intervention, 39 earned the CMSINFOLIT badge
(50%). Of the 39 who earned this badge, 26 showed growth (67%). From within the
instruction group of students, 21 came from Standard classes. Of the 21 Standard
students, only two earned badges and only one showed growth. This drastic decline in the
percentage of badge earners among Standards students was probably due to the decrease
in the instructional time. There were also seven students who earned a passing score from
the assignments and quizzes but did not complete all of the activities to earn a badge. Of
these seven students identified with passing scores, only two showed growth.
Students who completed the Canvas course. Independent sample t-tests were
conducted on the data for students who received direct instruction to determine the
effectiveness of the Canvas course modules on IL skill development. Of the direct
instruction students, 39 students (37 honors, two standard) completed all of the Canvas
modules. Data illustrate that students who completed Canvas scored higher than other
students with direct instruction. These differences were significant for four domain
strands (1. Develop A Topic, 3. Utilizing Search Strategies, 5. Responsible and Ethical
Use of Information and Total Gain; see Table 5). The implications for this significance is
a need to adjust by either adding or extending the content in the canvas LMS for sections
2. Identifying Sources, and 4. Evaluating Sources and Information. Also, the impact of

63
this significance is that it is beneficial for students complete the instructional activities
embedded into Canvas LMS.

Table 5
Comparison of Mean Students Who Completed Canvas LMS and Non-Completers by IL
Strand and Total Gain
Strand

M

SD

t

df

p

1.05
-1.44

1.43
1.52

3.15

76

.002

Strand 3: Utilizing Search Strategies (n=39)
Completers
Non-Completers

.28
-.77

1.81
1.60

2.06

76

.043

Strand 5: Responsible and Ethical Use of
Information (n=39)
Completers
Non-Completers

.05
-.77

1.65
1.51

2.29

76

.025

1.05
-1.44

3.97
4.80

2.50

76

.015

Strand 1: Develop A Topic (n=39)
Completers
Non-Completers

Total Gain (n=39)
Completers
Non-Completers

Student and Stakeholder Perceptions of IL
With an eye toward continuous improvement, reflective interviews were
conducted at the culmination of each collaborative instructional session. The purpose was
to ascertain what worked well, what could be improved, and what transpired that is
different from the usual methods of teaching research. While collaborative English
teachers are an important part of the design and implementation teams, this research
design recognizes their feedback and real-time evaluation of the intervention as an
invaluable data source for understanding how to create sustainable improvements.

64
Analysis of Coded Key Stakeholder Interviews
At the end of the study, I conducted interviews with the key stakeholders
participating in the project as part of the design and implementation teams. Their insight
into the project provides an opportunity to collect qualitative information specific to key
attributes associated with the creation, delivery, interaction, and reaction to the
intervention resources and goals for the study. From the interview questionnaires, I used
descriptive and magnitude coding to identify words that aligned specifically to addressing
the focus of each question (Miles, Huberman, & Saldaña, 2014, p. 80). Each item on the
questionnaire was assigned a control phrase that represented the primary idea for each
question. From this group of coded responses, I identified three primary word attributes,
creating a specific subcode list. The primary descriptive terms that came from these
results include “process, impact, outcome, features, ready, reaction, importance, and
vision.” Of these terms, the most relevant is process, which embodies the elements of
“ready and reaction” and features that encompass “impact, outcome, and importance.”
Inferring from the data, it appears that stakeholders value the process and features
associated with this study (see Table 6 for Coding and Subcoding details).

65
Table 6
Stakeholder Interview Question Coding and Subcoding
Primary Stakeholder
Interview Questions

Control
Phrase or
Word

Subcode Word
List and
Frequency

1: Please share your current expectations for
student awareness and skill for information
literacy?

Expectation

Process (9)
Success (6)
Learning (5)

2: How has this changed with your participation in
this project?

Change

Impact (6)
Recognize (5)
Value (2)

3: Based on your stakeholder role, please share
your observations of the pre-planning activities that
have taken place for this project (conversations,
Observation
meeting, lesson or curriculum development, online
resource tools, or implementation of the TRAILS
assessment)?

Outcome (6)
Participation (5)
Plan (3)

4: Considering the relationship between
information literacy, research skills, and problem
solving, what do you believe to be the most
important attribute that we can provide to students
to prepare them for college and/or careers?

Attribute

Features (9)
Traits (4)
Quality (3)

5. How has this project impacted your thoughts on
preparing students for the future?

Prepared

Ready (7)
Pending (4)
Future (2)

6. Reflecting on the scope of this project to
evaluate a select group of students for their level of
skill related to information literacy and five key
strands as compared to national standards,
implementing an intervention that includes
Impressions
instruction and/or online access to resources related
to developing and enhancing these skills, please
share your overall impressions of this project and
the potential outcomes.

Reaction (6)
Influence (4)
Idea (3)

7. Are there any other insights or information that
you would like to share related to your experience
supporting this project?

Importance (4)
Vision (4)
Value (3)

Insight

66
Relevant Feedback Responses from the Stakeholder Interviews
Part of the process for conducting the key stakeholder interviews was the
opportunity to uncover additional feedback specific to participation and insight gained
from the project. Post-study interviews were conduct with two teachers, two SLMS, and
two project support team members. Recognizing the importance of this study, one project
support team member shared that “Students should know the steps to follow, how to
share their work and how to use their self-reflections to successfully complete the
process,” and a teacher stated “recognizing that strong information literacy and problem
solving skills is important for all students to solve everyday information problems.” This
reflection confirms that collaborative partners understood the purpose of the study. When
asked about challenges they observed by participating in the study, one teacher stated that
the project was “well organized, clear expectations upfront, low impact, and easy to do.”
A project support team member voiced that “it is valuable for others in the profession to
learn about this research and work, in particular using TRAILS as an assessment tool.”
Another teacher commented that providing “curriculum for research skills is a great idea.
[It’s] helping students to be better prepared for college.” One SLMS stated she would
have liked to participate in the instructional process. Another SLMS was quoted “since
Canvas is still new to the district, it is challenging to partner with collaborative teachers
to use this LMS for instruction.” Other statements presented that have relevance to the
project include “students need to understand how to use resources, what are the
questions, keywords, not just Googling,” as indicated by a participating teacher and a
“deeper understanding of information literacy, conducting research in an efficient way,”

67
which was stated by a SLMS, which suggests stakeholders recognize the potential
changes that can take place when interventions are successful.
Student Survey
I used the online system from Qualtric Survey Software provided by Western
Carolina University to deliver a perception-driven survey using a Likert scale response to
gain insight into how students view their understanding, value, importance, and interest
in IL. An overview of this survey is shown in Figure 12 (for complete survey totals see
Appendix H, Table 8). A review of the data shows students generally have a favorable
attitude toward IL and its overall importance. The most revealing elements from the
survey came from question 11, which focused on how students valued the instruction for
the strand area in “the responsible, ethical, and legal use of information.” With almost a
third of the students either neutral or disagreeing in their responses to this question, an
inference can be made that this is an area in which students need additional support and
training.

Figure 12. Student Perception Survey overview.

68
Discussion
The findings from this study provide an interesting and thoughtful context to
examine IL as seen through the lenses of high school juniors and key stakeholders. By
addressing the three initial questions presented in this study, the results provide a
pathway to understand how to enhance IL development in a K-12 setting. The preassessment segment of the study showed that the AP students scored the highest on the
initial assessment, followed by Honors and then Standard students. The parallels between
these groups are also interesting in that the margin between the two AP school sites is
within one point. This changes when compared to the Honors and Standards students at
the multiple locations.
The pre-assessment data shows that all student category groups struggled with the
concepts and knowledge associated with utilizing search strategies. Specifically, within
the Honors and Standard class groups, these students found the process for identifying
sources challenging. All students performed better on the assessment for the process of
evaluating resources and using information responsibly and ethically. These two strands
are the most recognized because they are associated with finding resources in the library
collection or online databases as well as concepts of ownership and copyright. Most
students in their junior year of high school have visited the school library media center to
attempt to find information. Copyright and plagiarism are repeatedly reviewed with
students because this is part of the student code of conduct. In general, students struggled
most with concepts associated with developing a topic. Asking critical questions and
developing skills to drill deeper within a topic may not have been processes students were
exposed to before the assessment.

69
Evaluation of pre- and post-test TRAILS data revealed students with 400 minutes
of instruction had the greatest gains in IL skills. When examined closely, interventions
had significant differences when compared against total gains in IL, and strands related to
developing a topic, identifying sources, utilizing search strategies, and use of information
responsibly and ethically. The post-survey for students and stakeholder interviews
provided additional insight into the value of IL.

70
CHAPTER FIVE: REPORT AND RECOMMENDATIONS

The complexity of information delivered to society requires the ability to
integrate, analyze, and synthesize an increased amount of data from multiple outlets. The
development of the literacy skills associated with navigating this wealth of information is
paramount. IL as a skill included within the North Carolina curriculum policies and
procedures is weakly supported. There is limited actual practice and curriculum
integration for IL skills within the curriculum. The driver for this deficit appears to be the
expectation that teachers integrate IL skills into curriculum and collaborate with school
library professionals.
The lack of formal state mandated assessment data specific to IL skills for K-12
students also restricts the potential for requiring school districts and teachers to include
this important skill set into the curriculum and instructional practice. The guiding purpose
of this improvement project was to establish a baseline of student skill level for
information seeking, utilization, dissemination, and communication; to determine student
and teacher perceptions of the importance of IL; and to determine what instructional
strategies (including self-paced resources, collaborative instruction, and assessments) aid
in increasing student information skill levels. This study illustrated the potential for
increasing secondary students’ skills for IL by implementing a benchmark process,
leveraging instruction, providing a pathway for resources as aligned to standards, and
assessment tools.

71
Discussion
The initial benchmark data from TRAILS provided valuable insight into the
existing IL skill level for a diverse student population within CMS. All students,
regardless of course level, were below an ideal proficiency rate of 80% on the initial
assessment, demonstrating students need coursework that cultivates these skills. From the
three groups participating in this study, the direct instruction group had the most
significant gains compared to other interventions employed.
Use of the IBM SPSS (Statistical Package for the Social Sciences) and a one-way
ANOVA to evaluate the post-test TRAILS data enabled me to closely examine gains that
occurred across interventions used for this study. This examination was perplexing
because, overall, the gains were very small, and some students went down instead of up.
Factors that could explain these issues might include the use of a different test available
from the TRAILS system for the post-test. The second assessment test used the same
domains as the first assessment but utilized different items and examples. Another factor
could be that because students were not required to participate, there may not have been a
dedicated commitment to perform well on the post-assessment. There was no real
accountability, particularly for the website group, where I have no knowledge of who
actually utilized the website.
It was exciting to see some students’ scores demonstrated gains in the domains for
developing a topic and conducting search strategies. These two strands generated the
most questions from students during direct instruction. The two domains that saw no
statistically significant gains were the use of and evaluation of resources, which were also
the most challenging to integrate into the curriculum. As I was creating the online LMS

72
CMSINFOLIT modules, I found these two areas were the most difficult for which to
design curriculum and create related activities as part of having students practice what
they were learning. This observation provides an opportunity for SLMS to develop new
curriculum resources to support these specific IL strands. As more content is delivered
electronically via online resources, teaching students how to navigate resources as
investigators to uncover relevant and reliable facts is critical. Educators must continue to
examine these domains to create learning resources meeting the needs of today’s
students.
The most challenging aspects during the administration of this study were delays
during the summer session before the study began to maintain contact with stakeholders,
changes to the IRB process required by CMS, and the need to reduce direct intervention
time for direct instruction from 400 to 200 minutes. Though I was proactive with keeping
my primary stakeholders updated during the proposal stage of this project, needing to
expedite communication with these essential partners during the summer was sometimes
challenging. Luckily, I kept a documentation log of events and was able to bring
everyone on the design and implementation teams up to speed at the start of the new
school year. Because CMS required their own IRB process which would only be
processed after WCU made their approval, there were several months when I did not
know which protocol would be required for the study. Once CMS began the review
process for the study, I was told that I would need to include additional schools before
approval. This new demand occurred during the summer, which resulted in additional
delays due to difficulty in follow-up with school contacts.

73
Part of the start-up process was to give the pre-assessment to all students (no
matter what the course category). After analyzing the pre-assessment data and making
arrangements to begin instructions with each school site and course type, I learned that I
would only have half the time planned for the Standard instruction group (200 minutes)
as compared to the Honors classes (400 minutes). This was disappointing because I
believe this would add an uncontrollable variable. Because the instructional time was
shortened from 400 minutes to 200 minutes for the Standard group, I would use the time
provided as efficiently as possible but encouraged students to spend extra time on their
own exploring the online modules available within the Canvas LMS as well as the related
practice quizzes and activities. As the results show, the 200-minute direct instruction
group did not perform as well as the 400-minute group. Even with the gain that was seen
across intervention groups based upon direct instruction and those students who
completed the full blending learning coursework, it is shocking that students still
demonstrated IL knowledge below an ideal proficiency level. This provides a strong
argument that IL instruction requires adequate time to help students become
knowledgeable on the topics and provide opportunities for students to practice skills and
concepts.
The end-of-study surveys taken by students and the interviews with key
stakeholders provided valuable feedback for the project. By gauging the opinions and
ideas of these two groups, I have a better understanding of how this study supported the
intellectual needs of participants. Students clearly find the topic important and believe
acquiring IL skills is relevant for their future. The delivery of content had mixed results
on the survey, which could be a reflection of their overall understanding of the Canvas

74
LMS system or the process that students were required used use to complete the course
modules (single sign-on through NCEdCloud). Stakeholders interviewed for the study
provided a passionate argument for why the study was needed and the value placed on
developing IL skills with students. I have recently been asked to help other instructional
teams develop online curricula in the Canvas LMS, which came as a direct result of a
stakeholder referral.
The findings from this study have many implications for the future of research for
IL. Because there are currently only a small number of studies conducted within the PK12 setting, this research adds to the body of work needed to continue the investigation
into impacting student learning as part of a multiliteracy function. This study also
advocates for the inclusion of IL as part of the required curriculum that is mandated by
the state. By including IL as a measured component within related curriculum areas,
student performance data can be evaluated and provide additional insight on this topic.
The connection to social justice is also relevant because this study illustrates the impact
of sacrificing relevant skill-building instruction for students who are classed in a lowperforming group, which took place with the Standard classes engaged in this study.
Students who were given less time for instruction may have benefitted from additional
exposure and practice of IL, which could have had a positive influence in other academic
areas.
Limitations
As part of creating resources for this study, I developed an external website
students could use for their own self-directed learning. As the study was ending, I became
aware there was no way to track how many actual students visited the website or, for that

75
matter, if they were even in the intervention class group. Future studies using this method
should provide a login process that could be used as a tracking system. Another limitation
of this study is that not all categories of intervention have Honors and Standard students
in each group. The website intervention did not have a class represented by Honors
students. As previously mentioned, there was a challenge that required the redesign of the
Canvas LMS when the study moved to the phase with the Standard class groups. An
additional limitation was an unannounced break during one of the post-test assessments
that required stopping the test, creating a new assessment session, and restarting the
assessment for the student group impacted. Though these limitations provided challenges
throughout the study, they also provided opportunities to learn from these experiences, as
possible future studies may attempt to replicate this method.
Recommendations
Continuation of research in IL for PK-12 students would benefit from a
comparison between direct instructions without the use of a LMS. This could provide
insight into whether or not a blended learning approach impacts student knowledge. An
additional aspect in this area would be to examine what variables kept students from
completing the LMS modules towards earning a badge or course completion recognition.
Without a specific requirement for students to participate during the life of the study, I
believe that some students lost interest or decided to not take the training serious when
they realized that there was no penalty or high stakes conditions required. This may also
account for the high level of “no consent” forms not returned during the intervention
phase of the project. Even having the benefit of developing skills towards completing the
graduation project as an incentive to encourage participation, some students may have

76
realized that if their English course grade was high enough, they might be able to reduce
or eliminate all of this requirement.
This study also focused on the cognitive process that takes place as students begin
to learn, develop, and enhance their knowledge and understanding of IL. An ideal
extension of this would be an evaluation of skills used toward the application of
knowledge in creating a product. As part of the NC high school graduation project, there
is a rubric process used to evaluate students’ completed research. Comparing skills
students gained through IL instruction to the results of their final graduation project
would yield relevant insight potentially benefitting the development of new IL instruction
and assessments. An ideal opportunity exists to collect graduation project evaluation data
to understand how students perform on the overall product produced during their senior
year as compared to the IL standards. It would also be appropriate to use the evaluation
process for the graduation project to determine if there is a correlation between mastery
and the assessed skills found within the TRAILS strands. This study utilized only the
online TRAILS assessment from Kent State University. Applying similar methods and
using different assessment tools would be of value (e.g., using either the ILT test from
James Madison University).
The development of IL instruction for this project relied heavily on integrating the
Big6™ research process model across the curriculum. There may be a benefit to using a
different research model or multiple models to assess student learning and engagement.
Using a different research model may provide insight into how research models align to
standards or support instruction. This study would also benefit from replication in
different K-12 settings (private, charter, or early college). A significant influence during

77
the early development stage for this study was the work conducted by the Project
Information Literacy group from Washington State University. An ideal opportunity
would be to have this study become a part of the PIL resources focused on building a
base of knowledge and spurring a conversation toward PK-12 IL education within this
domain or seek an agency that could provide a similar platform. In addition, it is my hope
that this study creates opportunities to influence how the NCDPI treats IL. It is
recommended that NCDPI prioritize IL as a scaffold and taught curriculum area
throughout all grade levels. While following the recommendations from Common Core to
embed IL can address this need, a clear plan for assessment must also be embraced. It is
my recommendation that NCDPI use the existing assessment and benchmark process to
include items that can be measured and desegregated specifically for IL.
Conclusion
The results of this study showed that direct instruction was significant in helping
students gain IL knowledge. The data also indicate that having at least 400 minutes of
dedicated time to provide instruction is key to helping students attain this knowledge. The
perceptions demonstrated by students and stakeholders illustrate an awareness of the
importance of IL and need for additional instruction. As an essential skill, IL must be
included in existing assessment opportunities to gain insight into “if” and “how” these
skills are being embedded into general curriculum. In developing a goal to create lifelong
learners, a curriculum must be designed that successfully integrates IL as scaffolded
knowledge taught throughout the K-12 experience. As educators and leaders continue to
define policies and practices for student growth and development, IL must be included at

78
a higher level equal to other literacies focused on preparing students to be career and
college ready.

79
REFERENCES

Addison, C., & Meyers, E. (2013). Perspectives on information literacy: A framework for
conceptual understanding. Information Research, 18(3), paper C27. Retrieved
from http://InformationR.net/ir/18-3/colis/paperC27.html
American Association of School Librarians. (2007). AASL standards for the 21st-century
learner. Chicago, IL: Author. Retrieved from http://www.ala.org/aasl/sites/
ala.org.aasl/files/content/guidelinesandstandards/learningstandards/AASL_
LearningStandards.pdf
American Association of School Librarians. (2009). AASL standards for the 21st-century
learner in action. Chicago, IL: Author.
American Association of School Librarians. (2011). Learning standards & Common
Core State Standards crosswalk. Retrieved from http://www.ala.org/aasl/
standards-guidelines/crosswalk
American Library Association. (1989, January 10). Presidential committee on
information literacy: Final report. Chicago, IL: Author. Retrieved from
http://www.ala.org/acrl/publications/whitepapers/presidential
American Library Association. (1998). Information standards for student learning. In
American Association of School Librarians, Information power: Building
partnerships for learning (pp. 8–44). Chicago, IL: Author.
Association of College & Research Libraries. (2000, January 18). Information literacy
competency standards for higher education. Retrieved from
http://home.ubalt.edu/ub78l45/My%20Library/storage/QQD324ZP/
informationliteracycompetency.html
Canvas Instructure. (2016). Canvas. Retrieved from https://www.canvaslms.com/
Carey, J. O. (1998). Library skills, information skills, and information literacy:
Implications for teaching and learning. School Library Media Research, 5.
Retrieved from http://www.ala.org/aasl/sites/ala.org.aasl/files/content/
aaslpubsandjournals/slr/vol1/SLMR_LibrarySkills_V1.pdf
Carnegie Foundation for the Advancement of Teaching. (2016). Our ideas: Using
improvement science to accelerate learning and address problems of practice.
Retrieved from http://www.carnegiefoundation.org/our-ideas/
Cazden, C., Cope, B., Fairclough, N., Gee, J., Kalantzis, M., Kress, G., & Nakata, M.
(1996). A pedagogy of multiliteracies: Designing social futures. Harvard
Educational Review, 66(1), 60–92.

80
Central Michigan University. (2016). Research Readiness Self-Assessment/3.1. Retrieved
from http://rrsa.cmich.edu/cgi-bin/rrsalib.cgi/
Charlotte-Mecklenburg Schools. (2009, November 4). Strategic plan 2014. Retrieved
from http://www.cms.k12.nc.us/mediaroom/strategicplan2014/Pages/default.aspx
Chu, C., Yeung, A., & Chu, S. (2012). Assessment of students’ information literacy: A
case study of a secondary school in Hong Kong. Retrieved from http://citers2012.
cite.hku.hk/en/paper_553.htm
Common Core State Standards Initiative. (2012). Common Core State Standards
Initiative. Retrieved from http://www.corestandards.org/the-standards
Eisenberg, M., & Berkowitz, B. (1990). A Big6™ skills overview. Retrieved from
http://big6.com/pages/about/big6-skills-overview.php
Eubanks, J. (2014). Potential ramifications of Common Core State Standards adoption on
information literacy. Communications in Information Literacy, 8(1), 23–31.
Farmer, L., & Henri, J. (2008). Information literacy assessment in K-12 settings. Lanham,
MD: Scarecrow Press.
Gross, M., & Latham, D. (2007). Attaining information literacy: An investigation of the
relationship between skill level, self-estimates of skill, and library anxiety.
Library & Information Science Research (07408188), 29(3), 332–353.
doi:10.1016/j.lisr.2007.04.012
Head, A. J. (2012, October). Learning curve: How college graduates solve information
problems once they join the workplace. SSRN Electronic Journal, 2012, 1–38.
doi:10.2139/ssrn.2165031
Head, A. (2013a). Learning the ropes: How freshmen conduct course research once they
enter college. Project Information Literacy. University of Washington. Retrieved
from http://projectinfolit.org/images/pdfs/pil_2013_freshmenstudy_fullreport.pdf
Head, A. (2013b). Project Information Literacy: What can be learned about the
information-seeking behavior of today’s college students? Association of College
and Research Libraries Proceedings, 2013, 472–482. Retrieved from
http://www.ala.org/acrl/sites/ala.org.acrl/files/content/conferences/
confsandpreconfs/2013/papers/Head_Project.pdf
Head, A., & Eisenberg, M. (2011). How college students use the web to conduct
everyday life research. First Monday, 16(4), 1–23. Retrieved from
http://journals.uic.edu/ojs/index.php/fm/article/view/3484/2857
IFLA/UNESCO. (2002). School library guidelines. Retrieved from http://www.ifla.org/
publications/the-iflaunesco-school-library-guidelines-2002

81
James Madison University, & Madison Assessment. (2010). Information Literacy Test.
Retrieved from http://www.madisonassessment.com/assessment-testing/
information-literacy-test/
James-Maxie, D. (2007). Information literacy skills in elementary schools: A review of
the literature. Journal of Instruction Delivery Systems, 21(1), 23–26.
Kent State University Libraries. (2014). TRAILS: Tool for Real-time Assessment of
Information Literacy Skills. Retrieved from http://www.trails-9.org/
Kovalik, C., Yutzey, S., & Piazza, L. (2013). Information literacy and high school
seniors: Perceptions of the research process. School Library Research, 16, 1–26.
Retrieved from http://www.ala.org/aasl/sites/ala.org.aasl/files/content/
aaslpubsandjournals/slr/vol16/SLR_Information_Literacy_High_School_Seniors_
V16.pdf
Lawrence, J. (2013, May 23). Gap between perception and reality in college readiness
remains wide. Education News. Retrieved from http://www.educationnews.org/
higher-education/gap-between-perception-and-reality-in-college-readinessremains-wide
Loertscher, D., & Woolls, B. (1997). The information literacy movement of the school
library field: A preliminary summary of the research. Paper presented at the
Annual Conference of the International Association of School Librarianship Held
in Conjunction with the Association for Teacher-Librarianship, Vancouver,
British Columbia, Canada. Retrieved from http://files.eric.ed.gov/fulltext/
ED412972.pdf
McCarthy, C. A. (2003). Alice Yucht’s FLIP it!: An information literacy framework that
really works for all ages! School Library Media Activities Monthly, 19(7), 22–
23+. Retrieved from http://search.proquest.com/docview/237133390?accountid=
13217
Milam, P. (2002). Moving beyond technology with strategic teaching: Jamie McKenzie’s
Research Cycle. School Library Media Activities Monthly, 19(4), 22–23, 34.
Miles, M., Huberman, A., & Saldaña, J. (2014). Qualitative data analysis: A methods
sourcebook. Thousand Oaks, CA: Sage.
National Forum on Information Literacy. (1989). Information literacy skills. Retrieved
from http://infolit.org/information-literacy-projects-and-programs/
NC School Report Card. (2014). Charlotte-Mecklenburg Schools. Retrieved from
http://www.ncpublicschools.org/src/
North Carolina Department of Public Instruction. (2006). IMPACT: Guidelines for North
Carolina media and technology programs. Retrieved from
http://www.ncwiseowl.org/Impact/

82
North Carolina Department of Public Instruction. (2007). An overview of 21st century
skills in North Carolina. Retrieved from http://www.ncpublicschools.org/docs/
profdev/resources/skills/overview.pdf
North Carolina Department of Public Instruction. (2011). Information and Technology
Essential Standards. Retrieved from http://www.ncpublicschools.org/dtl/
standards/ites/
North Carolina Department of Public Instruction. (2014a). North Carolina Testing
Program Overview. Retrieved from http://www.ncpublicschools.org/docs/
accountability/1415testoverview.pdf
North Carolina Department of Public Instruction. (2014b). The North Carolina
graduation project. Retrieved from http://www.ncpublicschools.org/docs/
accountability/testing/eoc/gradproject14.pdf
North Carolina Department of Public Instruction. (2015a). Accountability and testing
results for 2014-15 state, district, and school level summary data. Division of
Accountability Services. Retrieved from
http://www.ncpublicschools.org/accountability/reporting/
North Carolina Department of Public Instruction. (2015b). NCEdCloud IAM Service.
Retrieved from http://www.dpi.state.nc.us/homebase/faq/overall/#nced-cloud
North Carolina Department of Public Instruction. (2015c). New learning management
system convenience contract [Letter written March 27, 2015 to Superintendents
and Charter School Directors]. Retrieved from http://dtlr8.ncdpi.wikispaces.net/
file/view/New+Learning+Management+System+(LMS)+Contract.pdf
North Carolina Department of Public Instruction. (2015d). Understanding North
Carolina’s annual measurable objectives. Division of Accountability Services.
Retrieved from http://www.ncpublicschools.org/docs/accountability/
reporting/amoinfo15.pdf
Pappas, M., & Tepe, A. E. (2002). Pathways to knowledge and inquiry learning.
Greenwood Village, CO: Libraries Unlimited.
Park, S., & Takahashi, S. (2013). 90-day cycle handbook. Retrieved from
http://www.carnegiefoundation.org/wp-content/uploads/2014/09/90DC_
Handbook_external_10_8.pdf
The Deming Institute. (2014). The Plan, Do, Study, Act (PDSA) cycle. Retrieved from
https://deming.org/theman/theories/pdsacycle
The Partnership for 21st Century Skills. (2002). Retrieved from http://www.p21.org/
Pinto, M., Cordon, J. A., & Gomez Diaz, R. (2010). Thirty years of information literacy
(1977–2007): A terminological, conceptual and statistical analysis. Journal of

83
Librarianship and Information Science, 42(1), 3–19.
doi:10.1177/0961000609345091
Project Information Literacy. (2016). What is PIL? Seattle, WA: University of
Washington, Information School. Retrieved from http://projectinfolit.org/about
Race to the Top fund. (2011). Race to the Top Fund. Retrieved from http://www2.ed.gov/
programs/racetothetop/index.html
Salem, J. (2014). The development and validation of all four TRAILS (Tool for Real-Time
Assessment of Information Literacy Skills) tests for k-12 students. (Electronic
Thesis or Dissertation). Retrieved from https://etd.ohiolink.edu/
S.O.S. for Information Literacy. (2014). Retrieved from http://www.informationliteracy.
org/
South Mecklenburg High: Overview. (2015). Retrieved from http://www.usnews.com/
education/best-high-schools/north-carolina/districts/charlotte-mecklenburgschools/south-mecklenburg-high-14573
Warner, R. (2013). Applied statistics: From bivariate through multivariate techniques
(2nd ed.). Thousand Oaks, CA: Sage Publications, Inc.
Western Carolina University, & Washburn, K. (2015, November 10). Qualtrics Survey
Software, CMSINFOLIT Insight (Version 1.145s) [Online Computer software].
Retrieved from https://wcu.az1.qualtrics.com/jfe6/form/
SV_3qHz6nPAiMSYYyV
Wichita State University Libraries. (2014). EMPOWER: Information Literacy. Retrieved
from http://library.wichita.edu/empower/supplementalscreens/modules.htm
Wolf, S., Brush, T., & Saye, J. (2003). The big six information skills as a metacognitive
scaffold: A case study. School Library Media Research, 6. Retrieved from
http://www.ala.org/aasl/sites/ala.org.aasl/files/content/aaslpubsandjournals/slr/
vol6/SLMR_BigSixInfoSkills_V6.pdf
Zimmerman, B. (2016). Make Beliefs Comix! Online educational comic generator for
kids of all ages. Retrieved from http://www.makebeliefscomix.com/

84
APPENDICES

APPENDIX A: PROJECT INFORMATION LITERACY INFOGRAPH
APPENDIX B: FRAMEWORK FOR 21ST CENTURY LEARNING INFOGRAPHIC
APPENDIX C: PRE-ASSESSMENT SAMPLE SCORE REPORT
APPENDIX D: CMSINFOLIT CANVAS LMS MODULES SCREENSHOTS
APPENDIX E: CMSINFOLIT CANVAS MODULES CONTENT OUTLINE
APPENDIX F: CMSINFOLIT COURSE MODULE REFERNCE LIST
APPENDIX G: CMSINFOLIT WEEBLY WEBPAGE
APPENDIX H: STUDENT PERCEPTION SURVEY RESULTS

85
APPENDIX A: PROJECT INFORMATION LITERACY INFOGRAPH

Figure 13. Project Information Literacy infographic (2016).

86
APPENDIX B: FRAMEWORK FOR 21ST CENTURY LEARNING INFOGRAPHIC

Figure 14. P21 Framework for 21st Century Learning.

87
APPENDIX C: PRE-ASSESSMENT SAMPLE SCORE REPORT

Information Literacy TRAILS9 Score Report for:
How are scores determined?
The five information literacy strand areas (Develop
Topic, Identify Sources, Search Strategies, Evaluate
Resources and Information, and Use Information
Responsibly and Ethically) have six questions within
each assessment area.
If a student answered all questions correctly, the score
in that strand would be 100%.

Score:
100
83
66
50
33
17
0

0000
Calculated based upon:
6 out of 6 correct
5 out of 6 correct
4 out of 6 correct
3 out of 6 correct
2 out of 6 correct
1 out of 6 correct
0 out of 6 correct

Results for the TRAILS9 Assessment Test for October 2, 2015
Develop Topic: Recognize need for information to address assignment. Develop questions to clarify
and focus topic. Identify individuals and resources to help develop manageable topic based on the
parameters of an assignment. Recognize the hierarchical relationships of broader and narrower topics
to aid in revising the topic.

% Correct: 50
Identify Sources: Understand information comes in various forms: textual, visual, audio, or data.
Appreciate that each form offers differing types of information sources produced in a variety of formats
(e.g., print or electronic books, film or streaming video). Understand the roles and limitations of differing
types of information sources and the finding tools needed to access them (e.g., libraries, search
engines, online catalogs). Select the most appropriate information sources and finding tools to address
a given information need.

% Correct: 83
Search Strategies: Create and revise search strategies. Understand how to use the features of an
information source in order to retrieve the information needed (e.g., index and table of contents in a
book, database filters). Develop a search strategy fitting for the given finding tool. Choose appropriate
terms and keywords for searching a topic. Understand how to use search expanders and search
limiters (e.g., logical operators) when too few, too many, or irrelevant results are returned.

% Correct: 67
Evaluate Resources and Information: Be able to determine the currency, relevance, authority,
accuracy, and purpose of information or information sources. Recognize divergent perspectives.
Recognize bias. Differentiate between fact and opinion.

% Correct: 33
Use Information Responsibly and Ethically: Understand the concepts of intellectual property
(especially copyright, fair use, and plagiarism) and of intellectual freedom. Understand how to cite and
list sources using an appropriate style manual. Recognize how to take notes and paraphrase correctly.

% Correct: 83

Total Overall Score (% Correct): 63
Assessment and Report Information from TRAILS: Tool for Real-time Assessment of Information Literacy Skills
Copyright © 2015 Kent State University Libraries

Figure 15. TRAILS pre-assessment score report sample.

88
APPENDIX D: CMSINFOLIT CANVAS LMS MODULES SCREENSHOTS

Intervention Instructional Resource: Online Course for this study (CMSINFOLIT)
Screenshots

Figure 16. CMSINFOLIT Canvas course screenshot main webpage.

89

Figure 17. CMSINFOLIT Canvas course screenshot overview.

Figure 18. CMSINFOLIT Canvas course screenshot #1.

90

Figure 19. CMSINFOLIT Canvas course screenshot #2.

Figure 20. CMSINFOLIT Canvas course screenshot #3.

91

Figure 21. CMSINFOLIT Canvas course screenshot #4.

Figure 22. CMSINFOLIT Canvas course screenshot #5.

92

Figure 23. CMSINFOLIT Canvas course screenshot #6.

Figure 24. CMSINFOLIT Canvas course screenshot #7.

93

Figure 25. CMSINFOLIT Canvas course screenshot #8.

Figure 26. CMSINFOLIT Canvas course screenshot #9.

94

Figure 27. CMSINFOLIT Canvas course screenshot #10.

Figure 28. CMSINFOLIT Canvas course screenshot #11.

95

Figure 29. CMSINFOLIT Canvas course screenshot #12.

Figure 30. CMSINFOLIT Canvas course screenshot #13.

96

Figure 31. CMSINFOLIT Canvas course screenshot #14.

Figure 32. CMSINFOLIT Canvas course screenshot #15.

97

Figure 33. CMSINFOLIT Canvas course screenshot #16.

Figure 34. CMSINFOLIT Canvas course screenshot #17.

98

Figure 35. CMSINFOLIT Canvas course screenshot #18.

Figure 36. CMSINFOLIT Canvas course screenshot #19.

99

Figure 37. CMSINFOLIT Canvas course screenshot #20.

Figure 38. CMSINFOLIT Canvas course screenshot #21.

100

Figure 39. CMSINFOLIT Canvas course screenshot #22.

Figure 40. CMSINFOLIT Canvas course screenshot #23.

101

Figure 41. CMSINFOLIT Canvas course screenshot #24.

Figure 42. CMSINFOLIT Canvas course screenshot #25.

102

Figure 43. CMSINFOLIT Canvas course screenshot #26.

Figure 44. CMSINFOLIT Canvas course screenshot #27.

103

Figure 45. CMSINFOLIT Canvas course screenshot #28.

Figure 46. CMSINFOLIT Canvas course screenshot #29.

104

Figure 47. CMSINFOLIT Canvas course screenshot #30.

Figure 48. CMSINFOLIT Canvas course screenshot #31.

105

Figure 49. CMSINFOLIT Canvas course screenshot #32.

Figure 50. CMSINFOLIT Canvas course screenshot #33.

106

Figure 51. CMSINFOLIT Canvas course screenshot #34.

Figure 52. CMSINFOLIT Canvas course screenshot #35.

107
APPENDIX E: CMSINFOLIT CANVAS MODULES CONTENT OUTLINE

Table 7
CMSINFOLIT Canvas Module Course Content
Module Page/Section Number
Module Home Page
Course Overview

Assignments
Announcements
Resources
Contact Info.
Module 1
Module 1.2

Module 1.3

Module 1 Assignment
Module 2

Module 2.2

Module 2 Assignment
Module 2.3

Module 2 Quiz

Description
Information Literacy Skills Training: Module
Resource Buttons and Links
 Course Summary
 About the Instructor
Syllabus
List of Assignments
Posted Course Announcements
List and Hyperlinks to Additional Resources to
Support Information Literacy Instruction
Contact Information for Kevin Washburn
Introduction
MakeBeliefsComix created.
Defining Information Literacy
 Definition for information literacy from the
American Library Association
Definition of information literacy from the State
University of New York, Plattsburg
 Mike Eisenberg Vodcast #1: “What is
Information Literacy?” From ABC-Clio found
on SchoolTube
Big6™ Image: 1. Task Definition
 Answer information literacy questions
What is Inquiry?
 Big6™ Image: 2. Information seeking strategies
 Statement from ACRL Framework
 Big6™ Handout: Inquiry Learning
Inquiry
Choosing Your Topic (Links to an external site.)
online learning activity from Wichita State
University Library: EMPOWER website
Provide an example for how to define a topic based
upon broad, narrow, to specific criteria
Refining the Question
Kent University Recommended video from Calgary
University: Developing a good research question.
Define the question
Two practice questions modeled from the TRAILS

108
Module Page/Section Number
Module 3

Module 3.2

Module 3.3

Module 3.4

Module 3: Assignment
Module 3 Quiz

Module 4.1

Module 4.2

Module 4.3

Module 4.4

Module 4 Quiz

Module 4 Assignment

Description
database.
Working with Sources
 EMPOWER - Starting Your Research:
Webpages 2 - 7
Big6™ Image: 3. Location and access
Finding Sources
 Kent State University Libraries: Transitioning to
college website
Resource webpage from the Charlotte Mecklenburg
Library
Primary and Secondary Sources
Primary sources on the web: Finding, evaluating,
using webpage from the Reference and User
Services Association (ALA)
Locating Sources
Using CMS Follett Destiny online library catalog
system
List one useful source that can be used for research
not already listed in CMSINFOLIT
Identify and Search for Potential Sources
Three practice questions modeled from the TRAILS
database.
Using Informational Resources
 MakeBeliefsComix created
 Purdue Online Writing Lab webpage for
Searching the World Wide Web: Overview
Easybib recommendations for searching websites
Searching Periodicals
 EBSCOHost tutorial video: Using the advanced
search feature
Big6™ Image: 4. Use of Information
Selection
 Easybib introduction website
SchoolTube Video: How to create a project in
Easybib
Additional Easybib Information
 Easybib School Edition Webinar video
 Website from Nancy Florio, librarian at The
Canterbury School
Gale Cengage Learning: Handout for using Easybib
Develop, use, and revise search strategies
Six practice questions modeled from the TRAILS
database.
Using NC WiseOwl for the Student Research

109
Module Page/Section Number

Module 5

Module 5.2

Module 5.3

Module 5 Quiz

Module 6.1

Module 6.2

Description
database, find articles on Global Warming, use
broad to narrow strategies and count number of
returns.
Synthesis
 Video from James Madison University:
Research Toolkit - Using Information
 Big6™ Image: 5. Synthesis
 Flicker Evaluate Image
 John Hopkins University Sheridan Library
website for Evaluating Information
 Virginia Tech University Libraries website for
Evaluating Internet Information
 Purdue Online Writing Labs website for
Evaluating During Reading
Penn State University Libraries website for How to
Evaluate Information
Next Step: Organizing Your Research from Multiple
Sources
 Easybib Notebook Overview
 Duke University: Writing Studio PDF on
Organizing a Research Project
University of Maryland University College Online
Guide to Writing and Research
Responsibility
 Copyright symbol image from Flickr
 Copyright definition from the U.S. Copyright
Office
 EMPOWER Citing Sources - Plagiarism:
Webpages 4 - 8
 Copyright for Students from NC WiseOwl
Interactive copyright website from the Library of
Congress
Evaluate sources and information
Four practice questions modeled from the TRAILS
database.
Assessing: Evaluation of your work
 Research paper process image from Flickr
 Big6™ Image: 6. Evaluation
Big6™ Checklist for a writing assignment
Sharing
 Research Project Calculator
 CMS Graduation Project: Presentation
Guidelines
 CMS Graduation Project: Portfolio

110
Module Page/Section Number

Module 6 Quiz

Module 6: Assignment
End of Course Review

Completed Module

Description
North Carolina Service Learning Wiki: NC
Graduation Project
Use information responsibly, ethically, and legally
Three practice questions modeled from the TRAILS
database.
Define copyright
 Interactive Information Literacy from the
Institute for research and Innovation in Social
Services
 CMSINFOLIT Module Resource Page
 Big6™ Evaluate Your Skills Worksheet
MakeBeliefComix illustration
 Badge Recognition

111
APPENDIX F: CMSINFOLIT COURSE MODULE REFERENCE LIST

Below is a list of resources that were used within the CMSINFOLIT Canvas modules as
curriculum and content related resources.

ABC Clio. (2011, March 23). Mike Eisenberg Vodcast #1: What is information literacy?
Retrieved from http://bit.ly/WZCqy5
Association of College and Research Libraries. (2015, February 2). Framework for
information literacy for higher education. Retrieved from
http://www.ala.org/acrl/standards/ilframework
Badke, W., Baer, R., & University of Calgary. (2008, June 4). Developing a good
research question. Retrieved from https://youtu.be/vK6_U4SCZSc
Canvabadges. (2015). Retrieved from https://www.canvabadges.org/
Canvas by Instructure. (2016). Retrieved from https://www.canvaslms.com/
Charlotte Mecklenburg Library. (2015). Resources. Retrieved from
https://www.cmlibrary.org/resources
Charlotte-Mecklenburg Schools. (2014, April 29). Graduation project: The presentation
guidelines. Retrieved from http://www.cms.k12.nc.us/cmsdepartments/ci/gradproject/Pages/ThePresentation-Guidelines.aspx
Charlotte-Mecklenburg Schools. (2014, April 29). Graduation project: The portfolio
components. Retrieved from http://www.cms.k12.nc.us/cmsdepartments/ci/
grad-project/Pages/ThePortfolio-Components.aspx
Easybib: Imagine Easy Solutions LLC. (2013, October 31). School edition webinar.
Retrieved from https://youtu.be/B5hE413tUbM
Easybib: Imagine Easy Solutions LLC. (2014, November 25). General notebook
overview. Retrieved from http://imagineeasy.freshdesk.com/support/solutions/
articles/4000036391-general-notebook-overview
Easybib: Imagine Easy Solutions LLC. (2015a). EasyBib: Introductory tutorial.
Retrieved from http://www.easybib.com/help/intro
Easybib: Imagine Easy Solutions LLC. (2015b). Research: Software tools, tips, and
techniques. Retrieved from http://content.easybib.com/students/writing-guide/iiresearch/d-software-tools-tips-and-techniques/

112
EBSCO Support. (2015, August 19). EBSCOhost advanced searching: Tutorial.
Retrieved from https://youtu.be/kT1kzWfWxiE
Edwards, D. (2009, June 24). NC service learning wiki: North Carolina graduation
project. Retrieved from http://ncservicelearning.pbworks.com/w/page/
7128938/North%20Carolina%20Graduation%20Project
Eisenberg, M., & Berkowitz, B. (1990). A Big6™ skills overview. Retrieved from
http://big6.com/pages/about/big6-skills-overview.php
Eisenberg, M., & Berkowitz, R. (1996a). Big6™ coloring pages: 1. Task definition
[Worm and apple design]. Retrieved from http://big6.com/media/freestuff/
lwormtrans1.gif
Eisenberg, M., & Berkowitz, R. (1996b). Big6™ coloring pages: 2. Information seeking
strategies [Worm and apple design]. Retrieved from http://big6.com/media/
freestuff/lwormtrans2.gif
Eisenberg, M., & Berkowitz, R. (1996c). Big6™ coloring pages: 3. Location and access
[Worm and apple design]. Retrieved from http://big6.com/media/freestuff/
lwormtrans3.gif
Eisenberg, M., & Berkowitz, R. (1996d). Big6™ coloring pages: 4. Use of information
[Worm and apple design]. Retrieved from http://big6.com/media/freestuff/
lwormtrans4.gif
Eisenberg, M., & Berkowitz, R. (1996e). Big6™ coloring pages: 5. Synthesis [Worm and
apple design]. Retrieved from http://big6.com/media/freestuff/lwormtrans5.gif
Eisenberg, M., & Berkowitz, R. (1996f). Big6™ coloring pages: 6. Evaluation [Worm
and apple design]. Retrieved from http://big6.com/media/freestuff/
lwormtrans6.gif
Eisenberg, M., & Berkowitz, R. (2014, January 28). Inquiry learning Big6™-style: It all
starts with asking great questions! Retrieved from http://big6.com/media/
Eisenberg-Berkowitz-Inquiry%20Learning%20Questioning%20Webinar%20
2014(1).
Follett Destiny locations for Charlotte-Mecklenburg Schools. (2015, March 5). Retrieved
from https://char-meck.follettdestiny.com/common/welcome.jsp?context=saas52_
3215696
Institute for Research and Innovation in Social Services. (2010). Information literacy
interactive tutorial. Retrieved from http://content.iriss.org.uk/informationliteracy/
index.html

113
James Madison University Libraries. (2014, August 23). Madison research essential
toolkit: Using information. Retrieved from https://www.youtube.com/
watch?v=85I2kFR15Bs&feature=youtu.be
Jansen, B. (2003, April 30). Check list for a writing assignment: Grades 7–12. Retrieved
from http://big6.com/media/files/CheckList_Writing_7-12.
Jansen, B., & Berkowitz, R. (2003, September 24). Evaluate your research skills using
the Big6™. Retrieved from http://big6.com/media/files/Big6_Evaluation.pdf
John Hopkins University: The Sheridan Libraries. (2015). Evaluating information.
Retrieved from http://guides.library.jhu.edu/evaluatinginformation
Kent State University Libraries. (2015). Transitioning to college: Identify potential
sources. Retrieved from http://libguides.library.kent.edu/c.php?g=278041&p=
1855101
Library of Congress. (2015, September 14). Taking the mystery out of copyright.
Retrieved from http://www.loc.gov/teachers/copyrightmystery/
Longley, D. (2009, May 22). The research paper process [Digital image file from Flickr,
Creative Commons License]. Retrieved from https://secure.flickr.com/photos/
20724275@N03/3553821535
NC WiseOwl. (2012, June 29). Copyright for students. Retrieved from
http://www.ncwiseowl.org/zones/copyright/default.htm
North Carolina Department of Public Instruction. (2014). The North Carolina graduation
project. Retrieved from http://www.ncpublicschools.org/docs/accountability/
testing/eoc/gradproject14.pdf
North Carolina Department of Public Instruction. (2015). NCEdCloud IAM Service.
Retrieved from http://www.dpi.state.nc.us/homebase/faq/overall/#nced-cloud
Penn State University Libraries. (2013, May 22). How to evaluate information. Retrieved
from https://www.libraries.psu.edu/psul/lls/students/research_resources/evaluate_
info.html
Purdue Online Writing Lab. (2013, February 13). Evaluation during reading. Retrieved
September 6, 2015, from https://owl.english.purdue.edu/owl/resource/553/03/
Purdue Online Writing Lab. (2015). Searching the World Wide Web: Overview.
Retrieved from https://owl.english.purdue.edu/owl/resource/558/01/
Reference and User Services Association (a division of the American Library
Association). (2015). Primary sources on the web: Finding, evaluating, using.

114
Retrieved from http://www.ala.org/rusa/sections/history/resources/pubs/
usingprimarysources
Retrokatz. (2013, August 4). Evaluate [Dalek Cyborg Image (Creative Commons License
Use)]. Retrieved from https://www.flickr.com/photos/retrokatz/9434470484
Ricker, J. (2011, December 12). Easybib instructions from Gale Cengage Learning.
Retrieved from http://www.galesites.com/uploads/
cd3650d5830c7a89a305d436cbe12ce12766
SchoolTube. (2013, February 26). How to create a new project in EasyBib. Retrieved
from http://bit.ly/XYZDnJ
S.O.S. for Information Literacy. (2014). Retrieved from http://www.informationliteracy.
org/
State University of New York, Plattsburg. (2016). Definition of information literacy.
Retrieved from http://www.plattsburgh.edu/library/instruction/
informationliteracydefinition.php
Seyfang, M. (2008, November 11). Copyright symbols [Digital image file from Flickr,
Creative Commons License]. Retrieved from https://secure.flickr.com/photos/
31477768@N00/3020966268
Tools for real-time assessment of information literacy skills (TRAILS). (2014). Retrieved
from http://www.trails-9.org/
U.S. Copyright Office. (2008, July 1). Information circular 1a. Retrieved from
http://copyright.gov/circs/circ1a.html
University of Maryland University College. (2011). Online guide to writing and research.
Retrieved from https://www.umuc.edu/writingcenter/onlineguide/tutorial/
chapter4/ch4-16.html
University of Minnesota. (2015, August 10). Research project calculator. Retrieved from
https://rpc.elm4you.org/
Virginia Tech University Libraries. (2010, April 21). Evaluating internet information.
Retrieved from http://www.lib.vt.edu/instruct/evaluate/
Western Carolina University, & Washburn, K. (2015, November 10). Qualtrics Survey
Software, CMSINFOLIT Insight (Version 1.145s) [Online Computer software].
Retrieved from https://wcu.az1.qualtrics.com/jfe6/form/
SV_3qHz6nPAiMSYYyV

115
Wichita State University Libraries. (2011a). EMPOWER: Choosing your topic. Retrieved
from http://library.wichita.edu/empower/module2/choosingYourTopic.htm
Wichita State University Libraries. (2011b). EMPOWER—Citing Sources: Plagiarism.
Retrieved from http://library.wichita.edu/empower/module6/Plagiarism.htm
Wichita State University Libraries. (2011c). EMPOWER—Starting your research.
Retrieved from http://library.wichita.edu/empower/module1/
informationSources.htm
Writing Studio. (2009, May 21). Organizing a research project. Durham, NC: Duke
University. Retrieved from https://twp.duke.edu/uploads/assets/
research_project.pdf
Zimmerman, B. (2016). Make Beliefs Comix! Online educational comic generator for
kids of all ages. Retrieved from http://www.makebeliefscomix.com/

116
APPENDIX G: CMSINFOLIT WEEBLY WEBPAGE

Figure 53. CMSINFOLIT Home Weebly Webpage.

APPENDIX H: STUDENT PERCEPTION SURVEY RESULTS

Table 8
Student Perception Survey Questions and Results

Item #

Agree

Somewhat
Agree

Neither

Somewhat
Disagree

Disagree

Total
Responses

Mean

Question 1. Information literacy is an essential skill area that enables me to successfully conduct research.
1

49

31

9

1

2

92

1.65

Question 2. The TRAILS: Tools for Assessing Information Literacy Skills test is a good indicator of knowledge for information
literacy.
2

40

28

18

3

3

92

1.92

Question 3. The training and resources that I received after the first assessment using the TRAILS test increased my knowledge
on the process for developing a research topic.
3

33

37

14

3

4

91

1.99

Question 4. Examining all of the potential elements, factors, keywords, and historical context to a topic can provide ideas.
4

50

29

8

2

3

92

1.68

Question 5. The training and resources that I received after the first assessment using the TRAILS test increased my knowledge
on locating valid source information.
5

41

30

14

3

4

92

1.90

117

Table 8
Cont.

Item #

Agree

Somewhat
Agree

Neither

Somewhat
Disagree

Disagree

Total
Responses

Mean

Question 6. Using authoritative and scholarly books, databases, and publications increases the potential for locating relevant
and accurate information.
6

44

27

14

5

2

92

1.85

Question 7. The training and resources that I received after the first assessment using the TRAILS test increased my knowledge
on utilizing successful search strategies.
7

38

31

15

6

1

91

1.91

Question 8. Conducting research in an organized manner as part of an overall plan increases the potential for success.
8

52

24

12

3

1

92

1.66

Question 9. The training and resources that I received after the first assessment using the TRAILS test increased my knowledge
for selecting the best sources for information.
9

38

30

18

3

3

92

1.95

Question 10. Finding relevant primary resources is a critical component for successfully conducting research.
10

48

31

11

1

1

92

1.65

Question 11. The training and resources that I received after the first assessment using the TRAILS test increased my knowledge
on the responsible, ethical, and legal use of information.
11

33

30

23

3

2

91

2.02

118

Table 8
Cont.

Item #

Agree

Somewhat
Agree

Neither

Somewhat
Disagree

Disagree

Total
Responses

Mean

Question 12. I have an obligation to cite and follow recommended practices for using source information as part of an overall
research process.
12

49

22

17

3

1

92

1.75

Question 13. I value the training that I received after the first information literacy assessment and time well spent.
13

33

34

18

5

2

92

2.01

Question 14. Expanding my information skill knowledge will better enable me to conduct research and solve problems.
14

41

32

12

4

2

91

1.84

4

91

1.93

Question 15. I feel more prepared to complete my graduation research project.
15

41

27

15

4

119

