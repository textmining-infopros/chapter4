FLORIDA STATE UNIVERSITY
COLLEGE OF COMMUNICATION AND INFORMATION

A NEW FRAMEWORK OF WEB CREDIBILITY ASSESSMENT AND
AN EXPLORATORY STUDY OF OLDER ADULTS’ INFORMATION BEHAVIOR
ON THE WEB

By
WONCHAN CHOI

A Dissertation submitted to the
School of Information
in partial fulfillment of the
requirements for the degree of
Doctor of Philosophy

2015

ProQuest Number: 3724204

All rights reserved
INFORMATION TO ALL USERS
The quality of this reproduction is dependent upon the quality of the copy submitted.
In the unlikely event that the author did not send a complete manuscript
and there are missing pages, these will be noted. Also, if material had to be removed,
a note will indicate the deletion.

ProQuest 3724204
Published by ProQuest LLC (2015). Copyright of the Dissertation is held by the Author.
All rights reserved.
This work is protected against unauthorized copying under Title 17, United States Code
Microform Edition © ProQuest LLC.
ProQuest LLC.
789 East Eisenhower Parkway
P.O. Box 1346
Ann Arbor, MI 48106 - 1346

Wonchan Choi defended this dissertation on July 7, 2015.
The members of the supervisory committee were:

Besiki Stvilia
Professor Directing Dissertation
Neil Charness
University Representative
Kathleen Burnett
Committee Member
Lorraine Mon
Committee Member

The Graduate School has verified and approved the above-named committee members, and
certifies that the dissertation has been approved in accordance with university requirements.

ii

TO MY PARENTS

iii

ACKNOWLEDGMENTS
I am a blessed person who has met great teachers in my journey in academics. My sincerest
gratitude goes to my major professor Dr. Besiki Stvilia for his great support throughout my Ph.D.
course at Florida State University (FSU). He is a mentor of my life who has not only taught me
how to research, but also shown me how much an advisor can mean to a student who is
preparing his or her future in the Ph.D. program. When I get the opportunity to guide my own
students in the future, I will always think back to the days I was under his mentorship and remind
myself of how he guided me. My thanks also go to my other committee members, Dr. Kathleen
Burnett, Dr. Lorraine Mon, and Dr. Neil Charness, for their support and insightful feedback,
which have enabled me to complete my Ph.D. program including my dissertation project. It was
such great luck to have them as my advisory committee in my Ph.D. program.
I also would like to extend my appreciation to my professors at Pusan National
University (PNU) in Korea. My special thanks go to Dr. Durk Hyun Chang, who was my advisor
in the Master’s program at PNU, for his continuous support and encouragement.
I thank my faculty, staff, and fellow doctoral students in the School of Information at
FSU, as well as my friends outside of the school for their intellectual, emotional, and social
support, without which I would have never been able to survive this long journey.
My sincere gratitude goes to my research participants. They willingly spent their time
with me, sharing their thoughts and experience regarding my research topic, which enabled me to
answer my research questions in the dissertation project.
Last but not least, my very special thanks go to my parents and family. I would like to
express my deepest love and respect to my father, Kiseok Choi, and my mother, Jiwon Park, for
their endless love and support.
iv

TABLE OF CONTENTS
List of Tables ...................................................................................................................................x
List of Figures ............................................................................................................................... xii
Abstract ........................................................................................................................................ xiii
1.

2.

INTRODUCTION ...................................................................................................................1
1.1

An Introduction to Web Credibility ...............................................................................1

1.2

Statement of the Problem ...............................................................................................3

1.3

Purpose of the Study ......................................................................................................8

LITERATURE REVIEW ......................................................................................................10
2.1

Web Credibility............................................................................................................10
2.1.1

Definition of Web Credibility ..........................................................................10
2.1.1.1

Dimensions of credibility..................................................................11

2.1.1.2

Key dimensions of credibility: trustworthiness and expertise ..........15

2.1.2

Objects of Web Credibility Assessment ..........................................................16

2.1.3

User Characteristics in Web Credibility Assessment ......................................18

2.1.4

2.1.3.1

Demographics ...................................................................................18

2.1.3.2

User involvement ..............................................................................19

2.1.3.3

Technology proficiency ....................................................................20

Theoretical Frameworks of Web Credibility Assessment ...............................22
2.1.4.1

Fogg’s P-I theory ..............................................................................22

2.1.4.2 Wathen and Burkell’s model for how users judge the credibility of
on-line information ..........................................................................................23

2.2

3.

2.1.4.3

Hilligoss and Rieh’s unifying framework of credibility assessment 24

2.1.4.4

Sundar’s MAIN model......................................................................25

2.1.4.5

Metzger’s dual processing model of credibility assessment .............26

2.1.4.6

Lucassen et al.’s revised 3S-model of credibility evaluation ...........27

Older Adults .................................................................................................................29
2.2.1

Definition of Older Adults ...............................................................................29

2.2.2

Characteristics of Older Adults’ Information Behavior...................................31

METHODOLOGY ................................................................................................................35
v

3.1

Overview of Research Design .....................................................................................35

3.2

Phase I: A Qualitative Meta-Study ..............................................................................37
3.2.1

3.4

4.

3.2.1.1

Criteria for inclusion .........................................................................39

3.2.1.2

Criteria for exclusion ........................................................................40

3.2.2

Appraisal of Primary Research Reports ...........................................................41

3.2.3

Qualitative Meta-Analysis of Data, Method, and Theory................................42

3.2.4
3.3

Selection of Primary Research Reports ...........................................................38

3.2.3.1

Analytic approach .............................................................................42

3.2.3.2

Data management..............................................................................42

Qualitative Meta-Synthesis ..............................................................................43

Phase II: Semi-Structured Interviews ..........................................................................45
3.3.1

Target Population and Sampling Criteria ........................................................46

3.3.2

Participants Recruitments ................................................................................48
3.3.2.1

Recruitment site ................................................................................49

3.3.2.2

Prescreen ...........................................................................................49

3.3.3

Interview Protocol Development .....................................................................50

3.3.4

Data Collection ................................................................................................52
3.3.4.1

Semi-structured interview .................................................................52

3.3.4.2

Interview administration ...................................................................53

3.3.5

Data Analysis ...................................................................................................55

3.3.6

Ethical Consideration of Semi-Structured Interview .......................................56

Trustworthiness of Qualitative Research .....................................................................57
3.4.1

Credibility ........................................................................................................58

3.4.2

Transferability ..................................................................................................60

3.4.3

Dependability ...................................................................................................61

3.4.4

Confirmability ..................................................................................................63

FINDINGS ............................................................................................................................64
4.1

Findings from Phase I: A Qualitative Meta-Study ......................................................64
4.1.1

Assessment of Web Credibility .......................................................................65
4.1.1.1

Measures of operator trustworthiness ...............................................66

4.1.1.2

Measures of operator expertise .........................................................68

4.1.1.3

Measures for content trustworthiness ...............................................69
vi

4.1.2

4.1.3

4.2

4.1.1.4

Measures for content expertise .........................................................71

4.1.1.5

Measures for design trustworthiness .................................................74

4.1.1.6

Measures for design expertise ...........................................................75

Variability of Web Credibility Assessment .....................................................78
4.1.2.1

Context ..............................................................................................78

4.1.2.2

User characteristics ...........................................................................79

Process of Web Credibility Assessment ..........................................................80
4.1.3.1

Initial evaluation stage ......................................................................85

4.1.3.2

Final evaluation stage .......................................................................86

Findings from Phase II: Semi-Structured Interviews...................................................87
4.2.1

4.2.2

4.2.3

4.2.4

4.2.5

Profiles of Research Participants .....................................................................88
4.2.1.1

Demographic characteristics .............................................................88

4.2.1.2

Health condition ................................................................................90

4.2.1.3

Cultural capital ..................................................................................91

4.2.1.4

Social capital .....................................................................................93

4.2.1.5

Internet usage and experience ...........................................................95

Way of Life and Mastery of Life .....................................................................96
4.2.2.1

Way of life ........................................................................................97

4.2.2.2

Mastery of life ...................................................................................98

Older Adults’ Information Needs in the Everyday Life Context ...................104
4.2.3.1

Health/wellness ...............................................................................104

4.2.3.2

Travel ..............................................................................................108

4.2.3.3

Culture and education .....................................................................109

4.2.3.4

Entertainment ..................................................................................110

4.2.3.5

Random topics: looking for factual information .............................110

4.2.3.6

Finance ............................................................................................111

4.2.3.7

Others ..............................................................................................111

Older Adults’ Health Information Behavior ..................................................113
4.2.4.1

Interpersonal sources ......................................................................113

4.2.4.2

Online sources.................................................................................114

4.2.4.3

Older adults’ health information use and share ..............................119

Older Adults’ Credibility Assessment of Online Health Information ...........121
4.2.5.1

Operator trustworthiness .................................................................121
vii

4.2.6

5.

4.2.5.2

Operator expertise ...........................................................................123

4.2.5.3

Content trustworthiness ..................................................................124

4.2.5.4

Content expertise ............................................................................125

4.2.5.5

Design trustworthiness ....................................................................126

4.2.5.6

Design expertise ..............................................................................126

Ratings on Credibility Markers......................................................................128
4.2.6.1

Positive, neutral, and negative credibility markers .........................130

4.2.6.2

Ratings by user characteristics ........................................................131

DISCUSSION......................................................................................................................135
5.1

5.2

A New Framework of Web Credibility Assessment..................................................135
5.1.1

Assessment of WC .........................................................................................136

5.1.2

Process of WC................................................................................................140

5.1.3

Variability of WC ..........................................................................................144

Characteristics of Older Adults’ ELIS .......................................................................147
5.2.1

5.2.2

5.2.3

Older Adults’ Health/Wellness Information Needs .......................................148
5.2.1.1

Health condition and health information needs ..............................150

5.2.1.2

Social contact network and health information needs ....................151

Orienting Information Seeking (Incidental Information Acquisition) ...........151
5.2.2.1

Non-expert interpersonal sources ...................................................152

5.2.2.2

Cognitive hobbies ...........................................................................153

5.2.2.3

Additional training and research-related activities .........................155

Practical Information Seeking........................................................................155
5.2.3.1 Medical professionals as the primary source for practical health
information seeking .......................................................................................156
5.2.3.2 Partner as a source or an object of practical health information
seeking .........................................................................................................158
5.2.3.3

5.3

Online sources for practical health information seeking ................160

Older Adults’ Credibility Assessment of Online Health Information .......................162
5.3.1

Credibility Markers/Cues and Heuristics .......................................................163
5.3.1.1

Operator expertise ...........................................................................163

5.3.1.2

Operator trustworthiness .................................................................164

5.3.1.3

Content trustworthiness ..................................................................165
viii

5.3.1.4

6.

Content trustworthiness ..................................................................167

5.3.2

Credibility Markers by User Characteristics: Variability of WC ..................168

5.3.3

Web Credibility Assessment in ELIS ............................................................170

LIMITATIONS, FUTURE RESEARCH DIRECTION, AND CONCLUSION ................174
6.1

Limitations of the Research .......................................................................................174

6.2

Future Research .........................................................................................................175

6.3

6.2.1

Positioning Web Credibility Assessment in Information Behavior ...............175

6.2.2

Operationalizing Web Credibility in Various Contexts .................................176

6.2.3

Validating the New Framework for Web Credibility Assessment ................177

Conclusion ................................................................................................................178

APPENDICES .............................................................................................................................181
A.

HUMAN SUBJECTS COMMITTEE APPROVAL MEMORANDUM ............................181

B.

UPDATED HUMAN SUBJECTS COMMITTEE APPROVAL MEMORANDUM ........182

C.

PRESCREEN TEST TOOLS ..............................................................................................183

D.

SEMI-STRUCTURED INTERVIEW PROTOCOL ...........................................................187

E.

INFORMED CONSENT FORM ........................................................................................193

F.

RESEARCH QUESTIONS, THEORETICAL FRAMEWORKS, AND METHODS .......196

G.

CODING SCHEME ............................................................................................................197

REFERENCES ............................................................................................................................199
BIOGRAPHICAL SKETCH .......................................................................................................211

ix

LIST OF TABLES
2.1

Underlying Dimensions of Credibility in Interpersonal Communication..........................14

2.2

Objects of Web Credibility Assessments based Fogg’s Framework .................................17

2.3

User Characteristics Influencing Web Credibility Assessment .........................................22

2.4

Characterization of Six Existing Theoretical Frameworks of Web Credibility Assessment
by Four Facets ....................................................................................................................28

2.5

Definitions of Older Adults ...............................................................................................31

3.1

Criteria for Inclusion and Exclusion of the Qualitative Meta-Study .................................41

3.2

Design of the Appraisal Tool Used in the Qualitative Meta-Study ...................................41

3.3

Summary of Phase I (Qualitative Meta-Study) ..................................................................44

3.4

Target Population and Sampling Criteria ..........................................................................48

3.5

Design of the Interview Protocol .......................................................................................51

3.6

Summary of Phase II (Semi-Structured Interviews) ..........................................................56

4.1

Measures of Web Credibility Assessment .........................................................................77

4.2

Variability of Web Credibility Assessment .......................................................................79

4.3

Process of Web Credibility Assessment Identified in Existing Frameworks ....................84

4.4

Two-Stage Model of Web Credibility Assessment Process ..............................................87

4.5

Profiles of Research Participants by Age Group ...............................................................89

4.6

Research Participants’ Ages ..............................................................................................90

4.7

Research Participants’ Health Conditions .........................................................................91

4.8

Research Participants’ Cultural and Cognitive Capital .....................................................92

4.9

Research Participants’ Social Contact Network ................................................................94

4.10

Research Participants’ Internet Usage ...............................................................................96

4.11

Nature of Hobbies ..............................................................................................................98
x

4.12

Research Participants’ Background Information and Mastery of Life Types .................103

4.13

Older Adults’ Information Needs in Everyday Life Context...........................................112

4.14

Sources for Health Information Seeking..........................................................................119

4.15

Credibility Markers/Cues Mentioned by the Research Participants ................................127

4.16

Ratings on Credibility Markers/Cues by the Research Participants ................................129

4.17

Positive, Neutral, and Negative Credibility Markers .......................................................131

4.18

Ratings on Credibility Markers by User Characteristics .................................................131

5.1

Conceptual Typology of WC Assessment .......................................................................139

5.2

Conceptual Relationship between Information Foraging Theory and WC Framework ..144

5.3

Variability of Web Credibility Assessment .....................................................................147

5.4

Couples’ Information Behavior and Mastery of Life ......................................................159

5.5

Ratings on Operator’s Expertise-Related Items ...............................................................164

5.6

Ratings on Operator’s Trustworthiness-Related Items ....................................................165

5.7

Ratings on Content Trustworthiness-Related Items ........................................................167

5.8

Ratings on Content Expertise-Related Items ...................................................................168

5.9

Process of Web Credibility Assessment of Online Health Information ..........................173

xi

LIST OF FIGURES
3.1

Overall Design of the Research .........................................................................................36

4.1

Ratings on Credibility Markers by Mastery of Life.........................................................133

4.2

Ratings on Credibility Markers by Internet Usage ..........................................................134

xii

ABSTRACT
This dissertation research aims to provide a better understanding of people’s credibility
assessment of online information (i.e., Web credibility assessment), which is an important part of
their information behavior. In particular, this research focuses on older adults as a research
population as they are a less studied user group in the literature on information credibility.
Considering the ever increasing presence of older adults on the Web and their needs for health
information in their daily lives, this research explores older adults’ credibility assessment of
online health information within the context of everyday life information seeking (ELIS).
The methodology employed in this research consists of a qualitative meta-study (Phase I)
and semi-structured interviews (Phase II). In Phase I, the researcher analyzed 84 primary
research reports on information credibility, identifying conceptual typologies of important facets
of credibility assessment such as conceptualization, operationalization, variability (user
characteristics and contexts), and process. Based on these conceptual typologies, the researcher
proposed a new, extended framework of Web credibility assessment, named WC framework, that
contains three main components, Assessment of WC—i.e., conceptualization and
operationalization to measure Web credibility; Variability of WC—i.e., variables regarding
individual and context; and Process of WC—i.e., the overall process of Web credibility
assessment.
In Phase II, the researcher conducted semi-structured interviews with twenty-one older
adults whose ages ranged from 61 to 80 (M = 70.3) in the manner of one-on-one, in-person. The
purposeful sampling methods, such as convenience sampling and snowball sampling, were used
to recruit older adults who meet the sampling criteria of the study: age (55 years old or older),
residency (Florida residents for an in-person interview), and Internet use experience. Also, a

xiii

prescreen test was carried out via a telephone interview to make sure that the participant’s
cognitive function was adequate for the study. Those who met all the sampling criteria and
passed the prescreen test were recruited for an in-person interview which lasted around 45
minutes.
The interview data revealed that older adults needed health/wellness information
regarding medication and supplements, symptoms of and cures for specific diseases, medical
quality assurance, health insurance, nutrition, and exercise. In seeking health information, they
used both interpersonal and online sources. As for the interpersonal sources, the research
participants mentioned medical professionals (e.g., doctors and physicians), partners, family, and
friends. On the Web, they referenced information from non-profit (i.e., non-commercial)
institutions’ websites such as government websites (e.g., NIH, CDC) and university hospitals’
websites (e.g., Johns Hopkins Medicine, Harvard Medical School). The most frequently
mentioned commercial website was WebMD. Depending on the purposes of health information
seeking, some interviewees mentioned that they used pharmacists’ websites (e.g., Walgreens and
CVS).
When judging the credibility of online health information, they employed various
cues/markers and heuristics that are related to the attributes of the operator (i.e., source), content
(i.e., message), and design (i.e., media) of Web resources. Based on the new framework
developed in Phase I, the informants’ Web credibility assessment process was characterized with
the two stages: initial and final evaluations.
Lastly, both theoretical and empirical implications of the research and future research
directions were discussed. Specifically, the new Web credibility assessment framework (i.e., the
WC framework) advanced our understanding of the conceptualization of Web credibility and can

xiv

be used as a knowledge resource in developing context specific credibility assessment models as
well as information system interfaces that provide effective support for information credibility
evaluation by users. Likewise, findings from the semi-structured interviews can inform online
information system developers and librarians about how older users search for online health
information and how they assess its credibility. Ultimately, the findings of this research should
help the development of more effective online systems, services, and, training modules that are
aligned with the online information behaviors of this rapidly growing, important user
population—i.e., older adults.

xv

CHAPTER 1
INTRODUCTION
1.1 An Introduction to Web Credibility
The notion of credibility has been a focus of examination from as early as the Aristotelian
era, focused on the characteristics of the communicator’s ability to inspire confidence and belief
in what was being said: ethos, pathos, and logos, corresponding to the source’s credibility,
emotional appeal toward the audience, and the logic used to support a claim. In particular, ethos
refers to the character of a speaker in the context of interpersonal communication, which is
mainly based on the listener’s perception of the speaker’s credibility. Aristotle mentioned that
“his [the speaker’s] character ethos is the most potent of all the means to persuasion” (Cooper,
1932, p. 9). Many of the later scholars also consider the source credibility one of the important
variables affecting attitude change of a receiver (Hovland, Janis, & Kelley, 1953; Petty &
Cacioppo, 1981). In other words, the perceived credibility of the communicator has been
identified as a crucial criterion for judging the credibility of the statement itself. Even in cases
where a statement was approved by highly respected persons or organizations, the statement may
be perceived as credible as if it was originated by them (Hovland et al., 1953). The elaboration
likelihood model (ELM), which is an instrumental model explaining communication and
persuasion in the field of psychology, identifies credibility as one of the important variables for
persuasion (Petty & Cacioppo, 1981); thus, a receiver’s attitude change (i.e., persuasion) may be
a recognizable outcome of credibility (Rieh & Danielson, 2007).
While the ‘traditional’ studies on credibility in the context of interpersonal
communication have focused heavily on the characteristics of the speaker’s ability to inspire
confidence and belief in what was being said (i.e., source credibility), the literature focusing on
1

information seeking and retrieval, credibility has been viewed as a criterion for judging the
relevance of information (i.e., information credibility). Relevance has been viewed as playing a
significant role in users’ decisions about whether to accept or reject information (Cool, Belkin,
Frieder, & Kantor, 1993). The term quality has been used in this field to indicate the value that
leads people to choose and utilize certain information (Rieh & Belkin, 1998; Stvilia, Gasser,
Twidale, & Smith, 2007). The literature using the term quality tends to consider credibility as a
dimension of quality—a set of characteristics, which allows indirect (vs. direct) evaluation or
prediction of information quality. That is, when users do not have sufficient knowledge to judge
the quality of the given information and/or those who are not deeply involved with the given
task, they may rely on the markers/cues and heuristics of information credibility, rather than
directly evaluating the information quality. Based on the theoretical lens of “Information
Foraging” by Pirolli and Card (1999), the indirect evaluation of information quality by credibility
markers/cues may be understood as following ‘information scents’ (Pirolli, 1997) or ‘residues’
(Furnas, 1997), which are imperfect representations of the information quality based on proximal
cues. Using the analogy, credibility markers/cues may exude either a positive, negative, or even
neutral ‘scent’ in sense-making around the quality of the information.
Credibility of online information, which is also called Web credibility (Fogg, 2003a),
involves various markers/cues and heuristics that are related to the unique features of Web-based
platforms. Undoubtedly, source-related features, which have been the main focus in the
interpersonal communication context, still have a significant impact on the perception of
credibility even in the Web context. However, many scholars point out that the traditional cues
and measures of source credibility may not be able to fully reflect the credibility of Web
resources considering the unique features of the current Web such as the proliferation of peer-

2

production mechanisms (e.g., Wikis) and social content creation communities using those
mechanisms, and increasingly interactive ways of information dispersion (Flanagin & Metzger,
2008; Hong, 2006b; Jessen & Jørgensen, 2012). For instance, Hong (2006b) draws attention to
media and genre-specific structural features of Web resources, such as domain names, navigation
tools, and hyperlinks to other sites, that may not be addressed appropriately by the measures
focused on source-related attributes in the interpersonal communication setting. Also, Jessen and
Jørgensen (2012) highlight that authors’ credential and authority markers/cues (i.e., sourcerelated features) are not always available in online information, especially in user-generated
resources; therefore, Web credibility assessment does not necessarily rely on source credibility
markers/cues. There are other cases where the provided source information is hard to interpret
due to the convergence of multiple sources and the easiness of reproducing and redirecting one
source to another (Flanagin & Metzger, 2008). Therefore, although the concept of Web
credibility is rooted in the traditional concept of credibility (i.e., source credibility in
interpersonal communication), it is necessary to identify appropriate credibility markers/cues,
heuristics, and measures that can be enabled by the distinct features of Web systems and genres.
Chapter 2 provides a more in-depth discussion on conceptualization and operationalization of
credibility.
1.2 Statement of the Problem
Web credibility has been defined and measured largely based on the three main objects of
assessment: source, message, and media. Several underlying dimensions have been identified to
address each type of credibility, such as trustworthiness, expertise, dynamism, and attractiveness
for source credibility; accuracy, comprehensiveness, currency, reliability, and validity for
message credibility; and stability, consistency, and easiness of use for media credibility. Even

3

though each type of credibility and associated dimensions seemed to focus on different objects of
assessments, ultimately, the root or overarching concept under investigation has been credibility.
Therefore, logically, the root concept, credibility, has to be conceptualized first, and the
conceptualization should be applied to all types of credibility operationalizations. However,
discussions of credibility in both interpersonal and Web contexts in the literature have been
shaped more by the sources and/or objects of credibility assessment rather than guided by a
systematic conceptualization. This may lead to a conflation and confusion of the conceptual
structure, with measurements, and objects of measurements of the concept of credibility.
Therefore, it is necessary to articulate the relationship among the key dimensions of Web
credibility, related measures, and objects of those measures.
Web credibility assessment is often theorized as an iterative process that is influenced by
user characteristics (e.g., age, gender, education, and so on) and contextual variables (e.g., task,
user involvement, motivation, ability, and so on). Several existing theoretical frameworks
conceptualized the process of Web credibility assessment, taking the impacts of different
variables on users’ perceptions of Web credibility into account (Fogg, 2003b; Hilligoss & Rieh,
2008; Metzger, 2007; Sundar, 2008; Wathen & Burkell, 2002). However, as the existing
theoretical frameworks have somewhat different perspectives on how people assess credibility
when they seek for information on the Web, it is useful to analyze common and unique features
of the existing theoretical frameworks, providing a better understanding of what is known and
what needs to be further studied regarding the given topic—i.e., people’s Web credibility
assessment.
Some previous studies have found that age may have a significant impact on Web
credibility assessment (Choi, 2013; Fogg et al., 2001; Zulman, Kirch, Zheng, & An, 2011).

4

However, older adults are a relatively less studied user group, even though their use of Internet
services has been expanding rapidly over the years: As of April 2014, 59% of American older
adults who are 65 years old or older used the Internet (Smith, 2014), up from 15% in 2000, 37%
in 2008, and 56% in 2013 (Smith, 2013); in terms of social networking sites (SNSs) use, 46% of
the older Internet users used SNSs in 2014 (Smith, 2014), up from 5% in 2005, 11% in 2008, and
43% in 2013 (Brenner & Smith, 2013). In particular, about 30% of American older adults sought
health information online as of 2012 (Fox & Duggan, 2013); considering the fact that 54% of
those who were between 50 and 64 years old and 67% of those who were between 30 and 49
years old went online for health information online, future cohorts of older adults are expected to
do so in much higher proportions. However, most of the previous studies on information
credibility assessment have been either based on or focused on younger generations’ perceptions,
such as college students or high school students (Eastin, 2001; Eysenbach, 2008; Flanagin &
Metzger, 2000, 2008; Hargittai, Fullerton, Menchen-Trevino, & Thomas, 2010; Hilligoss &
Rieh, 2008; Hong, 2006a, 2006b; Iding, Crosby, Auernheimer, & Klemm, 2009; Liu, 2004; Liu
& Huang, 2005; Rieh & Hilligoss, 2008; Rowley & Johnson, 2013). Other studies have been
conducted with the general population, consisting of a wide range of age groups (Fogg et al.,
2003; Princeton Survey Research Associates, 2002; Rieh, Kim, Yang, & St. Jean, 2010;
Savolainen, 2011; Yang & Rieh, 2013). Even though some studies have dealt with older adults’
credibility assessment of online health information, they tended to focus on the factors
responsible for older adults’ distrust of online information (Robertson-Lang, Major, &
Hemming, 2011; Zulman et al., 2011), rather than looking into the underlying dimensions and
structures of their perceptions of Web credibility.

5

In fact, older adults, who have relatively less experience with the Internet than younger
generations, tend to have more concerns or doubts about the credibility of health-related
resources on the Web. Kaiser Family Foundation (2005) reported that only 26% of Internet users
who were 65 years old and older trusted online health information “a lot” or “some” to provide
accurate health information, while 57% of younger adults did. Fisk, Rogers, Charness, Czaja,
and Sharit (2009) mention that this distrust may be because their expectations about how a
system should work is based on how previous versions (i.e., non-electronic) were structured. In
other words, they may assume that the credibility of online information is relatively lower than
that of printed information because they are more familiar with printed information that is
produced and disseminated through a stricter process that requires enough authority and capital
to justify and sell information products.
However, the fact that older adults have more concerns with credibility issues does not
necessarily mean that they are able to seek out indicators of credibility when exploring a
Website. For instance, Robertson-Lang et al. (2011) found that 93% of older adults in their study
considered the websites they chose credible, but only 29% of them actually checked the source
of the sites to make sure they were credible. The study also reported that some participants
automatically trusted online health information simply because it was on the Web (RobertsonLang et al., 2011). Zulman et al. (2011) also highlighted that older adults’ lack of Internet
experience and unfamiliarity with technology influenced their trust of online resources. More
specifically, the relationship between age and distrust in the Internet as a source of health
information was moderated after adjusting for Internet experience and technical difficulties with
the computer or Internet. Therefore, older adults who have relatively lower levels of experience

6

and proficiency with the Internet than other age other age groups may need to be studied
separately.
Lack of information technology (IT) proficiency, which would influence Web credibility
assessment, may be also found in younger age groups. Ross, Grossmann, and Schryer (2014)
argue that there is no sufficient evidence showing a higher possibility of being defrauded in the
older adults group than younger population segments. However, older adults may still be
considered one of the groups vulnerable to online fraud because they tend to lag behind younger
people in adopting new technologies, regardless of whether or not they are familiar and confident
with the current IT, due to perceptual, cognitive, and psychomotor declines (Charness & Boot,
2009; Fisk et al., 2009; Smith, 2014). Furthermore, there have been studies examining older
adults’ vulnerability to fraud from the perspectives of neural activity. In Castle et al.’s (2012)
study, older adults rated untrustworthy faces as significantly more trustworthy and approachable
than younger adults did. Also, older adults showed lesser activation in the anterior insula (AI),
which is known as a region that contributes to decision-making by instantiating subjective
feeling states, when making explicit judgments of trustworthiness and when perceiving
untrustworthy faces. The authors argue that reduced AI activation seen in older adults may be a
neural indicator of a weaker warning signal regarding cues of untrustworthiness, which could
cause a higher rate of being victimized by online fraud in older age groups.
To fill the aforementioned gap in the literature on information credibility in terms of both
theoretical and empirical aspects, this dissertation research first synthesized existing theoretical
frameworks pertinent to Web credibility assessment to propose a new framework (Phase I) by
conducting a qualitative meta-study (Paterson, Thorne, Canam, & Jillings, 2001); then used the
new framework to explore older adults’ perceptions of Web credibility and their behavioral

7

characteristics in Web credibility assessment through semi-structured interviews (Phase II). The
findings from this dissertation, therefore, will contribute to the information behavior and HCI
literature and provide theoretical and empirical considerations for future research on various user
groups’ Web credibility assessment.
1.3 Purpose of the Study
This dissertation research has three goals to accomplish: First, the research seeks to
enhance our understanding of what information credibility is, especially in the Web context (i.e.,
Web credibility), how theoretical frameworks in the literature conceptualize people’s Web
credibility assessments, and which user-related variables come into play in the process of Web
credibility assessment. By conducting a qualitative meta-study (Paterson et al., 2001) on the
literature of information credibility, this research proposes a new framework that is useful to
understanding people’s of credibility assessment of information in the Web context.
Second, this research explores older adults’ (i.e., the target research population’s) typical
information needs in their everyday lives, and, in particular, their health information needs and
related information behaviors using semi-structured interviews. Based on Savolainen’s (1995)
everyday life information seeking (ELIS) model, various types of individual and social factors
are taken into account in order to have a holistic understanding of older adults’ information
behaviors in the everyday life context.
Third, this research aims to provide a deeper understanding of older adults’ Web
credibility assessment as a crucial part of health information seeking in their everyday lives
settings. The new framework of Web credibility assessment that is proposed by the qualitative
meta-study guides the construction of the interview protocol as well as the data analysis and
interpretation. To accomplish these goals, the following research questions have been developed:

8

RQ1: How is the process of Web credibility assessment conceptualized in existing
theories and models?
RQ1-1: What are the common and unique features of existing theories and models
of Web credibility assessment?
RQ1-2: How can the existing theoretical frameworks of Web credibility
assessment be improved?
RQ2: In general, what are older adults’ common information needs in their everyday
lives?
RQ3: What are older adults’ health information needs and related information behaviors?
RQ3-1: What sources do older adults use to find health information both on- and
offline, and why do they use those sources?
RQ3-2: How do they use the information they find?
RQ4: How do older adults assess the credibility of health-related information on the
Web?
RQ4-1: What are older adults’ perceptions of Web credibility?
RQ4-2: What are some of the psychological, social, and/or cultural mechanisms
that underlie and/or affect those perceptions?
RQ4-3: What are some of the markers/cues and heuristics used by older adults to
assess the credibility of health-related websites?

9

CHAPTER 2
LITERATURE REVIEW
This chapter reviews the literature on two areas that lay the groundwork for this
dissertation research: (1) Web credibility and (2) older adults’ information behavior. The first
subsection of the chapter reviews both theoretical and empirical studies on Web credibility
published in journals of information studies as well as communication, psychology, and HCI. At
the same time, however, this section reviews some of the older/classic studies of credibility that
provide original definitions and conceptualizations of the concept of credibility, which then have
been used in more recent studies and reviews of credibility assessment on the Web. Also, this
section devotes a good deal of attention to reviewing theories and models that conceptualize the
process of web credibility assessment.
The second subsection defines ‘who older adults are’ by reviewing different definitions in
the literature. Also, this section describes older adults’ typical information needs and related
information behaviors in their daily lives. In particular, this section focuses on distinct
characteristics of their online information use and seeking behaviors, which may be different
from younger generations.
2.1 Web Credibility
2.1.1 Definition of Web Credibility
According to the Oxford English Dictionary, credibility is defined as “the quality of
being trusted and believed in” (Credibility, n.d.). As the dictionary definition indicates, the

concept of credibility is based on specific qualities or virtues (i.e., underlying
dimensions), such as trustworthiness, that are involved in making people believe

10

something. In the literature, the “something,” an object of credibility assessments, has
varied depending on the main interests and approaches of research. The “something” can
be a speaker in interpersonal communication and psychological research; an organization
or group, as in the management sciences; media, such as television or the Internet in mass
communication research; or information resources in the information sciences; and so on
(Rieh & Danielson, 2007).
Even though the main objects of credibility assessments have varied among fields,
scholars seem to agree on the idea that credibility is based on a person’s (e.g., a listener, a user,
or a recipient) perception, rather than the objects of assessments (O'Keefe, 1990). That is, the
essential part of credibility assessments concerns how people perceive “something” as credible,
which then should theoretically be applicable for all types of credibility assessments regardless
of the objects under investigation (e.g., source, message, media). In this regard, the initial
investigations on credibility performed in interpersonal communication and psychological
research can provide a foundation for conceptualizing credibility.
2.1.1.1 Dimensions of credibility. In his Rhetoric, Aristotle identifies three
characteristics of the speaker, which affect the listener’s perception of the ethos of a speaker:
intelligence, character, and goodwill (Cooper, 1932). His conceptualization of ethos having the
three dimensions seems to serve as the fundamental reference in the following research that
attempts to clarify the concept of credibility, while labels of the dimensions identified in the later
studies were not always the same. Several of theoretical and empirical studies have suggested
different models of credibility, consisting of different numbers and names of underlying
dimensions.

11

Gaziano and McGrath (1986) propose the one-factor model of source credibility. The
factor analysis in their study shows that 12 items (e.g., being fair, unbiased, trustworthy,
complete, factual, accurate, public-beneficial, well-trained) are grouped together in a single
factor, named credibility. However, most scholars consider credibility a multifaceted concept
with underlying dimensions. Hovland et al. (1953) define the trustworthiness and expertness of
the communicator (i.e., source) as determinants of the credence given to them, which highly
influences the receiver’s judgment of whether to accept or reject the message generated by the
communicator. Wilson (1983), who coined the term cognitive authority, mentions that cognitive
authority is clearly related to credibility, as people would perceive the authority’s argument or
idea as proper when the authority is thought credible. He highlights that cognitive authority is not
necessarily people, but may be information resources, such as books, dictionaries, journals, etc.
Thus, credible resources are regarded as potential cognitive authority. He identifies
trustworthiness and competence (i.e., expertise) as two main components of credibility, which
are in line with Hovland et al. (1953).
Petty and Cacioppo (1981) view credibility as one of the important variables for source
effects in their model for persuasion (ELM of persuasion), along with attractiveness/likeableness,
power, and number of sources. In particular, they examined the effect of credibility on
persuasion by manipulating the source’s trustworthiness and expertise, assuming that they are the
core attributes of source credibility. Similarly, O'Keefe (1990) also considers these two the key
dimensions of credibility.
McCroskey and Teven (1999) add a third dimension, goodwill, to the two-factor model of
source credibility (i.e., trustworthiness and expertise). This three-factor model is semantically the
same as Aristotle’s conceptualization of ethos having three underlying dimensions, intelligence

12

(correctness of opinions), character (reliability), and goodwill (intent toward receiver). The
authors pointed out that goodwill has been often either excluded from the key dimension lists or
viewed as a sub-dimension of trustworthiness. They argued that Aristotle’s separation of
character (general reliability) of a person from the perception of his intentions toward a specific
trusting person (i.e., goodwill) is a valid and fundamental one because a person may be perceived
as reliable (or trustworthy) and at the same time be perceived as opposed to the personal goals of
the person who must rely upon him as a source of information. Their study results, which were
derived from a factor analysis on 783 undergraduate students’ perceptions, also supported their
assertion that goodwill is a separate component of the ethos/ source credibility construct.
Berlo, Lemert, and Mertz (1969) suggest another three-factor model of source credibility,
safety, qualification, and dynamism. The authors mention that this three-factor model is not
necessarily incompatible with Hovland et al.’s (1953) two-factor model, consisting of
trustworthiness and expertness. They explain that the safety factor is closely related to, yet more
general than the concept of trustworthiness in Hovland et al.’s model, as safety includes a
general evaluation of the source’s congeniality, such as kind and friendly; the second factor,
qualification, is more likely to be identical to the expertness dimension. They suggest dynamism
of the speaker as the dimension of source credibility, which includes items such as aggressive,
emphatic, bold, active, and energetic.
Whitehead Jr. (1968) proposes a four-factor model, consisting of trustworthiness,
competence, dynamism, and objectivity. The author also uses the factor analytic techniques to
verify the previously identified dimensions of ethos (source credibility). Sixty-five scales
regarding ethos or credibility from the literature were selected and factor analyzed based on 152
college students’ responses. Items which were significantly loaded to each dimension were: right,

13

honest, trustworthy, and just for trustworthiness; experienced and had professional manner for
competence; aggressive and active for dynamism; and open-minded and objective for objectivity.
Lastly, Giffin (1967) argues that five factors play important roles in forming the
perceived credibility of the source: expertness (authoritativeness or intelligence), character,
goodwill, dynamism (or activity), and personal attraction (likability or affiliation). As shown
above, expertness and character (trustworthiness) are the two core components of source
credibility that have been identified in most of the previous studies examining what are the core
components of ethos/source credibility. In addition, the third dimension, goodwill, was also
identified as an important dimension of ethos/source credibility by Andersen and Clevenger Jr
(1963); Cooper (1932); McCroskey and Teven (1999), and the fourth factor, labeled dynamism,
was found by Berlo et al. (1969). With regard to the fifth factor, personal attraction, Giffin
mentions that even though it did not account for a significantly large portion of the definition of
credibility in the literature, it was an identifiable factor, and there is an indication of a possible
influence of personal attraction, such as likability and affiliation. Table 2.1 summarizes the
dimensions of credibility suggested in the literature.

Table 2.1 Underlying Dimensions of Credibility in Interpersonal Communication

Hovland et al.
(1953)
Berlo et al.
(1969)
McCroskey &
Teven (1999)
Whitehead Jr.
(1968)
Giffin (1967)

Trustworthiness

Expertise

P	  

P	  

P	  

P	  

P	  

P	  

P	  

P	  

P	  

P	  

Dynamism

Goodwill

Objectivity

Personal
attraction

P	  
P	  
P	  

P	  
P	  

14

P	  

P	  

2.1.1.2 Key dimensions of credibility: trustworthiness and expertise. Some scholars
point out that the disagreement in defining the concept of credibility can be derived from the
limitations of the factor analytic methods used to identify the construct’s core dimensions
(O'Keefe, 1990; Taraborelli, 2008). As shown above, the most frequently used approaches for
analyzing the construct (i.e., credibility) were creating candidate items relevant to credibility and
validating them by using the factor analytic techniques. The variances in creating and validating
those candidate items might have caused the disagreement. Acknowledging the limitations in
defining the concept of credibility, the researcher aimed to identify the most frequently used
terms or ‘expressions’ used in the literature to conceptualize credibility: trustworthiness and
expertise (see Table 4.1 below). Several scholars addressing credibility issues in the Web context
also see that credibility is fundamentally based on two primary dimensions of credibility,
trustworthiness and expertise (Fogg, 2003a; Metzger, 2007). More recently, the Encyclopedia of
Library and Information Sciences (EoLIS) has defined that trustworthiness and expertise are the
two key dimensions of credibility, as well (Rieh, 2010). Thus, the researcher uses this twodimensional conceptualization of credibility as an operational definition for organizing the rest of
this review.
Trustworthiness captures the perceived goodness and morality of the source (Fogg,
2003a). Thus, the perception that a source is fair, unbiased, and truthful contributes to the
trustworthiness of information (Rieh, 2010). Wilson (1983) mentions that a person is regarded as
trustworthy if he or she is honest, careful about what he or she says, and disinclined to deceive.
Hovland et al. (1953) say that the degree of confidence in the communicator’s intent to
communicate a valid assertion is considered as the communicator’s trustworthiness.

15

Expertise is defined as the perceived knowledge, skill, and experience of the source
(Fogg, 2003a). From the perspective of source credibility in the interpersonal communication,
expertise is considered as the extent to which a communicator is perceived to be a source of valid
assertion (Hovland et al., 1953). Wilson (1983) says that a person is competent in some area of
observation or investigation if a person is able to observe accurately or investigate successfully.
2.1.2 Objects of Web Credibility Assessment
Fogg’s (2003a) Web Credibility Framework can guide the discussion on
operationalization of Web credibility, as it suggests three main objects of Web credibility
assessment that can be used to categorize various measures identified in the literature: (1)
operator, (2) content, and (3) design. Even though these three categories correspond to the
traditional typologies of credibility, such as source, message, and media credibility (Metzger,
Flanagin, Eyal, Lemus, & McCann, 2003; Rieh & Danielson, 2007), Fogg’s framework is
specialized for Web credibility assessment, labeling and defining each category appropriately for
Web-based resources.
The first category of Web credibility framework, operator, is defined as “the organization
or person offering the site” (p. 173). Operator can be interpreted as source in the conventional
setting. As credibility of a speaker (i.e., source) is considerably important evidence for people to
judge the credibility of the message from him or her in interpersonal communication, credibility
of an operator, who runs a website, is an important object of assessment for judging the
credibility of the website. For instance, whether or not the operator of the website is respectful or
whether it is a profit or nonprofit organization can be some of the markers/cues that influence
users’ perceptions of websites’ credibility.

16

Content is the second category of the Web credibility framework. Fogg (2003a) defines
content as “what the site provides in terms of information and functionality” (p. 173). Currency,
accuracy, relevance of content, and endorsement by a respected outside agency (e.g., the Health
on the Net foundation; HON) are the message-related markers/cues that boost Web credibility. In
addition, Fog (2003a) considers functionalities that a website can provide for users as the other
aspect of content. Examples include the archive function that allows users to search past content
on the website and customizability that allows tailoring pages to individual users.
Design is the third category of the framework, which is largely about the structural
attributes of websites. Fogg (2003a) specifies that four key design elements come into play for
Web credibility assessment: information design (the structure of information on each page and
throughout the site – e.g., organization of information); technical design (how the site works
from a technical standpoint – e.g., search function is powered by a respected search engine);
aesthetic design (how things look, feel, or sound – e.g., whether or not the site is professionally
designed); and interaction design (moment-by-moment experience of users as they go through
the steps to accomplish their goals – e.g., easy to use, navigability). Table 2.2 provides the
definitions and examples of the three objects (types) of Web credibility assessment.

Table 2.2 Objects of Web Credibility Assessments based Fogg’s Framework (2003a)
Assessment object
Operator (source)

Definition
The organization or person offering the site

Content (message)

What the site provides in terms of
information and functionality
The structural attributes of the site

Design (media)

17

Types and examples
• Persons (individuals)
• Organizations
• Message-related markers/cues
• Website’s functionalities
• Information design
• Technical design
• Aesthetic design
• Interaction design

2.1.3 User Characteristics in Web Credibility Assessment
The process of assessing information credibility relies on users’ perceptions, which may
be affected by different user characteristics. These include age, gender, socio-economic status,
technology proficiency, information literacy, and so on (Ahmad, Komlodi, Wang, & Hercegfi,
2010a; Arazy & Kopak, 2011; Fogg, 2003b; Kim, 2012; Lucassen, Muilwijk, Noordzij, &
Schraagen, 2013; Metzger, 2007; Robertson-Lang et al., 2011; Zulman et al., 2011). Therefore,
to gain a better understanding of users’ assessments of the credibility of online resources and the
effects of various demographic, cultural, and physiological variables on information behaviors,
researchers have devised studies of populations defined by those variables.
2.1.3.1 Demographics. Age may be one of the important factors influencing Web
credibility assessment. Fogg et al. (2001) found that participants of 28 years old or younger
tended to be more critical of amateurism on a site, compared to those who were 37 years older or
older. Also, the older respondents in the study rated credibility markers regarding expertise,
trustworthiness, and tailoring (i.e., personalized services) more positively than their younger
counterpart did. Castle et al.’s (2012) study that found the reduced AI activation in the older
group, which implied a lower visceral warning signal to cues of untrustworthiness, can be
understood in line with the findings from Fogg et al. (2001) that older users tended to be more
generous in rating credibility markers than younger users.
In addition, older adults tended to have more concerns or doubts about the credibility
issues on the Web (Kaiser Family Foundation, 2005). Considering the fact that the age effect on
the perception of credibility of online health information was attenuated after controlling for
technology proficiency (Zulman et al., 2011), age seems to have a both direct and indirect effect
on people’s perception of Web credibility and their behaviors in Web credibility assessment.

18

Gender is another demographic factor that influences credibility perception on the web.
Johnson and Kaye (1998) mentioned that gender was the only variable that was significantly
associated with perceptions of credibility of all four types of sources (e.g., online newspapers,
online news magazines, online candidate literature, and online issue sources) among the
demographic variables under investigation, such as age, education, and income. In particular,
female participants in the study viewed the web as more credible than males did. Fogg et al.
(2001) also found that male participants rated credibility markers/cues more negatively than
females did. However, gender does not have a consistent effect—i.e., men being more critical
than women—on perceptions of Web resources. For instance, Flanagin and Metzger (2003)
reported opposite findings, that males rated the given websites as more credible (i.e., more
positive) than did females. Furthermore, Johnson and Kaye (2000) found that gender did not play
a significant role in assessing Web credibility, while other studies have controlled gender in their
data analyses (Hong, 2006a; Johnson & Kaye, 2009; Metzger, Flanagin, & Zwarun, 2003).
2.1.3.2 User involvement. Several models for Web credibility assessment view user
involvement (e.g., motivation and ability) as a decisive factor having an impact on the overall
process of Web credibility assessment (Fogg, 2003b; Lucassen et al., 2013; Lucassen &
Schraagen, 2011; Metzger, 2007). When people have high motivation to evaluate Web
credibility, they are more likely to look into the content-related features of the web resource,
such as the arguments presented and/or source of the information. However, when they have low
motivation, people tend to evaluate Web credibility based on more superficial features of the
web resource, such as design, color schemes, and functionalities of the website (Fogg, 2003b;
Metzger, 2007).

19

Furthermore, when people have the ability to evaluate the web resource, in addition to
motivation, people use a more rigorous and systematic strategy for credibility assessment.
However, if a user does not have motivation, no credibility assessment will happen; if he does
not have the ability, yet has the motivation, to evaluate, the user would rely on the surface
characteristics (i.e., peripheral cues) or heuristics to judge the credibility of the information (see
Metzger, 2007, p. 2088). Thus, user motivation and ability are crucial factors that determine the
extent to which users will critically evaluate the web resource.
Familiarity on a given topic is known as a contributing factor that influences Web
credibility assessment, as well (Arazy & Kopak, 2011; Chesney, 2006; Lucassen et al., 2013).
More specifically, Lucassen et al. (2013) found that people who were familiar with a given topic
tended to pay more attention to the semantic features of the information (i.e., central cues),
whereas people who were not familiar with the topic focused more on surface features of the
information (i.e., peripheral cues). Arazy and Kopak (2011) also highlighted that assessing the
accuracy of the content required knowledge of relevant facts (i.e., semantic features), whereas
assessing the presentation of the Wikipedia article (i.e., surface features) did not require such
expertise. Thus, his level of familiarity with the subject matter may affect the user’s
interpretation of the web resource under examination.
2.1.3.3 Technology proficiency. Users’ information literacy levels play a significant role
in assessing the structural and message features that influence credibility perceptions and
evaluations of websites (Ahmad et al., 2010a; Lucassen et al., 2013; Zulman et al., 2011). With
Julien and Barker’s (2009) definition of information literacy, “the set of skills required to
identify information sources, access information, evaluate it, and use it effectively, efficiently,
and ethically” (p. 12), expert users who have higher information literacy tend to use not only the

20

structural features of websites (e.g., links, policy, affiliation, sponsor, domain names,
advertisements, and aesthetics) but also the message features (e.g., information timeliness,
information language, information organization, information citation, information consistency,
testimonials, author, and author expertise) to verify the credibility of websites. However, novice
users having lower information literacy rely mainly on the visual appearance and structural
features of websites.
Scholars in the field of communication have used the term media reliance, examining
mainly the relative influence of reliance on different media types for credibility perceptions
(Johnson & Kaye, 2009; Johnson, Kaye, Bichard, & Wong, 2007; Kiousis, 2001). These studies
have focused on the relationship between users’ media reliance (or media use) and the perceived
credibility of the medium under investigation. Overall, reliance has been found to be one of the
influential factors for credibility perception both in traditional media, such as television, radio,
and newspaper, and in the web sources such as websites in general, blogs, and SNSs. For
instance, Flanagin and Metzger (2000) reported that more experienced users tended to consider
the Internet more credible than those who had less experience with it. In blogs, in particular,
reliance was the strongest predictor of blog credibility even after controlling for demo- graphics
and Internet use (Johnson & Kaye, 2004; Johnson & Kaye, 2009; Johnson et al., 2007):
experienced Internet users considered blogs more credible than those having less experience with
blogs (i.e., general Internet users) because the experienced users were familiar with the purpose
of blogs or the style of writing. These findings seem to indicate that, the more users rely on a
certain source, the more likely they are to judge the information from the source as credible.
Table 2.3 lists the factors having an impact on Web credibility assessment in three categories:
demographics, involvement, and technology proficiency.

21

Table 2.3 User Characteristics Influencing Web Credibility Assessment
Types
Demographics

Definition
User’s demographic backgrounds that
influence Web credibility assessment

Involvement

The degree to which users know and care
about specific topics under
examination

Technology proficiency

The degree to which users are familiar
and comfortable with the technology
(Internet) to identify, access, evaluate,
and use information resources

Variables
• Age
• Gender
• Education
• Motivation
• Ability
• Domain expertise
• Information literacy
• Media reliance

2.1.4 Theoretical Frameworks of Web Credibility Assessment
Several of theoretical frameworks have been proposed to explain Web credibility
assessment taking various attributes of online resources into consideration, in terms of source
(i.e., operator), message (i.e., content), and structure (i.e., design), as well as the dynamic nature
of the ‘process’ of assessing the information credibility. This section reviews the following six
theoretical frameworks pertinent to Web credibility assessment: Wathen and Burkell’s (2002)
Model for How Users Judge the Credibility of Online Information; Fogg’s (2003b) ProminenceInterpretation Theory (P-I Theory); Metzger’s (2007) Dual Processing Model of Website
Credibility Assessment; Hilligoss and Rieh’s (2008) Unifying Framework of Credibility
Assessment; Sundar’s (2008) MAIN Model; and Lucassen et al.’s (2013) Revised 3S-Model. In
particular, the common and unique features of the theoretical frameworks are recapitulated in
Table 2.4.
2.1.4.1 Fogg’s P-I theory. The P-I Theory proposed by Fogg (2003b), posits that two
things happen when people assess credibility: a person notices something (i.e., prominence) and
makes a judgment about it (i.e., interpretation). The fundamental idea of this theory is that people

22

would evaluate the given website as much as they have noticed based on their involvement,
motivation, ability, etc.
The first concept of the theory, prominence, is defined as “the likelihood that a website
element will be noticed or perceived” (Fogg, 2003b, p. 722). The author mentions that before a
website element can affect a user’s credibility assessment of the site, the user must first notice
the element. In other words, if certain website elements are not noticed by users, information in
the website cannot have an impact on credibility assessment of the site. He identifies five factors
that affect the prominence phase: involvement of the user, content, task, experience, and
individual differences.
The second concept of the P-I theory is interpretation. Fogg (2003b) defines the concept
as “a person’s judgment about an element under examination” (p. 723). In the interpretation
phase, the user evaluates website elements as good or bad. For example, a user may interpret a
broken link on a web either as the operator does not care for the site or the site was not carefully
created in the first place. In either case, the broken link will contribute to a lower credibility
perception of the site. Fogg mentions that at least three factors affect interpretation: user’s
assumptions (e.g., culture, past experiences, and heuristics), skill/knowledge of a user (e.g., a
user’s level of competency in the site’s subject matter), and context (e.g., the user’s environment,
expectations, and situational norms).
2.1.4.2 Wathen and Burkell’s model for how users judge the credibility of on-line
information. Wathen and Burkell (2002) view the credibility assessment of online resources as
an iterative process. Particularly, they conceptualize the process of Web credibility assessment
with two distinct phases: (a) evaluation of surface credibility and (b) evaluation of message
credibility. According to their model, people begin the process by making immediate judgments

23

about the surface characteristics of the site, such as appearance (e.g., color, graphics, lack of
error, etc.), usability (e.g., navigability, menus, download speed, etc.), and organization of
information (e.g., layers, ease of access, and choice of detail level). The factors identified in this
model are in line with the surface credibility markers suggested by Tseng and Fogg (1999). As
motioned above, people consider the professional appearance of a website an important cue to
judge its overall credibility.
In the second phase of credibility assessment (i.e., evaluation of message credibility),
people evaluate the credibility of the message delivered by the website in terms of source and
message. The authors identify expertise, competence, trustworthiness, and credentials as factors
that influence source credibility. Also, they identify content, accuracy, currency, and relevance to
the user need as factors that influence message credibility.
2.1.4.3 Hilligoss and Rieh’s unifying framework of credibility assessment. Hilligoss
and Rieh (2008) suggested a unifying framework of credibility assessment in an attempt to
consider diverse information seeking goals, tasks, and contexts in everyday life. They identified
three distinct levels of credibility judgments: construct, heuristic, and interaction. In the
framework, construct is the highest and the most abstract level as it is concerned with how
people define (or perceive) the concept of credibility. Hilligoss and Rieh suggest five constructs
of credibility – trustfulness, believability, trustworthiness, objectivity, and reliability – and
highlight that people may conceptualize credibility in different ways depending on the situation
they are facing and the types of information encountered.
The second level of the framework is the heuristic level. Heuristics involve general rules
of thumb that are utilized in cases where people are unwilling or unable to evaluate the content
of the message because of time, motivation, and ability. Hilligoss and Rieh categorize heuristics

24

for credibility assessment into four types: media-related heuristics (e.g., book, peer-reviewed
journal articles, Web, etc.), source-related heuristics (e.g., familiar vs. unfamiliar sources,
primary vs. secondary sources), endorsement-based heuristics (e.g., recommendation by
knowledgeable and trusted individuals), and aesthetics-based heuristics (e.g., design in websites).
The third level of Hilligoss and Rieh’s framework is interaction. The authors define
interaction as “specific attributes associated with particular information objects and sources for
credibility judgments” (p. 1473). This level differs from the previous level (i.e., heuristics), in
that credibility judgments in this level are based on specific source or content cues that are
unique to a specific context. Three types of interactions are identified: interactions with content
cues, peripheral source cues, and peripheral information object cues. Content cues are directly
related to evaluating the credibility of the message itself. Peripheral source cues are sourcerelated features that can affect the credibility assessment of information, such as affiliation,
reputation, author’s education background, type of institution, etc. Peripheral information object
cues are about the appearance or presentation of the information object, such as advertisements
or language used in the website, etc. Hilligoss and Rieh mention that the three levels of
credibility assessment are interlinked, affecting each other in both directions from the abstract
level (i.e., construct) to the specific level (i.e., interaction), rather than functioning exclusively.
2.1.4.4 Sundar’s MAIN model. Sundar (2008) pays attention to the technology effects
on credibility assessments. In particular, as multiple sources are often interlinked in online
information, source credibility, which has been conventionally regarded as the most important
clue to judge the believability of information, may not play a clear role in the Web context.
Therefore, information receivers have to consider message credibility as well as the credibility of
the medium itself to assess credibility of online information. In such information environment

25

where people have to take more things into consideration to find credible information, they get to
face the information overload and the lack of uniformity in content quality. In this regards, he
highlights the importance of roles of cognitive heuristics that people take advantage of to make
judgments of credibility in the Web context. His MAIN model accommodates various heuristics
pertaining to credibility assessments, categorized in four types of affordances in digital media,
such as Modality (M), Agency (A), Interactivity (I), and Navigability (N).
Affordance is a particular capability possessed by the medium to facilitate a certain
action, and the affordances exist in most digital media to some different degrees. The modality
affordance is closely related to the structural aspects of the medium, rather than the content –
e.g., “realism heuristic” that people tend to trust audiovisual modality because its content has a
higher resemblance to the real world; the agency affordance-related heuristics are utilized to
identify the source, which affect the perceived credibility of the information provided by the
source – e.g., “machine heuristic” that people consider the objectivity of chosen news to be more
credible if it is recommended by a machine; the interactivity affordance involves both concepts
such as interaction and activity, which are the characteristics usually lacking in most traditional
media – e.g., “activity heuristic” that influences users’ credibility judgments by the dynamism;
navigability affordance is about interface features of digital media, such as organization of sites
and hyperlinks – e.g., “browsing heuristic” that encourage users to take a look at the site by
checking out the various links.
2.1.4.5 Metzger’s dual processing model of credibility assessment. Metzger’s (2007)
dual processing model takes user motivation and ability into account in theorizing the process of
Web credibility assessment. This model adopts the main idea of the dual processing models, such
as Petty and Cacioppo’s (1981) Elaboration Likelihood Model of persuasion (ELM), that divide

26

the process of information processing and assessment into two routes, (a) central and (b)
peripheral routes, depending on the depth of the user’s motivation and ability to scrutinize it. The
dual-processing-perspective is a useful approach to understand Web credibility assessment as it
is mainly based on user perceptions, which are formed (influenced) by various audience factors
(i.e., user characteristics) such as demographic background, involvement (motivation and
ability), topic familiarity, and information skills. In other words, since user perceptions are not
necessarily the same for all types of users, nor for various situations, it is reasonable to specify
the evaluation process by considering the dynamisms in Web credibility assessment. In the
exposure phase of the model, user’s motivation and ability decide whether or not they will go to
the next phase, evaluation phase. When a user has motivation and ability to evaluate the
information he or she is being exposed to, the user will take more rigorous and systematic
strategies to credibility assessment (i.e., central route); however, if a user does not have
motivation, no credibility assessment will happen; in case the user does not have ability, yet has
motivation to evaluate, he or she will rely on the surface characteristics (i.e., peripheral cues) or
heuristics to judge the credibility of the information (see Metzger, 2007, p. 2088).
2.1.4.6 Lucassen et al.’s revised 3S-model of credibility evaluation. Lucassen and
Schraagen (2011) suggested the 3S-model, where 3S indicated the three information
characteristics, semantic, surface, and source features. Lucassen et al. (2013) improved the initial
version of the model by further examining the influences of the topic familiarity and information
skills (i.e., information literacy). Defining topic familiarity (or domain expertise) as “having
knowledge on the topic at hand” (Lucassen et al., 2013, pp. 256-257), people who have the
higher level of knowledge on the topic tended to focus more on the semantic features (i.e.,
message of the information), while the novice users who are not familiar with the topic relied

27

more on surface features (i.e., structural features). Furthermore, when defining information skills
as “the skills required to identify information sources, access information, evaluate it, and use it
effectively, efficiently, and ethically” (Julien & Barker, 2009, p. 12), users with better
information skills more often attempted to evaluate information quality, while those with poorer
information skills did not.
The Revised 3S-Model shares common ideas with the dual processing model by Metzger
(2007), in that credibility assessment may vary depending on the levels of motivation and
abilities. In particular, ability can be seen as the same concept as information skills mentioned in
the Revised 3S-Model. Thus, involving the two models together, users go through the different
routes by focusing on different types (i.e., semantic vs. surface) and levels (i.e., number of cues)
of credibility cues embedded in the given information depending on the information skills (i.e.,
ability).

Table 2.4 Characterization of Six Existing Theoretical Frameworks of Web Credibility
Assessment by Four Facets
Models

Facet 1:
Context

Facet 2:
User characteristics

Facet 3:
Operationalization

Judgment
Model
(Wathen &
Burkell,
2002)

Situation

• Previous knowledge
• Topic familiarity
• Willingness to
believe and use the
information

• Surface credibility
• Source credibility
• Message credibility

• Enter websites
• Evaluation of surface
credibility
• Evaluation of
message credibility
• Content evaluation

P-I Theory
(Fogg,
2003b)

• Context
• Task

• Involvement
• Experience
• Individual difference
• Assumption
• Skill/knowledge

Content

• Prominence
• Interpretation

28

Facet 4:
Process

Table 2.4 – Continued
Dual Model
(Metzger,
2007)

N/A

• Motivation to
evaluate
• Ability to evaluate

• Heuristic evaluation
• Systematic
evaluation

• Exposure phase
• Evaluation phase
• Judgment phase

Unifying
Model
(Hilligoss &
Rieh, 2008)

• Context
• Goal
• Task

Information seeker:
motivation & ability

• Media heuristics
• Source heuristics
• Endorsement
heuristics
• Aesthetics heuristics

• Construct
• Heuristic
• Interaction

MAIN
Model
(Sundar,
2008)

N/A

N/A

• Modality cues
• Agency cues
• Interactivity cues
• Navigability cues

• Affordance
• Heuristics
• Quality
• Credibility judgment

Revised 3SModel
(Lucassen et
al., 2013)

N/A

• Domain expertise
• Information skills
• Source expertise

• Semantic features
• Surface features
• Source features

N/A

2.2 Older Adults
2.2.1 Definition of Older Adults
Full retirement age based on U.S. Social Security Administration, 65 years old, has been
often used to define senior citizens (or older adults). The normal retirement age had been 65 for
many years, but it has gradually increased to 67 for people born after 1959 (U.S. Social Security
Administration, 2015). This age classification has been widely used for nationwide demographic
surveys such as U.S. Census. However, when it comes to studying older adults as a user group of
information systems (e.g., websites, search engines, and SNSs), it may not be appropriate to
define this group solely by age because of their wide range of abilities and experiences with
technology (Fisk et al., 2009; Moffatt, 2013).
Instead of using the chronological age of 65-year-old as a cut-off point, Laslett (1989)
suggests four ages of human’s life span to define age groups: (1) the first age is the time between
29

birth and 20 to 25 years when education, socialization, and preparation for work occurs; (2) the
second age is the period between taking on the obligations of a job, marriage, and retirement
from paid work; (3) the third age is usually ushered in by retirement when people have time for
self-fulfillment; and (4) the fourth age is the stage, which is characterized by illness, frailty,
dependence, and the imminence of death.
Neugarten (1974) mentions that existing age norms, such as 65-year-old by the Social
Security system, are increasingly irrelevant as precise guides for behavior particularly in late
adulthood. She differentiates them broadly into young-old and old-old. Although chronological
age may not be a satisfactory criterion, it is nevertheless an indispensable one. Her original
formulation emphasizes patterns of functioning and involvement that are correlated but not
isomorphic with age.
The young-old can be defined as the group composed of those who are approximately 55
to 75 years old. This group of people (i.e., young-old) is characterized as healthy, active older
people, who are energetically engaged in leisure and social activities. Distinguished from youngold, the old-old are the group of people who are 75 and over with significant disabilities, whose
health and social needs require an ever growing number of resources. Neugarten (1996) mentions
that there is a set of stereotypes about old age that are based primarily upon the old-old, such as
sick, poor, enfeebled, isolated, and desolated.
Williamson and Asla (2009) mention that people who are 75 years older or older can be
part of the fourth age and they use significantly fewer sources than relatively younger seniors.
Wicks (2004) also highlight that people in this age group often face significant changes in their
lives, such as retirement from the workplace, declines of cognitive and physical abilities, and
changes in technology and society that affect their ability to utilize online information.

30

Table 2.5 Definitions of Older Adults
Source
U.S. Social Security
Administration
(2015)

Term
Full retirement age

Definition/Characteristics
The age at which a person may
first become entitled to full or
unreduced retirement benefits.

Age (year-old)
65 – 67 (or older)

Laslett (1989)

Third age

The population without a
socially predetermined role but
healthy, energetic, and ripe
with experience.
Also called “old age” or “the
disability zone,” characterized
by illness, frailty, dependence,
and the imminence of death.

From retirement

The group composed of those
who are approximately 55 to
75 years old, characterized as
healthy, active older people,
who are energetically engaged
in leisure and social activities.
The group of people who are 75
and over with significant
disabilities, whose health and
social needs require an ever
growing number of resources.

Approx. 55 – 74

Fourth age

Neugarten (1996)

Young-old

Old-old

Approx. 60 – 100

Approx. 75+

2.2.2 Characteristics of Older Adults’ Information Behavior
Wicks (2004) points out that older adults may have particular information seeking
behavior as they often face significant changes in their lives, such as retirement and declining
health. Thus, older adults may need a relatively small range of information topics than younger
adults due to physiological, cognitive, and/or physical declines (Asla, Williamson, & Mills,
2006). In terms of information needs, previous studies consistently find that the most necessary
and important information topic for older adults is health- and wellness-related information, as
they get old and accumulate various diseases and illnesses (Su & Conaway, 1995; Taha, Sharit,
& Czaja, 2009; Wicks, 2004; Williamson, 1997, 1998; Williamson & Asla, 2009). For instance,
31

Williamson and Asla (2009) found that health was the most popular topic for the subjects in the
study, followed by income and finance as second, and recreation as third; Getz and Weissman
(2010) reported that older adults in their study were mainly interested in receiving information
on economic subjects and preferred that the information included explanations about law and
services as well as technical details such as addresses and opening hours of information
suppliers.
As highlighted by Neugarten (1996), it also need to be noted that older adults are a more
heterogeneous user group than any other age groups due to age-related declines in perception,
cognition, and movement control (Charness & Boot, 2009; Fisk et al., 2009). Thus, even within
the older adults group, there exist significant age effects (i.e., differences) on their information
seeking behaviors: Williamson (1997) found that participants aged 85 and older tended to have a
fewer number of topics of interest than younger adults in general.
An important finding was that the subjects tended not to share their problems with others
until they encounter difficulty in receiving information. Williamson (1997) also found that older
adults relied most heavily on people within their social networks; however, friends are not a key
information source for some of the very old (aged 85+), particularly because they are becoming
increasingly isolated due to failing health and the loss of friends through death or other
circumstances. Even within older adults, the older seniors used significantly fewer sources than
relatively younger seniors.
In the Web context, older adults, who have relatively less experience with the Internet
than younger generations, tend to have more concerns or doubts about the credibility. Zulman et
al. (2011) showed that Internet users who were 65 years or older were significantly less likely
than younger than 65 to report trusting the Internet for health information. However, the

32

association between age and trust disappeared after accounting for two significant factors:
confusion and lack of awareness. The results indicate that experience and familiarity with the
Internet play a significant role in assessing the credibility of online health information.
A previous study on older adults’ health information use also found that regardless of
whether they were online users or non-users, they relied mostly on their health care providers
than any other types of sources under investigation, such as pharmacist; newspapers and popular
magazines; medical journals, medical books, and popular books; television and radio; and friends
and family (Taha, Sharit, & Czaja, 2009).
Selwyn, Gorard, Furlong, and Madden (2003) found that using the Internet for
information search was less popular (common) activity for older adults in their everyday lives,
and people who were older 70 years old used the Internet less than those who were between 60 to
69 years old.
Furthermore, Getz’s (2010) study on Israeli older adults’ legal information seeking
behaviors found that older adults preferred to receive information from informal sources such as
family members and mass media over formal sources such as counsellors from the National
Insurance Institute and the Citizens’ Information Service. This tendency that older adults prefer
informal sources for information seeking was also found in Su and Conaway (1995) focused on
Chinese older adult immigrants’ information seeking behaviors; the participants in the study not
only used their family and friends as information sources for various topics of information needs
including health most frequently, but also they perceived them as more helpful sources than
experts. Clubs and religious centers also serve as informal sources of information for older adult
immigrants in the United States, who also prefer to receive information from neutral sources,
such as libraries (Rait, 1989).

33

Theses additional training and research-related activities provided them with the
opportunities to obtain more in-depth medical knowledge, such as symptoms and cures for
certain diseases of which they played a role as a patient having the diseases in the standardized
patient program and even (incidentally) acquired knowledge about typical medical procedures.
These people showed self-confident in seeking and understanding health and wellness
information. This finding is supported by the literature on the effects of health literacy programs
for older adults. In particular, health literacy intervention significantly improved the research
participants’ (M = 69.99; SD = 8.12) knowledge, skill, and eHealth literacy efficacy (Xie, 2011a,
2011b, 2012).
In terms of technology adoption and use, Charness and Boot (2009) highlight that
regardless of whether older adults are confident with dealing with technology, there is a lag in
older adults’ technology adoption and use due to perceptual, cognitive, and psychomotor
declines. Thus, older adults are an important user group in the area of information behavior.

34

CHAPTER 3
METHODOLOGY
This chapter elaborates on the methodology used to achieve the three main research
goals: developing a conceptual framework of Web credibility assessment; and providing a deeper
understanding of older adults’—the research population—information needs and related
information behavior in the everyday life context; and their perception of information credibility
in the Web context and their behaviors relating to credibility assessment of online health
information. Considering the interpretive and exploratory nature of this research, qualitative
research approaches and methods were employed: (1) a qualitative meta-study (Phase I) and (2)
semi-structured interviews (Phase II).
Qualitative research is appropriate and useful to understand how people make sense of
the phenomenon of interest, construct the meaning, and interpret their experiences regarding the
phenomenon, rather than to determine the causal relationships among the identified variables and
predict similar cases in the future (Merriam, 2009). In the current research, therefore, the
qualitative approach will allow the researcher to explore how the concept of credibility and the
process of information credibility assessment are understood in the literature and by the target
population, i.e., older adults. The following subsections explain the overall research design and
provide details of each research procedure.
3.1. Overview of Research Design
The overall research design consists of two phases (Figure 3.1). In Phase I, a qualitative
meta-study of the information credibility literature was carried out. The main purpose of the
qualitative meta-study was to have a better-organized theoretical framework of Web credibility
assessment based on a systematic literature analysis—it produced conceptual typologies of Web
35

credibility assessment. The researcher followed the guidelines of a qualitative meta-study
suggested by Paterson et al. (2001) that involve (a) selection and appraisal of primary research
reports, (b) meta-analysis of data, methods, and theory, and (d) meta-synthesis (see Table 3.3).
In Phase II, semi-structured interviews were conducted with older adults to explore their
credibility assessment of online health information in their daily life contexts. The conceptual
typologies of Web credibility assessment that were proposed in Phase I and the ELIS model by
Savolainen (1995) were used to guide the construction of the interview protocol as well as the
data analysis and interpretation. Thus, the interview data collected in Phase II helped the research
not only study older adults’ Web credibility assessment in the context of ELIS, but also test and
refine the new framework with empirical data.
The following subsections provide details of the research design. In particular, each
phase’s methodological approach, sampling technique, data collection procedure, and data
analysis methods are explained. Most importantly, the last subsection describes how the two
phases of the research are combined, synthesizing the findings from each phase and drawing the
integrated implications out of this dissertation project as a whole.

Figure 3.1 Overall Design of the Research
36

3.2 Phase I: A Qualitative Meta-Study
As mentioned above, a qualitative meta-study was conducted in Phase I to analyze the
literature on information credibility with the particular goal of developing a new framework for
Web credibility assessment. More specifically, the survey of the literature in the study identified
conceptual typologies of the four core components of Web credibility assessment:
conceptualization, operationalization, variability, and process. Ultimately, Phase I was designed
to answer the following research questions:
RQ1: How is the process of Web credibility assessment conceptualized in existing
theories and models?
RQ1-1: What are the common and unique features of existing theories and models
of Web credibility assessment?
RQ1-2: How can the existing theoretical frameworks of Web credibility
assessment be improved?

Qualitative meta-study is “a research approach involving analysis of the theory, methods,
and findings of qualitative research and the synthesis of these insights into new ways of thinking
about phenomena” (Paterson et al., 2001, p.1). This method synthesizes qualitative research
reports taking into consideration the theoretical, methodological, and societal contexts of the
original studies. Meta-study does not merely synthesize the results of the previous research (i.e.,
primary reports) on the given topic or simply combine the results of a collection of similar
studies. Thus, meta-study is an interpretive qualitative research approach that is useful to
critically interpret the findings from various disciplines with regard to a particular phenomenon
under investigation (Paterson et al., 2001). In particular, Paterson et al. (2001) highlight that the

37

primary goal of meta-study is to develop a new (or expanded) theoretical framework concerning
the phenomenon under investigation, which is well matched with the first goal of the current
research—developing a new framework for Web credibility assessment.
The qualitative meta-study conducted in this research consists largely of three sequential
processes, based on the methodological guidelines established by Paterson et al. (2001): (1)
selection and appraisal of primary research reports; (2) meta-analysis of data, method, and theory
in the primary research reports; and (3) meta-synthesis of the meta-analysis results. The
following subsections describe the criteria for inclusion and exclusion of primary research
reports, the process of developing the appraisal tool used in the study, and data analytic approach
used in meta-analysis and meta-synthesis (Table 3.3).
3.2.1 Selection of Primary Research Reports
The literature analyzed in Phase I includes both theoretical and empirical studies on
credibility assessment of online information conveyed through various Web-based platforms,
such as more traditional and static websites and the newer and more collaborative social websites
such as SNSs, social Q&A sites, Wikis, and blogs. Also, this study reviewed some of the
older/classic studies of credibility which provided original definitions and conceptualizations of
the concept of credibility, which then have been used in more recent studies and reviews of
credibility assessment on the Web. It is important to review the older/classic studies as they
provide the foundational discussions on the conceptualization of credibility, identifying key
dimensions of credibility, which are directly related to the conceptualization of Web credibility.
To retrieve research reports examining Web credibility assessment, online databases
specializing in library and information studies (LIS), communication, psychology, and humancomputer interaction (HCI) were searched: Web of Knowledge; Association for Computing

38

Machinery (ACM) Digital Library; Library, Information Science & Technology Abstracts
(LISTA); and Journal Storage (JSTOR). Keywords used in the search included “credibility,”
“information credibility,” OR “Web credibility.” Thus, primary research reports that assigned
one or more of these words/phrases as keywords were included in the initial lists for further
analysis. Further, the researcher also performed a manual search of reference lists of pertinent
review articles on information credibility, such as Metzger, Miriam J. et al. (2003); Rieh and
Danielson (2007). All searches were limited to English-language publications between the years
of 2000 to 2014. A total of 83 primary research reports were included for analysis. The criteria
for inclusion and exclusion of the articles identified are detailed below.
3.2.1.1 Criteria for inclusion. It is crucial to have clearly defined criteria for inclusion in
accordance with the research purpose and research questions of the meta-study to locate the
appropriate primary research reports (Paterson et al., 2001). In order to identify the appropriate
primary research reports for the current qualitative meta-study, aiming to develop a new
framework of Web credibility assessment, the following three inclusion criteria were used: (1)
articles published in peer-reviewed journals or conference proceedings; (2) studies examining
users’ perceived credibility of online information; and (3) studies that propose, use, and/or test
theories and models of Web credibility assessment.
First, articles published in peer-reviewed journals or conference proceedings in LIS,
communication, psychology, and HCI are included. Unpublished reports, including dissertations
and works in progress are not included. The researcher agreed with the conventional point of
view that considers primary research reports that have not been accepted for publication in a
peer-reviewed journal (or conference proceedings) as initial statements about the quality of the
research, rather than concrete findings on the given topic (Beatty, Reay, Dick, & Miller, 2011).

39

This approach seems to be more practical and in common with the decisions made in previous
qualitative meta-studies (Beatty et al., 2011; Jørgensen & Shepperd, 2007; Paterson et al., 2001).
Second, studies examining people’s perception of information credibility and their
information behaviors in evaluating the credibility of various Web-based resources are included.
More specifically, this study focuses on information credibility assessment in the Web context,
rather than persuasion and/or attitude change in the interpersonal communication context.
Further, the study includes the primary research reports that approach the Web credibility
assessment issues from the user-centered perspective, as opposed to the system- or computeroriented perspective, being more interested in people’s perception of information credibility in
the Web context and their behavioral characteristics in looking for credible information online.
As mentioned above, since credibility has long been studied in various disciplines from different
perspectives, it is crucial to limit the boundary and pinpoint the focal area within the boundary.
Third, theoretical research studies that propose theories and models pertinent to Web
credibility assessment are included—i.e., studies of which the theories or models are outcomes of
the studies. Moreover, empirical studies that either use or test existing theories and models,
examining the influence of various factors on Web credibility assessment (e.g., user
characteristics, content topics, media types, etc.) are also included.
3.2.1.2 Criteria for exclusion. Paterson et al. (2001) highlight that including primary
studies that fail to clearly report their research designs and findings can jeopardize the overall
quality of a meta-study. Therefore, the current meta-study excludes: studies based on unusual or
skewed samples; studies that omit significant data or details of the research design; and studies
that come up with conclusions that are not supported by the data provided in the study.

40

Table 3.1 Criteria for Inclusion and Exclusion of the Qualitative Meta-Study
Inclusion criteria
o Articles published in peer-reviewed journals or
conference proceedings
o Studies examining users’ perceived credibility
of online information
o Studies that propose, use, and/or test theories
and models of Web credibility assessment

Exclusion criteria
× Studies based on unusual or skewed samples
× Studies omitting significant data or details of
the research design
× Studies with conclusions that are not supported
by the data provided in the study

3.2.2 Appraisal of Primary Research Reports
An appraisal tool was developed to facilitate a systematic review and assessment of the
primary research studies based on the guidelines of reporting empirical studies (Kitchenham et
al., 2008; Paterson et al., 2001) and appraisal tools used in previous meta-studies (Beatty et al.,
2011; Jørgensen & Shepperd, 2007). The appraisal tool was mainly used to describe, rather than
evaluate, the primary research reports in terms of main objects of credibility assessments (e.g.,
source, message, and media), nature of sample, and research design. Thus, the appraisal tool
helped the researcher determine whether or not each of the primary research studies was eligible
for inclusion in the meta-study, as well as record pertinent data about the study under
examination. Table 3.2 shows the overall design of the appraisal tool used in the study, which
consists of four main sections and their subsections.

Table 3.2 Design of the Appraisal Tool Used in the Qualitative Meta-Study
Sections
Reference Information

Content
Research Design
Nature of Sample

Required information/evaluation items
•
•
•
•
•
•
•
•
•

Bibliographic information
Publication type
Discipline of the study
Credibility type investigated—objects of credibility assessment
Topics of the online resources under study
Data collection methods
Types of platforms
Total number
Demographics (age & gender)

41

3.2.3 Qualitative Meta-Analysis of Data, Method, and Theory
3.2.3.1 Analytic approach. The first step to analyze a corpus of primary research studies
is determining the appropriate data analytic approach. Even though any systematic interpretive
approach can be applicable to meta-study, choosing the appropriate data analysis strategy that
fits the given research questions and design remains as the researcher’s role (Paterson et al.,
2001). Considering the purpose of the current study aiming at developing a new framework of
Web credibility assessment by synthesizing common and unique components of existing theories
and models, the researcher used (1) conceptual classification (i.e., typology), which is a useful
approach to analyze existing theoretical frameworks, as it groups entities (i.e., main components
of theories and models of Web credibility assessment) by similarity (Bailey, 1994). In particular,
the researcher used substruction as the method of typology construction, which is the “process of
extending the dimensions of a single type in order to form the full typology of which it is a part”
(Bailey, 1994, p. 24).
Furthermore, (2) content analysis was used to summarize the contents of the selected
primary research reports in terms of objects (or types) of credibility assessments, measures used
to operationalize credibility, and characteristics of the samples. This analytic approach was
intended to identify and describe these features in the primary research reports, rather than
conducting inferential statistics, showing patterns or relationships among the variables identified
out of the qualitative meta-study. When content-analyzing the primary research reports, the
typologies developed through the conceptual classification using substruction in the previous
phase of the data analysis.
3.2.3.2 Data management. Qualitative analysis of the collected data was performed
using software applications such as NVivo 10 and EndNote X6. NVivo 10 was used for a

42

systematic coding process, facilitating the content analysis on the primary research reports,
categorizing them based on the typologies developed in the study; additionally, EndNote X6 was
used to manage bibliographic information of a large number of primary research reports used in
the qualitative meta-study.
3.2.4 Qualitative Meta-Synthesis
Qualitative meta-synthesis brings the insights from the prior processes of a qualitative
meta-study together (i.e., meta-data-analysis, meta-method, and meta-theory) to suggest a new
way of understanding the given research topic (Paterson et al., 2001). One of the distinguishable
outcomes of the exercise is forming a new theory, rather than a simple aggregation the findings
from the prior phases of a qualitative meta-study. The overall process of the Web credibility
assessment framework development is guided by Bailey’s (1994) Three-Level Measurement
model, which is a useful approach for typology construction. This model defines the ways of
understanding a certain concept (or phenomenon) under investigation (i.e., Web credibility) with
three levels, (1) conceptual, (2) empirical, and (3) indicator levels. The conceptual level is based
on theoretical or hypothetical (or even imaginary) constructs with no empirical cases, while the
empirical level identifies purely empirical examples with no theoretical counterpart. The
operational or indicator level is a combination of the conceptual level with the empirical level.
This exercise of mapping both the conceptual and empirical levels into the indicator level can be
done through two ways, deductively or inductively. The deductive strategy—it was termed the
classical strategy in Bailey (1973)—first identifies conceptual types of the concept under
investigation and then searches for empirical examples for each type. In other words, it is a topto-bottom approach. The inductive strategy, which is a bottom-to-top approach, first identifies

43

empirical clusters (the empirical level) and then assigns conceptual labels to them. This approach
is also considered a strategy of grounded theory (Glaser & Strauss, 1967).
The current qualitative meta-study takes advantage of the both strategies, doing iterative
comparison and refinement of the conceptual framework through matching the empirical
examples. More specifically, the qualitative meta-study deductively proposes a conceptual
framework of Web credibility assessment by synthesizing existing theoretical frameworks. In
particular, the framework suggests conceptual (or hypothetical) typologies of key dimensions of
credibility, measures used to operationalize credibility dimensions, influences of user
characteristics and context, and the process of Web credibility assessment. This conceptual
framework is then used to analyze the literature on information credibility, inductively forming
clusters and categorizing the appropriate content into the typologies. The framework is updated
when new credibility dimensions, measures, and/or variability factors (e.g., user characteristics
and context) that cannot be accommodated by the conceptual framework are found.

Table 3.3 Summary of Phase I (Qualitative Meta-Study)
Procedure

Activities

Data analysis techniques
and instruments/tools
Inclusion and exclusion
criteria (Table 3.1)

Selection of primary
research reports

Primary research reports
search via online
database hand search

Appraisal of primary
research reports

Qualitative metaanalysis of data,
method, and
theory

83 primary research
reports identified

Initial (descriptive)
analysis of the
identified primary
research reports

Appraisal tool (Table
3.2)

Descriptive analysis of
the research reports

Meta-analysis of the
findings, methods, and
theories used in the
primary research
reports

• Conceptual
classification
• Content analysis
• EndNote X6
• QSR NVivo 10

Initial typologies of the
common and unique
components of Web
credibility assessment

44

Results/Products

Table 3.3 – Continued
Qualitative metasynthesis

Synthesis of the findings
from the meta-dataanalysis, meta-method,
and meta-theory

• Three-Level
Measurement model
(Bailey, 1994)

Conceptual typologies
for the main facets of
Web credibility
assessment identified:
• Conceptualization
• Operationalization
• Contexts
• User Characteristics
• Process of assessment

3.3 Phase II: Semi-Structured Interviews
In Phase II, the researcher employed semi-structured interviews as the effective means of
data collection to explore older adults’ credibility assessment of online health information. The
study focused on health information as it is one of the most common and necessary topics of
information in older adults’ everyday life (Czaja et al., 2006; Su & Conaway, 1995; Taha et al.,
2009; Williamson, 1998). Moreover, credibility is a particularly crucial factor that requires
consideration in health information seeking as the decision to make use of health information can
affect the overall quality of human life (Gustafson & Wyatt, 2004).
The new framework proposed in Phase I and the ELIS model by Savolainen (1995) were
used in the process of developing the interview protocol, which then guided the data analysis and
interpretation in the study. Phase II aimed to gain a better understanding of older adults’ common
information needs and information seeking behaviors in their daily lives (i.e., ELIS) with special
interests in their perceptions of credibility in the Web context. As the research participants were
all retirees (one participant was preparing her retirement, which was scheduled for three days
after the interview), it is appropriate to look into their information needs and related information
behaviors through the lenses of ELIS based in the ‘non-work’ context, as opposed to the workrelated context. In particular, this study frames older adults’ ELIS into two dimensions using the
45

ELIS model by Savolainen (1995): (1) seeking of orienting information that covers older adults’
general information needs and related information seeking behaviors that are closely related to
their hobbies and leisure activities; and (2) seeking of practical information that addresses older
adults’ specific information needs for problem solving and related information-seeking
behaviors—as mentioned above, the current study focused on health-related information needs
for older adults (Taha et al., 2009; Williamson & Asla, 2009).
Overall, Phase II was designed to answer the following research questions:
RQ2: In general, what are older adults’ common (ELIS) information needs?
RQ3: What are older adults’ health information needs and related information behaviors?
RQ3-1: What sources do older adults use to find health information both on- and
offline, and why do they use those sources?
RQ3-2: How do they use the information they find?
RQ4: How do older adults assess the credibility of health-related information on the
Web?
RQ4-1: What are older adults’ perceptions of Web credibility?
RQ4-2: What are some of the psychological, social, and/or cultural mechanisms
that underlie and/or affect those perceptions?
RQ4-3: What are some of the markers/cues and heuristics used by older adults to
assess the credibility of health-related websites?

3.3.1 Target Population and Sampling Criteria
The target population of this study is older adults who have sought for health information
online. This study defines older adults as people who are 55 years old or older based on

46

Neugarten’s (1974) definition, which is relatively more specific and comprehensive than other
definitions reviewed in Chapter 2 (Table 2.5). In particular, Neugarten’s subdivision of older
adults (i.e., ‘young-old’ and ‘old-old’) is a useful framework to have a deeper understanding of
the target population, identifying differences in perceptions of information credibility on the
Web even among older adults. Fisk et al. (2009) mention that older adults tends to be more
heterogeneous than younger age groups due to age-related declines in perception, cognition, and
movement control that might affect their interaction with computers and technology. Therefore,
it would be more appropriate to use the definition of older adults by Neugarten that covers a
wide range of ages (55+) and specifies the sub-groups (i.e., young-old and old-old), rather than
relying on a single chronological age cut-off.
Among the total population of American older adults, this research limits the population
to older adults who reside in Florida. Considering the location of the researcher (i.e., Tallahassee,
Florida) and the desired manner of interviews with participants (i.e., one-on-one, face-to-face),
the researcher focused on Florida residents for recruitment. Furthermore, since the proportion of
older adults (aged 55+) in Florida (29.78%) is not only significantly higher than the average
proportion of the age group in the country (24.86%), but is also the highest among all fifty states
(U.S. Census Bureau, 2012), Florida is a good place to study older adults.
Lastly, since the study focused on the credibility assessment of ‘online’ health
information, older adults who had not looked for information online about health/wellness issues
were not included in the research. Focusing on older adults who use online health information
specified the interview processes and helped to ensure the validity of the data. In particular, the
researcher recruited ‘active’ online health information users who searched for health information

47

online at least once during last six months. This sampling criterion was also beneficial from
recall perspective. Table 3.4 summarizes the target population and the criteria for sampling.

Table 3.4. Target Population and Sampling Criteria
Target population
American older adults, using
online health information on a
regular basis

Criteria for sampling
• Older adults who are 55 years old or older
• Florida residents, as of the data collection point
• Older adults who have used/searched for online health
information at least once over the past 6 months

3.3.2 Participant Recruitments
Considering the purpose of this qualitative investigation that aimed to explore the specific
target group’s credibility perception of online health information and related behaviors, the
researcher used the (1) purposive sampling and (2) snowball sampling methods to recruit
subjects who could provide the appropriate, rich and in-depth information for the purpose of the
study (Merriam, 2009; Schutt, 2009). Purposive sampling is “a nonprobability sampling method
in which elements are selected for a purpose, usually because of their unique position” (Schutt,
2009, p. 173). This sampling technique helped the researcher select informants who met the three
sampling criteria mentioned in Table 3.4. Snowball sampling is “a method of sampling in which
sample elements are selected as they are identified by successive informants or interviewees”
(Schutt, 2009, p. 174). This sampling technique was useful to recruit “hard-to-reach”
participants.
The researcher received approval for the study from the Florida State University Human
Subjects Committee (Appendix A: Human Subjects Committee Approval Memorandum) before
the participant recruitments were initiated.

48

3.3.2.1 Recruitment site. The Osher Lifelong Learning Institute (OLLI) at Florida State
University was used as the initial recruitment site where the researcher identified first few
participants and looked for further participants using the snowball sampling technique. OLLI is a
program of classes targeted to older adults who are 50 years old or older. On average, this
institute offers 75 to 80 classes per year within the timeframes of a 6-week Spring- and a 6-week
Fall Term and a 30week session in May. Topics covered in the OLLI classes include current
trends and issues, art, world history, technology, and so forth (Osher Lifelong Learning Institute,
2015). The researcher contacted the administrative staff of OLLI and gained permission to meet
their members. The administrative staff sent out an invitation message to their members via
email, introducing the research purpose and design, and those who were interested in the study
were asked to contact the researcher to schedule an interview. Also, the researcher asked the
initial study participants, recruited from OLLI and completed the interview, to spread the word to
their friends who might be interested in the study.
3.3.2.2 Prescreen. When potential participants expressed their interests in the research
via emails or calls, the researcher scheduled a telephone interview with each person and
conducted a prescreen test. The main purpose of conducting a prescreen test was to make sure
that the participant’s cognitive function was adequate for the study, as the participant was
supposed to answer interview questions based on their previous experiences, reminding
themselves of how they did to seek for information and judge the credibility of online health
information. The researcher used two established instruments for the prescreen test over the
telephone: (1) Pfeiffer’s (1975) Short Portable Mental Status Questionnaire (SPMSQ) and (2)
Wechsler’s (1997) Wechsler Memory Scale III (WMS-III). SPMSQ (Pfeiffer, 1975) is an
instrument developed to assess the presence and degree of any intellectual impairment using ten

49

questions that the participants must answer, without referencing outside materials. Questions
asked in the instrument include the date, recalling the names of former presidents, a small
subtraction problem, and so on. The following scale is the suggested cut-off points to determine
if the participant has passed the prescreen test: 0 to 2 errors = intact; 3 to 4 errors = mild
intellectual impairment; 5 to 7 errors = moderate intellectual impairment; 8 to 10 errors = severe
intellectual impairment (Appendix C). WMS-III (Wechsler, 1997) involves reading aloud a short
story that the participants must then recall as much as they can. If the participant does not pass
the first story, a second story is then read aloud. The following scale is used to determine if the
participant has passed: Story A = 6, if failed, Story B = 4. Only those who passed the prescreen
test was invited to the interview study (Appendix C). There was no one disqualified based on the
prescreening.
Overall, twenty-one participants were recruited. Each of the first nineteen participants
received $10 and the last two received $25 in exchange of their participation. The researcher
increased the amount of incentive to encourage the potential subjects’ participations from
minority groups; the necessarily IRB update was approved (Appendix B).
3.3.3 Interview Protocol Development
Interview protocol is a set of questions that guide the interview (Barriball & While,
1994). As mentioned above, the interview protocol used in the current study was designed based
on the new framework proposed in Phase I and the ELIS model by Savolainen (1995).
Specifically, the three components of the new framework, i.e., assessment, variability, and
process of Web credibility assessment, elicited necessary information that ultimately answered
RQ4 and its sub-questions of the study, which asked about older adults’ credibility assessment of

50

online health information. The ELIS model guided the data analysis and interpretation process
regarding older adults’ information behaviors in the everyday life context.
To develop items (questions) that would be included in the interview protocol (Appendix
D), the researcher referenced the items used in two existing survey instruments designed by the
Stanford Persuasive Technology Lab (Stanford Persuasive Tech Lab, 2015) and the Center for
Research and Education on Aging and Technology Enhancement (CREATE, 2015). Since these
two instruments focused on the key components of the current study (i.e., older adults and Web
credibility assessment), and had been used with a large group of people, the validity and
reliability of the questionnaires were assumed ensured. Furthermore, the interview protocol was
pilot-tested in a preceding research (Choi, 2013) that explored how older adults perceive various
credibility markers/cues on health-related websites. Table 3.5 below overviews the main sections
and questions (items), requested information by each question, and question types. Appendix F
also shows the relationships among the research questions, theoretical frameworks used, and
interview questions.

Table 3.5 Design of the Interview Protocol
Item No.

Requested Information

Question Type

Section A
A1
A2
A3

Internet Use
Hours of Internet use a week
Years of Internet use experience
Health information seeking on the Web

Section B

Older Adults’ Information Needs

B1
Section C
C1
C2
C3

Closed-ended (multiple choice)
Closed-ended (multiple choice)
Closed-ended (yes-or-no)

Topics of ELIS

Open-ended

Older Adults’ Health Information Needs
and Related Behaviors
Health information needs
Sources of health information
Reasons for using certain source(s)

Open-ended
Open-ended
Open-ended

51

Table 3.5 – Continued
C4

Health information use

C5

Health information share

Section D
D1
D2

Closed-ended (yes-or-no);
Open-ended, if answered ‘yes’
Closed-ended (yes-or-no);
Open-ended, if answered ‘yes’

Credibility of Online Health Information
Credibility markers/cues
Credibility markers/cues

Open-ended
Closed-ended (5-Likert scale; 35 items)

3.3.4 Data Collection
3.3.4.1 Semi-structured interview. A semi-structured interview was employed as the
data collection method to have a comprehensive understanding of older adults’ credibility
assessment of online health information. A semi-structured interview is a type of interview that
is well-suited for the exploration of the perceptions and opinions of respondents regarding
complex issues and provides an additional opportunity to probe for more information and
clarification of answers (Barriball & While, 1994). In terms of types of questions that can be
asked in the interview, the semi-structured interview incorporates both open-ended and more
theoretically driven questions, eliciting data grounded in the experience of the participant as well
as data guided by existing theoretical frameworks regarding the particular research topic
(Galletta, 2013). Thus, the semi-structured interview was a useful data collection method not
only to understand older adults’ Web credibility assessment process, but also to test and refine
the new framework developed in Phase I.
Another significant benefit of using the semi-structured interview was that the researcher
had the opportunity to help participants understand any ambiguous and/or difficult-to-understand
terms in the Liker-type items included in the last section of the interview protocol (see Appendix
D). Since older adults have relatively less experience and familiarity with the Internet than
younger generations and a wide range of cognitive and physical abilities (Fisk et al., 2009),
52

employing the self-administrated survey may not be an appropriate method to accurately capture
older adults’ perceptions of Web credibility. Thus, the semi-structured interview, which allowed
the researcher to utilize the pre-determined questions as well as have the additional opportunity
to probe participants’ responses for clarification, was beneficial for examining older adults’ Web
credibility assessment.
3.3.4.2 Interview administration. Interviews were conducted at sites of participants’
choosing (e.g., participants’ houses, public libraries, etc.). The researcher met with participants
in a face-to-face, one-on-one interaction to produce considerable reciprocity between the
participant and the researcher, which allows an interviewee the freedom to express his or her
thoughts and creates space for the researcher to probe the interviewee’s responses for
clarification, meaning making, and critical reflection (Galletta, 2013; O'Leary, 2005). The final
sample size of the study was twenty-one. Each interview lasted for approximately 38 minutes.
Data collection lasted from June 2014 to April 2015. Each interview was conducted through
three segments: (1) opening segment, (2) middle segment, and (3) final segment.
In the opening segment, the researcher explained the purpose of the study, having the
participant read and sign an informed consent form (Appendix E), and letting the participant
know their rights in the study they were participating in. In particular, each participant was
reminded of the fact that the interview would be voice-recorded, as stated in the consent form.
After the participant understood and agreed upon the research design, questions regarding their
Internet use and experience were asked using the interview protocol Section A. Since the
perception of Web credibility and the process of Web credibility assessment may be affected by
the user’s technology proficiency and experience with the Internet (Ahmad, Komlodi, Wang, &
Hercegfi, 2010b; Lucassen et al., 2013; Zulman et al., 2011), the researcher further asked about

53

the interviewee’s background information regarding Internet use and technology proficiency,
such as their past career—whether they were supposed to use a computer and the Internet in the
workplace. This question elicited the information regarding the research participants’
occupational backgrounds, which was useful to better understand their information beahviors.
The middle segment had main questions that were more specific for the research
questions. More specifically, questions in this segment asked about older adults’ common
information needs in their everyday lives (Section B); older adults’ health information needs and
related behaviors (Section C); and Web credibility assessment of online health information
(Section D). In particular, when asking about the cues/markers and heuristics by using openended questions in Section D, the researcher helped the interviewee remind themselves of the
specific cues/markers and heuristics that they employed in Web credibility assessment by
showing the website(s) they identified as credible, using the researcher’s tablet or laptop.
In the final segment, each interview ended by asking a set of closed-ended questions,
which were based on a 5-point Likert-type scale. The research participants were asked to rate
each of the thirty-five pre-identified credibility markers/cues based on whether they would
increase or decrease their perceptions of Web credibility, by indicating the appropriate number in
a 5-point Likert-type scale with –2 being “much less credible;” –1 being “less credible;” 0 being
“neutral;” +1 being “more credible;” and +2 being “much more credible.” As mentioned above,
the items used in the final segment of the interview (i.e., credibility markers/cues) were
developed based on the existing survey questionnaire that was used in large survey studies (Fogg
et al., 2001; Fogg et al., 2003) and pilot-tested in a preceding study (Choi, 2013). After each
participant fished answering all the questions in the final segment, the researcher addressed
questions that participants had. Lastly, each participant was rewarded for his or her

54

participation—as mentioned in 3.3.2 Participant Recruitments, the first nineteen participants
received $15 and the last two participants received $25 in cash.
3.3.5 Data Analysis
First of all, qualitative data collected from twenty-one semi-structured interviews were
transcribed for data analysis. The initial lists of codes were identified based on the framework
proposed in Phase I and the ELIS model by Savolainen (1995): (a) six types of credibility
markers/cues and heuristics, (b) individual and contextual variables of Web credibility, (c)
process of Web credibility assessment from the new framework; and (d) “way of life,” (e)
“mastery of life,” (f) orienting and practical information-seeking, (g) social capital, and (h)
cultural/cognitive capital (Appendix G). Even though the coding process was mainly guided by
the frameworks (theoretical coding technique), the researcher also looked for emerging themes
that could not be coded by the frameworks (open-coding technique). The computer software,
QSR NVivo 10, facilitated the management of various types of raw data such as the voicerecorded interview files, transcripts, and memos, and aided a systematic analysis of the data sets,
identifying and assigning codes, categories (themes), and relationships between codes.
Furthermore, descriptive (e.g., mean, median, range, standard deviation) as well as
inferential statistics (e.g., one-sample t test) techniques were applied for the numeric data
generated from the last segment of the semi-structured interviews, which were based on the 5point Likert type scales (Appendix D). As mentioned above, the thirty-five items used in the
current study were tested in previous studies (Choi, 2013; Fogg et al., 2001; Fogg et al., 2003).
The ratings on these items were combined with and compared to the data acquired from the
open-ended question asked about older adults’ Web credibility assessment process. IBM SPSS

55

Statistics 22 was used for the quantitative analysis of the numeric data. Table 3.6 below
summarizes the overall process of the semi-structured interview.

Table 3.6 Summary of Phase II (Semi-Structured Interviews)
Procedure

Activities

Data analysis techniques
and instruments/tools
• 55+ years old
(Neugarten, 1974)
• Looked for health
information online over
the past 6 months

Results/Products

Sampling

Defining the target
population and sample

Sampling criteria (Table
3.4)

Participant
recruitments

• Identifying potential
subjects based on the
sampling criteria
• Conducting the
prescreen test

• Sampling criteria
• SPMSQ
• WMS-III

21 participants recruited

Data
collection

• Developing the
interview protocol
• Conducting semistructured interviews:
(1) Opening segment
(2) Middle segment
(3) Final segment

• The new framework
developed in Phase I
• Two existing survey
instruments (CREATE,
2015; Stanford
Persuasive Tech Lab,
2015)
• Voice-recording
• Memo

• Interview protocol
(Appendix D)
• Qualitative data
collected

Data analysis

• Transcribing and
coding the interviews	  
• Conducting descriptive
and inferential statistics
on the ratings of 35
closed-ended items

• Interview protocol
• QSR NVivo 10
• IBM SPSS Statistics 22

RQs 2, 3, and 4 answered

3.3.6 Ethical Consideration of Semi-Structured Interview
Since the semi-structured interviews in this dissertation research involved human
subjects, the researcher took into considerations of the ethical issues that may occur. First of all,
56

this research did not include any harmful or sensitive contents or materials to the participants. As
the main purpose of conducting semi-structured interviews was to explore how older adults
assess Web credibility to identify reliable and relevant online resources for their information
needs, the researcher asked only about their opinions and perceptions of online information
credibility, rather than testing or experimenting on any aspects of their cognitive or physical
abilities. Thus, risks associated with the research were very low and were considered no greater
than those of everyday life.
Also, the researcher collected the minimum range of personal information of the
participants that was necessary for the research purpose, such as age, gender, ethnicity,
educational background, and Internet experience. All collected data were made anonymous, and
only pseudonyms were used in data analysis and reports. Thus, minimal risk was associated with
the impact on privacy even if excerpts from interviews reveal information that may be
considered to affect an individual’s privacy.
Lastly, all participation was voluntary; participants were asked to read and sign a consent
form testifying to their willingness to voluntarily engage in the study. The consent form clearly
informed about the research design and the right to privacy. The overall research design and
associated research materials including the consent form, pretest instruments, and the interview
protocol were reviewed and approved by the Florida State University’s Human Subjects
Committee (Appendices A and B).
3.4 Trustworthiness of Qualitative Research
Qualitative research based on an interpretive/constructivist perspective assumes that there
is no single reality that can be observed, measured, or ‘found’ as seen from a positivist/postpositivist perspective; rather, there are multiple realities of a single event that should be

57

understood, interpreted, and constructed (Merriam, 2009). In other words, qualitative research
does not aim to find generalizable knowledge that is replicable (i.e., reliability) and applicable
(i.e., valid) to understand different types of populations; rather, it aims to gain a deep
understanding of a particular group of people’s social, psychological, and/or behavioral
characteristics regarding a certain topic—e.g., older adults’ perceptions of Web credibility and
their behaviors in assessing the credibility of online health information. Therefore, even though it
is obvious that all types of research are supposed to produce reliable and valid knowledge, the
ways to ensure the “trustworthiness” of qualitative research are necessarily different from those
for quantitative research, as they stand upon different philosophical assumptions about the
phenomenon or reality under examination. As the nature of the current research was qualitative,
rather than quantitative, the researcher consulted four criteria suggested by Lincoln and Guba
(1985) to ensure the trustworthiness and rigor in the qualitative investigation: (1) credibility, (2)
transferability, (3) dependability, and (4) confirmability. The following subsections provide the
definitions of the quality criteria and explain how the current research meets those criteria.
3.4.1 Credibility
The quality criterion credibility is concerned with “truth value” of findings from a
qualitative study that represents the construction of the reality under investigation. In order to
deal with the credibility criterion, one can pose a question of “how can one establish confidence
in the trust of the findings of a particular inquiry for the subjects (respondents) with which and
the context in which the inquiry was carried out?” (Lincoln, 1995, p. 290). When using the
traditional term used in qualitative research, credibility may be understood as the internal
validity (Lincoln & Guba, 1985; Merriam, 2009). In order to satisfy this criterion (i.e.,
credibility), this research used the technique of triangulation using multiple methods, sources of

58

data, and theories (Denzin, 1989). First of all, this research was designed to collect different
types of data from multiple sources to address the research questions regarding people’s Web
credibility assessment. More specifically, the qualitative meta-study (Phase I) analyzed both
theoretical and empirical research results in the literature on information credibility. The metasynthesized findings out of the study formed a new framework of Web credibility assessment,
which then guided the semi-structured interviews with older adults (Phase II). As elaborated in
the preceding section, 3.3.3 Interview Protocol Development, the interview protocol used in the
semi-structured interviews included not only open-ended questions that allowed participants to
freely talk about the topic under study (i.e., credibility assessment of online health information),
but also closed-ended questions based on 5-point Likert scales helped the participants show their
opinions about varied types of cues/markers and heuristics that might not be reminded of by
themselves—i.e., triangulation using multiple methods was made.
In addition, in Phase II, the researcher put a significant amount of effort into recruiting
different types of informants in terms of age, gender, and education in order to enhance the
credibility of the findings, accommodating the variety in the target population’s perceptional and
behavioral features by demographic variables. Nationally representative data about health
information searching (Fox & Duggan, 2013) showed that there were significant differences
between gender groups (female Internet users looked online for health information more than
male Internet users); age groups (older Internet users who were 65+ looked online for health
information less than younger age groups); and education levels (college graduates looked online
for health information more than any other lower levels of education groups). Thus, the findings
that are reported in the following chapters are mainly based on, but not limited to, the

59

intersectional information by multiple interview participants having different socioeconomic
backgrounds—i.e., triangulation using multiple sources was made.
Lastly, this research was built upon multiple well-established theories and models
pertinent to human information behaviors, such as the ELIS model (Savolainen, 1995), the IF
theory (Pirolli & Card, 1999), and the synthesis of six existing models of Web credibility
assessments (Table 2.4). Thus, findings from Phase II regarding older adults’ credibility
assessment of online health information were interpreted using multiple angles of lenses, which
helped enhancing the credibility of the findings—i.e., triangulation using multiple theories was
made.
3.4.2 Transferability
Transferability is concerned with the “applicability” of the findings from a particular
study to other studies in different contexts or with different subjects (Lincoln & Guba, 1985). In
the traditional paradigm, transferability is named external validity (Lincoln & Guba, 1985;
Merriam, 2009). However, from the interpretive/constructivist perspective (or the naturalistic
paradigm) that aims to establish a deep understanding of a particular population’s informationseeking behaviors in a certain context, rather than seeking for generalizable findings that can be
applicable to different settings, external validity is not the goal of the qualitative investigation
(Merriam, 2009). Furthermore, since the transferability of the findings from one study to another
depends on the degree of similarity between the two (Lincoln & Guba, 1985), the researcher
cannot measure the degree of transferability of the findings from the current study to any future
research that will be carried out in different contexts. Thus, “rich, thick description” can help
readers and following researchers know whether the settings of the given study can be reused or
referenced in other studies and whether the findings of the study can be transferred to other

60

studies in similar contexts (Merriam, 2009). Therefore, the researcher expatiated about the
context where the study is situated and the processes of which the study went through, so that
future researchers can use (i.e., transfer) the findings to understand the topic under similar
contexts—i.e., people’s Web credibility assessment.
In particular, the new framework proposed by the qualitative meta-study (Phase I) is
particularly transferable to other studies focusing on research topics regarding people’s
credibility assessments in the Web context. As the framework was developed based on an
extensive review of the literature on Web credibility in the disciplines of LIS, HCI,
communication, and psychology, it may be useful and usable for studies in these fields that
address Web credibility issues from the perspective of how people perceive the credibility of
online information in the process of seeking relevant information for their information needs.
The semi-structured interview study with older adults in this dissertation (Phase II) can be
considered a case study that shows how to use the new framework along with existing theories or
models pertinent to the given context. In particular, the attached appendices provide more
detailed information about the research, such as interview protocol (Appendix D); relationships
between research questions, theoretical frameworks, and methods (Appendix F); and coding
schemes for data analysis (Appendix G).
3.4.3 Dependability
Dependability concerns with the question of whether the process of the inquiry is
acceptable as well as whether the product (e.g., data, findings, interpretations, and
recommendations) is supposed by the data collected and is coherent; dependability is a
substitutive term for “reliability” (Lincoln & Guba, 1985). From the conventional research
paradigm (i.e., positivist/post-positivist perspective) that assumes that there is a tangible and

61

unchanging reality, reliability is the extent to which research findings can be replicated with the
same (or similar) subjects in the same (or similar) context. Lincoln and Guba (1985) suggested
inquiry audit as a strategy to ensure the dependability of qualitative research. As the name of the
technique implies—it is metaphorically based on the fiscal audit—auditors (i.e., reviewers)
examine the process and product of the qualitative research determining whether they are sound
and coherent. In this dissertation research, four members in the dissertation committee, including
a university representative from outside of the school where the researcher was located, played
the role as auditors in the stages of designing the research (i.e., prospectus defense) as well as
conducting the proposed research and reporting the findings (i.e., dissertation defense).
As for the product’s dependability, the new framework of Web credibility assessment
proposed in Phase I was reviewed by two external experts in the topic area (i.e., information
credibility) for journal publication: Choi and Stvilia (in press). Through the two rounds of
reviews and revisions between the reviewers and the researcher, the framework’s dependability
was significantly improved.
In addition, the coding scheme used to analyze the interview data collected in Phase II
was examined through the following four phases. First, the initial coding scheme was reviewed
by the researcher’s academic advisor who was well aware of the overall research design as well
as the theoretical frameworks used to develop the coding scheme. Second, a fellow doctoral
student, who was familiar with the literature on human information behavior and the coding
process, was recruited to check the dependability of the coding scheme and the interview data. In
particular, the researcher provided the third-party coder (i.e., the doctoral student) with an
instruction session, informing him about not only the overall research design (e.g., the main
purpose of the study, theoretical frameworks used, and general characteristics of the

62

interviewees), but also the definition of each code and rules of assigning the codes and
corresponding examples. After then, the researcher and the recruited coder coded an interview
transcript independently and compared their coding results to identify any disagreements. Third,
the researcher and the coder had an in-person meeting to discuss about the differences in their
codes and resolved them. Using the refined coding scheme based on the discussion, both coders
coded another interview together, reading the transcript line by line, to have further refinements
in the coding scheme by exchanging opinions immediately when any disagreements or
discussion points emerged. Fourth, the researcher used the final coding scheme refined in the
third phase to code all the interviews collected.
3.4.4 Confirmability
Confirmability addresses the question of whether the characteristics of the data can be
confirmed by other people who review the qualitative research, rather than the question of
whether the researcher can establish the objectivity in the research findings; thus, confirmability
concerns with whether the given study is free from the researcher’s biases, motivations, interests,
or perspectives (Lincoln & Guba, 1985). The confirmability audit that examines the specification
of the research data and process is a technique that allows readers to review and confirm the
quality of a qualitative investigation (Lincoln & Guba, 1985). In order to enhance the
confirmability of the current research, the four members of the dissertation committee, including
a university representative from outside of the school where the researcher was located, reviewed
the overall process of the research from designing the research to reporting findings.

63

CHAPTER 4
FINDINGS
4.1 Findings from Phase I: A Qualitative Meta-Study
This section reports on the findings from the qualitative meta-study conducted in Phase I.
In Chapter 2, the researcher provided a thorough review of the literature on information
credibility in terms of conceptualization (i.e., key dimensions of credibility), operationalization
(i.e., measures for the dimensions); influences of user characteristics and contextual variables on
Web credibility assessment; and theoretical frameworks that theorize the process of Web
credibility assessment.
By synthesizing the main findings reported, the researcher proposes a new framework of
Web credibility (WC) that consists of three main components: (1) Assessment; (2) Variability;
and (3) Process of WC. As the WC framework stems from the typologies produced by the
literature analysis (a qualitative meta-study), it is still in the conceptual stage. However, this new
framework can be a useful theoretical lens to look into how people deal with credibility issues in
the Web for several reasons.
First of all, this new framework combines all the important facets of theoretical
frameworks of Web credibility assessment identified in the literature analysis (i.e.,
conceptualization and operationalization of credibility, context and user characteristics that
influence Web credibility assessment, and the process of Web credibility assessment). In
particular, the “Assessment” component of the WC framework provides a more elaborated
typology of Web credibility measures by cross-mapping the two key dimensions of credibility
(i.e., trustworthiness and expertise) and the objects of assessment (i.e., operator, content, and
design). In other words, the extended (cross-mapped) typology further categorizes the measures
64

used to assess Web credibility in the literature, specifying which measures could be applicable
for each of the credibility dimensions, i.e., trustworthiness and expertise (Table 4.1).
In addition, the WC framework takes both context- and user-related variables into
account as they have significant impacts on the overall process of Web credibility assessment.
Particularly, the WC framework can go well with models of human information behavior
especially in the social paradigm that takes social factors into consideration (Pettigrew, Fidel, &
Bruce, 2001). In this dissertation research, Savolainen’s ELIS model (1995) was employed to
understand the target population’s (i.e., older adults’) health information behaviors in the context
of ELIS, in which people have to select the most credible sources to fulfill their information
needs. Several factors regarding an individual’s characteristics as well as social environments
around the users identified in the ELIS model can be used to understand the dynamic, rather than
static, nature of people’s perceptions of information credibility and the process of Web
credibility assessment.
4.1.1 Assessment of Web Credibility
As mentioned above, the Assessment component of the WC framework deals with the
conceptualization and operationalization of Web credibility. To connect the dimensions of the
credibility conceptualization with relevant credibility measures found in the literature, the
framework organizes those measures by the three categories of the Web Credibility Framework
of Fogg (2003a) and then mapped to the two key dimensions of credibility (i.e., trustworthiness
and expertise). This cross-mapping exercise produces six categories (i.e., operator
trustworthiness; operator expertise; content trustworthiness; content expertise; design
trustworthiness; and design expertise) that form a more elaborate conceptualization for

65

understanding relationships among the key dimensions of credibility, related measures, and
objects of those measures (Table 4.1).
4.1.1.1 Measures of operator trustworthiness. The trustworthiness of a website can be
assessed based on the trustworthiness of its operator. Several of the previous studies have
identified sub-dimensions of trustworthiness by asking participants to rate the importance or
appropriateness of related adjectives: neutral, balanced, unbiased, even-handed, fair, ethical,
believable, consistent, well-respected, trusted, honest, and sincere (Cheung & Lee, 2006; Hong,
2006a; Johnson & Kaye, 2000, 2009; Liu & Huang, 2005; Westerwick, 2013). In this subsection,
measures used in the literature that are related to the operator’s trustworthiness are reviewed
within four categories: (1) commercial implication, (2) perceived integrity, (3) transparency, and
(4) decency.
One of the frequently mentioned measures in the literature that can be grouped in the
category of an operator’s trustworthiness is a website’s “commercial implication,” checking
whether it is a commercial or non-commercial site. In particular, it examines the website’s
URL—whether it ends with .com or .gov, .org, or .edu; ads on the website; whether it has pop-up
windows with ads; whether it requires paid subscription to gain access (Fogg et al., 2001). Using
these measures, Choi (2013) found that older adults perceived non-commercial websites as more
credible than commercial ones for health information because they did not provide information
to make a profit for themselves. Participants in the study mentioned that when they saw ads on
health-related websites, they perceived the websites as trying to sell something, rather than
providing the public with useful information.
Operator’s trustworthiness also matters in online shopping sites. Cheung and Lee (2006)
measured “perceived integrity,” which was a significant factor for the trustworthiness of Internet

66

merchants, based on users’ ratings on whether the vendors charge the same price for Internet
shoppers. Jansen and Resnick (2006) focused on the effect of sponsored links (vs. non-sponsored
links) on Web searching behavior in the context of online shopping. This study showed that
when using a Web searching engine for e-commerce searching, participants were more likely to
view the non-sponsored links first and evaluate them as more relevant than sponsored links. On
search results pages on Web searching engines (e.g., Google and Yahoo), sponsored links appear
because a company, organization, or individual purchased the key words, while non-sponsored
(i.e., organic) links show up based on their proprietary matching algorithms. Therefore,
searchers’ perceived credibility of the given links, either sponsored or non-sponsored, can play
an important role in their online information seeking behaviors.
On SNSs and user-generated content sites, where authors’ identity information is not
always available, nor is their expertise necessarily assured, author trustworthiness and expertise
could be assessed based on the records/logs of their past contributions and behaviors (Stvilia et
al, 2008). For instance, the trustworthiness of a Wikipedia user who had been observed behaving
maliciously in the past (e.g., inserting false content in article) and whose edits had been reversed
often could be evaluated lower. Thus, whether or not the author opens their profile to the public
can be used as a useful credibility marker/cue. In particular, users may look for an author’s
online profile, the background information for online identity, such as a LinkedIn profile, Twitter
stream, or personal website or blog, to evaluate (or presume) the author’s trustworthiness (Jessen
& Jørgensen, 2012). Rieh, Jeon, Yang, and Lampe’s (2014) study focused on the credibility of
bloggers also showed that “transparency,” in terms of a blogger’s identity (e.g., the background
information of the blogger and the main purposes of running the blog) and open-modification
process (e.g., announcement regarding correcting inaccurate information, rather than deleting it),

67

was considered an important cue/marker that signals the credibility of the blog. Similarly,
Francke and Sundin (2012) mention that Wikipedia, which allows people to participate in
content production, is considered open and independent (i.e., transparent), therefore more
credible.
On social Q&A sites, the answerer’s (i.e., author’s) “intention” or “decency” can be an
important criterion for askers to judge the trustworthiness of the answerer (Jeon & Rieh, 2014).
For instance, when an answerer is perceived as facetious in answering a certain question, trying
to make a joke, his or her answer tends to be judged as less credible by people.
4.1.1.2 Measures of operator expertise. Operator’s expertise is the perceived
knowledge, skill, and experience of the operator. Sub-dimensions of expertise that are related to
an operator are: name recognition, reputation, fame, authoritativeness, and competence (Cheung
& Lee, 2006; Fogg et al., 2003; Liu & Huang, 2005; Westerwick, 2013; Zhang, 2014). In this
subsection, we review various measures that are used in the literature to examine the operator’s
expertise within three categories: (1) perceived reputation, (2) search engine ranking, and (3)
history of author’s activity.
Operator’s expertise can be measured by checking whether or not the site lists author
credentials for each article (Fogg et al., 2001). For scholarly information, in particular, the
author’s affiliation information, qualification and credentials, and publications in printed journals
are employed by people to evaluate the author’s expertise – i.e., “reputation” (Hargittai et al.,
2010; Liu, 2004; Liu & Huang, 2005). In other contexts, such as seeking entertainment
information, the positive reputation still plays an important role in forming the authority of the
site (Huvila, 2013).

68

In addition, “search engine rankings” can play a significant role in judging source
credibility (Hargittai et al., 2010; Huvila, 2013; Pan et al., 2007; Westerwick, 2013). Westerwick
(2013) showed that Google top-ranking affected information credibility through the significant
impact on perceived sponsor (i.e., operator) credibility. Other scholars also report the result that
people tend to trust a website when it was suggested in the first result by a search engine
(Hargittai et al., 2010; Huvila, 2013; Pan et al., 2007). These findings may be interpreted that
users’ trust in search engines can influence their credibility perceptions of the search results.
“Historical data of member activities” in peer-production systems such as Wikipedia
have been used for predicting the quality of information and/or identifying the expertise and
interests of a member (Adamic, Zhang, Bakshy, & Ackerman, 2008; Cosley, Frankowski,
Terveen, & Riedl, 2007; Stvilia, Twidale, Smith, & Gasser, 2005). In a recent study Jeon and
Rieh (2014) reported that an answerer’s involvement in a given topic (e.g., top contributor
badge) tended to be considered a positive marker/cue in credibility assessment of the answer in a
social Q&A site, as the answerer was assumed to have at least some knowledge to answer the
question.
4.1.1.3 Measures for content trustworthiness. Trustworthiness of content mainly
concerns whether the given message or information itself on a website is perceived by users as
fair, unbiased, and truthful. Measures used in the literature that are related to content’s
trustworthiness can be grouped into four categories: (1) neutral/unbiased information, (2)
aggregated social opinion, (3) consistency in content provision, and (4) currency/recency.
Fogg et al. (2001) mention that people tend to perceive content as trustworthy when it
contains links to outside materials and sources, especially links to its competitors’ sites. Also,
when policy on content is available on a website (i.e., “consistent in content provision”), the

69

content from the site is perceived to be trustworthy, providing consistent information (Princeton
Survey Research Associates, 2002).
In terms of health-related topics, users tend to perceive a website as most credible when it
provided both pros and cons on the given topic or issue (e.g., medication, side effects, etc.)
because the site is viewed as attempting to provide “neutral/unbiased information” regarding the
given topic (Choi, 2013). Neutrality also seems to be an important marker/cue to judge the
trustworthiness of user-generated contents, such as social Q&A sites, blogs, online discussion
forums, etc. In Metzger’s (2010) study, the proportion of negative to positive reviews on
feedback systems or reputation systems was an important cue/marker that they paid attention to
make credibility evaluations. Giudice (2010) also showed that mixed stances of user feedback on
an issue (i.e., both positive and negative) influenced perceptions of Web credibility. In the study
(Giudice, 2010), a Web page with positive or mixed user feedback was perceived as more
credible than a Web page with negative feedback only; however, there was no statistically
significant difference in credibility ratings between positive and mixed feedback. Therefore,
interestingly, even counterclaims and rebuttals seem to be considered useful cues/markers for
judging the credibility in user-generated content. These findings may be understood that the
content having at least some negative reviews is perceived by users as more balanced, and
therefore, more trustworthy.
Also, “aggregated social opinions” from other users (as opposed to experts) seem to play
an important role in forming the perceived trustworthiness of content (Fernquist & Chi, 2013;
Flanagin & Metzger, 2013; Jessen & Jørgensen, 2012). Since aggregated opinions of other users
are assumed to be honest (i.e., not manipulated by someone), the social information may be
perceived as more credible. Flanagin and Metzger (2013) showed that the volume of ratings

70

provided by other users on a movie rating site was positively associated with perceived
credibility. This result indicates that opinions on a certain topic or issue from a large number of
‘general’ users may suggest trustworthy information, while presumably a smaller number of
‘experts’ may provide expert information.
In the literature, “currency” (or recency) has been considered one of the important
message-related features that may influence the perception of information credibility (Bernstam,
Shelton, Walji, & Meric-Bernstam, 2005; Hargittai et al., 2010; Sundar, Knobloch-Westerwick,
& Hastall, 2007). For online news, in particular, currency can be an important criterion to judge
its trustworthiness because the validity of news information is time sensitive. Sundar et al. (2007)
measured the number of minutes since the news story broke, named upload recency, to see
whether or not currency affects the perceived credibility of the news lead itself. The authors
found that users considered the most recently uploaded news more credible when the news was
from a low-credibility-source, while they did not care much about the currency of news when the
news was from a high-credibility-source.
4.1.1.4 Measures for content expertise. Content expertise, which is based on
evaluations of whether or not the information on the website is accurate, clear, comprehensive,
informative, factual, in-depth, useful, etc., plays an instrumental role in Web credibility
assessment (Eastin, 2001; Hong, 2006a; Kim, 2010; Liu, 2004; Savolainen, 2011; Sundar, 1999;
Zhang, 2014). Measures regarding content’s expertise that are used in the literature can be
grouped into four categories: (1) provision of citations and references, (2) social validation, (3)
thoroughness of content, and (4) reinforcement of content expertise.
When a website has articles that list citations and references – i.e., “provision of
evidence,” people tend to perceive the site as more credible (Fogg et al., 2001). People seem to

71

view citations and references as scientific supports/evidence for the arguments made in the
articles, assuming that these markers/cues guarantee the accuracy and completeness of the
content. In their empirical study, Sundar et al. (2007) examined whether the number of related
articles would have a significant impact on the perceived credibility of online news leads. The
authors found that a news lead from a low-credibility-source was perceived as more credible
when it included a larger number of related articles. However, there was no significant
relationship between the number of related articles and perceived credibility of a news lead when
it came from a high-credibility-source. In other words, in a case where source’s credibility is not
guaranteed, people tend to give higher credibility to the argument being supported by a larger
number of citations and references.
Various types of “social validation,” such as Facebook ‘Likes,’ social bookmarks, ratings
(Jessen & Jørgensen, 2012), best answers in social Q&A sites (Kim, 2010), and annotations by
other users (Kulkarni & Chi, 2013), are useful cues in assessing the expertise of content.
Kulkarni and Chi (2013) showed that social annotations play an important role in the
persuasiveness of online news articles, even though the impact varied depending on by whom
and in which situation the annotations were generated: annotations by friends were persuasive in
a logged-in context where users were able to recognize their friends; in a logged-out context,
annotations by computer and companies were more persuasive than those by unknown users.
Even though the effect of social annotations is not homogeneous, the findings imply that the
existence of social annotations on online news articles may influence the persuasiveness of
content, as social annotations are seen as endorsements of content by other users (Kulkarni &
Chi, 2013).

72

When it comes to user-generated content, such as answers in social Q&A sites, Kim
(2010) found that questioners paid more attention to content-related markers/cues than sourcerelated ones to judge the quality of answers. In particular, users considered a ‘Best Answer’
rating an important credibility marker, as it may guarantee the expertise of the answer
(informativeness, accuracy, usefulness, etc.). Thus, the Best Answer in Yahoo! Answers can be
regarded as a representative form of social validation in social Q&A sites.
In determining the best answers in social Q&A sites, the length of reply and the number
of competing answers (i.e., number of other answers the questioner has to choose from) were
identified as significant predictors; in particular, the answer length was the most influential factor
for predicting the ‘Best Answer’ in Yahoo! Answers (Adamic et al., 2008; Agichtein, Castillo,
Donato, Gionis, & Mishne, 2008). Given that, people seem to consider length of answer and
number of competing answers as cues/markers signaling the “thoroughness” of the usergenerated content.
There is a unique type of information behavior on the Web that influences the credibility
of user-generated content. St. Jean, Rieh, Yang, and Kim (2011) paid attention to the fact that
people have a chance to reinforce the credibility of their content even after they post it online.
The authors found that many content contributors posted additional information – i.e.,
“reinforcement of context expertise,” including supplementary images, and/or provide feedback
to their audience, and these interactions with audience provided the content contributors with an
additional chance to promote their knowledge and expertise to the audience in the social sites.
Savolainen (2012) finding that people tend to perceive user-generated content (e.g., answers in a
social Q&A site) more credible when the content provides further evidence for or competing
answer candidates is in line with the findings from St. Jean et al. (2011).

73

4.1.1.5 Measures for design trustworthiness. As mentioned above, Fogg’s (2003a)
Web credibility framework defines the design aspect of Web credibility with four sub-categories:
information design – the structure of information on each page and throughout the site; technical
design – technical functionalities; aesthetic design – layouts, graphics, and colors of the site; and
interaction design – usability. Based on the framework, design ‘trustworthiness’ can be
determined by whether or not the website’s performance is stable and consistent; whether or not
it ‘looks’ trustworthy, etc. This subsection examines measures used in the literature that are
related to a design’s trustworthiness according to real-world feel and stability of the website.
An early study on Web credibility assessment (Fogg et al., 2003) reported that the most
frequently mentioned criteria for evaluating the credibility of a website were design look and
information design/structure. The result tells us that visual aspects of a website, such as pleasing
graphics, higher quality look and feel, and professional appearance, may have significant impacts
on users’ perceptions of the credibility of Web-based resources. In particular, scholars have
examined the effects of trustworthiness of a website’s design in terms of “real-world feel,” such
as whether or not the site lists the operator’s physical address, contact number, and email address
(Choi, 2013; Fogg et al., 2001; Fogg et al., 2003), and whether or not it includes a picture of the
operator (e.g., organization’s members; authors of certain articles) (Fogg et al., 2001; Liu, 2004;
Liu & Huang, 2005). In particular, posting a profile picture may have a positive effect on users’
perceived credibility of user-generated content, as well. Jeon and Rieh (2014) reported that some
participants noticed answers’ profile pictures in Yahoo! Answers, considering them being more
involved in the site. When people feel that there are actual people behind the website, they may
perceive the website as more trustworthy.

74

Moreover, technical design also has a significant impact on the perceived trustworthiness
of a website. In particular, users seem to be sensitive to the “stability” of a website, such as how
often the site is down, or whether or not links from all pages work properly (Fogg et al., 2001).
Previous studies using these measures showed that people considered the websites that are often
unexpectedly unavailable or have broken links less credible because these are perceived as less
consistent and reliable—i.e., less trustworthy (Choi, 2013; Fogg et al., 2001; Fogg et al., 2003;
Liu, 2004; Liu & Huang, 2005).
4.1.1.6 Measures for design expertise. Web resources involve various design-related
features, such as the structure of information, search functions, aesthetics design, and usability
(Fogg, 2003a). Even though these design-related features are not particularly about content or its
source(s), which have been traditionally considered as the main objects of credibility
assessments, several scholars have shown that design may have a significant effect on the overall
perception of Web credibility (Fogg et al., 2001; Robins & Holmes, 2008; Wathen & Burkell,
2002). In particular, a website’s credibility can be communicated by its quality design (i.e.,
design’s expertise) in terms of aesthetics based on first impressions of surface traits and usability
based on first-hand experience (Fogg, 2003a). Measures regarding a design’s expertise identified
in the literature are grouped into three categories: (1) visual aesthetics, (2) appropriateness of
design, and (3) professionalism.
Robins and Holmes (2008) found that people tended to judge the content with a higher
aesthetic treatment as having higher credibility than the same content with a lower aesthetic
treatment – i.e., impact of “visual aesthetics” on Web credibility assessment. Also, based on
Wathen and Burkell’s (2002) model for Web credibility assessment, people begin the process of
assessing Web credibility with making immediate judgments about the surface characteristics of

75

the website, which is directly related to the structural features of websites. Surface characteristics
mentioned in this research include appearance of the site (e.g., color, graphics, no errors, etc.),
usability (e.g., navigability, menus, download speed, etc.), and organization of information (e.g.,
layers, ease of access, and choice of detail level). Rieh et al. (2014) also mentioned that visual
aesthetics can play an instrumental role in signaling credibility of blogs – e.g., changing the
background of a blog and including pictures. In particular, the “appropriateness of design,” being
in harmony with the type of content and tone of writing, was suggested as an important aspect of
a credible blog, as blogs are a type of user-generated content sites where bloggers and audience
are closely engaged, interacting with each other regarding their common concerns and interests.
In particular, professional design of a website can give a positive first impression to
users, which then can influence their credibility judgments of the website itself. In this regard,
Fogg et al. (2001) showed that a lack of “professionalism,” such as typographical errors in the
site or small size (e.g., having less than 5 pages), made the participants rate the website as less
credible. These cues tend to have a negative effect on the perceived expertise of a website.
One interesting finding regarding the impact of design features on Web credibility is that
quality design and site organization tend to be considered as a basic requirement (or even
prerequisite) that a health-related website must have (Choi, 2013). In other words, a high level of
website design may or may not increase the overall credibility of a health-related website, while
poor surface credibility significantly decreased the overall credibility of a website. Westerwick
(2013) also showed that credibility perceptions could not be enhanced by more appealing
website design when the site has lower source credibility; instead, higher source credibility
increased users’ ratings of the website’s design appeal.

76

Table 4.1 Measures of Web Credibility Assessment
Operator

Content

Trustworthiness
Commercial implication:
• URL ends with .com
• Ads on the site
• Pop-up windows with ads
• Paid subscription required
Perceived integrity:
• Same condition for both on- and offline shopping products
• Sponsored links (vs. non-sponsored
links)
Perceived transparency:
• Accessibility of author’s online profile
(e.g., LinkedIn profile, Twitter stream,
personal website or blog, etc.)
• Announcement (notice) on correcting
inaccurate information (not deleting it)
Perceived decency:
• Whether the author is serious or
facetious in information provision.

Expertise
Perceived reputation:
• Author credentials
• Affiliation information
• Author’s publications in printed
journals
Search engine ranking:
• Google top-ranking
History of author’s activity (in social
Q&A sites):
• Number of answers
• Number of best answers
• Top contributor badges

Neutral/unbiased information:
• Links to outside materials and sources
• Links to its competitors’ sites
• Mixed stances of user feedback on an
issue (both pros and cons are
provided)
• Proportion of positive and negative
comments on user-generated contents
Aggregated opinion/social validation:
• Duplication (i.e., certain information is
found on multiple websites.)
• Social annotations and rating from
other people
• ‘Best Answer’ rating (e.g., ‘Best
Answer’ in Yahoo! Answers and
‘Likes’ in Facebook)
Consistency in content provision:
• Posting policy on content
Currency/Recency:
• Upload recency (number of minutes
since a message was posted)

Provision of evidence:
• Provision of citations and references
• Number of related articles
Intrinsic quality:
• Typographical errors
• Size of the site (e.g., how many pages
does it have?)
• Length of the content (e.g., length of
the Wikipedia article; length of the
answer in Yahoo! Answers)
• Number of competing answers (in
social Q&A sites)
Reinforcement of content expertise:
• Additional information posted by the
author (e.g., replies, comments, or
supplemental images and videos, etc.
in social Q&A sites)

77

Table 4.1 – Continued
Design

Real-world feel:
• Operator’s contact information (e.g.,
physical address, contact number,
email address, etc.)
• Picture of operator
Stability of the website:
• The site is rarely down.
• All links in the site are working
(number of broken links).

Visual aesthetics:
• Professionally designed (e.g., color,
graphics, etc.)
Appropriateness of design:
• Well-matched with content (topic)
• Well-matched with tone of writing

4.1.2 Variability of Web Credibility Assessment
The Variability component of the WC framework combines the user characteristics such
as demographics, user involvement, and technology proficiency (Table 2.3) and the context facet
included in the existing theories and models for Web credibility assessment (Table 2.4). Since
both the user- and context-related factors can affect the overall process of Web credibility
assessment, they are grouped together and named “Variability” in the WC framework.
4.1.2.1 Context. Among the six frameworks reviewed in Chapter 2 (Table 2.4), the P-I
Theory (Fogg, 2003b)and Unifying Framework of Credibility Assessment (Hilligoss & Rieh,
2008) explicitly included context as an independent variable that influenced Web credibility
assessment. Fogg (2003b) highlights that the interpretation of identical website elements for Web
credibility assessment is not necessarily the same, as it is based on a person’s judgment, which
can be influenced by his or her environment and situational norms (i.e., context). In particular, he
mentions that context may be (1) user context and/or (2) task context. For instance, if a person
has to look for the lowest price for airfare online with a time constraint at work, he or she would
more likely perceive pop-up advertisements as negatively than when in a situation where he or
she is browsing travel sites for planning vacation with no time constraint at home. In other
words, the different user contexts (i.e., at work with a time constraint vs. at home without a time
78

constraint) and task context (i.e., looking for the best airfare vs. browsing travel sites for vacation
idea) would influence one’s perception of Web credibility.
Hilligoss and Rieh (2008) also mention that context is an important factor that influences
the overall process of Web credibility assessment. In particular, the context in which the
information need emerged (i.e., the context of the given task such as information need for school
homework) influences people’s credibility judgment and guide (or even limit) their source
selection practices. In their study, college students tried not to use Web-based resources for their
homework, assuming that their class instructors would consider the Internet a less credible
source for academic work. This case could be seen as an example showing the influence of task
context on credibility perception, rather than user context, based on the categorization of contextrelated variables mentioned by Fogg (2003b)—i.e., user context vs. task context.
4.1.2.2. User characteristics. Most of the theoretical frameworks reviewed (5 out of 6)
identified some of the user characteristics-related factors as important variables that may have
significant impacts on Web credibility assessment. The MAIN model by Sundar (2008),
however, focused primarily on the cues/markers and heuristics that can be used for Web
credibility assessment, without considering user characteristics. In the current research that
focuses on older adults as a target research population, user characteristics were considered one
of the most important variables that would produce meaning findings; thus, user characteristics
are included in the new framework (Table 4.2).

Table 4.2 Variability of Web Credibility Assessment
Type
Context

Variables
User context
Task context

Definition
User’s environment that influence the
process of Web credibility
Types of tasks that influence the process
of Web credibility

79

Examples
• Time
• Situational norm
• Topic
• Goal

Table 4.2 – Continued
User
Demographics
characteristics

User’s demographic backgrounds that
influence Web credibility assessment

Involvement

The degree to which users know and
care about specific topics under
examination

Technology
proficiency

The degree to which users are familiar
and comfortable with the technology
(Internet) to identify, access,
evaluate, and use information
resources

• Age
• Gender
• Education
• Motivation
• Ability
• Domain expertise
• Information literacy
• Media reliance

4.1.3 Process of Web Credibility Assessment
The third component of the WC framework focuses on how people go through the
process of Web credibility assessment. Hilligoss and Rieh (2008) make a distinction between
credibility assessment and credibility judgment by defining that “credibility assessment is seen as
an iterative process involving one or more credibility judgments” (p. 1468). Using the
distinction, the ‘process-oriented’ frameworks of Web credibility assessment encompass the
overall process of credibility assessment, while the ‘judgment-oriented’ frameworks focus on the
effects of certain factors on users’ perceptions of information credibility. Among the six
theoretical frameworks reviewed in Chapter 2, five out of six described Web credibility
assessment as a process.
Metzger’s (2007) Dual Model depicts the process of Web credibility assessment with
three phases: exposure, evaluation, and judge phases. Overall, this model considers users’
personal and situational contexts, especially motivation and ability, as crucial factors deciding to
what extent a person will critically evaluate Web information they found. More specifically, in
the exposure phase of the model, the user’s motivation and ability decide whether or not they
will go to the next phase, the evaluation phase. When a user has motivation and ability to
80

evaluate the information he or she is being exposed to, the user will take a more rigorous and
systematic strategies to credibility assessment; however, if a user does not have motivation, no
credibility assessment will happen; in case the user does not have ability, yet has motivation to
evaluate, he or she will rely on the surface characteristics (i.e., peripheral cues) or heuristics to
judge the credibility of the information (see Metzger, 2007, p. 2088).
Fogg’s (2003b) P-I theory also describes the process of Web credibility assessment with
two phases (or stages). As reviewed in Chapter 2 above, this theory posits that a person notices
something (i.e., prominence) and makes a judgment about it (i.e., interpretation). Thus, the first
phase of the theory is the stage where information seekers select a potentially relevant website
for their information needs based on the elements that they notice from the website. In other
words, any elements that are not noticeable to users cannot have any impact on users’ credibility
assessments of the website. Fogg (2003b) mentions that at least five factors can have significant
impacts in this phase: involvement, topic, task, experience, and individual differences (e.g.,
learning style, literacy level). In the second phase (i.e., interpretation), users evaluate the
credibility of the website based on available markers/cues and heuristics. The author identifies
three factors that can influence the interpretation phase such as users’ assumptions,
skill/knowledge, and context (e.g., user’s environment, user expectations, situational norms).
Wathen and Burkell (2002) also conceptualize the process of Web credibility assessment
with three phases: evaluation of surface credibility, evaluation of message credibility, and
content evaluation. In the first phase of this model, people make immediate judgments about the
surface characteristics of the site, such as appearance (e.g., color, graphics, lack of error, etc.),
usability (e.g., navigability, menus, download speed, etc.), and organization of information (e.g.,
layers, ease of access, and choice of detail level); the markers identified in this model are in line

81

with the surface credibility markers suggested by Tseng and Fogg (1999). In the second phase,
then, people evaluate the message credibility using various markers for source (operator) and
message (content). The authors identify expertise/competence, trustworthiness, and credentials as
factors that influence source credibility; content, accuracy, currency, and relevance to the user
need are identified as factors that influence message credibility. The last phase assesses the
interaction of the surface, source, and message credibility, completing the process of credibility
assessment. In this phase, the user’s cognitive status (e.g., knowledge, familiarity, time, and so
on) comes into play as mitigating factors.
Even though Hilligoss and Rieh’s (2008) framework identifies three levels of credibility
assessment (i.e., construct, heuristics, and interaction levels), rather than two phases, this
framework can be also understood with the two-phase process model, as well. Construct is the
highest and the most abstract level in the framework, in which people form a particular point of
view for judging credibility. More specifically, depending on the given context of credibility
assessment (e.g., types of tasks), people conceptualize credibility based on different constructs—
the authors identified five constructs such as trustfulness, believability, trustworthiness,
objectivity, and reliability. For instance, those who are evaluating a news article would consider
objectivity a more important construct than the others (Hilligoss & Rieh, 2008). Thus, what
people would do on the construct level can be understood with the third facet of the new
framework proposed in the current research, operationalization, in which people identify the
appropriate measures based on the key dimensions of credibility.
The second and third levels of this framework (i.e., heuristics and interaction levels)
mainly deal with how people measure the constructs (or dimensions) of credibility. Particularly,
in the second level, heuristics, people utilize general rules of thumb, especially when they are

82

unwilling or unable to evaluate the content of the message because of time, motivation, and
ability. In the third level, interaction, people examine specific attributes of information objects
and sources for credibility judgments, while the judgments in the heuristics level are based on
more general and widely-applicable rules of thumb gained from individual’s experience
(Hilligoss & Rieh, 2008). The idea that people take advantage of heuristics when they have low
motivation and/or ability to engage in the credibility assessment process is in line with other
frameworks reviewed in this research, such as Metzger’s (2007) Dual Processing Model and
Wathen and Burkell’s (2002) Judgment Model; fundamentally, it is based on the ELM of
persuasion (Petty & Cacioppo, 1981) that addresses the broader issues of how people change
their attitudes.
Lastly, Sundar’s (2008) MAIN model also highlights the roles of heuristics in credibility
assessment in the digital media context. This model views that the process of credibility
assessment consists of four stages: affordance, heuristics, quality, and credibility assessment.
Affordance is the starting point in the process where people recognize a particular capability
possessed by the medium, which can facilitate completion of the desired task—in the context of
ELIS, the task would be information seeking to fulfill certain information needs. Therefore, even
though the author does not particularly articulate it, some of the user characteristics-related
factors (e.g., proficiency/experiences with technology) would have effects on the affordance
stage.
In the following stage, heuristics, various types of heuristics are triggered by the cues
embedded in or offered by the medium used in the affordance stage. This model identifies varied
heuristics under the four types, Modality (M), Agency (A), Interactivity (I), and Navigability
(N), which are related to the structural aspects of the medium; source of information; interaction

83

and activity; and interface features (or design), respectively. As the name of the model, MAIN
model, implies, the MAIN model considers that the four types of heuristics play the instrumental
role in judging the quality of content in the third stage. Finally, some of the quality criteria, such
as trustworthiness and reliability, are linked to credibility assessment. Overall, the MAIN model
has the similar viewpoint on the process of credibility assessment with Hilligoss and Rieh’s
Unifying Framework in particular, and with other models reviewed in the current research at
large, in that it sees that people begin the process with less expensive methods in terms of time
and cognitive load. Table 4.3 summarizes the characteristics regarding the process facet in the
existing theories and models reviewed in this research.

Table 4.3 Process of Web Credibility Assessment Identified in Existing Frameworks
Theory/Model	  
Stage 1: Initial Evaluation	  
P-I Theory
Prominence: notices elements of a
(Fogg, 2003b)
website that may influence Web
credibility assessment. Factors that may
have impacts include:
• Involvement
• Topic; Task
• Experience
• Individual differences

Stage 2: Final Evaluation	  
Interpretation: evaluates the credibility of
the website based on the
interpretations of the elements noticed
in Prominence. Factors that may have
impacts include:
• User’s assumption
• Skill/knowledge
• Context

Judgment
Model
(Wathen &
Burkell, 2002)

Content Evaluation: assessing the
interaction of the message presentation
and content with the user’s cognitive
state:
• Knowledge
• Familiarity
• Time

Evaluation of Surface Credibility: makes
immediate judgments about the surface
characteristics of the site:
• Appearance/presentation
• Usability/interface design
• Organization of information
Evaluation of Message Credibility:
evaluates the source and message
based on various markers:
• Expertise/competence,
trustworthiness, etc. for source;
• Relevance, currency, accuracy, etc.
for message

84

Table 4.3 – Continued
Unifying
Model
(Hilligoss &
Rieh, 2008)

Construct Level: conceptualizes
credibility using constructs such as:
• Trustfulness
• Believability
• Trustworthiness
• Objectivity
• Reliability
Heuristics Level: utilizes general rules of
thumb to measure the construct(s)
defined in the Construct Level.
• Media-related heuristics
• Source-related heuristics
• Endorsement-based heuristics
• Aesthetics-based heuristics

Interaction Level: examines specific
attributes of information objects and
sources for credibility judgments.
• Content cues
• Peripheral source cues
• Peripheral information object cues

MAIN Model
(Sundar, 2008)

Affordance: a particular capability
possessed by the medium that
facilitates a certain action. Types of
affordances include:
• Modality
• Agency
• Interactivity
• Navigability
Heuristics: judgment rules that result in
estimation of content quality.
Heuristics are triggered by certain cues
in the affordances.

Credibility Judgment: users’ perceptions
of the credibility of the information
based on some of the considerations
for content quality:
• Trustworthiness
• Reliability
• etc.

Dual Model
(Metzger,
2007)	  

Exposure Phase: a person decides to
what extent he or she critically evaluate
Web credibility based on:
• Motivation
• Ability

Evaluation Phase:
• Heuristic/peripheral evaluation relies
on the surface characteristics.
• Systematic/central evaluation takes
more rigorous and systematic
strategies to credibility assessment.
Judgment Phase: the final judgment of
Web credibility

4.1.3.1 Initial evaluation stage. Based on the qualitative meta-analysis of the processes
of Web credibility assessment identified in the existing frameworks (Table 4.3), the researcher
came up with a two-stage model of Web credibility assessment process, which consists of (1)
85

initial and (2) final evaluation stages. As this model is based on the literature analysis, which is
still in the conceptual level, it needs to be tested with empirical data in future research.
In the initial evaluation stage, the researcher hypothesizes that people begin their Web
credibility assessment process with identifying the initial lists of websites that might convey
credible information for their information needs. This process may be influenced by the types of
tasks, topics of information, levels of user involvement, and individual presumptions and firsthand experiences. People may go directly to the trusted websites that they already know or use,
such as Google and Yahoo! When they use a search engine, they would examine the search
results to narrow down the number of candidate websites by using credibility markers/cues and
heuristics such as checking the URL of the site and reading the short description of the site. They
would make a decision of the website on which they would spend time and effort in the
following stage, final evaluation stage.
When using the analogy of Information Foraging Theory by Pirolli and Card (1999),
what people do in the initial evaluation stage may be understood as following “Credibility
Scents,” which are imperfect representations of the information quality based on proximal cues,
such as source credentials, hyperlinks in a Website, etc. In other words, credibility markers/cues
may exude either a positive, negative, or even neutral ‘scent’ in sense-making around the quality
of the information to the given information-seeking task.
4.1.3.2 Final evaluation stage. In the final evaluation stage, the researcher hypothesizes
that people select the ‘most credible’ and the relevance (value) information to the given
information-seeking task. People would examine the content of the chosen websites in the
previous stage (i.e., initial evaluation) to make the final decision of whether or not they would
use the information from the site for their information needs. Also, in the process of navigating

86

the website in this stage, they would receive impressions about design and functionality of the
site, which could influence their perception of the site’s credibility. In this stage, they would go
through the iterative process of credibility assessment of the candidate websites, which were
identified in the initial stage. The final evaluation stage may be influenced by individual factors
such as familiarity and knowledge about the given topic (e.g., specific diseases and symptoms),
IT proficiency, and time.

Table 4.4 Two-Stage Model of Web Credibility Assessment Process
Stage
Initial evaluation
Final evaluation

Definition
The stage in which people identify initial lists of the most relevant online
resources that can fulfill their information needs.
The stage in which people go through the iterative process of assessing the
credibility of the candidate websites identified in the initial evaluation stage.

4.2 Findings from Phase II: Semi-Structured Interviews
The purpose of the semi-structured interview study (Phase II) was to explore older adults’
credibility assessment of online health information within the context of ELIS. In particular, this
study aimed at exploring how older adults define information credibility and how they assess it
especially in the Web context. Another important goal of Phase II was to test the new framework
of Web credibility assessment (i.e., the WC framework) developed in Phase I and identify any
refinement suggestions. The average interview time was about 30 minutes; interview times
ranged from about 8 minutes to 51 minutes.
The following subsections report findings based on the WC framework and the ELIS
model proposed by Savolainen (1995). In particular, the coding scheme (Appendix G) guided
this process, identifying (1) individual and contextual variables that may influence the overall
process of Web credibility assessment (i.e., Variability of WC); (2) older adults’ information

87

needs and related information behaviors in their everyday lives (i.e., ELIS); (3) older adults’
credibility assessment of online health information, in terms of the utilization of credibility
markers/cues and heuristics (i.e., Assessment of WC) and the process of identifying credible
sources to fulfill their health information needs (i.e., Process of WC); and (4) any emerging
themes that are useful to refine the WC framework.
4.2.1 Profiles of Research Participants
A total of twenty-one older adults (n = 21) participated in the semi-structured interview
study (Phase II). Table 4.5 provides an overview of the research participants’ profiles in terms of
age, gender, ethnicity, education level, occupational background, and the Internet usage and
experience. In particular, the profiles of the research participants are organized based on the two
sub-groups of older adults suggested by Neugarten (1974)—the ‘young-old’ (55 to 74 years old)
and the ‘old-old’ (75+ years old). The age subdivision may be a useful framework to look into
the qualitative interview data from older adults who are known as a more heterogeneous age
group, than its younger counterparts, due to age-related declines in perception, cognition, and
movement control (Fisk et al., 2009). Overall, 71.4% (15 out of 21) of the research participants
were young-olds and 28.6% (6 out of 21) were old-olds (75+). The following sections will
provide more detailed descriptions of the research participants’ profiles.
4.2.1.1 Demographic characteristics. 76.2% (16 out 21) of the interview participants
were females and 23.8% (5 out of 21) were males. Looking into the gender distribution by the
age groups, 57.1% (12 out of 21) were female young-olds and 19% (4 out 21) were female oldolds; as for male participants, 14.3% (3 out of 21) were male young-olds and 9.5% (2 out of 21)
were male old-olds. The majority of the interview participants (90.5%; 19 out 21) were White
Caucasians; 9.5% (2 out of 21) were Black/African Americans (Table 4.5).

88

Table 4.5 Profiles of Research Participants by Age Group
Variables

Total
n (%)

Age group
Young-old
(n = 15)

Gender
Female
Male
Ethnicity
Black/African American
White Caucasian
Education
High school graduate/GED
Some college/Associate’s degree
Bachelor’s degree
Master’s degree
Doctoral degree
Occupational experience
Education/research-related fields
Freelancer (e.g., artist, writer)
Government employees
Health/medical profession
Legal profession
Not specified
Internet use
Between 1 hour and 5 hours a week
Between 6 hours and 10 hours a week
Between 11 hour and 15 hours a week
Between 16 hour and 20 hours a week
More than 20 hours a week
Internet experience
More than 5 years

Old-old
(n = 6)

16 (76.2)	  
5 (23.8)	  

12 (57.1)	  
3 (14.3)	  

4 (19.1)	  
2 (9.5)	  

2 (9.5)	  
19 (90.5)	  

2 (9.5)	  
13 (61.9)	  

0 (0.0)	  
6 (28.6)	  

1
5
5
4
6

(4.8)	  
(23.8)	  
(23.8)	  
(19.0)	  
(28.6)	  

0
3
4
4
5

(0.0)	  
(14.3)	  
(19.0)	  
(19.0)	  
(23.8)	  

1
2
1
1
1

(4.8)	  
(9.5)	  
(4.8)	  
(4.8)	  
(4.8)	  

5
2
4
2
4
4

(23.8)	  
(9.5)	  
(19.0)	  
(9.5)	  
(19.0)	  
(19.0)	  

4
2
4
0
2
3

(19.0)	  
(9.5)	  
(19.0)	  
(0.0)	  
(9.5)	  
(14.3)	  

1
0
0
2
2
1

(4.8)	  
(0.0)	  
(0.0)	  
(9.5)	  
(9.5)	  
(4.8)	  

3
3
3
5
7

(14.3)	  
(14.3)	  
(14.3)	  
(23.8)	  
(33.3)	  

0
2
2
4
7

(0.0)	  
(9.5)	  
(9.5)	  
(19.0)	  
(33.3)	  

3
1
1
1
0

(14.3)	  
(4.8)	  
(4.8)	  
(4.8)	  
(0.0)	  

21 (100.0)	  

15 (71.4)	  

6 (28.6)	  

The mean age of the twenty-one research participants was 70.3 years old (M = 70.3;
median = 70.0; SD = 5.6) within the range from 61 to 80 years old (range = 19). The ages of the
young-old group ranged from 61 to 72 years old (range = 11) with the mean of 67.4 years old (M
= 67.4; median = 67.0; SD = 3.0). The ages of the old-olds ranged from 75 to 80 years old (range

89

= 5), with the mean of 77.7 years old (M = 77.7; median = 77.5; SD = 1.8). Table 4.6 provides a
summary of the interviewees’ genders and ages.

Table 4.6 Research Participants’ Ages
Variable
Young-old
Female
Male
Old-old
Female
Male
Total
Female
Male

n (%)
15
12
3
6
4
2
21
16
5

(71.4)	  
(57.1)	  
(14.3)	  
(28.6)	  
(19.1)	  
(9.5)	  
(100.0)	  
(76.2)	  
(23.8)	  

Age (year-old)	  
Median	  
SD	  
67.0	  
3.5	  
67.0	  
3.6	  
70.0	  
2.6	  
77.5	  
1.8	  
79.0	  
1.5	  
76.5	  
2.1	  
70.0	  
5.6	  
69.0	  
5.9	  
71.0	  
4.6	  

M	  
67.4	  
67.0	  
69.0	  
77.7	  
78.3	  
76.5	  
70.3	  
69.8	  
72.0	  

Range	  
11 (61–72)	  
11 (61–72)	  
5 (66–71)	  
5 (75–80)	  
3 (77–80)	  
3 (75–78)	  
19 (61–80)	  
19 (61–80)	  
12 (66–78)	  

4.2.1.2 Health condition. Most of the research participants (17 out of 21; 81.0%)
mentioned that they had chronic illnesses, such as arthritis and blood pressure, and/or more
serious diseases, such as different types of cancers (e.g., breast cancer, bladder cancer, lung
cancer) and stroke. In particular, all the participants who were grouped in the old-old (75+)
mentioned at least one or more chronic illness and/or serious diseases experiences (6 out of 6;
100%), while about 73.3% (11 out of 15) in the young-old group did (Table 4.7). Due to their
experiences with these illnesses and diseases, older adults seemed to have accumulated a
significant amount of related knowledge and kept monitoring their health conditions and the
medication information to deal with their health conditions. Those who had not had particular
illnesses and diseases were still interested in common geriatric illnesses (e.g., dementia) and
wellness information (e.g., healthy diet, nutrition, exercise). A more detailed discussion on their
health information behavior including Web credibility assessment in relation to their health
conditions will be provided in Chapter 5.
90

Table 4.7 Research Participants’ Health Conditions
ID	  
P20
P21
P01
P17
P18
P04
P12
P11
P08
P10
P09
P03
P14
P05
P07
P06
P02
P19
P16
P15
P13

Age	  
61
63
64
65
65
66
66
67
69
69
70
71
71
72
72
75
77
77
78
79
80

Age group	  
Young
Young
Young
Young
Young
Young
Young
Young
Young
Young
Young
Young
Young
Young
Young
Old
Old
Old
Old
Old
Old

Gender	  
F
F
F
F
F
F
M
F
F
F
M
M
F
F
F
M
F
F
M
F
F

Ethnicity	  
Black
Black
White
White
White
White
White
White
White
White
White
White
White
White
White
White
White
White
White
White
White

Chronic illnesses/diseases mentioned	  
High blood pressure
Skin disease
Chronic kidney disease
Breast cancer survivor; arthritis
Chronic ailments
Replacements (artificial parts)
Foot operation
High blood pressure
Skin cancer
Arthritis; lower back problem
Breast cancer survivor
Low blood pressure
Lung-related disease
Lung cancer survivor; stroke (TIA)
Arthritis
Bladder cancer survivor; knee replacement
Lyme disease; arthritis; spinal problem
-

4.2.1.3 Cultural capital. Based on Savolainen’s (1995) ELIS model, cultural capital is
“cognitive resources acquired through education and life experience” (p. 269). Thus, cultural
capital may be accumulated through formal school education as well as extra training
opportunities, including self-study. Savolainen (1995) considers cultural (or cognitive) capital an
important factor that influences one’s ELIS.
As shown in Table 4.5 above, the research participants’ education level was overall very
high: 28.6% (6 out of 21) had a doctoral degree; 19% (4 out of 21) had a Master’s degree; 23.8%
(5 out of 21) had a Bachelor’s degree; indicating that 71.4% (15 out of 21) were college educated.
The rest also took some college courses, but did not complete (23.8%; 5 out of 21); and only one
participant in the study sample (4.8%) did not experience any higher education (i.e., high school

91

graduate). Besides the formal education, there were several participants who gained additional
education experiences or trainings. These included the Red Cross training in first aid,
cardiopulmonary resuscitation (CPR), and fever therapy; IT-related training by IBM; courses in
health informatics; the standardized patient program in the College of Medicine at Florida State
University (Table 4.8).
Their occupational experiences were closely related to their educational backgrounds.
Most of them worked in professions that required cognitive ability and/or professional training,
such as a university professor, attorney, and nurse. Specifically, 23.8% (5 out of 21) of the
research participants were in the teaching and researching field such as a university professor and
researcher in the higher educational institutions. 19% (4 out of 21) worked as legal professionals
such as an attorney and legal assistant. 9.5% (2 out of 21) worked in the health/medical field as a
nurse and a pathologist. There were four people (19%) who worked as federal or state
government employees; the other four (19%) did not specify the types of professions they had.
Table 4.8 provides the summary of the research participants’ educational and occupational
experiences.

Table 4.8 Research Participants’ Cultural and Cognitive Capital
ID	  

Age	  

Gender	  

P06	  
P05	  
P14	  
P15	  
P19	  
P21	  
P01	  
P04	  
P10	  
P13	  
P17	  

75	  
72	  
71	  
79	  
77	  
63	  
64	  
66	  
69	  
80	  
65	  

M	  
F	  
F	  
F	  
F	  
F	  
F	  
F	  
F	  
F	  
F	  

Educational experience	  
Highest degree	  
Extra training/experience	  
High school
Some college
Some college
Some college
Some college
Senior center classes
Some college
Bachelor
Bachelor
Standardized patient
Bachelor
Standardized patient
Bachelor
Online course on Spanish
Bachelor
IT training by the IBM

92

Occupational
experience	  
Not specified
Not specified
Government employee
Researcher
Legal assistant
Not specified
Writer
Court reporter
Government employee
Nurse
Government employee

Table 4.8 – Continued
P02	  
P11	  
P18	  
P20	  
P03	  

77	  
67	  
65	  
61	  
71	  

F	  
F	  
F	  
F	  
M	  

Master
Master
Master
Master
Doctorate

P07	  
P08	  
P09	  

72	  
69	  
70	  

F	  
F	  
M	  

Doctorate
Doctorate
Doctorate

P12	  

66	  

M	  

Doctorate

P16	  

78	  

M	  

Doctorate

OLLI classes
Red Cross training in first aid,
CPR, and fever therapy
Online courses on archaeology
Master’s courses on health
informatics; online courses on
vital signs
Post-doctoral work in history,
economics, and social studies
-

Pathologist
Not specified
Researcher
Government employee
Professor
Professor
Artist
Attorney

Professor
Attorney

4.2.1.4 Social capital. Savolainen (1995) defines social capital as “the nature of contact
networks” (p. 269). He considers social capital one of the factors that shape one’s “way of life”
and “mastery of life” as it may play a role as an important source for ELIS. In the current study,
the most significant type of social contact network (i.e., social capital) mentioned was partner
(61.9%; 13 out of 21). Those who were either in a marital relationship or a romantic relationship
mentioned that they share (provide and/or receive) necessary information for various topics with
their partners in their everyday lives. In the study sample, there were three couples who
participated in the study together. One common characteristic of these couples’ information
behavior was that in each couple, one person played a role of the primary information ‘provider’
(i.e., source) for the other; and the other tended to rely on the ‘provider’s information as a
‘receiver.’ The ‘receivers’ in each couple often mentioned in the interviews that their partners
(i.e., providers) were the most credible sources for their ELIS.
There were seven other participants who were in couple relationships, but whose partners
did not participate in the current study. These people mentioned that they were all information
‘providers’ (as opposed to receivers) for their partners in their daily lives. Even though it is
93

premature to make a generalizable argument considering the small sample size, gender, seniority,
and the relative education level in a partner relationship did not seem to be decisive factors in
assigning a role, either a provider or a receiver, in older adult couples’ ELIS. Rather, assigning a
role in the couple’s ELIS context seemed to be more influenced by a person’s orientation
towards a problem solving situation—based on the ELIS model by Savolainen (1995), it can be
understood with the concept, “Mastery of Life.” Table 4.9 below shows each participant’s
general orientation in practical information seeking, and these findings will be interpreted in
Chapter 5.
Another type of social relationship mentioned by the participants was friend: 66.7% (14
out of 21) answered that they were keeping touch with their friends. When they needed
information about health and wellness, for instance, they consulted friends who were in the same
(or similar) situation with them (e.g., had the same symptoms with theirs) or who worked as
medical professionals (e.g., doctor and nurse).
Lastly, there was a participant who was actively involved in a local chapter of the
National Parkinson Foundation. She was sharing information with the community members who
had family members suffering from the disease.

Table 4.9 Research Participants’ Social Contact Networks
ID	  
P01	  
P02	  
P03	  
P04	  
P05c1	  
P06c1	  
P07	  
P08c2	  
P09c2	  
P10	  

Age	  
64	  
77	  
71	  
66	  
72	  
75	  
72	  
69	  
70	  
69	  

Gender	  
F	  
F	  
M	  
F	  
F	  
M	  
F	  
F	  
M	  
F	  

Ethnicity	  
White	  
White	  
White	  
White	  
White	  
White	  
White	  
White	  
White	  
White	  

Nature of social contact network	  
Partner; mother
Friends (sharing common interests)
Partner; children & grandchildren
Friends (worked in the medical field); children;
Partner; friends; children
Partner; friends
Partner (worked in the medical field); grandchildren
Partner; sister; relatives; other
Partner; nephews (working in the medical field)
Family; friends

94

Table 4.9 – Continued
P11	  
P12	  
P13	  

67	  
66	  
80	  

F	  
M	  
F	  

White	  
White	  
White	  

P14	  
P15c3	  
P16c3	  
P17	  
P18	  
P19	  
P20	  
P21	  

71	  
79	  
78	  
65	  
65	  
77	  
61	  
63	  

F	  
F	  
M	  
F	  
F	  
F	  
F	  
F	  

White	  
White	  
White	  
White	  
White	  
White	  
Black	  
Black	  

Partner; family; friends
Family; friends
Local community for a specific disease—Parkinson’s
disease; friends (worked in the medical field)
Single; family; friends
Partner; friends; grandchildren
Partner; friends (sharing common interests)
Partner; friends (suffering from the same disease)
Family; friends (sharing the common interests)
Partner; children & grandchildren
Partner; family
Family

Note. There are three couples who participated in the study together. IDs of these couples are noted with superscripts,
such as c1, c2, and c3, to indicate the couple relationships.

4.2.1.5 Internet usage and experience. As this study is particularly interested in older
adults’ credibility assessments in the Web context, which involves computer-related skills, the
research participants’ Internet usage and experience were considered important factors that may
influence their perceptions of Web credibility and assessment behaviors. About 33.3% (7 out of
21) of the research participants answered that they used more than 20 hours in a given week, and
23.8% (5 out of 21) used the Internet between 16 and 20 hours a week. In other words, about
57.1% (12 out of 21) of the interviewees were online at least 16 hours a week, meaning that more
than a half of the research participants spend at least more than two hours and a half using the
Internet everyday. Only 13.3% (3 out of 21) answered that they used the Internet between 1 and
5 hours a week (Table 4.5).
Overall, the research participants’ Internet usage ranged from 3 to 7 with the mean of 5.5
(M = 5.5; median = 6.0; SD = 1.47), where 1 indicated “never;” 2 indicated “less than one hour a
week;” 3 indicated “between 1 hour and 5 hours a week;” 4 indicated “between 6 hours and 10
hours a week;” 5 indicated “between 11 hours and 15 hours;” 6 indicated “16 hours and 20 hours
a week;” and 7 indicated “more than 20 hours a week” (Table 4.10). When looking into the
95

Internet use by the sub-age groups, the old-old group seemed to spend relatively less time than
the young-old group did. More specifically, the mean of the young-old group’s Internet use was
6.1 hours a week (M = 6.1; median = 6.0; SD = 1.10), while the old-old group used 4.0 hours a
week (M = 4.0; median = 3.5; SD = 3.5). In particular, those who answered that they were using
the Internet more than 20 hours a week (n = 7) were all from the young-old group; and those who
answered that they were using the Internet less than 5 hour a week (n = 3) were all from the oldold group. In terms of experience with the Internet—the period of time that a person has been
using the Internet—all of them (100%; 21 out of 21) had more than five years of experience with
the Internet (Table 4.10).

Table 4.10 Research Participants’ Internet Usage
Age group
Young-old
Old-old
Total

n (%)
15 (71.4)
6 (28.6)
21 (100.0)

Internet usage (per week)
Median
SD
6.0
1.10
3.5
1.27
6.0
1.47

M
6.1
4.0
5.5

Range
3 (4–7)
3 (3–6)
4 (3–7)

4.2.2 Way of Life and Mastery of Life
Based on the ELIS model by Savolainen (1995), people’s information behavior is closely
related to how one’s daily life is organized, which can be reflected by the relationships between
work and leisure, models of consumption, and the nature of hobbies—i.e., “way of life.” In
addition, one’s information behavior can be characterized by his or her orientation toward
problem-solving situations—i.e., “mastery of life.” The subsequent sections therefore describe
the research participants’ “way of life” (Table 4.11) and “mastery of life” (Table 4.12) by
analyzing the comments they made in interviews regarding how they gathered health-related
information in the everyday life context.

96

However, the exercise of categorizing the types of the informant’s “way of life” and
“mastery of life” was not intended to examine inferential statistics yielding generalizable
findings, due to the small sample size of the study (n = 21) and a lack of validated instruments.
Rather, considering the qualitative nature of the research data, the main intent of this
examination was to explore the informants’ information behaviors regarding how they find and
use credible sources in their daily lives. The two concepts in the ELIS, “way of life” and
“mastery of life,” therefore provided the researcher with useful theoretical lenses for looking into
the characteristics in the informants’ (i.e., older adults’) ELIS in terms of both passive and active
information-seeking in their everyday lives.
4.2.2.1 Way of life. Savolainen (1995) mentioned that “the analysis of hobbies sheds
light on the substance of way of life because the nature of hobbies informs us of the things which
people find most pleasant; the analysis also reveals the role of informational interests, for
instance, newspaper reading in leisure time” (p. 263). Thus, information about the nature of
hobbies can be useful for understanding people’s information needs and the ways in which they
acquire information in their daily lives. Savolainen (1995) suggested two types of hobbies,
cognitive and affective types. (1) Cognitive types of hobbies include reading newspapers and
books on topics such as politics, science, and culture. (2) Affective types of hobbies include
watching television (TV), movies, and entertainment programs and listening to rock music.
Using this typology, the researcher coded the types of hobbies mentioned in the interviews. In
addition, many participants mentioned that they often spent time engaging in social networking
with their social contact networks both in the interpersonal and online contexts. Social
networking activities, such as meeting with friends and using SNSs (e.g., Facebook and
LinkedIn), were coded as (3) social types of hobbies (Table 4.11).

97

Overall, about 66.7% (14 out of 21) of the research participants mentioned the cognitive
types of hobbies, such as reading newspapers and/or online news, reading books, and watching
cultural and educational presentations (TED Talks); 85.7% (18 out of 21) mentioned the
affective types of hobbies, such as playing games, watching movies and TV shows, and listening
to music; and 95.2% (20 out of 21) mentioned the social types of hobbies, such as email, inperson social networking, and SNSs. However, considering that some informants were more
willing to share information about their daily lives, including their hobbies, it should be noted
that the main intent of this data analysis was to explore the exhaustive lists of older adults’
hobbies in their everyday lives, rather than categorizing each informant’s hobbies by type.

Table 4.11 Nature of Hobbies
Type
Cognitive

N (%)
14 (66.7%)

Examples
• Reading newspapers/online news articles
• Reading books on cultural and scientific topics
• Watching cultural and educational presentations (e.g., TED Talks)

Affective

18 (85.7%)

• Playing games
• Watching movies and TV shows
• Listening to music

Social

20 (95.2%)

• Email
• SNSs (e.g., Facebook, Twitter, LinkedIn)
• In-person social

4.2.2.2 Mastery of life. Savolainen (1995) identifies two dimensions that describe
people’s problem-solving behaviors: cognitive versus affective, and optimistic versus
pessimistic. The first dimension, cognitive vs. affective, characterizes whether an individual’s
approach to a problem-solving situation that occurred in his or her everyday life is systematic
and analytic (i.e., cognitive) or emotional and unpredictable (i.e., affective). The second

98

dimension, optimistic vs. pessimistic, categorizes whether an individual believes that he or she
can solve most of the problems they face in their everyday lives (i.e., optimistic) or
acknowledges the possibilities that some problems might not be solved (i.e., pessimistic). These
two dimensions are combined into four types of “mastery of life:” (1) optimistic-cognitive; (2)
pessimistic-cognitive; (3) defensive (optimistic)-affective; and (4) pessimistic-affective
(Savolainen, 1995, pp. 265-266).
Even though the four ideal types of “mastery of life” were used in the current study as a
useful framework in analyzing the interview data to have a better understanding of the
informants’ tendencies in ELIS, it was neither the main focus of the study to examine the
informants’ coping strategies, nor possible to accurately measure one’s propensity without using
a validated instrument. However, throughout the interviews, the informants naturally implied
how they usually dealt with problems that occurred in their everyday lives in the past, in terms of
whether they were cognitively versus affectively oriented and optimistic versus pessimistic
towards the problem-solving situations. In particular, some comments made in the interviews
revealed whether they had a high or relatively lower level of self-confidence in information
seeking to fulfill their information needs in everyday life contexts. The subsequent sections
provide the definitions of the four types of “mastery of life” based on Savolainen (1995) and
explain how each informant in the current study was categorized into one of the four types of
“mastery of life.”
4.2.2.2.1 Optimistic-cognitive. Individuals who are optimistic-cognitive (O-C) toward a
problem-solving situation are “characterized by a strong reliance on positive outcomes for
problem solving” (Savolainen, 1995, p. 265). This type of person tends to have a strong belief
that he or she can solve the problem by consulting different sources and channels; thus, this type

99

of person would have a good possibility of selecting the most appropriate information to solve
the given problem.
When categorizing the research participants in the current study in terms of “mastery of
life,” if someone said that he or she always attempted to make sense of new (unfamiliar) health
information by searching online by themselves, the person was considered cognitive, in that the
person was approaching the given situation by researching, seeking for useful information to
understand the unfamiliar topic. Furthermore, if the person used various sources to verify certain
health information and explicitly said that he or she had expertise in the medical fields (based on
their occupational background or accumulated knowledge in the area), the person was considered
optimistic about the solvability of the problem. The following are some of the comments from
those who were categorized as O-C:
“If I’m taking a medication, say, I’m taking one medication and I want to take an over-thecounter medication, too. Then I’ll check and see what the interaction is. If my doctor has given
me a diagnosis and I don’t understand it, I’ll come home and research it. She may have used some
terminology, which I didn’t understand, or she may have said something like, ‘Well, you have
tendonitis in your left arm and maybe it’ll heal up in six months, but then sometimes it takes
longer,’ I might come home and look up bicep tendonitis and read about it and then I would make
a decision as to whether I wanted to go back and see her again about something or ask for a
reference to a physical therapist, so yes I’ll act on the information that I get.” (P09, Male, 70)
“I sometimes will double check doctor’s diagnoses … a couple of months ago, a dermatologist, I
had a rash, and he told me that it was Eczema, and I just didn’t think it was because I’ve done
research on Eczema before. So, I got online and did a lot of research. I realized that was not
Eczema at all. And then, I did alone experimenting on my own and I realized it was an allergy to
a laundry detergent that I had started using.” (P10, Female, 69)

100

“I think my professional background makes it easier for me to decide … again, I generally, for
example, I have a problem with [my health condition] … I have had a chance to see a doctor
about that, but I wanted to find out what does it mean, and what is that about. So, I went to
probably four or five different websites among those I mentioned.” (P14, Female, 71)

4.2.2.2.2 Pessimistic-cognitive. Individuals who are categorized as pessimistic-cognitive
(P-C) are characterized as relatively less confident in problem solving than those who are
optimistic-cognitive even though they also take the cognitive and systematic approaches to
information seeking (Savolainen, 1995). People in this category acknowledge the situation where
the given problem may not be solved.
The researcher categorized the research participants into the P-C type of “mastery of life”
when they mentioned that they sought for necessary information by themselves using multiple
sources, but relied more on others’ (e.g., doctor’s or partner’s) suggestions for the final decisions
of which information they would use to solve the given problem. Below are some excerpts from
the interviews data that were used as evidence for the P-C type of “mastery of life:”
“Because of reading about that [newer drugs] in a Mayo clinic health letter, then I looked it up
and learned about it, which leads to questioning him. So, very practical usage of that … I’ve
looked up stuff online about that. I generally would look up health something information when it
concerns me or somebody I care about … I don’t feel like I have the credibility myself to solve
between reliable and unreliable information, I mean some stuff, yes you can.” (P02, Female, 77)
“If I’m in pain and it tells how to relieve that, I’m going to do that right away. Something on new
medications for blood pressure, I’m not going to change my medication usually it’s, you just
don’t do that with blood pressure medications. I might mention it to my doctor or if I’ve looked
up the side effects of the drugs that I’m taking, I might mention those to my doctor to see if he
wants to change the medication.” (P08, Female, 69)

101

4.2.2.2.3 Defensive-affective. Individuals who are defensive-affective (D-A) in a problemsolving situation are oriented towards the optimistic perspective on the solvability of the
problem; yet, their coping strategies are mainly based on affective (emotional), as opposed to
cognitive (systematic), factors. One distinctive characteristic of this type of person is that he or
she may either avoid the given situation when it is perceived as too challenging to solve by
themselves, or treat the situation lightly having ‘optimistic’ wish, rather than being based on
realistic considerations (Savolainen, 1995).
In the interviews, those who selected or deselected certain information based on feelings
or simple inspection were considered affective, rather than cognitive. Also, individuals who rely
heavily on their partners or other people in seeking necessary information, unwilling to search
information by themselves were considered pessimistic, rather than optimistic. Based on these
coding rules, there were two participants who were D-A “mastery of life.” Below are excerpts of
their comments made in the interviews that were used to group them in the D-A type:
“I don’t know. I use it, Drugs.com. It has all medications and supplements there. It tells me if
there’s any interaction. And, I believe that. Don’t ask me why … For some reason, I believe what
they tell me. I guess it’s because I have all my drugs in there, and it will tell me “don’t take this”
because you’re taking this and they interact one another.” (P05, Female, 72)
“I don’t know, I don’t necessarily know if it [WebMD] is credible or not. But, it gives me the
information that I need and it’s easy to read. You know, I’m not a doctor, if I put something and
say having problem breathing, it will give me whole list. And I think it starts with the things and
goes down … Sometimes, it makes me feel better when I know I’m not dying. And sometimes it
makes, you can tell what you should do.” (P11, Female, 67)

4.2.2.2.4 Pessimistic-affective. Lastly, pessimistic-affective (P-A) is also characterized by
the affective nature of information-seeking behavior. However, individuals who are pessimistic102

affective are different from those who are defensive-affective, in that they do not appreciate the
usefulness of systematic information seeking—the author used the expression of “learned
helplessness” (Savolainen, 1995, p. 266).
In the interviews, those who selected or deselected certain information based on feelings
or simple inspection were considered affective, rather than cognitive. Also, individuals who rely
heavily on their partners or other people in seeking necessary information, unwilling to search
information by themselves were considered pessimistic, rather than optimistic. In the current
study, there was only one informant (1 out of 21; 4.8%) who was categorized as P-A “mastery of
life:”
“There’s too much information, and everybody has got their whatever they’re promoting.
Everybody has got promotion. Who knows whether you’re getting truth or not? It may be 3/4
truth? … I go to see a doctor and take what the doctor tells me. I live like that … and, I ask my
girlfriend, she looks a lot.” (P06, Male, 75)

Overall, about a half of the research participants (11 out of 21; 52.4%) were grouped in
the O-C type; 33.3% (7 out of 21) were in the P-C type; 9.5% (2 out of 21) were in the D-A type;
and there was one participant (4.8%) who was characterized as the P-A type. Table 4.12
summarizes the informants’ “mastery of life,” along with their age, gender, educational
background, and Internet usage.

Table 4.12 Research Participants’ Background Information and Mastery of Life Types
ID

Age

Gender

P01
P03
P09
P10
P12
P13

64
71
70
69
66
80

F
M
M
F
M
F

Education
Bachelor
Doctorate
Doctorate
Bachelor
Doctorate
Bachelor

Internet use
20+ hours
16-20 hours
20+ hours
16-20 hours
6-10 hours
11-15 hours

103

Mastery of Life
O-C
O-C
O-C
O-C
O-C
O-C

Table 4.12 – Continued
P14
P15
P17
P20
P21
P02
P04
P07
P08
P16
P18
P19
P05
P11
P06

71
79
65
61
63
77
66
72
69
78
65
77
72
67
75

F
F
F
F
F
F
F
F
F
M
F
F
F
F
M

Some College
Some College
Bachelor
Master
Some College
Master
Bachelor
Doctorate
Doctorate
Doctorate
Master
Some College
Some College
Master
High School

16-20 hours
16-20 hours
20+ hours
20+ hours
20+ hours
6-10 hours
16-20 hours
20+ hours
11-15 hours
1-5 hours
11-15 hours
1-5 hours
20+ hours
6-10 hours
1-5 hours

O-C
O-C
O-C
O-C
O-C
P-C
P-C
P-C
P-C
P-C
P-C
P-C
D-A
D-A
P-A

4.2.3 Older Adults’ Information Needs in the Everyday Life Context
When asked about information needs in the context of everyday life, the informants
mentioned several different topics of information needs, such as health/wellness, travel, finance,
leisure activities, and others. Obviously, older adults had great interests in health/wellness topics
as they had experienced serious illness and/or were suffering from some of the diseases,
including geriatric diseases. Furthermore, since the research participants were all retired from
their work (except one person, as mentioned above), their information needs seemed to be
closely related to their leisure activities in the settings of everyday life. For instance, many
informants mentioned that they travel. The following sections report the topics of information
needs frequently mentioned by the research participants in the study (Table 4.13).
4.2.3.1 Health/wellness. The most frequently mentioned orienting information needs
were about health/wellness-related topics. All the research participants (21 out of 21; 100%)
answered that they were interested in and had sought health/wellness-related topics in their daily
lives for various reasons such as feeding general wellness knowledge on a regular basis, solving
104

general curiosity about health/wellness-related myths or rumors, gaining knowledge about
healthy diet and nutrition, and information about exercise. These health/wellness information
needs, however, were usually about general knowledge that is useful to know to have a healthy
life, but relatively less serious and urgent than those for the practical information seeking that
will be discussed in the following section. In seeking these types of information, they seemed to
be relatively more open to use their ‘non-expert’ social connections such as family and friends,
as well as more traditional sources such as newspapers. For instance, one female participant who
was interested in reading a local newspaper as a hobby mentioned that:
“I follow health things that are featured in our local newspaper. There’s generally one day a week
where the section devoted to health will include exercise, diet, activities that are healthy.” (P04,
Female, 66)

Another female participant who was actively engaged in a social group with friends said
that:
“I was losing my hair. It’s an age problem and my fingernails were breaking, so when my friends
met in a group, I mentioned something about it and they said, ‘Oh, just go get some biotin. It’s
over the counter. It’s like a health supplement thing.’ At first I thought, everybody says take
health supplements and I’m not really into that, but I took it and I was amazed. Yeah, it really
helped.” (P08, Female, 69)

Within health-related topics, most of them (17 out of 21; 85.7%) were interested in
information about medications and supplements regardless of the seriousness of the healthrelated issues the research participants were suffering from (see Table 4.7, health conditions).
More specifically, they mentioned that they had sought information to compare the prices of
drugs they were prescribed: to better understand the prescriptions; to know about the appropriate
dose for themselves; and to research any side effects and drug interactions. Below are two
105

examples that demonstrate older adults’ health information needs regarding medications and
supplements:
“I’ll look the medication up and see what the side effects are and how you should take the
medicine and interactions, anything just so I’ll be really informed about the prescription.” (P18,
Female, 65)
“When I change my medications or do anything along that line, I look at it online. Or, if I’m
having what I think are strange reactions, I might go online to see if it’s normal, particularly if it’s
a medicine, if I’ve never taken it before.” (P19, Female, 77)
“I spent a lot of time doing research on the cancer drugs they want you to take. I quit taking them
based on my research. They want you to take these low-level chemo drugs for 5 years after I
made my decision to have a lumpectomy. Research showed very little proof that being on those
drugs for 5 years made any difference, but the quality of life, it gets to the point where you
sometimes … for me, I couldn’t even walk. The pain was so great because it inflames all the fluid
in your body. I found all that out online. After a year of trying to make it work, I quit taking it and
I’m still cancer-free and it’s been since 2007. I did all the research online.” (P17, Female, 65)

Also, those who had certain diseases or ailments sought information regarding the
diseases or ailments (17 out of 21; 81%). If it was a chronic disease or a disease that could not be
cured completely, they kept paying attention to their conditions (or the patient’s condition),
updating with any new information on the diseases/ailments, such as new medication or
operating techniques.
“I have high blood pressure, so I keep up with the latest on that and the latest in blood pressure
cuffs and things like that, and medications.” (P08, Female, 69)
“When my husband had a stroke I started doing research on stroke improvement techniques,
physical therapy, things we can do at home. I also did research on diabetes when he started going
to the diabetes clinic.” (P20, Female, 61)

106

In some cases, older adults played a role as a caregiver for other people, mostly their
family members, looking for necessary information for them (6 out of 21; 28.6%). Moreover,
those who had a pet needed information for their pet’s health, as well (3 out of 21; 14.3%).
“As we have grown older and have children and now grandchildren, I’m often interested in their
health, pregnancy, and a variety of things related to health.” (P03, Male, 71).
“Recently not much, but my husband had Parkinson and I used the information about that a lot to
find out a lot about Parkinson.” (P13, Female, 80)
“I have a sister who is battling cancer, and, if I had done this with my first sister who was battling
cancer I would’ve known more and I would’ve been able to help her more during her lifetime,
during her illness before she died. I learned a lot about chemo and radiation treatments that they
give cancer patients and how they’re supposed to take care of themselves and all this kind of stuff.
I did not know all of this stuff with my first sister so I couldn't help. You know, the first sister
who had cancer, I couldn’t help her. Then I have got my nephew who has a rare blood disease;
the doctor says it’s rare. It’s similar to sickle cell. I learned a lot about that by searching the
Internet. I could give him advice.” (P21, Female, 63)

Some of the research participants pursued information to better understand or even to
verify doctor’s diagnoses (11 out of 21; 52.4%).
“I’m continually interested in preserving our own health. So, before I take any prescription or
after talking with a doctor, I’m very careful to do my own investigations before starting a new
medicine.” (P03, Male, 71)
“If my doctor has given me a diagnosis and I don't understand it, I'll come home and research it.
She may have used some terminology which I didn't understand or she may have said something
like, "Well, you have tendonitis in your left arm and maybe it'll heal up in six months, but then
sometimes it takes longer." I might come home and look up bicep tendonitis and read about it and
then I would make a decision as to whether I wanted to go back and see her again about

107

something or ask for a reference to a physical therapist. So, yes, I'll act on the information that I
get.” (P09, Male, 70)
“So, whatever medications the doctor gives me and whatever they say is wrong with me, I’ll look
that up so I can understand it better. A lot of times I don’t understand what the doctors are saying,
so, if I look it up and read it, I can understand it better.” (P21, Female, 63)

Another type of health information need mentioned by the research participants was
health insurance (8 out of 21; 38.1%).
“Well, you know, I do go to the ... Of course, I’m Blue Cross Blue Medicare and Blue Cross Blue
Shield. We have, so far, had wonderful experiences with our ... Pretty much wonderful. I find that
the print outs that you get from Blue Cross Blue Shield and Medicare, you are almost unable to
decipher what they're telling you. They say make sure that you’ve been billed properly. Make
sure this. You look at it. They’re terrible. I’m looking to see if there’s one around, but ... I mean
my husband’s an attorney. He’s accustomed to really looking into things. I’ve done research. I
mean you can say yes, you were at the office. Well, it’s just pertinent. They have a lot of
nonsense, rather than saying you had this. They use all the codes … you can look up the codes
and it’s determined whether that was in fact what you had done, but we’ve been lucky with the
Medicare and our supplemental insurance takes care of most everything.” (P15, Female, 79)
“I also used the Internet for different programs to find out the different services that’s available,
what could be provided for her at Medicaid. Then when my husband turned 65 I started doing
research for Medicare building up, getting more knowledge. The more you know when you’re
going to sit with somebody trying to get services, the more you know I found that it’s helpful. It’s
better to go in prepared with thorough information about a particular situation.” (P20, Female, 61)

4.2.3.2 Travel. Travel-related information was mentioned by the research participants as
one of the necessary information topics for their everyday lives (13 out of 21; 61.9%). In
particular, they used online sources to compare airline fares, hotel rates, and rental car rates. Also,
108

they sought for general information about the places they were going, such as transportation and
good restaurants. One participant who was taking a mission trip to Central America checked the
websites of the Central Intelligence Agency (CIA), the World Health Organization (WHO) and
the Centers for Disease Control and Prevention (CDC) to have information about endemic
diseases and local contacts in case he had any emergency issues. Below are some of the mentions
regarding travel-related information needs:
“I do a lot of price searching. End of this month, I am going to Las Vegas with some of my
friends. And, so, I was looking for Las Vegas sites for shows to go to that kind of thing.” (P01,
Female, 64)
“I use it for travel, yes, a lot for travel. Maps, GPS, researching places I’m going. Washington DC
has a bunch of things like, it has all the bus schedules, it has train schedules, and it has city works.”
(P07, Female, 72)
“I take students on trips, mission trips. I do research with the State Department, CIA. Maybe
that’s government. When I’m taking students to a Central American country, I do research there
about the country, make sure it’s safe, make sure we know whom to contact in case we have
issues that arise.” (P12, Male, 66)

4.2.3.3 Culture and education. Several research participants in the study (13 out of 21;
61.9%) were also interested in getting information regarding cultural and educational topics such
as history, archeology, architecture, languages, and so on. Some of these people were reading
books and following online news on the topics mentioned. Some of them took online courses or
watched TED Talks as well. They also looked for reviews of books and movies and background
information about authors. Below are some of the statements showing the participants’
information needs on cultural/educational topics:

109

“I spent a lot of time looking at historical, geographical topics, and read background information
about authors of interest to me. I’m also interested in archeology, so I read a lot on archeology.”
(P03, Male, 71)
“Sometimes I want to know, I’m interested in the history of everything. I’ll be reading a book and
something will come up.” (P14, Female, 71)

4.2.3.4 Entertainment. Another type of information needs that the research participants
sought in their everyday lives was regarding entertainment (12 of 21; 57.1%). They enjoyed
watching TV shows, movies, sports games, and performances on YouTube; listening to radio and
music; and playing games. Some of them needed information about exhibition and music
concerts:
“I watch ESPN all the time. Entertainment, sports! … I used it [the Internet] to play Scrabble, a
word game.” (P07, Female, 72)
“I use it [the Internet] to watch Netflix for entertainment. I also use it for radios. I have iTunes
radio so we use that. Well, that’s entertainment. I guess you’d put that under entertainment.” (P15,
Female, 79)
“I sometime check, I’d like to know any art exhibits, music… I like music, so concerts, different
bands playing.” (P10, Female, 69)

4.2.3.5 Random topics: looking for factual information. Some informants (8 out of 21;
38.1%) mentioned that they needed specific pieces of information for random topics such as
definitions of words, names of authors and movie stars they could not remember, opening times
of stores, locations of things, and so on. For this type of information needs, they usually
employed online sources using search engines such as Google. Below are two examples
demonstrating this case:
“I look up definitions of words I don’t know. I look for names of authors and movie stars I can’t
remember. So, that would be kind of pop cultural information.” (P04, Female, 66)

110

“I use it [Internet] for research. You’re sitting around, and ‘Oh, what was the name of the second
wife of the president in 1850?’ something like that. No more do you have to go looking for this
having an encyclopedia or have some kind of reference books. You just pick up your phone and
you go look on a website and get that information.” (P07, Female, 72).

4.2.3.6 Finance. There were some participants (5 out of 21; 23.8%) who mentioned
financial information needs such as economic situation in general and personal financial
management-related topics (e.g., online banking, credit cards, paying bills, and financial
investment):
“It might be for financial investment. It might be to look at products they make. It might be to
look for, get a feel for the company and how long it’s been in business or how trustworthy it is.”
(P18, Female, 65)

4.2.3.7 Others. Other information needs mentioned included cooking information such as
recipes (4 out of 21; 19%), voluntary activity-related information (3 out of 21; 14.3%), housingrelated information (1 out of 21; 4.8%), legal information (1 out of 21; 4.8%), and religionrelated information (1 out of 21; 4.8%).
“Let’s see, what else? Recipes? Absolutely! You know, it’s so much easier than going and
looking through my cabinet with all the books I have. I just type in what I want, and sometimes
it’s not so quick to find it, but ... recipes.” (P01, Female, 64)
“Well, you know, under the information, for instance, I’d look up… we like to garden. I’ll look
up different plants. I’ll use it a lot for bird ID, plant identification, organic gardening. Oh,
cooking. I used to have a huge collection of cookbooks. I don’t even bother with them anymore. I
just put it in. You know, like I want to make quinoa pilaf or something, which my cookbooks
never even used quinoa. You know? It’s amazing.” (P15, Female, 79)
“If I was doing a fund raiser for OLLI, I’ve done one fund raiser last year, and the year before
two fund raisers for OLLI. And a lot of there was done online, keeping up with how many tickets

111

I gave to whom, and if they’ve sold, tracking whole that information, which I don’t like doing.”
(P02, Female, 77)

Table 4.13 Older Adults’ Information Needs in Everyday Life Context
Topics of information needs
Health/wellness

n (%)
21 (100.0)

Examples mentioned
• Medication & supplements
• Diseases (symptoms)
• Medical quality assurance
• Health insurance and price
• Nutrition
• Exercise
• Pet’s health

Travel

13 (61.9)

• Price search and reservation (airline, rent car, hotel)
• Map
• Restaurant
• Endemic diseases

Culture/education

13 (61.9)

• Books
• Online courses (futurelearn.com; coursera.org)
• Family history (ancestry.com)
• TED Talks

Entertainment

12 (57.1)

• Music
• Movies
• Games

Random topics (general search
on factual information)

8 (38.1)

• Definitions of words
• Pop culture information (e.g., names of authors,
actors and actresses)
• Hours and locations of local stores

Finance

5 (23.8)

• Personal banking, credit card, billing
• Financial investment

Others

8 (38.1)

•
•
•
•
•

Cooking (e.g., recipes)
Voluntary activity
Housing
Law
Religion

112

4.2.4 Older Adults’ Health Information Behavior
Among the varied topics of information needs in older adults’ everyday life, this study
focused on health-related topics and related information behavior. The following subsections
provide the findings of (1) how older adults seek necessary health information, identifying the
relevant sources, and (2) how they use and share the acquired health information.
4.2.4.1 Interpersonal sources. Most of the research participants considered medical
professionals such as a doctor and a primary care physician reliable sources for practical health
information seeking. In particular, they went to see a doctor, rather than searching online by
themselves, when they have serious problems or for regular check-ups.
“If there’s a significant health event, such as serious abdominal pain or something like that, then
obviously I’ll be in the emergency room to see a physician.” (P09, Male, 70)
“I go and see the doctor if I’m sick for sure. I have a primary care physician. I go for quarterly
check-ups. I will talk to my doctor about symptoms or issues or whatever but I also, at the same
time, will go online and look stuff up. It’s simultaneous, but I’m not like definitely, when I go to
the computer to see what I’ve got, I go to the doctor.” (P18, Female, 65)

As another human source for health information seeking, some of the research
participants mentioned their partners. They seemed to rely on the health information from their
partners based on accumulated trust over a long period of time, rather than their expertise in the
given topic—when asked about their partners’ educational backgrounds or occupational
experiences, they were not necessarily experts in the medical fields. However, the research
participants did not necessarily rely on what they were told by other human sources, such as
family members or friends, unless otherwise they were trained as medical professionals. Thus,
older adults seemed to value the health-related information from those of whom they have

113

accumulated a special level of trustworthiness with or from those who have sufficient expertise
in the topic area. Below statements demonstrate this tendency:
“I go to [P15] most of the time anyway. I do some of this on my own but I usually go and get her
advice on what to do.” (P16, Male, 78)
“Unfortunately, it’s [health information from friends or family] not very reliable information, but
yes, I sometimes get information from them.” (P08, Female, 69)

4.2.4.2 Online sources. The research participants considered various Web-based
resources as useful sources for health information seeking. Particularly, they highly valued the
websites run by government, such as NIH and CDC, and academic institutions, such as Johns
Hopkins Medicine (JHM) and Harvard Medical School (HMS). These types of websites seemed
to be considered the ‘Web’ representations of the public organizations and institutions that have
established their reputations as reliable sources for health information. Some participants
mentioned that they trust the health information from these websites because it is based on
research, rather than opinion (i.e., expertise), and their main intent is on improving public health
and wellness, rather than selling their products (i.e., trustworthiness).
“If I go on the Web, looking at, I tend to go to a site that is reputable like Hopkins or Mayo,
where you know that they’re not going to be putting up false information. You can depend on it,
having been pretty reputable stuff.” (P02, Female, 77)
“I’d think medical schools. They publish peer-reviewed, learned articles about things that I think
are a little bit more trustworthy than Wikipedia or some drug company who’s really just trying to
sell what their pills are.” (P04, Female, 66)
“I think the non-profit sites, their purpose is to educate generally, and they have public interests.
That’s not related to profit, whereas I think that some of the .com sites, well, they may have good
information, often reliable information, they generally have a profit motive.” (P14, Female, 71)

114

Also, some of the research participants considered the health information from official
associations for certain diseases (e.g., the American Association for Cancer Research, the
American Arthritis Society, and the National Parkinson Foundation) useful and reliable sources
that can be used for health information seeking. Similarly to the reasons for using .gov and .edu
sites, older adults liked the ‘non-commercial’ nature of the associations, as well as the researchbased information provision. Another important reason for using medical associations’ websites
mentioned was that they were able to share information with other patients (or caregivers) who
were suffering from the same ailments or diseases. This seemed to be more useful for them to
obtain the most recent as well as specific information about their information needs.
“Associations of people who have a particular illness or who have family members. So, I think
diabetes association, Parkinson’s association, heart society, and so on. They are probably more
reliable than others because they have only the interest of their members in mind, not trying to
sell them something.” (P03, Male, 71)
“The Parkinson’s website has what’s new, they have doctors, and they have several physicians,
and nutritionists and pharmacists. You can look at questions people have asked, or you can ask
questions and get answers.” (P13, Female, 80)

Even though most of the research participants preferred non-profit websites (i.e., .gov,
.edu, and .org sites), there were some participants who were using commercial websites (i.e.,
.com sites) for health information seeking, as well. The most frequently mentioned .com site by
the research participations in this study was WebMD, which provides health/wellness-related
news and information on variety of topics. Another type of commercial sites mentioned was a
pharmacist’s site such as Walgreens or CVS. They were using these websites to get information
about prescriptions and to compare the prices of medications they take.

115

“Main source would be WebMD. Well, I Google it, and things come up. I usually go to WebMD
because they seem to be most trustworthy … I don’t know, I don’t necessarily know if it
[WebMD] is credible or not. But, it gives me the information that I need and it’s easy to read.
You know, I’m not a doctor, if I put something and say I’m having a problem breathing, it will
give me whole list. And I think it starts with the things and goes down.” (P11, Female, 67)

When asked about social media as a source of health information, most of the research
participants gave skeptical responses. Even those who were using social media such as Facebook
or Twitter in their everyday lives pointed out that information posted on social media is often
based on personal opinions rather than facts—i.e., lack of expertise.
“Everybody has got their opinions but a lot of those opinions are not based on fact. They’re just
an opinion. They may have read a book and think they know everything.” (P17, Female, 65)
“I want to base my reliance on information on somebody that has expertise. And, most of my
friends on Facebook are not in the medical community. I mean, they can talk about the benefits of
nutrition and vitamins or they might talk about some disease they’ve had or something that
somebody’s experienced, but I’m going to then go and search for more expertise. I’m not just
going to take it at face value. And, most the people on Facebook really don’t talk about that kind
of stuff. I wouldn’t trust just like a Facebook page from a company or something. You just don’t
know who’s really doing the page.” (P18, Female, 65)

Moreover, some older adults highlighted privacy issues in using social media for health
information seeking, as they were concerned with the possibility that their medical situations
may be disclosed to unwanted groups of people:
“I use social media with my friends and I don’t discuss medical information with them. That’s not
something that I do and my social media is kind of exposed to the world and the world doesn’t
need to know that I have a medical situation, because privacy is a big part of that.” (P09, Male, 70)

116

One participant even mentioned that she did not use Facebook for health information
seeking because it was difficult to use:
“When I firstly go to use Facebook, it was easy to use. But, it changed and it’s harder for me to
use … it’s kind of difficult to use now. (P11, Female, 67)

Four informants (19%) mentioned that they looked at Facebook pages for certain healthrelated topics such as breast cancer and Lymphedema because they were able to gather and share
information with other people who had the similar questions and/or concerns about the same
health-related issues. However, even those who were using social media for health information
seeking were cautious of the “credibility” of the information as they were aware of the fact that
the health information from social media users may not be necessarily trustworthy and expert.
Below are some comments from the informants who were using social media as a source for
health information, with caution:
“I do use the Facebook pages for Lymphedema. I have Lymphedema. This is a symptom: red and
swollen. I connect with people who have Lymphedema.” (P05, Female, 72)
“I use Facebook and Twitter. No, I might follow a link to an article about a health-related issue. I
will frequently do that, but again, I don’t give it credibility and I say it comes from a reliable
source … I think there is so much bad information, circulating on the Internet. People have a
tendency to believe if they saw on Facebook or some other social media or websites and that’s
got to be true, and that isn’t the case … it lacks expertise, and it lacks accuracy.” (P14, Female,
71)

Also, social media was used as a tool for feeding news on general wellness topics such as
diet and nutrition. After they obtained certain information from social media, they further
verified whether the information was correct and researched details of how to apply the

117

information to their actual life. Thus, social media seemed to be considered a source for orienting
information seeking, rather than practical information seeking.
“Somebody on social media on Facebook put out a thing, a whole list of things with honey and
cinnamon would do for blood pressures, diabetes, lost weight, and gum disease, just all that. Then
when I went and did research, I found quantity, how much to take a day, different ways you can
take it and what the honey and sugar would do to help.” (P20, Female, 61)

Wikipedia was another type of social site that was mentioned as a source for health
information seeking (5 out of 21; 23.8%). Similarly to what they mentioned about Facebook
pages, the informants were cautious when they acquired certain information from Wikipedia.
They did not automatically trust the Wikipedia information, as they understood the open,
collaborative information production mechanism in Wiki pages:
“Occasionally, I look at Wikipedia just to get a simple explanation, but I don’t rely on that 100%
because it’s not verified.” (P01, Female, 64)
“I’m cautious about Wikipedia. I use it a lot, but I always verify the information because it’s not
always updated, you don’t know who did the editing.” (P14, Female, 71)

However, some participants appreciated the usefulness of Wikipedia for relatively simple
health information needs, such as definitions of diseases, because they perceived the Wikipedia
articles were written in a layman’s language, which helped them understand difficult medical
terms.
“It depends upon what I’m looking for. If it’s a medication, sometimes I even go to Wikipedia
because it’ll explain in laymen’s terms what the medication is used for.” (P08, Female, 69)
“They’re (Wikipedia) usually written in a little more plain language that’s easy to understand. So,
I use that to get a feel for the topic and then that helps me understand what other sites may be
saying. They will often times define terms where other sources don’t because other sources
assume you know what it is.” (P18, Female, 65)

118

Lastly, there were other types of Web-based sources for health information seeking
mentioned by the research participants in the study, such as local doctors’ websites, patient
portal, and consumer report.

Table 4.14 Sources for Health Information Seeking
Source types
Interpersonal sources
Medical professionals

n (%)

Examples mentioned

20 (95.2)

• Doctors
• Primary care physicians
• Wife and husband
• Girlfriend and boyfriend
• Friends in the similar situation
• Friends in the medical fields
• Local chapter for the National Parkinson
Foundation

Partners

4 (19.0)

Friends

3 (14.3)

Local community

1 (4.8)

Online sources
Non-commercial sites

18 (85.7)

Commercial sites

8 (38.1)

Social sites

8 (38.1)

Others

5 (23.8)

• Government (e.g., NIH, CDC, FDA)
• Organizations (e.g., Mayo Clinic, WHO)
• University hospitals (e.g., JHM, HMS,
FSU College of Medicine)
• Associations (e.g., National Kidney
Foundation, National Parkinson
Foundation, American Arthritis Society)
• Online databases (e.g., WebMD,
Cochrane Library, MERCK Manual)
• Pharmacists (e.g., Walgreens, CVS)
• Online communities on Facebook pages
(e.g., Breast Cancer, Lymphedema)
• Wikipedia
• Local doctors’ websites
• Patient portal
• Consumer report

4.2.4.3 Older adults’ health information use and share. When asked about whether
they used the acquired health information, all of them answered that they acted on the
information in general. However, they said that they did not necessarily use all the health
119

information they obtained from the Web—many of them answered, ‘it depends on situations.’
More specifically, they considered serious topics that might directly affect their health conditions
such as changing medications or treating certain symptoms that they felt serious. In this case,
they consulted with medical professionals first and used the information they found only as a
secondary reference.
“If I’m really sick, maybe I will call a doctor and see what’s going on.” (P11, Female, 67)
“If it’s an issue of medication, I’d call my doctor’s office and share what I had learned and say, is
it possible I need to take a different medication, or talk to my Pharmacist. Based on the
information, I’d generally try to get expert’s opinion first. A doctor or pharmacist.” (P14, Female,
71)

When the given situation is relatively less serious, such as looking for information
regarding healthy diet, nutrition facts, or good exercise for older adults, they applied the
information they found online to their everyday lives.
“I’ll print out the information and keep it as I look for more. For instance, recently I’ve been
interested in a plant-based diet. Not eating meat. More like vegan diet. I can’t eat wheat anymore
because of the Lyme.” (P15, Female, 79)

There was an exceptional case where a participant relied on the health information on the
Web regardless of the seriousness of the situation because she could not afford to have health
insurance that would allow her to see a doctor.
“Since I don’t have medical insurance, I can’t go to the doctor like I’m supposed to like I did
when I was working and had insurance through my job. I look up my symptoms that I am having
and I look for ways that I can take care of myself naturally. Somebody in the family who's taking
medication or whatever, I research that medication so I can find out what the side effects are and
tell them about that.” (P21, Female, 63)

120

In terms of sharing health information, most of the research participants mentioned that
they shared health information with family and friends only when they were interested in or
asked to do so. They were reluctant to automatically share health information with other people,
as it might be unnecessary or even inappropriate for them. Some of them were also concerned
about any unexpected problems that might be caused by their recommendations.
“Yeah, sometimes. If it’s pertinent. I don’t normally say, ‘I was just on the Mayo website and I
found out such and such’ unless I think it really is going to be relevant to people.” (P8, Female,
69)
“I am not an expert. When it comes right down to it, I really don’t know what I’m talking about. I
only know what I have found online and I don’t want anybody to take that information and then
have some kind of a problem. I’m not adept.” (P18, Female, 65)

4.2.5 Older Adults’ Credibility Assessment of Online Health Information
The research participants used varied types of markers/cues to select the most credible
sources in the process of health information seeking. The following sections organize the
findings by the six types of Web credibility measures identified in the Assessment of WC
framework: (1) operator trustworthiness; (2) operator expertise; (3) content trustworthiness; (4)
content expertise; (5) design trustworthiness; and (6) design expertise. Furthermore, (7)
descriptive statistics of the ratings on the 35 credibility markers/cues that were included in the
last segment of the semi-structured interviews (Appendix D) are provided in Table 4.16. A more
detailed discussion on the ratings will be given in Chapter 5.
4.2.5.1 Operator trustworthiness. Most of the participants in the study considered the
trustworthiness of the operator an important criterion for selecting a credible source for online
health information (19 out of 21; 90.5%). Particularly, older adults gave special attention to the
nature of the website—i.e., whether or not the website has commercial interests—when they
121

judged the credibility of the website and the health information within it. As mentioned above,
the research participants tended to perceive health information from a non-commercial and
academic institution’s website (i.e., .gov, .edu, .org site) as more credible than that from a
commercial company’s, for example, a pharmaceutical company’s website selling their products
(i.e., .com sites). Considering their common online information searching behavior that begins
with search engines (e.g., Google), it seemed to be the simplest as well as safest way for them to
first examine the non-commercial websites from the research results. Below quotes demonstrate
this finding:
“One is the nature of the organization that provides the website. Is it a nonprofit organization or is
it attempting to sell me a product?” (P09, Male, 70)
“When I’m looking for medical stuff, I want to know what doctors say, and, you know, a lot of
times the universities have in depth information because they do studies and the hospitals, what
are they called, research hospitals? … the majority of the time you could believe [universities or
research hospitals] because those people are doing research and that is all they do. And, I don’t
believe they will publish something that has not been proven. I don’t believe that.” (P21, Female,
63)
“I try to go first to .gov or .org sites because they don’t have something to sell to me. Also, I go to
academic sites … I’m generally trying to verify that information from a more independent source
and I perceive that an academic institution or a non-profit foundation, for example the Arthritis
Foundation, might be a better source than .com sources. I think the non-profit sites, their purpose
is to educate generally, and they have public interests. That’s not related to profit, whereas I think
that some of the .com sites, well, they may have good information, often reliable information,
they generally have a profit motive.” (P14, Female, 71)

In the same context, having commercial advertisements on a website seemed to
significantly decrease the perceived credibility of the health information on the website. Several
122

of the research participants were hostile to being exposed to unwanted advertisements having
commercial intents (e.g., pop-up windows with advertisements):
“It has happened pop-up on the computer when I’m using it; I pay no attention to it at all because
their interests and mine are not necessarily perfectly coincided.” (P03, Male, 71)
“Do you want to know some negative markers? No advertising at all!” (P04, Female, 66)

4.2.5.2 Operator expertise. About 71.4% (15 out of 21) mentioned that the operator’s
expertise is an important criterion for selecting credible sources for online health information—
i.e., whether or not the operator (source) of the Web resources is reputable, famous, authoritative,
and competent (Choi & Stvilia, 2015). In particular, many of the informants mentioned that they
would trust certain health information if it was from reputable and authoritative institutions in the
medical field, such as NIH, CDC, and Mayo Clinic or associations (foundations) of specific
diseases such as the American Cancer Society and the Arthritis Foundation. Thus, ‘name
recognition’ seemed to be an important criterion for selecting sources for online health
information for older adults:
“If I search, and pops up Johns Hopkins, I might not look at any of the cues. Okay? I’m going by
name recognition. I don’t look at cues.” (P10, Female, 69)
“I think credentials. They’re professional, like you said, Johns Hopkins … the producer probably,
the producer of them. The source, yeah, more than the producer … I think that’s [credentials of
the source] the most important for me for medical information.” (P08, Female, 69)

Informants also mentioned about operator’s credentials, especially when they were not
familiar with the operator of the website (or the author of an online article). For instance, when
the site was a private physician’s website or an institution that is less famous or less reputable,
older adults would look for the information about credentials of the physician and staff (i.e.,

123

operator) who were providing the health information on their websites to judge the credibility of
health information on the website:
“Yes, reputation and credentials, that’s the word … if I was looking at a private physician or
private practice’s website, I’ll go and look at their providers and see where they’re trained. Well,
if they’re certified, that kind of thing.” (P01, Female, 64)
“I’m going to look at the masthead kind of thing that has who’s running the site, are they doctors,
are they MDs or PhDs? You know, their experience, I guess. So, I look to see who’s on their staff
basically, how well trained, what kind of training their staff have. (P10, Female, 69)

4.2.5.3 Content trustworthiness. Some of the research participants (8 out of 21; 38.1%)
considered content trustworthiness-related markers/cues important criteria for credibility
assessment of online health information—i.e., whether or not the message/information being
provided in the website is neutral, unbiased, even-handed, consistent, and current (Choi &
Stvilia, 2015). Several of the research participants pointed out that the health information from
one source may not be reliable as it may be just his or her opinion, rather than fact. Thus, they
said that they verified certain information they obtained by double-checking if it was agreed (or
mentioned) by other sources (i.e., unbiased and consistent).
“I might look at more than one site on the same topic. And, sometimes, you can read what people
have had to say in response to the information.” (P02, Female, 77)
“I just keep looking. Sometimes I spend a whole day going from one website to another. I take
notes because I will find the same information on several websites. Then I believe, I don’t really
know, but then I believe that this information is correct.” (P21, Female, 63)

Also, some of the participants checked whether the health information is up to date when
they assess the credibility of online health information. Currency (or recency) of information in

124

health topics seemed to be considered crucial, as outdated health information may be neither
useful nor trustworthy.
“He [his doctor] is as old as I am a little older. Sometimes, [he is] not quite as current as he might
be … I know very well what CDC recommends for older people, how frequently to take a certain
vaccines or so on. And, occasionally, I have discovered my doctor’s not quite as current as they
are.” (P03, Male, 71)
“For this one [a Facebook page for a certain disease called Lyme disease], it’s very current.
Current and new findings, new news and then it has a website to go to so I can check it out myself
… Here are the things. They can’t find what I’m looking for. That’s nice, but they have recent
posts and contacts. There it is about the Lyme disease today, which I might do.” (P15, Female, 79)
“I’m cautious about Wikipedia. I use it a lot, but I always verify the information because it’s not
always updated, you don’t know who did the editing.” (P14, Female, 71)

4.2.5.4 Content expertise. There were also some comments regarding content expertise
(11 out of 21; 52.4%)—i.e., whether or not the message/information being provided in the
website is informative, complete, comprehensive, in-depth, accurate, correct, and clear (Choi &
Stvilia, 2015). In particular, when certain health information was based on academic research
findings, providing citations and references (i.e., evidence-based), the health information tended
to be perceived as more credible. As mentioned above, the informants valued highly the healthrelated information from well-known academic and government institutions, as opposed to
commercial institutions, as it was believed as being based on a good intention (i.e., promoting
public interests, rather than making profit), but it is also based on scientific evidence:
“It references high quality journal articles and they seem to be credible to me … the Cochrane
Collection collects journal articles where there’s actual evidence been collected, medical evidence
been collected, to verify that hypothesis. That’s their job and so it’s sort of the gold standard of
studies that are good and reliable and they’re testing their criteria very rigorously. (P09, Male, 70)

125

“What I was taught when I was getting my Master’s is that when you’re pulling information off
the Internet, if it’s an article, if you look at the very last, check the references. What I found on
health is, if there’re medical associations, then when you check those references, then it will tell
you whether this article is just an opinion.” (P20, Female, 61)

Another cue/marker for judging content’s expertise mentioned was regarding the intrinsic
quality of the information itself. One participant in the study pointed out that any typographical
errors on a website (or a webpage) would play a role as a trigger that decreases her perception of
the given health information on the website.
“If it’s well written, you know if something, I guess probably it’s more of negative one if it’s
poorly written, if there are errors in grammar, syntax, and punctuation. I immediately discount, I
just do that automatically with everything. I’m really suspect when people don’t write well. (P07,
Female, 72)

4.2.5.5 Design trustworthiness. Markers/cues regarding design trustworthiness was
relatively less mentioned as being influential to older adults’ perceptions of health information
credibility, than other types of credibility cues/markers (2 out of 21; 9.5%). In other words, older
adults did not care much about whether or not the website is stable (e.g., all links in the site are
working properly); or whether or not the website has ‘real-world feel’ (e.g., providing the
operator’s picture and contact information) when they select the credible source for health
information:
“I don’t care whether or not there are pictures. I don’t really feel that they need for somebody to
have big classy pictures.” (P04, Female, 66)

4.2.5.6 Design expertise. Design expertise-related cues/markers were relatively more
often mentioned than those for design trustworthiness (9 out of 21; 42.9%)—i.e., whether or not
the structure, functionality, aesthetic design, and interactivity of the website is well organized,

126

easy to use, and aesthetically put together (Choi & Stvilia, 2015). Specifically, several of them
highlighted that usability-related features could affect their perception of the credibility of the
website, as it might enhance (or hinder) finding necessary information out of it. Below are some
of the mentioned regarding the effect of a poorly designed website on the perceived credibility of
the website:
“[It was] very consumer- and user-friendly. And, it was set up as a question and answer format,
which you can just look for what you're looking for question-wise and there was the answer.
Much easier!” (P01, Female, 64)
“It’s got a terrible, terrible user interface. It’s awful. It’s horrible. It’s like something done in 1910
… the information on the bottom of it is pretty credible and good, but it’s awful to use.” (P09,
Male, 70)

Also, there were some informants who considered the aesthetic impression of a healthrelated website a useful cue/maker in judging the credibility of health information:
“Yeah, for me visuals. I can get a lot of information out of a visual, as well as what’s written
there. One of the things I like about the Mayo website is if you go and you look up something,
it’ll have just enough to read on a page, and then it’ll have other, you can click on another page to
find out more about part of the area. It doesn’t overwhelm you.” (P08, Female, 69)
“I’m influenced by the design, certainly. This is an excellent website. I go back to the home page.
It’s colorful, it’s not cluttered. It’s very easy to navigate.” (P14, Female, 71)

Table 4.15 Credibility Markers/Cues Mentioned by the Research Participants
Types of measures	  
Operator trustworthiness	  

n (%)	  
19	   (90.5)	  

Markers/cues mentioned	  
• Nature of website (e.g., government and academic sites
vs. commercial sites)	  
• Commercial intent (e.g., advertisements)	  

Operator expertise	  

15	   (71.4)	  

• Name recognition	  
• Credentials	  

127

Table 4.15 – Continued
Content trustworthiness	  

Content expertise	  

8	   (38.1)	  

11	   (52.4)	  

• Unbiased and consistent across sources	  
• Current information 	  
• Research-based evidence provision	  
• Intrinsic quality (e.g., typo)	  

Design trustworthiness	  

2	   (9.5)	  

Mentioned as non-influential (‘neutral’)	  

Design expertise	  

9	   (42.9)	  

• Easy-to-use and easy-to-navigate	  
• Aesthetic impression	  

4.2.6 Ratings on Credibility Markers
After the participants freely talked about their strategies to choose credible sources for
health information, identifying various types of credibility markers/cues and heuristics, they
were asked to rate each of the 35 items chosen from the literature using a five-point Likert scale.
They were to rate each item in terms of the extent to which the given item (marker/cue) would
increase or decrease their perceptions of health information credibility. Specifically, when a
given item would have a negative impact on their Web credibility assessment of a health-related
website, the participants were asked to select –1 or –2, depending on the perceived extent; they
were asked to choose +1 or +2 when a given item would have a positive impact on their
perceptions of credibility of a health-related website; when a given item would have neither a
positive nor a negative impact on their Web credibility perception, they were asked to choose 0
(zero).
The participants’ ratings on the given items were transformed into positive numbers for
data analysis: –2 to 1; –1 to 2; 0 to 3; 1 to 4; and 2 to 5. Using the transformed numbers, mean
values of the ratings on each item (i.e., credibility marker) ranged from 1.24 (median = 1.00; SD
= .539) to 4.76 (median = 5.00; SD = .436), where 1.00 indicated much less credible; 2.00
indicated less credible; 3.00 indicated neutral; 4 indicated more credible; and 5 indicated much
128

more credible. As mentioned above, the ratings were based on the participants’ perceptions of
whether the given markers/cues would have a positive, negative, or even neutral impact on their
Web credibility assessment. Thus, it should be noted that even though the items in Table 4.16
below are sorted by the mean values in descending order, the lowest value—i.e., D29 (M = 1.24;
median = 1.00)—does not indicate that the item has the least impact on Web credibility
assessment; rather, it means that the given item was perceived as the most negative credibility
marker for the research participants.
The researcher performed the one-sample t test to categorize the credibility markers/cues
into positive, negative, or neutral types based on the participants’ ratings (Table 4.16). More
specifically, the researcher examined whether or not a particular marker/cue was perceived as
significantly different from neutral (i.e., “3”), using the t statistic against a population value of 3.

Table 4.16 Ratings on Credibility Markers/Cues by the Research Participants
No.
D19
D03
D07
D02
D34
D17
D32
D04
D22
D33
D09
D23
D05
D08
D18

Item wordings
The site is by an organization that is well-respected
outside of the Internet.
The site lists authors’ credentials for each article.
The site was recommended to you by a doctor.
The site has articles containing citations and
references.
The site tries to cover all of the different approaches
to the issue.
The site represents an organization you respect.
This site is complete in the information it provides.
The site is arranged in a way that makes sense to you.
The site states its policy on content.
The site provides information that is neutral.
The site has ratings or reviews of its content.
The site is linked to by a site you think is believable.
The site has been updated since your last visit.
The site represents a nonprofit organization.
The site gives a contact phone number.

129

Ratings
M
SD
4.76
.436

t(20)
18.50

p
< .001

𝑟!
.945

4.48
4.48
4.43

.750
.602
.746

9.02
11.25
8.77

< .001
< .001
< .001

.803
.863
.794

4.43

.746

8.77

< .001

.794

4.43
4.24
4.19
4.05
4.05
3.95
3.95
3.90
3.86
3.81

.598
.831
.680
.805
.805
.805
.669
1.261
.727
.981

10.95
6.83
8.03
5.97
5.97
5.42
6.52
3.29
5.40
3.78

< .001
< .001
< .001
< .001
< .001
< .001
< .001
< .001
< .001
< .001

.857
.700
.763
.640
.640
.595
.680
.351
.593
.417

Table 4.16 – Continued
D21
D01
D10
D35
D20
D24
D06
D25
D26
D11
D13
D14
D28
D12
D27
D16
D15
D31
D30
D29

The site gives a contact email address.
The site lists the organization’s physical address.
The URL for the site ends with “.org”
The site is customizable according to your
preference.
The site looks professionally designed.
The site provides links to its competitors’ sites.
The site was recommended to you by a friend (nonexpert).
The site displays an award it has won.
The site is small (e.g., fewer than five pages).
The site recognizes that you have been there before.
The site requires a paid subscription to gain access.
The site takes a long time to download.
The site is sometimes unexpectedly unavailable.
The site has one or more ads on each page.
The site has a commercial purpose (as opposed to an
academic one).
The site links to a site you think is NOT credible.
The site is rarely updated with new content.
The site has links that do not work.
The site has typographical errors.
The site automatically pops up new windows with
ads.

3.81
3.67
3.67
3.52

.873
.796
.796
1.030

4.25
3.84
3.84
2.33

< .001
< .001
< .001
.03

.475
.424
.424
.213

3.38
3.33
3.14

.865
.856
.655

2.02
1.78
1.00

.06
.09
.33

.169
.137
.048

3.00
2.95
2.95
2.33
2.24
2.05
1.81
1.71

.837
.590
.740
.796
.889
.805
.814
.845

0.00
-0.37
-0.30
-3.84
-3.93
-5.42
-6.71
-6.97

1.00
.72
.77
< .001
< .001
< .001
< .001
< .001

.000
.007
.004
.424
.435
.595
.692
.708

1.48
1.38
1.38
1.24
1.24

.602
.498
.590
.539
.539

-11.61
-14.91
-12.58
-14.98
-14.98

< .001
< .001
< .001
< .001
< .001

.871
.917
.888
.918
.918

Note. The mean values indicate whether each of the items (markers/cues) has a positive, negative, or neutral impact
on people’s perceptions of Web credibility, not the extent of its importance. Thus, the lowest mean value indicates
that the given item (D29 in the study) had the most negative impact on the research participants’ perceptions of
credibility of a health website among the given 35 items. The items shaded in gray are neutral markers/cues that
were perceived as being neither positive nor negative (p >.05); thus, the items listed above the gray shading are
positive markers, and the items below the gray shading are negative markers.

4.2.6.1 Positive, neutral, and negative credibility markers. Based on the t test results,
19 out of 35 items were included in the positive credibility marker category with an average of M
= 4.09, SD = .381. Statistical analysis indicated that the ratings on the positively perceived items
were significantly greater than the neutral value “3,” t(20) = 13.08, p < .001, 𝑟 ! = .895. In the
negative credibility marker category, 10 out of 35 items were included with an average of M =
1.69, SD = .437. The t statistic showed that the ratings on these items were significantly lower

130

than the neutral value “3,” t(20) = –13.771, p < .001, 𝑟 ! = .905. Lastly, 6 out of 35 items were
grouped into the neutral credibility marker category with an average of M = 3.13, SD = .428.
These items were not perceived as being different from the neutral value “3” t(20) = 1.360, p
= .189, 𝑟 ! = .085. Table 4.17 reports the mean and standard deviation of each category.

Table 4.17 Positive, Neutral, and Negative Credibility Markers
Category
Positive markers
Neutral markers
Negative markers

Number of items
n (%)
19 (54.3%)
6 (17.1%)
10 (28.6%)

Ratings
M
4.09
3.13
1.69

SD
.381
.428
.437

t(20)
13.08
1.36
-13.77

𝑟!
.895
.085
.905

p
< .001
.19
< .001

4.2.6.2 Ratings by user characteristics. Based on the categorization of the credibility
markers (i.e., positive, neutral, and negative groups), Table 4.18 provides the average ratings by
user characteristics, such as age, gender, ethnicity, educational background, Internet usage, and
the types of “mastery of life.” Given the small sample size, the researcher did not conduct
inferential tests comparing subgroups, such as age groups (i.e., young-old vs. old-old), gender
groups (i.e., female vs. male), education levels, and so on. Rather, the researcher explored any
notable features that might influence the research participants’ perceptions of credibility markers,
which then could be examined through quantitative examinations in future research.

Table 4.18 Ratings on Credibility Markers by User Characteristics
User characteristics
Age group
Young-old
Old-old
Gender
Female
Male

n

Positive
M
SD

Neutral
M
SD

Negative
M
SD

15
6

4.21
4.09

.349
.521

3.05
3.26

.266
.383

1.53
1.54

.434
.459

16
5

4.25
3.93

.324
.531

3.11
3.09

.329
.265

1.48
1.72

.391
.541

131

Table 4.18 – Continued
Ethnicity
White Caucasian
African American
Education
High school
Some college
Bachelor
Master
Doctorate
Internet use (per week)
1–5 hours
6–10 hours
11–15 hours
16–20 hours
20+ hours
Mastery of life
Pessimistic-affective
Defensive-affective
Pessimistic-cognitive
Optimistic-cognitive

19
2

4.15
4.41

.391
.486

3.09
3.27

.319
.129

1.57
1.19

.432
.265

1
5
5
5
5

3.31
4.24
4.03
4.55
4.06

.263
.298
.281
.362

3.00
3.18
2.95
3.22
3.11

.386
.277
.378
.226

1.63
1.63
1.45
1.43
1.63

.508
.401
.456
.500

3
3
3
5
7

3.65
4.54
4.12
4.07
4.08

.243
.030
.185
.435
.354

3.11
3.61
3.17
2.93
3.05

.096
.419
.333
.450
.469

1.80
1.63
1.60
1.54
1.80

.557
.650
.265
.391
.473

1
2
7
11

3.37
4.34
4.02
4.15

.335
.299
.392

3.00
3.42
3.29
2.98

.354
.381
.246

1.70
1.30
1.67
1.76

.424
.415
.474

4.2.6.2.1 Mastery of life. Acknowledging the prematurity of arguing any generalizable
findings with the limited sample size of the study, there were some notable discussion points
regarding the relationship between the ratings on the credibility markers and the types of
“mastery of life.” One participant (n = 1) who had relatively less educational experience (high
school graduate) than other study participants did showed the typical characteristics of the
pessimistic-affective (P-A) type of “mastery of life,” such as a heavy reliance on others (e.g.,
doctor and partner) for information-seeking in his everyday life as well as a lack of confidence in
and appreciation of systematic and cognitive information-seeking. This participant’s ratings on
the positive credibility markers tended to be lower than those from other types of “mastery of
life” groups. In particular, his average rating on the positive items was 3.31 (M = 3.37), while the
average of the whole group was 4.12 (M = 4.12; SD = .353), which simply showed his suspicion

132

about the credibility of online health information (Figure 4.1). The comments he made in the
interview also supported the finding that he did not trust information on the Web because he had
difficulty with distinguishing credible health information from fraudulent and/or false
information.

Figure 4.1 Ratings on Credibility Markers by Mastery of Life
4.2.6.2.2 Internet usage. Three participants (n = 3) answered that they used the Internet
for one to five hours per week, which was the lower bound in the current study; the
aforementioned P-A-type participant was included in this group. The range of this group’s
ratings on the credibility markers tended to be narrower than those from other groups (Figure
4.2). Like the ratings by the P-A type of “mastery of life” mentioned above (Figure 5.1), this
“less-tech-savvy” group gave relatively more negative ratings on the positive credibility markers
(M = 3.65; SD = .243) than other groups did (M = 4.15; SD = .353).

133

Figure 4.2 Ratings on Credibility Markers by Internet Usage

134

CHAPTER 5
DISCUSSION
This chapter discusses the findings presented in Chapter 4. In Phase I, the researcher
analyzed the literature on information credibility to have a theoretical understanding of how
people assess the credibility of various types of online resources in different contexts. As a
product of the qualitative meta-study in Phase I, a new framework for Web credibility
assessment, called the WC framework, was proposed. The new framework was then used as a
theoretical framework, along with the ELIS model by Savolainen (1995), in the following study
focusing on older adults’ credibility assessment of online health information (Phase II). Findings
from Phase II provided a deeper understanding of how older adults judge the credibility of online
health information in the context of ELIS. The subsequent sections, therefore, discuss both the
theoretical and empirical implications of the dissertation research.
5.1 A New Framework of Web Credibility Assessment
The first set of research questions asked about how existing theories and models
conceptualized people’s credibility assessment of online information and how they could be
synthesized and improved for empirical studies:
RQ1: How is the process of Web credibility assessment conceptualized in existing
theories and models?
RQ1-1: What are the common and unique features of existing theories and models
of Web credibility assessment?
RQ1-2: How can the existing theoretical frameworks of Web credibility assessment
be improved?

135

A new framework of Web credibility assessment, named WC framework, was developed
based on a qualitative meta-synthesis of the core facets of Web credibility assessment identified
in Phase I, such as conceptualization, operationalization, user characteristics, context, and
process. The processes of synthesizing these facets of Web credibility assessment into three main
components of the WC framework (i.e., Assessment of WC; Variability of WC; and Process of
WC) were explained in Chapter 4. This section discusses the usefulness of the new framework.
5.1.1 Assessment of WC
The first component of the new framework, Assessment of WC, provides a conceptual
typology of Web credibility assessment, cross-mapping the two key dimensions of credibility
(i.e., trustworthiness and expertise) and three objects of credibility assessments (i.e., operator,
content, and design credibility; Table 4.1). As highlighted, the existing frameworks reviewed in
this study did not articulate the relationship between the key dimensions of credibility (i.e.,
conceptualization) and the measures that can be used to examine the credibility dimensions (i.e.,
operationalization). Rather, they tended to focus on deploying some of the markers/cues and
heuristics that are known as being influential to Web credibility assessment along with user
characteristics such as demographics and user involvement. For instance, Wathen and Burkell
(2002) depicted the Web credibility assessment process as two phases (i.e., evaluation of surface
credibility and evaluation of message credibility) and listed various markers/cues for each phase.
Specifically, they identified appearance/presentation, usability/interface design, and organization
of information for the first phase of credibility assessment, evaluation of surface credibility; they
also identified expertise/competence, trustworthiness, credentials, etc. for the second phase,
evaluation of message credibility. Even though this model provided a good understanding of how
people go through the process of Web credibility assessment, it did not define what credibility is

136

(i.e., conceptualization) and which measures can be used to measure each of the key dimensions
of credibility (i.e., operationalization).
Metzger (2007) proposed a three-phase model of Web credibility assessment, i.e.,
exposure phase, evaluation phase, and judgment phase. This model focuses mainly on the effects
of user motivation and ability on the three phases of Web credibility assessment—the overall
idea of the model is that a user goes through one of the two routes (peripheral or central
evaluation route) at the beginning of Web credibility assessment process depending on his or her
motivation and ability in the given context. The last phase of this model is “credibility
judgment,” but, it does not articulate what credibility is and how it is measured. This is the case
in the MAIN model (Sundar, 2008) and the P-I Theory (Fogg, 2003b); they theorize the process
of Web credibility assessment, but do not explain how the concept of credibility is defined in the
given model and which measures can be employed to examine the operationalized concept of
credibility.
Hilligoss and Rieh’s (2008) “Unifying Model” did identify a phase for conceptualization
of credibility, i.e., Construct. The authors conceptualized that a user would define credibility
using one or more dimensions of credibility such as trustfulness, believability, trustworthiness,
objectivity, and reliability. They argued that depending on how users define credibility in the
phase Construct, different heuristics are employed, which then influence their Web credibility
assessment. However, the five dimensions of credibility identified in this model were based on
the frequency of the research participants’ (n = 24) expressions (mentions), rather than a more
systematic conceptualization. For instance, some of the credibility dimensions identified in the
study could be grouped as synonyms (e.g., trustworthiness, trustfulness and believability) if they
used a factor analytic approach to the candidate terms. Furthermore, the heuristics identified in

137

the model were not explained in full regarding which of the five credibility dimensions were
specifically related with.
Besides the six frameworks reviewed in the current research, Tseng and Fogg’s (1999)
Four Types of Computer Credibility is arguably an initial framework that is focused on various
factors contributing to credibility of computer-based information system. As the Web has
become an important source of information, the four types of credibility were used to understand
users’ experiences on the Web and called Four Types of Web Credibility (Fogg, 2003a). The
authors suggest that four types of credibility may come into play: presumed, reputed, surface,
and earned (or experienced) credibility. Briefly summarizing the each type of credibility in the
framework, “presumed credibility” is defined as the extent to which a person believes someone
or something because of general assumptions in the person’s mind (e.g., some people assume
that nonprofit organizations are more likely to be trustworthy because they are not seeking
commercial gain); “surface credibility” is derived from simple inspection—people often make an
initial judgment about credibility based on first impressions of surface traits (e.g., people
perceive a website to be credible when the site looks professionally designed); “reputed
credibility” is defined as the extent to which a person believes someone or something because of
what third parties have reported (e.g., awards, seals of approval, links, and endorsements from
friends); “earned (or experienced) credibility” is assumed to be the most powerful form of
credibility that is derived from people’s interactions with systems over an extended period of
time (e.g., first-hand experience with a certain website). Even though the typology is useful to
understand types of credibility markers/cues and heuristics, it does not articulate how the core
concept, credibility, is defined and connected to the four types of Web credibility; it does not

138

explain the process of Web credibility assessment and the roles of context and user
characteristics in the process, either.
Because of the absence of a systematic conceptualization of the relationships between
key dimensions of credibility and measures that could be used to operationalize them in the
existing theories and models, their explanatory power to provide a comprehensive and integrated
interpretation of the findings from empirical studies was rather limited. The new framework
proposed in the current study (i.e., the WC framework), therefore, has its strength in articulating
the relationship between the key dimensions of credibility (i.e., trustworthiness and expertise)
and the main objects of credibility assessments (i.e., operator, content, and design). As shown in
Table 4.1, the new conceptual typology of Web credibility sorted out the markers/cues and
heuristics employed in empirical studies in the literature. For more generalizable application of
the new framework in empirical studies, Table 5.1 provides definitions of the six types of Web
credibility assessment proposed in the Assessment component of the WC framework and lists
sub-dimensions of each category. This new typology can be used as a more elaborate
organization of Web credibility, its key dimensions, and sub-dimensions of each key dimension,
which all have been identified and examined in the literature over the decades. In future research,
the framework needs to be tested, as to whether it is supported by empirical data.

Table 5.1 Conceptual Typology of WC Assessment
Operator

Trustworthiness	  
How trustworthy is the operator? – Whether
or not the operator’s character is:
• Ethical
• Honest/Sincere
• Fair
• Believable
• Well-respected
• Trusted

139

Expertise	  
How expert is the operator? – Whether or
not the operator (source) of the Web
resources is:
• Reputable
• Famous
• Authoritative
• Competent

Table 5.1 – Continued
Content

How trustworthy is the content? – Whether
or not the message/information being
provided in the website is:
• Neutral
• Unbiased
• Even-handed
• Consistent
• Current

How expert is the content? – Whether or
not the message/information being
provided in the website is:
• Informative
• Complete
• Comprehensive
• In-depth
• Accurate
• Correct
• Clear

Design

How trustworthy is the design? – Whether or
not the structure, functionality, aesthetic
design, and interactivity of information
and/or the website as a whole is:
• Stable
• Consistent
• Reliable

How expert is the design? – Whether or not
the structure, functionality, aesthetic
design, and interactivity of information
and/or the website as a whole is:
• Well-organized
• Easy to use
• Aesthetically put together

5.1.2 Process of WC
Process of WC is the second component of the new framework proposed in this
dissertation research. It conceptualizes the process of how people identify and select credible
information items to fulfill their information needs in the Web context. Based on the findings
from the qualitative meta-study on existing theories and modes of Web credibility assessment
(Phase I), two stages of Web credibility evaluations were identified: Initial Stage of Evaluation
and Final Stage of Evaluation. The definitions of the stages, actions taken by users in each stage,
and associated makers/cues and heuristics are provided in Chapter 4.
In a broad sense, Web credibility assessment is part of human information behavior that
seeks, uses, and shares information in the Web context. The process of Web credibility
assessment is thus influenced by various contextual, individual, and social variables that have
impacts on information behavior. Thus, in the current dissertation research, one of the useful
140

models of information behavior in the everyday life context, ELIS model by Savolainen (1995)
was employed as a theoretical framework, along with the new framework of Web credibility
assessment (i.e., the WC framework), which was developed in Phase I.
Considering the close relationships between information behavior and credibility
assessment, this section discusses how the new framework of Web credibility assessment can be
‘plugged into’ other theories and models of information behavior to better understand how
people optimize the use of resources, including cultural/cognitive and social capital, to acquire
necessary information in a given context. In particular, the two-stage model of Web credibility
assessment described in the Process of WC (Table 4.4)—i.e., initial and final evaluations—is
looked into in relation to existing theories of information behavior.
The “Information Foraging Theory” (Pirolli & Card, 1999), which addresses the overall
processes of people’s information seeking and using, may be a good theoretical lens to
understand the relationship between information behavior and information credibility
assessment. Using the analogy of animals’ food seeking and prey selection (i.e., food foraging),
the authors conceptualize people’s information-gathering and sense-making strategies with three
concepts: (1) information patch, (2) information diet, and (3) information scent. The underlying
assumption of the theory is that the optimal information forager (i.e., information seeker) would
acquire the necessarily information at a minimal cost (e.g., time) within the given circumstances.
By the analogy, information patches are various sources of information. For instance, when an
information forager seeks certain information, he or she navigates different information patches
such as piles of documents, books in libraries, and various online sources that may provide him
or her with the relevant information for the given task. Pirolli and Card (1999) argue that the
relevance of specific information patches (i.e., sources) may change depending on the context of

141

the task, in which the activity of information foraging is embedded. It is the same idea with how
other theories and models see people’s information seeking behavior—i.e., context-dependent.
For instance, the ELIS model by Savolainen (1995) focuses on people’s information behavior in
the context of everyday life, as opposed to a work-related context.
Information diet explains the situation where an information forager selects and uses
(consumes) certain information items from various information patches. Specifically, he or she
has to make decisions of which information patch should be examined first, and whether they
would focus on the chosen information patch (i.e., within-patch foraging) or move to another
information patch (i.e., between-patch foraging) to seek information relevant to the given task
(Pirolli & Card, 1999). In this process, the information forager estimates (defines) the potential
‘profitability’ of a certain information patch by considering the unit cost (e.g., time) of
processing the source, and it is alike the energy returned from certain food (prey)—i.e.,
information diet.
When navigating through various information patches, the information forager often has
to make the indirect, as opposed to direct, evaluation of information quality (or relevance) in an
attempt to identify the most ‘profitable’ information patch among many. The authors
conceptualize that the information forager would follow information scents (Pirolli, 1997) or
residues (Furnas, 1997), which are imperfect representations of the information quality (or
relevance) based on proximal cues. In the Web context, the proximal cues include source
credentials, hyperlinks in a Website, and icons representing the sources (Pirolli & Card, 1999),
which are closely related to the markers/cues and heuristics identified in the information
credibility literature (Table 4.1). If using the analogy, credibility markers/cues may exude either
a positive, negative, or even neutral ‘scent’ in sense-making around the relevant information to

142

the given task. Thus, Web credibility assessment could be understood as part of the process of
following the “information scents” to find the most profitable, high quality “information diet”
among the various types of “information patches.”
Applying the three main concepts of the Information Foraging Theory to interpreting the
findings from Phase II, the research participants (i.e., older adults) navigated various types of
online “information patches” such as websites run by government, academic institutions, and
other non-profit organizations (e.g., NIH, CDC, Mayo Clinic, JHM, HMS), as well as some of
the commercial sites (e.g., WebMD, Walgreens, CVS) to find the relevant information for their
health information needs. In choosing and pursuing the information patches on the Web, older
adults followed various “information scents” in terms of the perceived trustworthiness and
expertise of the operator (source), content (message), and design (media). In other words, they
used one or more of the six types of Web credibility markers/cues to identify the best
“information diet” for the given task. Considering the current online information environment, in
which users (i.e., online information foragers) have to seek out the relevant information from a
great amount and variety of information sources, Web credibility assessment seems to be a
crucial component of online information seeking.
Table 5.2 conceptualizes the relationships between the process of information behavior,
especially information seeking in the Web context, and Web credibility assessment, based on the
discussion above on the connections between the Information Foraging Theory by Pirolli and
Card (1999) and the WC framework. As mentioned above, since this theory-synthesis exercise is
still in the conceptual stage, future research should test and validate the conceptual relationship
between the two theoretical frameworks with empirical data.

143

Table 5.2 Conceptual Relationship between Information Foraging Theory and WC framework
Information Foraging
Scent-following activities:
• Information credibility scent
• Information Diet
Enrichment activities:
• Information Patch

WC Process
Initial Stage

Final Stage

WC Framework
WC Assessment
• Source trustworthiness
• Source expertise

•
•
•
•
•
•

Source trustworthiness
Source expertise
Content trustworthiness
Content expertise
Design trustworthiness
Design expertise

5.1.3 Variability of WC
One of the three components of the WC framework, Variability of WC, accounts for the
contextual and individual factors that have impacts on the overall process of Web credibility
assessment. For instance, the qualitative meta-study on information credibility literature (Phase
I) identified time, situational norm, topic, and goal as context-related factors; and demographics,
motivation, ability, domain expertise, information literacy, and media reliance as user-related
factors (Table 4.2). The findings from the semi-structured interviews with older adults in the
current research (Phase II) showed that some of the contextual and individual variables might
come into play when older adults assess the credibility of online health information. For instance,
some of the informants paid attention to the information from their friends or family who were
not experts in the medical field when it comes to seeking general wellness information (e.g.,
healthy diet, nutrition facts, and good exercise for elderly), while they did not necessarily trust
them when they were supposed to figure out more critical health-related issues/problems (e.g.,
symptoms and cure for certain diseases)—i.e., context. Also, those who were suffering from a
certain disease had more knowledge and experience with the disease than those who were not

144

involved with the disease, and, therefore, they were able and willing to judge the credibility of
the content as well as the source and design of the online health information—i.e., involvement
(user characteristic).
In addition to the aforementioned variables, one of the factors included in Savolainen’s
(1995) ELIS model “social capital” seems to be a useful factor that can expand the perspective
on the variability in Web credibility assessment. As defined above, social capital refers to “the
nature of contact networks” (Savolainen, 1995, p. 269) and it is considered a significant factor
that forms people’s “ways of life” and “mastery of life,” which influence their information
behaviors in daily life settings. Using the categorization of the conceptual frameworks for
information behavior by Pettigrew et al. (2001)—cognitive approaches examine “the individual
as the main driving force behind information behavior” and social approaches examine
“frameworks that focus on the social context” (p. 46)—the dynamic and variable nature of
people’s Web credibility assessment could be further explained by social approaches that
acknowledge the influences of social factors on information behavior.
More specifically, in the initial evaluation stage of the process of WC, people identify the
lists of candidate websites that may be useful to fulfill their information needs. In this stage,
several of the research participants in the semi-structured interviews mentioned that they might
have a positive impression of a certain website if it was recommended by their trusted social
contact networks such as doctors, partners, and friends. Even in the cases where they would not
necessarily trust the recommended website, they mentioned that they would at least take a look at
the site because they trust the recommenders. Thus, people’s social connects could have an effect
on the initial stage of Web credibility assessment.

145

Wilson’s (1983) cognitive authority, defined as “influence on one’s thoughts that one
would consciously recognize as proper” (p. 15), seems to be a useful concept that can add the
social perspectives to the WC framework. Wilson (1983) says that a cognitive authority could be
a friend, colleague, or expert, who has a significant effect on an individuals’ decision making.
Thus, various types of interpersonal sources that one relies heavily on (e.g., doctors and partners)
may have a significant impact on his or her Web credibility assessment. On the Web, cognitive
authorities may be online communities for specific topics (e.g., a Facebook page for breast
cancer).
The social approaches were useful to further interpret the interview data regarding older
adults’ Web credibility assessment (Phase II). As mentioned above, older adults who were in a
couple relationship tended to trust information from their partners. Thus, these people seemed to
perceive a website credible if it was recommended by their partners; they were at least willing to
spend time and effort to examine the recommended website for their information seeking. In
addition, as shown in Table 4.9 above, those who were participating in local communities for
specific diseases, such as a local chapter for the National Parkinson Foundation or a Facebook
page for breast cancer, regarded their social networks within the communities as credible sources
to acquire (and share) necessarily health information. Thus, recommendations from the
communities where people are involved in could influence their initial choice of websites to seek
for necessary health information.
In the new framework, therefore, the potential impacts of social factors (e.g., “cognitive
authority” by Wilson, 2003; and “social capital” by Savolainen, 1995) on Web credibility
assessment are taken into consideration as part of the Context. Table 5.3 provides the definitions
and examples of variables that influence people’s Web credibility assessment process.

146

Table 5.3 Variability of Web Credibility Assessment
Variability types
Context

Variables
User context

Definition
User’s environment that
influence the process of Web
credibility
Types of tasks that influence the
process of Web credibility
User’s social networks that
influence the process of Web
credibility

Examples
• Time
• Situational norm

Demographics

User’s demographic backgrounds
that influence Web credibility
assessment

Involvement

The degree to which users know
and care about specific topics
under examination

Technology
proficiency

The degree to which users are
familiar and comfortable with
the technology (Internet) to
identify, access, evaluate, and
use information resources

• Age
• Gender
• Education
• Motivation
• Ability
• Domain expertise
• Information literacy
• Media reliance

Task context
Social context

User characteristics

• Topic
• Goal
• Social capital
(Savolainen, 1995)
• Cognitive authority
(Wilson, 1983)

5.2 Characteristics of Older Adults’ ELIS
The second and third research questions examined older adults’ information behavior,
especially health information behavior, in their everyday lives:
RQ2: In general, what are older adults’ common (everyday life information seeking:
ELIS) information needs?
RQ3: What are older adults’ health information needs and related information behaviors?
RQ3-1: What sources do older adults use to find health information both on- and
offline, and why do they use those sources?
RQ3-2: How do they use the information they find?

147

As reported in Chapter 4, the research participants mentioned various topics of
information needs, such as health/wellness, travel, culture/education, finance, entertainment, and
others. Particularly, health/wellness seemed to be the most important topic of ELIS for older
adults, as it was mentioned by all of the research participants in the study. Furthermore, there
were some characteristics in their health information seeking behaviors that encompass not only
the ‘purposive’ meaning of information-seeking, but also the ‘passive’ ways of acquiring health
information—the latter is termed incidental information acquisition (Williamson, 1998) or
accidental information discovery (Wilson, 1977). Using the Savolainen’s (1995) ELIS model
that guided the current research in understanding older adults’ information behaviors in the
everyday life settings, the serendipitous way of health information acquisition is regarded as part
of ‘orienting’ information seeking, as opposed to ‘practical’ information seeking. As part of
health information behavior, older adults used and shared the acquired health information with
people in their social networks, as well. The subsequent sections provide discussions on: (1)
older adults’ health and wellness information needs in their daily lives; (2) the characteristics of
their orienting and (3) practical information seeking behaviors to fulfill health/wellness-related
information needs; and (4) the ways that older adults use (act on) and share the acquired
health/wellness information.
5.2.1 Older Adults’ Health/Wellness Information Needs
As reported in Table 4.13, the most frequently mentioned type of information needs by
the research participants was health and wellness. Considering the sampling criteria of the
research (i.e., older adults’ who have looked for health information online over the past 6
months), it was not a surprising result that all the research participants (21 out of 21; 100%) had
sought health/wellness information in their daily lives. Beyond as a requirement for participating

148

in the study, each of the participants showed strong interests and/concerns with various
health/wellness-related topics, which made them seek for necessary health information. Health
topics mentioned included medication and supplements; diseases and symptoms; verifying
and/or understanding doctor’s notes; health insurance; healthy diet and nutrition facts; exercise;
and even pets’ health.
The finding that shows older adults’ interests and concerns in health/wellness-related
topics was in line with previous studies on older adults’ information needs and information
seeking behaviors (Fox & Duggan, 2013; Hirth, Czaja, & Sharit, 2007; Su & Conaway, 1995;
Williamson, 1998). For instance, Williamson (1998) reported that all of the 202 research
participants who were 60 years old and older highlighted “health” as an important topic of
information needs in their daily lives, along with “income and finance.” More recently, Fox and
Duggan (2013) found that approximately 58% of Internet users who were 65 years old and older
looked for health information online as of 2012. The health topics identified in this study were
almost the same as those found in the current dissertation research: specific disease or medical
problem (55%); certain medical treatment or procedure (43%); how to lose weight or how to
control your weight (27%); health insurance (25%), and so on.
Hirth et al. (2007) found that older adults aged from 55 to 80 years old looked for health
information to better understand doctor’s notes. They gathered some background information of
the given health topic before they went to see a doctor, so that they could ask more specific
questions in the meeting with the doctor. They also sought for further information about the topic
after the visit. This type of health information seeking behavior was often found in the group of
people who were categorized into the cognitive type of “mastery of life” such as O-C and P-C in
the current study (see Section 4.2.2 Way of Life and Mastery of Life). In particular, those who

149

were cognitive and optimistic towards the information-seeking tasks often mentioned that they
attempted to double check all the health information that obtained from others, including doctors.
They preferred to validate health information by themselves by consulting with multiple sources
of information on the Web.
5.2.1.1 Health condition and health information needs. Most of the research
participants (17 out of 21; 81.0%) were suffering from some of the chronic geriatric diseases
and/or had experienced more serious diseases and illnesses such as cancer. The interview data
showed that those who were having such health issues were mainly interested in getting
information that was useful to deal with their health conditions, rather than more general
wellness-related information, while those who explicitly mentioned that they were healthy or
who did mention any specific health-related issues were interested in getting more general
wellness-related information (e.g., healthy diet, exercise). This finding can be understood within
the model of ELIS by Savolainen (1995). He pointed out that one’s “current situation of life” is
an important individual factor that has a significant impact on his or her ELIS along with other
factors such as material, cultural/cognitive, and social capital. Since the focus of the current
study was on health-related information behavior, one’s health condition was considered an
important aspect that shows the current situation of his or her life.
Zhang (2014) also highlighted that people’s health information behaviors are affected by
not only the characteristics of sources or the relevance and usefulness of the content itself, but
also the characteristics of users, such as knowledge status, personal preference, socioeconomic
status (SES), intention, characteristics of the health problem and information needs, problematic
situation. Among these factors identified in her study, the characteristics of the problematic
situation—whether it is acute or chronic; severe or not; common or rare; stigmatized or not—are

150

useful to understand the findings from the current study regarding the difference in the types of
health information needs between those who were having/had serious health issues and those
who were relatively healthier.
5.2.1.2 Social contact network and health information needs. Older adults sought for
health/wellness-related information not only for themselves, but also for others such as partners
(7 out of 21; 33.3%), other family members (5 out of 21; 23.8%), and friends (1 out of 21; 4.8%);
there were three participants who mentioned information needs for their pets (14.3%). In
particular, as reported in Chapter 4, partners were the second most frequently mentioned
interpersonal sources for health information in the current study, after medical professionals
(e.g., doctors and physicians), and one in a couple relationship played the main role as an
information ‘provider’ for the other in the relationship. These findings implied that older adults’
social contact networks, especially the partner relationships, would have a significant impact on
the types of information needs, and for the opposite ends (i.e., information ‘receivers’),
information seeking behaviors, as well. Based on the ELIS model (Savolainen, 1995), the nature
of contact networks (i.e., social capital) seemed to be a crucial factor that influenced older adults’
information behaviors, considering their relatively narrow social contact networks due to the
limited social activities—as mentioned above, the research participants were all retirees. Zhang
(2014) also found that whether the search is for self or for others was a situational factor that
formed the characteristics of information needs.
5.2.2 Orienting Information Seeking (Incidental Information Acquisition)
Orienting information seeking in ELIS is closely related to how one’s daily life is
organized (i.e., “Way of Life”), which may be reflected by the relationship between work and
leisure, models of consumption, and the nature of hobbies (Savolainen, 1995). This type of ELIS

151

is more passive than practical information seeking, which is to seek for specific information to
solve problems, and can be seen as incidental information acquisition (Williamson, 1998) or
accidental information discovery (Wilson, 1977). The following subsections discuss about some
of the characteristics of older adults’ orienting information seeking behaviors found in the
interview data.
5.2.2.1 Non-expert interpersonal sources. For orienting health information needs (e.g.,
general knowledge about healthy food, nutrition facts, news about drugs and supplements), older
adults were relatively more open to the information from their social connections, such as family
and friends, who were not experts in medicine, than when they were seeking for practical health
information. In this study, social networking was the most frequently mentioned type of daily
activities for older adults (17 out of 21; 81%), meaning that older adults spent their leisure time
with their friends talking about various topics in their everyday lives. Through the social
networking activities, they not only sought for practical information regarding their common
health/wellness issues, but also they incidentally ‘picked up’ unexpected information. For
instance, P08 (Female, 69) mentioned that she incidentally acquired health information about the
effect of biotin as a supplement for healthy hair in a casual conversation with her friends. In
other words, even though she did necessarily needed or particularly asked for the information
regarding biotin, she became to have the information from the social connections. She said that
she searched the Internet for more information about biotin to verify the information from her
friend, who was non-expert in the medical field.
This type of information acquisition was based on the nature of social contact networks in
the informants’ everyday life. In particular, as mentioned above, partners were one of the
important non-expert interpersonal sources for health information seeking for older adults. Also,

152

local communities for specific diseases/illnesses served as sources for health information seeking
for older adults, as well. For instance, P13 (Female, 80) was participating in the local chapter for
the National Parkinson Foundation in Tallahssee. Even though the main purpose of the local
chapter was to share useful information regarding Parkinson disease, it also played a role as a
social network, in which people who were in similar situations get together and develop a bond
of sympathy between members of the group—the informant considered it “a cordial retreat.”
This type of social connection provided the participant with the opportunities to gather
unsolicited information.
However, even though the participants in the study identified their non-expert social
connections as useful sources of health information, they did not rely solely on the information
from them. Rather, the ‘unexpected’ or ‘unsolicited’ new information from the non-expert
interpersonal sources (i.e., orienting information seeking or incidental information acquisition)
intrigued them to actively seek for further information to verify it (i.e., practical information
seeking). Thus, as Savolainen (1995) mentioned, the two different types of information seeking
seemed to be closed related to each other.
5.2.2.2 Cognitive hobbies. Older adults often obtained orienting health information
through their hobbies. Savolainen (1995) classifies individuals’ orientations to media use, which
is a crucial aspect of information seeking, into three categories by the nature of hobbies:
cognitive oriented, balanced, and affective oriented. As reported in Table 4.11, cognitive types of
hobbies included learning new things, reading books on cultural and scientific topics, and
reading newspapers to keep up with current affairs and cultural/social issues through TV or
radio; affective type of hobbies included playing games, enjoying entertaining programs on TV
and radio, watching movies, and listening music.

153

Even though the research participants were overall well educated—71.4% (15 out of 21)
graduated from a college—those who had relatively higher educational backgrounds (college
graduate or higher) seemed to have cognitive types of hobbies more often than those who had
relatively lower educational backgrounds (some college experience or high school graduate).
More specifically, 80% (12 out of 15) of the research participants who graduated from a college
(bachelor, master, and/or doctorate degree holders) mentioned that they enjoyed cognitive types
of hobbies such as reading newspapers and online new articles on current issues, reading books
on various topics (e.g., history, archeology, medicine, and so on), watching TED talks online,
and taking online courses. However, only 33.3% (2 out of 6) of those who did not graduate from
a college (some college and/or high school graduates) had cognitive types of hobbies (Table
4.11). In terms of affective and social types of hobbies, most of the participants seemed to
enjoyed these types of hobbies in their everyday lives: 85.7% (18 out of 21) of the research
participants mentioned the affective types of hobbies; and 95.2% (20 out of 21) mentioned the
social types of hobbies.
Thus, even though it is hard to generalize this finding with the limited sample size, it
seemed that those who had experiences in higher education tended to have both cognitive and
affective types of hobbies, while relatively less educated people were more into the affective
type of hobbies (Table 4.11). Given that the nature of hobbies is one of the important cues that
simply show how individuals organize their daily lives (Savolainen, 1995), which then directs
their information behaviors, especially source selection (Zhang, 2014), those who read books and
newspapers on various topics including health and wellness would have more opportunities to
incidentally (and purposefully) acquire reliable health information than those who only enjoy
affective hobbies.

154

5.2.2.3 Additional training and research-related activities. Except one participant who
worked as a nurse, none of them had formal school trainings or occupational experiences in the
medical field. However, there were some participants who had accumulated a substantial amount
of medical knowledge by participating in additional training activities such as the standardized
patient program and the Red Cross training in first aid, and even participating in a long-term
research project in a medical school as a participant. These additional training and researchrelated activities provided them with the opportunities to obtain more in-depth medical
knowledge, such as symptoms and cures for certain diseases of which they played a role as a
patient having the diseases in the standardized patient program and even (incidentally) acquired
knowledge about typical medical procedures. These people showed self-confidence in seeking
and understanding health and wellness information. This finding is supported by the literature on
the effects of health literacy programs for older adults. In particular, Xie (2011b) found that their
health literacy intervention significantly improved the research participants’ (M = 69.99; SD =
8.12) knowledge, skill, and eHealth literacy efficacy.
These people were all highly educated (holding a bachelor or a higher level degree) and
all had cognitive types of hobbies such as reading newspapers and books. Thus, cognitivelyoriented, rather than affectively-oriented, individuals seemed to be more often and naturally
exposed to the environment where they can acquire orienting health information purposefully as
well as incidentally.
5.2.3 Practical Information Seeking
Practical health information seeking happens when people need health information to
deal with specific medical issues and problems. For this type of information need, people
‘purposefully’ seek information to find answers, reduce uncertainty, and make sense of the given

155

health issues/problems that they are facing (Case, 2012). Since practical health information
seeking is more contextualized in specific problem-solving situations than orienting information
seeking, it is important to understand how people approach their problems in their everyday
lives—“mastery of life” (Savolainen, 1995). As mentioned above, “mastery of life” may be
understood as individuals’ information-seeking styles in terms of two dimension: (1) whether
they are cognitive or affective (emotional); and (2) whether they are optimistic or pessimistic
towards the solvability of the problem (Savolainen, 1995). As mentioned, even though
categorizing the research participants’ “mastery of life” types was not the main purpose of the
current research, the exercise of analyzing the participants’ coping styles towards problemsolving situations allowed the researcher to provide more in-depth interpretations of the
interview data. Thus, the characteristics of older adults’ health information behavior, including
Web credibility assessment, were understood in relation to their “mastery of life” types in some
of the following subsections.
5.2.3.1 Medical professionals as the primary source for practical health information
seeking. Considering the potential lethality of the misinformation of health issues, most of the
research participants regarded medical professionals such as doctors and primary care physicians
as the most reliable sources for practical health information seeking. They went to see a doctor,
rather than referencing family’s or friends’ advice or searching online sources by themselves
when they needed information about ‘acute’ or ‘serious’ health issues that might affect their
health condition in a negative way. In particular, when they were suffering from certain chronic
diseases and/or had experienced serious illnesses that needed to be monitored in their everyday
lives (e.g., cancer), they tended to rely on the information from their doctors, who had been
giving them medical treatments regarding their medical conditions. In this case, they were

156

supposed to take medicines and have regular checkups. As older adults often suffered from
minor and major illnesses and ailments, individuals’ health conditions seemed play an important
role in seeking practical health information, especially in choosing the primary source for health
information. A previous study on older adults’ health information use also found that regardless
of whether they were online users or non-users, they relied mostly on their health care providers
than any other types of sources such as pharmacist; newspapers and popular magazines; medical
journals, medical books, and popular books; television and radio; and friends and family (Taha et
al., 2009).
This information seeking behavior, however, seemed to be context-dependent. More
specifically, the seriousness of the given situation and privacy concerns seemed to influence
older adults’ behaviors in source selection. Getz’s (2010) study on Israeli older adults’ legal
information seeking behaviors found that older adults preferred to receive information from
informal sources, such as family members and mass media, over formal sources, such as
counselors from the National Insurance Institute and the Citizens’ Information Service. Getz and
Weissman (2010) mentioned that older adults in the study were cautious about the situation
where their problems were exposed to other people. Thus, unless they perceived the difficulties
in receiving necessary information from informal sources, they were reluctant to consult with
experts on the given topic area. This tendency that older adults preferred informal sources for
information seeking was also found in Su and Conaway’s (1995) study focused on Chinese older
adult immigrants’ information seeking behaviors: the participants in the study not only used their
family and friends as information sources for various topics of information needs including
health most frequently, but also they perceived them as more helpful sources than experts.

157

Based on these results, the Internet could be a useful channel for older adults who are
seeking information for relatively less acute and less serious medical issues/problems. If they
have sufficient levels of familiarity and confidence in using IT technologies, they could take
advantage of the anonymity of the Internet to obtain necessary information on the Web.
5.2.3.2 Partner as a source or an object of practical health information seeking.
Those who were in partner relationships mentioned that their partners were either the most
reliable source for practical health information (i.e., partner as an information provider) or the
object of information provision (i.e., partner as an information receiver). In the former case, they
relied heavily on the health information from their partners based on accumulated trust over a
long period of time, rather than their expertise in the given topic—when asked about their
partners’ educational backgrounds or occupational experiences, they were not necessarily experts
in the medical fields. On the opposite end, their partners were the dependents to whom they were
providing information for various topics including health and wellness.
Looking into the potential factors that might affect the role allocation in the couples’
health information seeking, being either information provider or receiver, “mastery of life” and
“Internet usage” seemed to be important factors; other factors, such as gender, age, education
level, and occupational experience, which were initially assumed to be influential to the role
allocation in a couple relationship, did not seem to be useful to explain these cases in the study.
Overall, those who were more optimistic towards information-seeking situations (i.e., “mastery
of life”) played the role as “information provider” in the couple relationships, while those who
were pessimistic and/or affective played the role as “information receiver” in the current study
(Table 5.4). In addition to the three couples who participated in the current study together, there
were five more participants who mentioned that they were the primary health information

158

providers for their partners in their everyday lives. They all showed strong self-confidence in
practical information seeking based on their cognitive/cultural capital (e.g., educational and
occupational experiences).
Furthermore, each of the three couples had the same orientation towards the nature of
information sources—cognitive vs. affective. In other words, both in a couple were either
cognitive or affective together; there was no couple that had different orientation in terms of
cognitive vs. affective. Thus, those participants who were more optimistic in practical health
information seeking seemed to play the information provider role in their couple relationships.
Lastly, all the “information providers” in the current study used the Internet more hours a
week than their partners did. Even though it is yet premature to generalize the relationships
between the couple role allocation, “mastery of life” types, and Internet usage given the small
sample of couples in the dataset (i.e., three couples), it seemed that those who were more familiar
with the Internet and Web-based resources tended to have higher confidence in seeking
necessary information online and played the role as “information providers” for their partners
(i.e., “information receivers”). Future research will need to examine the role allocation patterns
in older couples’ information behaviors with a large amount of quantitative data, yielding
generalizable findings.

Table 5.4 Couples’ Information Behavior and Mastery of Life
ID
P05 c1
P06 c1
P09 c2
P08 c2
P15 c3
P16 c3
P01

Age

Gender

Education

72
75
70
69
79
78
64

F
M
M
F
F
M
F

Some college
High school
Doctorate
Doctorate
Some college
Doctorate
Bachelor

Occupation
Not specified
Not specified
Attorney
Artist
Researcher
Attorney
Writer

159

Internet Use
(hours a week)
20+
1-5
20+
11-15
16-20
1-5
20+

Mastery
of Life
DA
PA
OC
PC
OC
PC
OC

Role
Provider
Receiver
Provider
Receiver
Provider
Receiver
Provider

Table 5.4 – Continued
P03
P11
P17
P20

71
67
65
61

M
F
F
F

Doctorate
Master
Bachelor
Master

Professor
Not specified
Government
Government

16-20
6-10
11-15
20+

OC
DA
OC
OC

Provider
Provider
Provider
Provider

Note. There are three couples who participated in the study together. IDs of these couples are noted with superscripts,
such as c1, c2, and c3, to indicate the couple relationships. The shaded cells in the table indicate the participants who
played the role as “information provider” for their partners in the couple relationships.

5.2.3.3 Online sources for practical health information seeking. The informants
utilized various Web resources to find factual information, seeking answers for specific
questions. In particular, many of them mentioned that they went online to verify certain health
information that they acquired from the non-expert interpersonal sources such as their family and
friends. Furthermore, there were some participants (8 out of 21; 38.1%) who sought health
information to verify and/or make sense of what doctors said to them (e.g., diagnosis,
prescription). These people were all categorized in the cognitively-oriented “mastery of life”
types, regardless of whether they were optimistic (i.e., optimistic-cognitive) or pessimistic (i.e.,
pessimistic-cognitive) about the solvability.
In general, the research participants highly valued the websites run by government (e.g.,
NIH, CDC) and academic institutions (e.g., JHM, HMS), as these types of websites were seen as
the Web representations of the highly respected organizations and institutions outside of the
Web. Some participants mentioned that they trust the health information from these websites
because it is based on research, rather than opinion, and their main intent is on improving public
health and wellness, rather than selling their products.
Also, some of the research participants considered the health information from official
associations for certain diseases (e.g., the American Association for Cancer Research, the
American Arthritis Society, and the National Parkinson Foundation) useful and reliable sources
that can be used for health information seeking. Similarly to the reasons for using .gov and .edu
160

sites, older adults liked the ‘non-commercial’ nature of the associations, as well as the researchbased information provision. Another important reason for using medical associations’ websites
mentioned was that they were able to share information with other patients (or caregivers) who
were suffering from the same ailments or diseases. This seemed to be more useful for them to
obtain the most recent as well as specific information about their information needs.
When asked about social media as a source of health information, most of the research
participants showed skeptical responses. Even those who were using social media such as
Facebook or Twitter in their everyday lives pointed out that social media posts are often based on
personal opinions rather than facts—lack of expertise. Moreover, some older adults highlighted
privacy issues in using social media for health information seeking, as they were concerned with
the possibility that their medical situations may be disclosed to unwanted groups of people.
Only a few mentioned that they looked at Facebook pages for certain health-related
topics such as breast cancer and Lymphedema. However, even those who were using social
media for health information seeking tended to consider social media a means for connecting
with other people who had the same diseases or for sharing the information with their friends or
family.
Lastly, even though most of the research participants preferred non-profit websites (i.e.,
.gov, .edu, and .org sites), there were some participants who were using commercial websites
(i.e., .com sites) for health information seeking. The most frequently mentioned .com site by the
research participatns in this study was WebMD, which provides health/wellness-related news
and information on variety of topics. Another type of commercial site mentioned was
pharmacists’ sites such as Walgreens and CVS. They were using these websites to have
information about prescriptions and to compare prices of medications they take.

161

5.3 Older Adults’ Credibility Assessment of Online Health Information
This section provides discussion on the findings from Chapter 4, which answered the last
set of research questions:
RQ4: How do older adults assess the credibility of health-related information on the
Web?
RQ4-1: What are older adults’ perceptions of Web credibility?
RQ4-2: What are some of the psychological, social, and/or cultural mechanisms
that underlie and/or affect those perceptions?
RQ4-3: What are some of the markers/cues and heuristics used by older adults to
assess the credibility of health-related websites?

As discussed in 5.1.2 Process of WC, assessing information credibility is an important
part of people’s information behavior. The interview data revealed major topics for information
needs in older adults’ everyday life, including health-/wellness-related topics, and several
different information-seeking strategies older adults used to fulfill their health information needs.
As for the -based resources for health information seeking, all the research participants
recognized the importance of and difficulties in identifying credible sources for health
information on the Web. Various types of credibility markers/cues and heuristics were
mentioned by the research participants in terms of operator (source), content (message), and
design (media). The following sections (1) look into older adults’ perceptions of the credibility
markers/cues and heuristics based on the six types of WC assessment, combining the verbal
interview data and ratings on the Likert-type items; (2) provide a discussion on the variability of
Web credibility assessment from both empirical and theoretical perspectives; and (3) discuss the

162

process of older adults’ Web credibility assessment using the two-stage model in the WC
framework.
5.3.1 Credibility Markers/Cues and Heuristics
Overall, the ratings on the 35 items by the research participants (Table 4.16) were in line
with what they mentioned in the interviews (Table 4.15). Using the six types of Web credibility
proposed in the WC framework, operator-related credibility markers/cues (e.g., commercial
intent and credentials of the operator or author) were mentioned in the interviews more
frequently than other types of credibility markers/cues (i.e., content- and design-related
credibility markers/cues); related items based on Likert-type scales were also rated as the most
influential in older adults’ Web credibility assessment. However, relatively fewer people
mentioned design-related credibility markers/cues (e.g., real-world feel, aesthetic design of a
site) as being influential in their Web credibility assessment (Table 4.15).
5.3.1.1 Operator expertise. Regarding the ratings and comments on the credibility
markers, the participants seemed to perceive expert knowledge in the given topic area as the
most important characteristic that boosts the credibility of online health information. As
mentioned above, many of the research participants had suffered from serious illnesses/diseases
(e.g., cancer) and/or had some chronic diseases as well. Also, they showed great concerns about
geriatric diseases they might be diagnosed with in the future. Therefore, most of them were
interested in having health information about new medicines and supplements that would be
helpful for them to control and cure the diseases/ailments from which they were suffering or
which they might prevent themselves from having in the future. Considering the potentially
lethal effects of incorrect health information on human life (Gustafson & Wyatt, 2004), it seemed
the expertise of the health information provider was considered the prerequisite to be assured of

163

the information’s credibility. As reported above, many participants in the interviews mentioned
they would rely on their doctors’ information, rather than information from the Web, especially
in regard to information about serious health-related issues. Thus, the ratings in Table 5.5 seem
to directly reflect the participants’ perceptions of health information credibility.

Table 5.5 Ratings on Operator Expertise-Related Items
No.
D19
D03
D07
D17
D23
D06
D25

Item wordings
The site is by an organization that is well respected outside of the
Internet.
The site lists authors’ credentials for each article.
The site was recommended to you by a doctor.
The site represents an organization you respect.
The site is linked to by a site you think is believable.
The site was recommended to you by a friend (non-expert).
The site displays an award it has won.

M
4.76

Rating
Median
5.00

SD
.436

4.48
4.48
4.43
3.95
3.14
3.00

5.00
5.00
4.00
4.00
3.00
3.00

.750
.602
.598
.669
.655
.837

5.3.1.2 Operator trustworthiness. Credibility markers/cues regarding the operator’s
trustworthiness also had an impact on older adults’ credibility perception of online health
information. As shown in Table 5.6 below, the three reversely worded items (D12, D27, and
D29) regarding operator trustworthiness were perceived very negatively; in other words, these
items had significant impacts, as opposed to being ignored or perceived as neutral.
As emphasized in the interviews, older adults paid close attention to whether the website
(or other Web-based source) tried to sell something to them or promote public interests (i.e., the
presence or absence of commercial intention). More specifically, 90.5% (19 out of 21) of
participants highlighted they preferred health information from academic institutions (e.g., JHM
and HMS) or government research centers (e.g., NIH and CDC) because of their “healthy”
intentions (i.e., operator trustworthiness), in addition to their expertise and reputations (i.e.,
operator expertise). Based on the ratings reported in Table 5.5 and Table 5.6, as well as their
164

comments in the interviews in Table 4.13, older adults seemed to evaluate online health
information based mainly on operator (i.e., source)-related markers/cues. This tendency is not
new to the credibility literature; rather, it seems reasonable, as the concept of Web credibility is
rooted in the source’s credibility in interpersonal communication settings, as reviewed in Chapter
2 (Table 2.1).

Table 5.6 Ratings on Operator’s Trustworthiness-Related Items
No.
D08
D10
D13
D12
D27
D29

Items
The site represents a nonprofit organization.
The URL for the site ends with “.org”
The site requires a paid subscription to gain access.
*The site has one or more ads on each page.
*The site has a commercial purpose (as opposed to academic).
*The site automatically pops up new windows with ads.

M
3.86
3.67
2.33
1.81
1.71
1.24

Rating
Median
4.00
3.00
3.00
2.00
2.00
1.00

SD
.727
.796
.796
.814
.845
.539

Note. An asterisk (*) indicates negatively rated items.

5.3.1.3 Content trustworthiness. As shown in Table 5.7, content trustworthiness-related
credibility markers/cues seemed to have significant impacts on older adults’ perceptions of
health information credibility. Specifically, the research participants seemed to perceive health
information that has an unbiased orientation, covering different perspectives on the given issue,
as a positive characteristic of credible health information on the Web (D34, M = 4.43, SD =
.746).
As discussed above, the participants in the current study had well-established
cognitive/cultural backgrounds in terms of education level and occupational experience (Table
4.8). Also, many of them had cognitive types of hobbies such as reading books on varied topics,
including health and wellness (Table 4.11), which provided them with the opportunity to acquire
knowledge on the topics with which they were concerned. In particular, those who were
characterized as the O-C “mastery of life” type (52.4%) preferred to gather necessary
165

information from multiple sources, including both interpersonal (e.g., doctors) and Web sources,
and to verify whether the obtained information was supported by other sources. As quoted in
Chapter 4, there were some participants who did not automatically trust health information even
from a doctor; rather, they did further research on the given topic to verify and make sense of the
information. These user characteristics might influence the research participants’
conceptualization of health information’s credibility, as well as how they verify the credibility of
online health information (i.e., their process of Web credibility assessment). In other words,
these people seemed to prefer judging the credibility of online health information for themselves,
examining multiple sources for verification, rather than relying on others’ suggestions.
“Cross-referencing multiple resources” seems to be one of the common strategies people
employ when seeking necessary information. For instance, Rieh and Hilligoss (2008) reported
most of the college students in their study assessed the credibility of the information they found
by checking multiple sources, as they believed judging information credibility could not be done
solely based on a single source’s argument. This credibility assessment strategy was also found
in Yi, Stvilia, and Mon’s (2012) study in which 45% of the research participants perceived
online health information as more trustworthy when the information was presented on multiple
websites; the authors used the term “duplication” (p. 49). People’s “cross-referencing multiple
resources” behavior is also in line with one of the credibility heuristics mentioned in Sundar’s
(2008) MAIN model, the bandwagon heuristic, in that people tend to perceive certain
information as more credible if it is considered credible by other people. In the SNS contexts,
this heuristic can be operated in the forms of “social validation” (Jessen & Jørgensen, 2012) and
“social annotations” (Kulkarni & Chi, 2013). Based on the interview data of the current study,

166

content trustworthiness-related credibility markers/cues and heuristics were used in the final
evaluation stage using the two-stage model in the WC framework.

Table 5.7 Ratings on Content Trustworthiness-Related Items
No.
D34
D22
D33
D09
D05
D24
D15

Items
The site tries to cover all the different approaches to the issue.
The site states its policy on content.
The site provides information that is neutral.
The site has ratings or reviews of its content.
The site has been updated since your last visit.
The site provides links to its competitors’ sites.
*The site is rarely updated with new content.

M
4.43
4.05
4.05
3.95
3.90
3.33
1.38

Rating
Median
5.00
4.00
4.00
4.00
4.00
3.00
1.00

SD
.746
.805
.805
.805
1.261
.856
.498

Note. An asterisk (*) indicates negatively rated items.

5.3.1.4 Content expertise. Content expertise concerns both scientific evidence
provisions for content and intrinsic quality. As reported in Table 5.8, the research participants
were concerned about whether the health information they found on the Web was based on
scientific evidence or someone’s personal opinion (D02, M = 4.43; SD = .746). The interview
data also showed older adults paid attention to content expertise when they sought health
information on the Web; 52.4% (11 out of 21) of the interviewees mentioned credibility
markers/cues regarding content expertise (Table 4.15).
Furthermore, from looking at the ratings in Table 5.8 below, a website would be seen as
much less credible if it had typographical errors (D30, M = 1.24; SD = .530). This finding is also
in line with previous studies regarding the effects of surface credibility markers on websites’
credibility: Fogg et al.’s (2001) study, which analyzed ratings on credibility markers/cues
collected from 1,410 people (M = 33 years old), showed having typographical errors on a
website significantly decreased users’ perceptions of the site’s credibility; in Choi’s (2013) study

167

with older adults (M = 71.6 years old), the same item (i.e., having typographical errors) was rated
as the most negative credibility marker/cue among 57 items on the questionnaire. These findings
were well supported by several participants’ comments in the semi-structured interviews that,
regardless of whether the content was good or not, the intrinsic quality of the content influenced
their perception of the health information’s credibility:
“I guess probably it’s more negative if it’s poorly written, if there are errors in grammar,
syntax, and punctuation. I immediately discount [the credibility of the information]. I just
do that automatically with everything. I really suspect when people don’t write well”
(P07, Female, 72).
As the excerpt from the interview above simply shows, the intrinsic quality of health
information appeared to be considered a basic requirement for providing credible health
information on the Web, as having no typos may or may not increase the perceived credibility of
the site, but having typos or any other errors would significantly decrease its credibility.

Table 5.8 Ratings on Content Expertise-Related Items
No.
D02
D32
D26
D16
D30

Items
The site has articles containing citations and references.
This site is complete in the information it provides.
The site is small (e.g., less than five pages).
*The site links to a site you think is NOT credible.
*The site has typographical errors.

M
4.43
4.24
2.95
1.48
1.24

Rating
Median
5.00
4.00
3.00
1.00
1.00

SD
.746
.831
.590
.602
.539

Note. An asterisk (*) indicates negatively rated items.

5.3.2 Credibility Markers by User Characteristics: Variability of WC
Even though no inferential statistics were conducted to compare the ratings by subgroups
due to the small sample size of the study, there were two factors that might have had significant

168

impacts on older adults’ Web credibility assessment if the sample size were large enough to
carry out inferential statistics: (a) “mastery of life” and (b) Internet usage. More specifically, one
participant (n = 1) who had relatively less educational experience (high school graduate) than the
other participants in the study showed the typical characteristics of the P-A type “mastery of life”
such as heavy reliance on others for information seeking in everyday life settings and lack of
confidence and appreciation of systematic and cognitive information seeking. On the ratings by
the orientation of the items (i.e., positive, neutral, or negative), this participant’s (P-A) ratings on
the positive credibility markers/cues were lower than those on the same items by the other three
groups (i.e., O-C, O-P, and D-A). As shown in Table 4.17, his average ratings on the positive
items was M = 3.37, while the other groups’ average ratings were M = 4.15 (O-C), M = 4.02 (PC), and M = 4.34 (D-A); however, his ratings on the negative items (M = 1.70) were similar to
the other groups’ ratings: M = 1.76 (O-C), M = 1.67 (P-C), and M = 1.30 (D-A). As shown in
Figure 4.1, the P-A type of participant’s range of average ratings on the positive and negative
items was narrower than other “mastery of life” types’ ranges. This simply showed the P-A type
of participant’s suspicion about the credibility of online health information and his inability to
distinguish the positive credibility markers from the negative markers. His comments made in
the interview also supported these findings that he did not trust information on the web because
of the difficulties in distinguishing the “trustworthy” and “expert” health information from false
information and/or layman’s opinion. Considering the relatively lower Internet usage of this
participant (1–5 hours a week) compared to other participants (median = 6, indicating 16–20
hours a week), his relatively lower familiarity with the Internet might also come into play in
forming his distrust of online health information, becoming more reliant on his partner and
doctors for health information seeking. As Robertson-Lang et al. (2011) highlighted, being

169

concerned with credibility issues seemed to be a different matter than being able to identify and
appreciate indicators of credibility on the Web for older adults.
There were three participants (n = 3), including the aforementioned participant (P-A),
who answered they used the Internet from 1 to 5 hours a week. The range of this group’s ratings
on the credibility markers tended to be narrower than that of the other groups (Figure 5.2). This
“less-tech-savvy” group tended to give more negative ratings (M = 3.65) on the positive
credibility markers, whereas they gave more positive ratings (M = 1.80) on the negative
credibility markers than others groups did. It seemed to be more challenging for those who used
the Internet less than others to rate the positive and negative markers differently. These findings
are in line with the findings from Zulman et al.’s (2011) study, which demonstrated users’
experience and familiarity with technology, including the Internet and computer, mitigated the
relationship between age and distrust of online sources for health information. This result
indicates that, depending on the levels of IT proficiency, older adults’ credibility perceptions of
online health information may change. Thus, the findings from the current study along with those
from the literature highlight the importance of information literacy education for older adults,
which would help them become more familiar with IT technologies in general and Web
credibility markers/cues and heuristics in particular.
5.3.3 Web Credibility Assessment in ELIS
This section discusses older adults’ Web credibility assessment process based on the
verbal interview data, which were based on the informants’ retrospective reports about their Web
credibility assessment. Within the context of ELIS, Web credibility assessment seems to be
closely related to the source selection. Using the two-stage model in the WC framework, in the
initial stage of Web credibility assessment, in which people make a decision about which online

170

sources to examine to fulfill their information needs, many of the participants already had
specific sources in mind based on their previous experiences. This can be understood as Fogg’s
(2003a) “experienced credibility.” They went to the preferred websites directly and searching for
necessary information. Websites frequently mentioned by the research participants included
those of the Mayo Clinic (10 out of 21; 47.6%), NIH (9 out of 21; 42.9%), CDC (4 out of 21;
19.0%), JHM (4 out of 21; 19.0%), and HMS (4 out of 21; 19.0%). They seemed to be familiar
with these online resources because of their general interests in and experiences with specific
medical issues (i.e., involvement in health topics).
Many of them also began their health information seeking with search engines, chiefly
Google, to identify a list of candidate sources potentially relevant to their information needs. In
order to narrow down the search results, they mentioned they conducted an initial evaluation by
using various credibility markers/cues and heuristics. In particular, the most frequently
mentioned types of credibility markers in the initial stage were source-related markers, such as
the nature of the website (i.e., operator’s trustworthiness) and the operator’s reputation and
authority (i.e., operator’s expertise), to form the initial lists of websites they would visit and read
the content. As mentioned above, older adults tended to rely on health information from wellknown, noncommercial institutions such as NIH, CDC, Mayo Clinic, JHM, and HMS, as well as
representative associations/foundations for specific ailments/diseases such as the American
Cancer Society, the American Diabetes Association, and the American Arthritis Society.
The participants reported that in the final evaluation stage of Web credibility assessment
they may put effort into verifying the health information they had identified in the initial
evaluation stage. Many of the participants in this study mentioned they examined more than one
source to see if certain health information they obtained from a source was consistent with that

171

from other sources. As discussed above, cross-checking multiple sources seemed to be a useful
strategy for the research participants, who were mostly nonexperts in the medical field. This
strategy seemed to help them validate the accuracy of the health information they acquired on the
Web by referencing the aggregated opinions (i.e., content’s trustworthiness). In the current study,
when the information from different sources was not consistent or even contradictory, the
research participants mentioned they either trusted the information from the more authoritative
source (i.e., the source with the greatest expertise) or sought further information to verify the
controversial issue.
In addition, as reported above, some older adults regarded the currency (or recency) as an
important cue/marker that could be used to judge the trustworthiness of health information. Also,
the research participants attempted to examine the expertise of the health information by
checking the intrinsic quality (e.g., typographical errors, size of the site) or by determining
whether certain health information—for instance, an article on a website—was based on
scientific evidence or not.
Lastly, older adults took into consideration not only source- and content-related
markers/cues but also design-related ones in the final stage of Web credibility assessment.
However, among those who mentioned the visual aspect of a website, more people answered
they were not influenced by visuals than those who answered they were. Thus, the influence of
visual aesthetics on older adults’ credibility assessment of online health information seemed to
vary depending on individual preferences. The participants’ ratings on the corresponding item
also showed the research participants did not care much about the “prettiness” of the website
when they judged the credibility of online health information (D20, M = 3.38; SD = .865, where
3 meant neutral, meaning the cue/marker did not have either a positive or a negative role in

172

judging the credibility of the online health information). In terms of the functional aspect of a
website (i.e., the usability of a website), however, all the participants who mentioned this aspect
considered it an important criterion that might influence the perception of the website’s
credibility. Table 5.9 summarizes what older adults mentioned in the interviews based on the
conceptual model of the Web credibility assessment process (i.e., two-stage model in the WC
framework). Thus, the process model needs to be tested with empirical data (e.g., observations or
log data) that could directly and more accurately capture how older adults go through the entire
process of Web credibility assessment.

Table 5.9 Process of Web Credibility Assessment of Online Health Information
Stage
Initial

Definition
The stage in which people
identify initial lists of the
most relevant online
resources that can fulfill
their information needs

Activities
• Directly go to trusted
websites (e.g., NIH, CDC,
Mayo)
• Begin with a trusted search
engine (e.g., Google)

Cue/Markers Used
• Source trustworthiness
• Source expertise

Final

The stage in which people go
through the iterative process
of assessing the credibility
of the candidate websites
identified in the initial
evaluation stage

• Compare multiple sources
(i.e., cross-check)
• Check the currency of the
health information
• Check the intrinsic quality
(e.g., for typographical
errors)
• Check visual aesthetics
• Check usability

•
•
•
•
•
•

173

Source trustworthiness
Source expertise
Content trustworthiness
Content expertise
Design trustworthiness
Design expertise

CHAPTER 6
LIMITATIONS, FUTURE RESEARCH DIRECTION, AND CONCLUSION
6.1 Limitations of the Research
This dissertation research examined the information credibility issues in the Web context
from both theoretical and empirical perspectives. Phase I of the research developed a new
framework for Web credibility assessment (i.e., the WC framework) by synthesizing existing
models and theories from the literature, and Phase II used the new framework to explore older
adults’ credibility assessment of online health information in everyday life settings (i.e., ELIS).
This research identified a number of important characteristics of older adults’ Web credibility
assessment as well as interesting theoretical considerations for Web credibility assessment.
However, there were several limitations in the research data. First of all, even though the
qualitative meta-analysis was an appropriate method to synthesize a new theoretical framework
for the given topic (i.e., Web credibility assessment), the qualitative nature of the verbal
interview data and limited sample size (n = 21) were inadequate to test and validate the new
framework. Thus, the framework is still in the conceptual stage.
In the same context, although the semi-structured interview data were useful to explore
the target population’s (i.e., older adults’) Web credibility perceptions and related behaviors, the
qualitative nature of the data and small sample size did not allow the researcher to apply any
inferential statistics to compare the participants’ Web credibility perceptions by subgroups such
as age (i.e., young-old vs. old-old), gender (i.e., female vs. male), “mastery of life” (i.e., the four
types suggested by Savolainen in 1995), or Internet usage.
Another limitation of the research stems from the methodology used. O'Leary (2005)
pointed out researchers may face challenges in managing the interview process, such as resisting
174

the urge to lead the participants and figuring out how the demographics (e.g., race, gender,
ethnicity, class, age) of the interviewer and the interviewee would affect the interview process.
Also, communication miscues are possible in the process of interviewing. Additionally, a lack of
anonymity might hinder the respondents from providing honest and open responses (Barriball &
While, 1994). Furthermore, given that the semi-structured interviews were solely based on the
interviewees’ retrospective recall, rather than direction observations, the consistency of the
information collected from each interviewee might be skewed in terms of accuracy.
6.2 Future Research
Based on the limitations of the current research mentioned above, the researcher suggests
future research directions in terms of both theoretical and empirical aspects of information
behavior research including Web credibility assessment: (1) positioning Web credibility
assessment in information behavior, (2) operationalizing Web credibility in various contexts, and
(3) validating the new framework for web credibility assessment.
6.2.1 Positioning Web Credibility Assessment in Information Behavior
One important future research agenda would be to determine a clearer relationship
between Web credibility assessment and information behavior in the Web context. As discussed
above, Pirolli and Card’s (1999) information foraging theory may be a useful theoretical
framework to help understand the right position of Web credibility assessment within the process
of information seeking, including both purposeful information seeking and more passive,
meaning of incidental information acquisition (Williamson, 1998). At the conceptual level, Web
credibility assessment seems to be the most useful in the source selection process because users
are supposed to identify credible sources of necessary information from a plethora of information
on the Web. When it comes to health-related information seeking (or other serious topics of

175

information seeking), it is critical to ensure the credibility of the information, as it may affect the
information user’s wellness.
However, in the empirical studies on information behavior, similar constructs have been
used interchangeably, such as information credibility, information quality, authority, and more.
For instance, in the literature on information credibility, many of the criteria for message
credibility overlap with those for information quality. These include: accuracy, currency,
reliability, and relevance. Scholars focusing on information quality have often considered
credibility a dimension of quality—a set of characteristics that allow indirect (vs. direct)
evaluation or prediction of information quality. That is, when users do not have sufficient
knowledge to judge the quality of the given information and/or are not deeply involved with the
given task, they may rely on the markers/cues and heuristics of information credibility rather
than directly evaluate the information quality. Therefore, future research will need to provide a
better understanding of the relationship between the related concepts, examining the unique role
of credibility assessment in people’s information behavior.
6.2.2 Operationalizing Web Credibility in Various Contexts
Future research will continue studying human information behavior regarding how
people judge information credibility in various contexts. In particular, as new types of websites
and information systems emerge continuously, the design of information scents or residues
(Furnas, 1997; Pirolli, 1997) to support heuristic evaluation of information credibility by users
will still remain a very active area of research in the future.
As mentioned above, Web credibility assessment is different from and more complex
than credibility assessment in interpersonal communication due to the dynamic nature of the
Web, its technologies, and document genres. Recent studies on Web credibility started paying

176

attention to user-generated content, such as posts on SNSs, blogs, including microblogs (e.g.,
Twitter), and questions and answers in social Q&A sites. Since the user-generated content often
lacks cues/markers for source credibility, it can be a challenge for users to evaluate whether the
given information (i.e., user-generated content) is credible or not. Therefore, future research on
Web credibility assessment will need to study the unique features of user-generated content and
related information behaviors on those sites. Appropriate measures need to be identified to
capture those user behaviors and enable Web credibility assessment in specific contexts.
6.2.3 Validating the New Framework for Web Credibility Assessment
One of the most significant contributions of the current dissertation research would be the
new framework for Web credibility, named the WC framework, consisting of three components:
WC assessment, WC variability, and WC process. In particular, the first component, WC
assessment, is based on the conceptual typology of measures for web credibility assessment. As
shown in Table 5.1, the conceptual typology contains six types and is based on two key
dimensions of credibility (i.e., trustworthiness and expertise) and three objects of credibility
assessment (i.e., operator, content, and design). This typology can be understood as a
reorganization of the concept, its key dimensions, and the subdimensions of each key dimension,
which all have been identified and examined in the literature over the decades. Thus, it is a
systematic and elaborate typology that can be used to understand how various markers/cues and
heuristics affect users’ perception of Web credibility.
However, since the typology is still in the conceptual stage and has only been tested with
the limited amount of qualitative interview data in the current study (Phase II), it has to be tested
with a larger amount of quantitative data. To test the typology, a research instrument needs to be
developed in accordance with the definitions and features of the six types of credibility measures

177

in the framework. This new conceptual typology and related research instrument will help
researchers operationalize Web credibility assessment in different contexts, as well as provide
system developers with cues/markers to design credible systems. Once the conceptual typology
and instrument are validated through such a study, they can be used as knowledge tools to
understand how people assess the credibility of various online information topics (e.g., health,
education, politics, entertainment), as well as media types (e.g., websites in general, social
networking sites, social Q&A sites, blogs).
6.3 Conclusion
The purpose of this dissertation research was to develop a new framework for Web
credibility assessment and use it to explore and describe older adults’ perceptions of information
credibility in the Web context, identifying markers/cues, heuristics, and other factors influencing
their credibility assessment of online health information. This research employed mixed methods
of a qualitative meta-study and semi-structured interviews to achieve the goals listed below.
First, this research sought to enhance our understanding of people’s perception of
information credibility in the Web context. A qualitative meta-study was conducted to analyze
the literature on information credibility regarding credibility conceptualization (key dimensions
of credibility), operationalization (measures used to operationalize credibility dimensions),
variability (influences of user characteristics and context), and the process of assessment. As an
outcome of the literature analysis, this research synthesized a new, extended framework for Web
credibility assessment, the WC framework.
Second, this research aimed to provide a better understanding of older adults’ credibility
assessment of online health information using semi-structured interviews. The researcher used
the new credibility assessment framework (i.e., the WC framework) to guide the construction of

178

an interview protocol used in the data collection and interpretation of results. In addition, as Web
credibility assessment was seen as a part of information behavior, the ELIS model by Savolainen
(1995) was employed to interpret the semi-structured interview study in terms of the research
participants’ “way of life” and “mastery of life,” as well as the social and cultural/cognitive
capital that have significant impacts on people’s information behavior in everyday life contexts.
This dissertation research has both theoretical and empirical implications. The new Web
credibility assessment framework advances our understanding of the conceptualization of Web
credibility and can be used as a knowledge resource in developing context-specific credibility
assessment models as well as information system interfaces that provide effective support for
information credibility evaluation by users. Furthermore, in that the new framework for Web
credibility assessment was used within the context of ELIS (Savolainen, 1995), the research
provided a better idea of the conceptual relationships between information behavior and
information credibility assessment.
Likewise, findings from the semi-structured interviews can inform online information
system developers and librarians on how older users search for online health information and
how they assess its credibility. Despite the increase in the population of older adults who use the
Internet and their vulnerability in perceiving and/or processing markers/cues to assess
information credibility they find online, there is a dearth of research on older adults’ credibility
assessment of online information. Therefore, the findings from this study contribute to the
information behavior and HCI literature and provide preliminary data for future research.
Further, studying the mechanisms of older adults’ Web credibility assessment has several
practical implications as well. First of all, Web credibility assessment contributes to a better
understanding of how older adults make decisions about the quality of information pertinent to

179

their information needs on the Web. When people do not have sufficient knowledge or expertise
to directly judge the quality of information, credibility markers (i.e., cues) and heuristics can play
roles in the decision-making process of acceptance or rejection of the information.
Also, findings from the study can inform online service developers and intermediaries
(e.g., search engines) about how older adults perceive credibility of online information and how
it affects their use of online systems. This can be used as a knowledge base in designing,
describing, indexing, ranking, and promoting Web-based services.
Lastly, the findings can be utilized for teaching purposes. Knowledge about the process
of Web credibility assessment and involved markers and heuristics could be used to teach people
how to evaluate information and recognize credible sources.

180

APPENDIX A
HUMAN SUBJECTS COMMITTEE APPROVAL MEMORANDUM

From: Human Subjects humansubjects@fsu.edu
Subject: Use of Human Subjects in Research - Approval Memorandum
Date: May 12, 2014 at 2:32 PM
To:
Cc:
The Florida State University
Office of the Vice President For Research
Human Subjects Committee
Tallahassee, Florida 32306-2742
(850) 644-8673 · FAX (850) 644-4392
APPROVAL MEMORANDUM
Date: 5/12/2014
To: Wonchan Choi
Address:
Dept.: INFORMATION STUDIES
From:

Thomas L. Jacobson, Chair

Re: Use of Human Subjects in Research
Older Adults' Credibility Assessment of Online Health Information
The application that you submitted to this office in regard to the use of human subjects in the proposal referenced above have been reviewed
by the Secretary, the Chair, and one member of the Human Subjects Committee. Your project is determined to be Expedited per per 45 CFR §
46.110(7) and has been approved by an expedited review process.
The Human Subjects Committee has not evaluated your proposal for scientific merit, except to weigh the risk to the human participants and
the aspects of the proposal related to potential risk and benefit. This approval does not replace any departmental or other approvals, which
may be required.
If you submitted a proposed consent form with your application, the approved stamped consent form is attached to this approval notice. Only
the stamped version of the consent form may be used in recruiting research subjects.
If the project has not been completed by 5/11/2015 you must request a renewal of approval for continuation of the project. As a courtesy, a
renewal notice will be sent to you prior to your expiration date; however, it is your responsibility as the Principal Investigator to timely request
renewal of your approval from the Committee.
You are advised that any change in protocol for this project must be reviewed and approved by the Committee prior to implementation of the
proposed change in the protocol. A protocol change/amendment form is required to be submitted for approval by the Committee. In addition,
federal regulations require that the Principal Investigator promptly report, in writing any unanticipated problems or adverse events involving
risks to research subjects or others.
By copy of this memorandum, the Chair of your department and/or your major professor is reminded that he/she is responsible for being
informed concerning research projects involving human subjects in the department, and should review protocols as often as needed to insure
that the project is being conducted in compliance with our institution and with DHHS regulations.
This institution has an Assurance on file with the Office for Human Research Protection. The Assurance Number is FWA00000168/IRB
number IRB00000446.
Cc: Besiki Stvilia, Advisor
HSC No. 2014.12763

181

APPENDIX B
UPDATED HUMAN SUBJECTS COMMITTEE APPROVAL
MEMORANDUM
From: Human Subjects humansubjects@fsu.edu
Subject: Use of Human Subjects in Research - Approval Memorandum
Date: February 18, 2015 at 12:54 PM
To:
Cc:
The Florida State University
Office of the Vice President For Research
Human Subjects Committee
Tallahassee, Florida 32306-2742
(850) 644-8673 · FAX (850) 644-4392
RE-APPROVAL MEMORANDUM
Date: 2/18/2015
To: Wonchan Choi
Address:
Dept.: INFORMATION STUDIES
From:

Thomas L. Jacobson, Chair

Re: Re-approval of Use of Human subjects in Research
Older Adults' Credibility Assessment of Online Health Information
Your request to continue the research project listed above involving human subjects has been approved by the Human Subjects Committee. If
your project has not been completed by 2/17/2016, you must request a renewal of approval for continuation of the project. As a courtesy, a
renewal notice will be sent to you prior to your expiration date; however, it is your responsibility as the Principal Investigator to timely request
renewal of your approval from the committee.
If you submitted a proposed consent form with your renewal request, the approved stamped consent form is attached to this re-approval
notice. Only the stamped version of the consent form may be used in recruiting of research subjects. You are reminded that any change in
protocol for this project must be reviewed and approved by the Committee prior to implementation of the proposed change in the protocol. A
protocol change/amendment form is required to be submitted for approval by the Committee. In addition, federal regulations require that the
Principal Investigator promptly report in writing, any unanticipated problems or adverse events involving risks to research subjects or others.
By copy of this memorandum, the Chair of your department and/or your major professor are reminded of their responsibility for being informed
concerning research projects involving human subjects in their department. They are advised to review the protocols as often as necessary to
insure that the project is being conducted in compliance with our institution and with DHHS regulations.
Cc: Besiki Stvilia, Advisor
HSC No. 2015.14892

182

APPENDIX C
PRESCREEN TEST TOOLS

Florida’s iSchool
Florida State University School of Information
Participant Pre-Screening
PRE-SCREENING ID: __ __ __ __
P1. Gender:

⃞ Male

DATE: __ __ / __ __ / __ __

⃞ Female

Age: ____

P2. What is your highest level of education? Please check the category.
⃞ No formal education
⃞ Less than high school graduate
⃞ High school graduate/GED
⃞ Vocational training
⃞ Some college/Associate’s degree
⃞ Bachelor’s degree (BA, BS)
⃞ Master’s degree (or other post-graduate training)
⃞ Doctoral degree (PhD, MD, EdD, DDS, JD, etc.)
P3. How would you describe your primary racial group? Please check the category.
⃞ No Primary Group
⃞ American Indian/Alaska Native
⃞ White Caucasian

⃞ Native Hawaiian/Pacific Islander

⃞ Black/African American

⃞ Multi-racial

⃞ Asian

⃞ Other (specify): _____________

P4. Is English your primary language? Please check the category.
⃞ Yes

⃞ No

a) If “No,” what is your primary language? ___________________________
P5. Do you have any problems with your vision that cannot be corrected?
⃞ Yes
⃞ No
a) If “Yes,” specify _____________________
P6. Do you wear glasses or contacts?
⃞ Yes
⃞ No
a) If “Yes,” specify _____________________

183

Florida’s iSchool
Florida State University School of Information
P7. Do you have any problems hearing that cannot be corrected?
⃞ Yes

⃞ No

a) If “Yes,” specify_____________________
P8. Do you wear a hearing aid?
⃞ Yes

⃞ No

P9. Do you have arthritis in your hands to the extent that it makes it difficult for you to
write?
⃞ Yes

⃞ No

a) If “Yes,” ask subject if they think they would be able to type on a keyboard:
⃞ Yes
⃞ No

184

Florida’s iSchool
Florida State University School of Information
Short Portable Mental Status Questionnaire (SPMSQ)
PRE-SCREENING ID: __ __ __ __

DATE: __ __ / __ __ / __ __
Wrong

S1

What is the date today?

S2

What day of the week is it?

S3

What is your street address?

S4

What is your telephone number?

S5

How old are you?

S6

When were you born?

S7

Who is the current president of the U.S.?

S8

Who was the President just before him?

S9

What was your mother’s maiden name?

S10

Subtract 3 from 20 and keep subtracting 3 from each
number, all the way down. 20, 17, 14, 11, 8, 5, 2

Total number of errors: ____
0 – 2 errors = intact
3 – 4 errors = mild intellectual impairment
5 – 7 errors = moderate intellectual impairment
8 – 10 errors = severe intellectual impairment
Allow one more error if subject had no grade school education.
Allow one fewer error if subject had education beyond high school.

Subject Passed:

⃞

Yes

⃞

No

185

Correct

Florida’s iSchool
Florida State University School of Information
Wechsler Memory Scale III (WMS-III)
PRE-SCREENING ID: __ __ __ __

DATE: __ __ / __ __ / __ __

Story A and B
Say I am going to read a short story to you. Listen carefully and try to remember it just
the way I say it, as close to the same words as you can remember. When I am through,
I want you to tell me everything I read to you. You should tell me all you can remember
even if you are not sure. Are you ready?
Read Story A
Anna / Thompson / of South / Boston, // employed as a cook / in a school / cafeteria, //
reported / at the police / station / that she had been held up / on State Street / the night
before / and robbed / of fifty-six dollars. // She had four / small children, // the rent was
due, / and they had not eaten / for two days. / The police, / touched by the woman’s
story, // took up a collection / for her. //
After reading the story, say Tell me everything you can remember about this story.
Start at the beginning.
Read Story B
At 6:00 / on Monday / evening, / Joe / Garcia / of San Francisco // was watching
television /// as he dressed / to go out. // A weather bulletin / interrupted the program //
to warn that thunderstorms / would move into the area / within the next two to three
hours / and remain until morning. // The announcer said / the storm could bring hail /
and up to four inches / of rain / and cause the temperature to drop / by fifteen degrees.
// Joe decided to stay home. / He took off his coat / and sat down / to watch old movies.
Number of elements Story A: ___
Number of elements Story B: ___
For subjects aged 18 – 54: Story A ≥ 7 elements. If failed, Story B ≥ 5.
For subjects aged 15 – 80: Story A ≥ 6 elements. If failed, Story B ≥ 4.

Subject Passed:

⃞ Yes

⃞ No

186

APPENDIX D
SEMI-STRUCTURED INTERVIEW PROTOCOL

Florida’s iSchool
Florida State University School of Information
STUDY #: __ __ __

DATE: __ __ / __ __ / __ __

Section A – Internet Use
The purpose of this set of questions is to ask your familiarity and experience with the
Internet. Please answer all questions by placing a check mark on or filling in the
appropriate response.
A1. About how many hours a week do you use the internet?
⃞ Never (skip the rest of the questionnaire)
⃞ less than one hour a week
⃞ between 1 hour and 5 hours a week
⃞ between 6 hours and 10 hours a week
⃞ between 11 hours and 15 hours a week
⃞ between 16 hours and 20 hours a week
⃞ more than 20 hours a week
A2. How long have you been using the internet?
⃞ Less than 6 months
⃞ between 6 months and 1 year
⃞ more than 1 year, but less than 3 years
⃞ more than 3 years, but less than 5 years
⃞ more than 5 years
A3. Have you looked for health-related information online during the past 6 months?
⃞ Yes
⃞ no
A) if “no,” when was the last time you searched for health information online? ____

187

Florida’s iSchool
Florida State University School of Information
Section B – Older Adults’ Information Needs
The purpose of this set of questions is to ask your information needs for your everyday
life. Please answer the following questions.
B1. What are your everyday life information needs, as opposed to work-related
information needs? Examples of topics may include health/wellness, shopping,
leisure, etc.

188

Florida’s iSchool
Florida State University School of Information
Section C – Older Adults’ Health Information Needs and Related Behaviors
The purpose of this set of questions is to ask your information needs for health-related
topics. Please answer the following questions.
C1. What are some of your health-related information needs?

C2. How do you search for health information?
C2-1. What source(s) do you use to obtain health information (e.g., physicians,
family and friends, websites, social media, etc.)?

C2-2. Which online sources do you use to look for health information (e.g.,
websites, social media, mobile applications, etc.)?
! Websites:
! Social media:
! Mobile applications:
C3. Why do you use the source(s) to obtain health information?

C4. Do you use (apply) or act on the health information you obtained?
⃞ Yes
⃞ No
4-1. If “Yes,” when, why, and how?

C5. Do you share the health information you obtain? If yes, when, why, how, and with
whom?
⃞ Yes
⃞ No
C5-1. If “Yes,” when, why, how, and with whom?

189

Florida’s iSchool
Florida State University School of Information
Section D – Credibility of Online Health Information
This section asks you about credibility markers/cues and heuristics on health-related
websites. Please tell me what makes health-related websites credibility for you.
D1. How do you evaluate credibility of a health-related website? Which markers/cues
on the website make you perceive it as credible?

D2. Please indicate the extent to which you disagree or agree with the following
statements by circling the appropriate number in the scale next to each statement:
+2
+1
0
-1
-2

=
=
=
=
=

Much More Credible
More Credible
Neutral / No Impact
Less Credible
Much Less Credible

Much
Less
Credible

Less
Credible

Neutral

More
Credible

Much
more
Credible

1

The site lists the organization’s physical
address.

-2

-1

0

+1

+2

2

The site has articles containing citations
and references.

-2

-1

0

+1

+2

3

The site lists authors’ credentials for each
article.

-2

-1

0

+1

+2

4

The site is arranged in a way that makes
sense to you.

-2

-1

0

+1

+2

5

The site has been updated since your last
visit.

-2

-1

0

+1

+2

190

Florida’s iSchool
Florida State University School of Information
Much
Less
Credible

Less
Credible

Neutral

More
Credible

Much
more
Credible

6

The site was recommended to you by a
friend (non-expert).

-2

-1

0

+1

+2

7

The site was recommended to you by a
doctor.

-2

-1

0

+1

+2

8

The site represents a nonprofit
organization.

-2

-1

0

+1

+2

9

The site has ratings or reviews of its
content.

-2

-1

0

+1

+2

10

The URL for the site ends with “.org”

-2

-1

0

+1

+2

11

The site recognizes that you have been
there before.

-2

-1

0

+1

+2

12

The site has one or more ads on each
page.

-2

-1

0

+1

+2

13

The site requires a paid subscription to
gain access.

-2

-1

0

+1

+2

14

The site takes a long time to download.

-2

-1

0

+1

+2

15

The site is rarely updated with new
content.

-2

-1

0

+1

+2

16

The site links to a site you think is NOT
credible.

-2

-1

0

+1

+2

17

The site represents an organization you
respect.

-2

-1

0

+1

+2

18

The site gives a contact phone number.

-2

-1

0

+1

+2

19

The site is by organization that is well
respected outside of the Internet.

-2

-1

0

+1

+2

20

The site looks professionally designed.

-2

-1

0

+1

+2

21

The site gives a contact email address.

-2

-1

0

+1

+2

191

Florida’s iSchool
Florida State University School of Information
Much
Less
Credible

Less
Credible

Neutral

More
Credible

Much
more
Credible

-2

-1

0

+1

+2

22

The site states its policy on content.

23

The site is linked to by a site you think is
believable.

-2

-1

0

+1

+2

24

The site provides links to its competitors’
sites.

-2

-1

0

+1

+2

25

The site displays an award it has won.

-2

-1

0

+1

+2

26

The site is small (e.g. less than five
pages).

-2

-1

0

+1

+2

27

The site has a commercial purpose (as
opposed to academic).

-2

-1

0

+1

+2

28

The site is sometimes unexpectedly
unavailable.

-2

-1

0

+1

+2

29

The site automatically pops up new
windows with ads.

-2

-1

0

+1

+2

The site has typographical errors.

-2

-1

0

+1

+2

The site has links that do not work.

-2

-1

0

+1

+2

32

This site is complete in the information it
provides.

-2

-1

0

+1

+2

33

The site provides information that is
neutral.

-2

-1

0

+1

+2

34

The site tries to cover all the different
approaches to the issue.

-2

-1

0

+1

+2

35

The site is customizable according to your
preference.

-2

-1

0

+1

+2

30
31

Thank You!
192

APPENDIX E
INFORMED CONSENT FORM

Older Adults’ Credibility Assessment of Online Health Information
Informed Consent Form
Introduction to the Study
This dissertation project will explore older adults’ health information seeking and Web
credibility assessment behaviors. In particular, this study will explore the ways older
adults search for and select health-related websites and will identify a set of
cues/markers and heuristics they use to judge credibility of health information.
A doctoral candidate, Wonchan Choi
, in the School of Information
at the Florida State University will be conducting this study.
W ha t W ill H a ppe n D ur ing t he S t udy
In a semi-structured interview, each participant will be asked to answer questions
regarding his or her health information needs and perception of health-related websites’
credibility. Each interview is anticipated to take around 45 minutes to complete.
Interviews will be scheduled at a time and place convenient to the participant. Interviews
will be tape recorded; transcripts will be prepared with names and any personal
identifiers changed. Participants have the right to have the tape turned off at any time
during the interview. In appreciation for participating in the study, each interviewee will
receive an honorarium in the amount of $25.
Signing this form constitutes informed consent for participation in the study.
If you have questions or concerns about participating in this study, please contact:
Principal Investigator:
Wonchan Choi, Doctoral Candidate
Florida State University School of Information
Email:
Tel:
Web:
Academic Advisor:
Besiki Stvilia, Ph.D.
Florida State University School of Information
Email:
Tel:
IRB Study#:

193

Risks
Risks associated with the research are very low and are considered no greater than
those of everyday life. All collected data will be confidential, and only pseudonyms will
be used in data analysis and subsequent reports and publications. Collected data will
be kept on a secure, password protected external hard drive. Primary data will be
disposed of 1 year from the end of the project. Minimal risk is associated with the impact
on privacy if excerpts from interviews reveal information that may be considered to
affect an individual’s privacy.
Benefits of this Project
This study will benefit the studies of older adults’ health information need and
associated behaviors, consumer wellness and health informatics, and credibility
assessment research. No promise or guarantee of benefits is made to encourage you to
participate.
Extent of Anonymity and Confidentiality
Confidentiality is assured to the participants to the extent allowed by law. Publications
about the findings from the study will mask the identity of the individual. Interviews will
be tape recorded; transcripts will be prepared with names and any personal identifiers
changed. Participants have the right to have the tape turned off at any time during the
interview. Tapes and transcripts will remain in the possession of the primary
investigator.
Participant’s R i g h t s
In accordance with Florida State University (FSU) policy, and as the principal
investigator, I would like to assure you that:
•
•

Participation in this study is entirely voluntary.
If you decide to participate, you are free to withdraw at any time without
consequence. You are free to decline to answer any questions that you
choose or to request that the tape recorder be turned off at any time during an
interview.

All research on human volunteers is reviewed by a committee that works to protect your
rights and welfare. If you have any questions or concerns regarding the study and would
like to talk to someone other than the researcher, you are encouraged to contact the
FSU IRB at telephone number 850-644-8633. You may also contact this office by email
at jjcooper@fsu.edu, or by writing or in person at 2010 Levy Street, Research Building
B, Suite 276, FSU Human Subjects Committee, Tallahassee, FL 32306-2742.

194

Participant’s Permission
By signing this form below, you acknowledge that you have read and understood the
above statement and consent to participate in this study.

If I participate, I may withdraw at any time. I agree to abide by the rules of this project.

________________________________________
Signature

195

____________________
Date

APPENDIX F
RESEARCH QUESTIONS, THEORETICAL FRAMEWORKS, AND
METHODS
Research Questions
RQ1: How is the process of Web credibility
assessment conceptualized in existing
theories and models?
RQ1-1: What are the common and
unique components of existing
theoretical frameworks of Web
credibility assessment?
RQ1-2: How can/should these
theoretical frameworks be synthesized?
RQ2: In general, what are older adults’
common information needs?
RQ3: What are older adults’ health
information needs and related
information behaviors?
RQ3-1: What sources do older adults
use to find health information both onand offline, and why do they use those
sources?
RQ3-2: How do they use the
information they find?

Theoretical Frameworks

Methods

Interview
Questions
N/A

3LM: Conceptual level

Qualitative metastudy

3LM: Empirical level

Qualitative metastudy

N/A

3LM: Indicator level

Qualitative metastudy
Semi-structured
interviews

N/A

Semi-structured
interviews

C1

Semi-structured
interviews

C2, C3

Semi-structured
interviews

C4, C5

• ELIS: Way of Life
• ELIS: Mastery of Life
• ELIS: ELIS—Seeking
orienting information
• ELIS: ELIS—Seeking
practical information
• ELIS: ELIS—Selection of
information sources and
channels
• ELIS: Individual Factors
• ELIS: Way of Life
• ELIS: Mastery of Life
• ELIS: Individual Factors
WC: Process

RQ4: How do older adults assess the
Semi-structured
credibility of health-related information
interviews
on the Web?
RQ4-1: What are older adults’
WC: Assessment
Semi-structured
perceptions of Web credibility?
interviews
RQ4-2: What are some of the
WC: Variability
Semi-structured
psychological, social, and/or cultural
interviews
mechanisms that underlie and/or affect
those perceptions?
RQ4-3: What are some of the
WC: Assessment
Semi-structured
markers/cues and heuristics used by
interview
older adults to assess the credibility of
health-related websites?
3LM: The three-level measurement model (Bailey, 1994)
ELIS: The model of everyday life information seeking (Savolainen, 1995)
WC: the new framework of Web credibility assessment (a product of the current dissertation research)

196

B1

D1

D1
A1, A2, A3
D1

D1, D2

APPENDIX G
CODING SCHEME
Theoretical
Concepts

Definitions

Codes and Examples

Nature of Hobbies

Order of things, which is
based on the choices that
individuals make in
everyday life

Mastery of Life

Keeping things in order;
typical ways of
approaching everyday
problems

Individual Factors

A set of social cultural,
and individual factors
that influence the
structure of one’s way of
life and mastery of life

• Cognitive (e.g., reading books on cultural and
scientific topics, reading newspapers, reading
online news, taking courses)
• Affective (e.g., watching TV shows and movies,
listening to music, playing games)
• Social (e.g., email, SNSs, in-person socializing)
• Optimistic-cognitive (O-C):
o People who are optimistic about the
solvability of a given problem based on their
familiarity and confidence in seeking
information using IT technologies; AND
o Who use multiple sources to seek for
necessary information
• Pessimistic-cognitive (P-C):
o People who use their cognitive abilities and
multiple sources to seek necessary
information; BUT
o They acknowledge the possibility that the
given problem may not be solved; relatively
less confident in information seeking than OC
• Defensive-affective (D-C):
o People who are optimistic toward problemsolving situations; BUT
o Their information seeking strategies are
based on affective, rather than cognitive,
means; thus, they often have wishful thinking
or avoid the situation
• Pessimistic-affective (P-A):
o People who do not appreciate the value of
systematic and cognitive information
seeking; AND
o Who heavily rely on their social contact
networks to seek necessary information (e.g.,
friends, partners, doctors)
• Social capital (i.e., social contact networks such as
families, partners, friends, experts in the given
topic area)
• Cultural/cognitive capital (e.g., educational
background, occupational background, extra
training experience)

197

Assessment of WC

A set of cues/markers and
heuristics that influence
users’ perception of
online information
credibility (i.e., web
credibility)

Variability of WC

A set of individual and
contextual variables that
influence the process of
web credibility
assessment

Process of WC

The iterative process of
how people select and
evaluate the credibility
of online resources

• Operator trustworthiness (O-T):
o Whether or not the operator’s character is
ethical, honest/sincere, fair, believable,
trusted
• Operator expertise (O-E):
o Whether or not the operator’s expertise is
reputable, famous, authoritative, competent
• Content trustworthiness (C-T):
o Whether or not the message/information
being provided on the website is neutral,
unbiased, evenhanded, consistent, current
• Content expertise (C-E):
o Whether or not the message/information
being provided on the website is informative,
complete, comprehensive, in-depth, accurate,
correct, clear
• Design trustworthiness (D-T):
o Whether or not the structure, functionality,
aesthetic design, and interactivity of the
information and/or the website as a whole is
stable, consistent, reliable
• Design expertise (D-E):
o Whether or not the structure, functionality,
aesthetic design, and interactivity of the
information and/or the website as a whole is
well organized, easy to use, aesthetically put
together
• Demographics (e.g., age, gender)
• Involvement (e.g., motivation, ability, domain
expertise)
• Technology proficiency (e.g., information literacy,
media reliance)
• Context (e.g., task, goal, situation)
• Initial evaluation: The stage where people identify
the initial lists of most relevant online resources
that can fulfill their information needs
o Directly going to trusted websites
o Using trusted search engines
• Final evaluation: The stage where people go
through the iterative process of assessing the
credibility of candidate websites identified in the
initial evaluation stage

ELIS: The model of everyday life information seeking (Savolainen, 1995)
WC: the new framework of Web credibility assessment (a product of the current dissertation research)

198

REFERENCES
Adamic, L. A., Zhang, J., Bakshy, E., & Ackerman, M. S. (2008). Knowledge sharing and
Yahoo Answers: Everyone knows something. In J. Huai, R. Chen, H.-W. Hon, & Y. Liu
(Eds.), Proceedings of the 17th International Conference on World Wide Web (WWW
'08) (pp. 665-674). New York, NY: ACM.
Agichtein, E., Castillo, C., Donato, D., Gionis, A., & Mishne, G. (2008). Finding high-quality
content in social media. In M. Najork (Ed.), Proceedings of the 2008 International
Conference on Web Search and Data Mining (WSDM '08) (pp. 183-194). New York, NY:
ACM.
Ahmad, R., Komlodi, A., Wang, J., & Hercegfi, K. (2010a). The impact of user experience levels
on web credibility judgments. Proceedings of the American Society for Information
Science and Technology, 47(1), 1-4. doi:10.1002/meet.14504701180
Ahmad, R., Komlodi, A., Wang, J., & Hercegfi, K. (2010b). The impact of user experience
levels on web credibility judgments. Proceedings of the American Society for Information
Science and Technology (ASIS&T), 47(1), 1-4. doi:10.1002/meet.14504701180
Andersen, K., & Clevenger Jr, T. (1963). A summary of experimental research in ethos.
Communications Monographs, 30(2), 59-78. doi:10.1080/03637756309375361
Arazy, O., & Kopak, R. (2011). On the measurability of information quality. Journal of the
American Society for Information Science and Technology, 62(1), 89-99.
doi:10.1002/asi.21447
Asla, T., Williamson, K., & Mills, J. (2006). The role of information in successful aging: The
case for a research focus on the oldest old. Library and Information Science Research,
28(1), 49-63. doi:10.1016/j.lisr.2005.11.005
Bailey, K. D. (1973). Monothetic and polythetic typologies and their relation to
conceptualization, measurement and scaling. American Sociological Review, 38(1), 1833.
Bailey, K. D. (1994). Typologies and taxonomies: An introduction to classification techniques.
Sage Publications, Inc.
Barriball, K. L., & While, A. (1994). Collecting data using a semi-structured Interview: A
discussion paper. Journal of Advanced Nursing, 19(2), 328-335.
Beatty, P., Reay, I., Dick, S., & Miller, J. (2011). Consumer trust in e-commerce web sites: A
meta-study. ACM Comput. Surv., 43(3), 1-46. doi:10.1145/1922649.1922651
Berlo, D. K., Lemert, J. B., & Mertz, R. J. (1969). Dimensions for evaluating the acceptability of
message sources. Public Opinion Quarterly, 33(4), 563-576. doi:10.1086/267745

199

Bernstam, E. V., Shelton, D. M., Walji, M., & Meric-Bernstam, F. (2005). Instruments to assess
the quality of health information on the World Wide Web: What can our patients actually
use? International Journal of Medical Informatics, 74(1), 13-19.
doi:10.1016/j.ijmedinf.2004.10.001
Brenner, J., & Smith, A. (2013). 72% of online adults are social networking site users.
Washington, DC: Pew Internet & American Life Project.
Case, D. O. (2012). Looking for information: A survey of research on information seeking,
needs, and behavior. Emerald Group Publishing.
Castle, E., Eisenberger, N. I., Seeman, T. E., Moons, W. G., Boggero, I. A., Grinblatt, M. S., &
Taylor, S. E. (2012). Neural and behavioral bases of age differences in perceptions of
trust. Proceedings of the National Academy of Sciences, 109(51), 20848-20852.
doi:10.1073/pnas.1218518109
Charness, N., & Boot, W. R. (2009). Aging and Information Technology Use: Potential and
Barriers. Current Directions in Psychological Science, 18, 253-258. doi:10.1111/j.14678721.2009.01647.x
Chesney, T. (2006). An empirical examination of Wikipedia's credibility. First Monday, 11(11).
doi:10.5210/fm.v11i11.1413
Cheung, C. M. K., & Lee, M. K. O. (2006). Understanding consumer trust in Internet shopping:
A multidisciplinary approach. Journal of the American Society for Information Science
and Technology, 57(4), 479-492. doi:10.1002/asi.20312
Choi, W. (2013). What makes online health information credible for older adults?: An
exploratory study. In W. E. Mackay (Ed.), CHI '13 Extended Abstracts on Human
Factors in Computing Systems (pp. 2671-2676). New York, NY: ACM.
Choi, W., & Stvilia, B. (2015). A new framework for Web credibility assessment. iConference
2015 Proceedings.
Choi, W., & Stvilia, B. (in press). Web credibility assessment: Conceptualization,
operationalization, variability, and models. Journal of the Association for Information
Science and Technology. doi:10.1002/asi.23543
Cool, C., Belkin, N. J., Frieder, O., & Kantor, P. (1993). Characteristics of texts affecting
relevance judgments. Proceedings of the 14th National Online Meeting.
Cooper, L. (1932). The rhetoric of Aristotle: An expanded translation with supplementary
examples for students of composition and public speaking. New York, NY: AppletonCentury-Crofts.
Cosley, D., Frankowski, D., Terveen, L., & Riedl, J. (2007). SuggestBot: Using intelligent task
routing to help people find work in wikipedia. In D. Chin, & M. Zhou (Eds.),
200

Proceedings of the 12th International Conference on Intelligent User Interfaces (IUI '07)
(pp. 32-41). New York, NY: ACM.
CREATE. (2015). Retrieved April 18th, 2015 from http://create-center.org
Credibility. (n.d.). In Oxford Dictionaries. Retrieved from
http://oxforddictionaries.com/view/entry/m_en_us1236946 - m_en_us1236946
Czaja, S. J., Charness, N., Fisk, A. D., Hertzog, C., Nair, S. N., Rogers, W. a., & Sharit, J.
(2006). Factors predicting the use of technology: findings from the Center for Research
and Education on Aging and Technology Enhancement (CREATE). Psychology and
aging, 21, 333-352. doi:10.1037/0882-7974.21.2.333
Denzin, N. K. (1989). The research act : A theoretical introduction to sociological methods (3rd
ed.). Englewood Cliffs, NJ: Prentice Hall.
Eastin, M. S. (2001). Credibility assessments of online health information: The effects of source
expertise and knowledge of content. Journal of Computer-Mediated Communication,
6(4). doi:10.1111/j.1083-6101.2001.tb00126.x
Eysenbach, G. (2008). Credibility of health information and digital media: New perspectives and
implications for youth. Digital Media, Youth, and Credibility, 123-154.
Fernquist, J., & Chi, E. H. (2013). Perception and understanding of social annotations in web
search. In D. Schwabe, V. Almeida, & H. Glaser (Eds.), Proceedings of the 22nd
International Conference on World Wide Web (WWW '13) (pp. 403-412). Republic and
Canton of Geneva, Switzerland: International World Wide Web Conferences Steering
Committee.
Fisk, A. D., Rogers, W. A., Charness, N., Czaja, S. J., & Sharit, J. (2009). Designing for older
adults: Principles and creative human factors approaches (2 ed.). Boca Raton, FL: CRC
Press.
Flanagin, A. J., & Metzger, M. J. (2000). Perceptions of Internet information credibility.
Journalism & Mass Communication Quarterly, 77(3), 515-540.
doi:10.1177/107769900007700304
Flanagin, A. J., & Metzger, M. J. (2003). The perceived credibility of personal Web page
information as influenced by the sex of the source. Computers in Human Behavior, 19(6),
683-701. doi:10.1016/S0747-5632(03)00021-9
Flanagin, A. J., & Metzger, M. J. (2008). Digital media and youth: Unparalleled opportunity and
unprecedented responsibility. In M. J. Metzger, & A. J. Flanagin (Eds.), Digital media,
youth, and credibility (pp. 5-27). Cambridge, MA: The MIT Press.
Flanagin, A. J., & Metzger, M. J. (2013). Trusting expert-versus user-generated ratings online:
The role of information volume, valence, and consumer characteristics. Computers in
Human Behavior, 29(4), 1626-1634. doi:10.1016/j.chb.2013.02.001
201

Fogg, B. J. (2003a). Persuasive technology: Using computers to change what we think and do.
San Francisco, CA: Morgan Kaufmann Publishers.
Fogg, B. J. (2003b). Prominence-interpretation theory. In G. Cockton, & P. Korhonen (Eds.),
CHI '03 Extended Abstracts on Human Factors in Computing Systems (pp. 722-723).
New York, NY: ACM.
Fogg, B. J., Marshall, J., Laraki, O., Osipovich, A., Varma, C., Fang, N., . . . Treinen, M. (2001).
What makes Web sites credible?: A report on a large quantitative study. In J. Jacko, & A.
Sears (Eds.), Proceedings of the SIGCHI Conference on Human Factors in Computing
Systems (CHI '01) (pp. 61-68). New York, NY: ACM.
Fogg, B. J., Soohoo, C., Danielson, D. R., Marable, L., Stanford, J., & Tauber, E. R. (2003).
How do users evaluate the credibility of Web sites?: A study with over 2,500 participants.
In J. Arnowitz, A. Chalmers, & T. Swack (Eds.), Proceedings of the 2003 Conference on
Designing for User Experiences (DUX '03) (pp. 1-15). New York, NY: ACM.
Fox, S., & Duggan, M. (2013). Health online 2013. Washington, DC: Pew Internet & American
Life Project.
Francke, H., & Sundin, O. (2012). Negotiating the role of sources: Educators' conceptions of
credibility in participatory media. Library & Information Science Research, 34(3), 169175. doi:10.1016/j.lisr.2011.12.004
Furnas, G. W. (1997). Effective view navigation. In S. Pemberton (Ed.), Proceedings of the ACM
SIGCHI Conference on Human Factors in Computing Systems (CHI '97) (pp. 367-374).
New York, NY: ACM.
Galletta, A. (2013). Mastering the semi-structured interview and beyond: From research design
to analysis and publication. New York, NY: New York University Press.
Gaziano, C., & McGrath, K. (1986). Measuring the concept of credibility. Journalism Quarterly,
63(3), 451-462.
Getz, I., & Weissman, G. (2010). An information needs profile of Israeli older adults, regarding
the law and services. Journal of Librarianship and Information Science, 42, 136-146.
doi:10.1177/0961000610361422
Giffin, K. (1967). The contribution of studies of source credibility to a theory of interpersonal
trust in the communication process. Psychological Bulletin, 68(2), 104-120.
doi:10.1037/h0024833
Giudice, K. D. (2010). Crowdsourcing credibility: The impact of audience feedback on Web
page credibility. Proceedings of the American Society for Information Science and
Technology, 47(1), 1-9. doi:10.1002/meet.14504701099
Glaser, B. G., & Strauss, A. L. (1967). The discovery of grounded theory: Strategies for
qualitative research. Chicago, IL: Aldine Publishing Company.
202

Gustafson, D. H., & Wyatt, J. C. (2004). Evaluation of ehealth systems and services. BMJ, 328,
1150. doi:10.1136/bmj.328.7449.1150
Hargittai, E., Fullerton, L., Menchen-Trevino, E., & Thomas, K. Y. (2010). Trust online: Young
adults' evaluation of web content. International Journal of Communication, 4, 468-494.
Hilligoss, B., & Rieh, S. Y. (2008). Developing a unifying framework of credibility assessment:
Construct, heuristics, and interaction in context. Information Processing & Management,
44(4), 1467-1484. doi:10.1016/j.ipm.2007.10.001
Hirth, J., Czaja, S. J., & Sharit, J. (2007). Older adults' health information needs and the effect of
the internet (pp. 15-19): SAGE Publications.
Hong, T. (2006a). Contributing factors to the use of health-related websites. Journal of Health
Communication: International Perspectives, 11(2), 149-165.
doi:10.1080/10810730500526679
Hong, T. (2006b). The influence of structural and message features on Web site credibility.
Journal of the American Society for Information Science and Technology, 57(1), 114127. doi:10.1002/asi.20258
Hovland, C. I., Janis, I. L., & Kelley, H. H. (1953). Communication and persuasion:
Psychological studies of opinion change. New Haven, CT: Yale University Press.
Huvila, I. (2013). In Web search we trust?: Articulation of the cognitive authorities of Web
searching. Information Research, 18(1). Retrieved from
http://www.informationr.net/ir/18-1/paper567.html - .VRnb31xq4r8
Iding, M. K., Crosby, M. E., Auernheimer, B., & Klemm, E. B. (2009). Web site credibility:
Why do people believe what they believe? Instructional Science, 37(1), 43-63.
Jansen, B. J., & Resnick, M. (2006). An examination of searcher's perceptions of nonsponsored
and sponsored links during ecommerce Web searching. Journal of the American Society
for Information Science and Technology, 57(14), 1949-1961. doi:10.1002/asi.20425
Jeon, G. Y., & Rieh, S. Y. (2014). Answers from the crowd: How credible are strangers in social
Q&A? In M. Seadle, & P. Hasle (Eds.), iConference 2014 Proceedings (pp. 663-668).
Urbana-Champaign, IL: iSchools.
Jessen, J., & Jørgensen, A. H. (2012). Aggregated trustworthiness: Redefining online credibility
through social validation. First Monday, 17(1). doi:10.5210/fm.v17i1.3731
Johnson, T. J., & Kaye, B. K. (1998). Cruising is believing?: Comparing internet and traditional
sources on media credibility measures. Journalism & Mass Communication Quarterly,
75(2), 325-340. doi:10.1177/107769909807500208

203

Johnson, T. J., & Kaye, B. K. (2000). Using is believing: The influence of reliance on the
credibility of online political information among politically interested Internet users.
Journalism & Mass Communication Quarterly, 77(4), 865-879.
Johnson, T. J., & Kaye, B. K. (2004). Wag the Blog: How Reliance on Traditional Media and the
Internet Influence Credibility Perceptions of Weblogs Among Blog Users. Journalism &
Mass Communication Quarterly, 81(3), 622-642. doi:10.1177/107769900408100310
Johnson, T. J., & Kaye, B. K. (2009). In blog we trust? Deciphering credibility of components of
the internet among politically interested internet users. Computers in Human Behavior,
25(1), 175-182.
Johnson, T. J., Kaye, B. K., Bichard, S. L., & Wong, W. J. (2007). Every Blog Has Its Day:
Politically-interested Internet Users’ Perceptions of Blog Credibility. Journal of
Computer-Mediated Communication, 13(1), 100-122. doi:10.1111/j.10836101.2007.00388.x
Jørgensen, M., & Shepperd, M. (2007). A systematic review of software development cost
estimation studies. Ieee Transactions on Software Engineering, 33(1), 33-53.
doi:10.1109/TSE.2007.256943
Julien, H., & Barker, S. (2009). How high-school students find and evaluate scientific
information: A basis for information literacy skills development. Library & Information
Science Research, 31(1), 12-17. doi:10.1016/j.lisr.2008.10.008
Kaiser Family Foundation. (2005). E-health and the elderly: How seniors use the Internet for
health information. Menlo Park, CA: Kaiser Family Foundation.
Kim, D. (2012). Interacting is believing? Examining bottom-Up credibility of blogs among
politically interested Internet users. Journal of Computer-Mediated Communication,
17(4), 422-435. doi:10.1111/j.1083-6101.2012.01583.x
Kim, S. (2010). Questioners' credibility judgments of answers in a social question and answer
site. Information Research, 15(1). Retrieved from http://informationr.net/ir/152/paper432.html
Kiousis, S. (2001). Public trust or mistrust?: Perceptions of media credibility in the information
age. Mass Communication and Society, 4(4), 381-403.
doi:10.1207/s15327825mcs0404_4
Kitchenham, B., Al-Khilidar, H., Babar, M., Berry, M., Cox, K., Keung, J., . . . Zhu, L. (2008).
Evaluating guidelines for reporting empirical software engineering studies. Empirical
Software Engineering, 13(1), 97-121. doi:10.1007/s10664-007-9053-5
Kulkarni, C., & Chi, E. (2013). All the news that's fit to read: a study of social annotations for
news reading. In W. E. Mackay (Ed.), Proceedings of the SIGCHI Conference on Human
Factors in Computing Systems (pp. 2407-2416). New York, NY: ACM.
204

Laslett, P. (1989). A fresh map of life: The emergence of the Third Age. Cambridge, MA:
Harvard University Press.
Lincoln, Y. S. (1995). Emerging criteria for quality in qualitative and interpretive research.
Qualitative Inquiry, 1(3), 275-289.
Lincoln, Y. S., & Guba, E. G. (1985). Naturalistic inquiry. Newbury Park, CA: Sage
Publications, Inc.
Liu, Z. M. (2004). Perceptions of credibility of scholarly information on the web. Information
Processing & Management, 40(6), 1027-1038. doi:10.1016/S0306-4573(03)00064-5
Liu, Z. M., & Huang, X. B. (2005). Evaluating the credibility of scholarly information on the
web: A cross cultural study. International Information & Library Review, 37(2), 99-106.
doi:10.1016/j.iilr.2005.05.004
Lucassen, T., Muilwijk, R., Noordzij, M. L., & Schraagen, J. M. (2013). Topic familiarity and
information skills in online credibility evaluation. Journal of the American Society for
Information Science and Technology, 64(2), 254-264. doi:10.1002/asi.22743
Lucassen, T., & Schraagen, J. M. (2011). Factual accuracy and trust in information: The role of
expertise. Journal of the American Society for Information Science and Technology,
62(7), 1232-1242. doi:10.1002/Asi.21545
McCroskey, J. C., & Teven, J. J. (1999). Goodwill: A reexamination of the construct and its
measurement. Communications Monographs, 66(1), 90-103.
doi:10.1080/03637759909376464
Merriam, S. B. (2009). Qualitative research: A guide to design and implementation. San
Francisco, CA: Jossey-Bass.
Metzger, M. J. (2007). Making sense of credibility on the Web: Models for evaluating online
information and recommendations for future research. Journal of the American Society
for Information Science and Technology, 58(13), 2078-2091. doi:10.1002/asi.20672
Metzger, M. J., Flanagin, A. J., Eyal, K., Lemus, D. R., & McCann, R. M. (2003). Credibility for
the 21st century: Integrating perspectives on source, message, and media credibility in the
contemporary media environment. In P. J. Kalbfleisch (Ed.), Communication yearbook
(Vol. 27, pp. 293-336). Mahwah, NJ: Lawrence Erlbaum Associates, Inc., Publishers.
Metzger, M. J., Flanagin, A. J., & Medders, R. B. (2010). Social and heuristic approaches to
credibility evaluation online. Journal of Communication, 60(3), 413-439.
doi:10.1111/j.1460-2466.2010.01488.x
Metzger, M. J., Flanagin, A. J., & Zwarun, L. (2003). College student Web use, perceptions of
information credibility, and verification behavior. Computers & Education, 41(3), 271290. doi:10.1016/s0360-1315(03)00049-6
205

Moffatt, K. (2013). Older-adult HCI: Why should we care? interactions, 20(4), 72-75.
Neugarten, B. L. (1974). Age groups in American society and the rise of the young-old. The
Annals of the American Academy of Political and Social Science, 415(1), 187-198.
Neugarten, B. L. (1996). The meanings of age: Selected papers. Chicago, IL: University of
Chicago Press.
O'Keefe, D. J. (1990). Persuasion : Theory and research. Newbury Park, CA: Sage Publications,
Inc.
O'Leary, Z. (2005). Researching real-world problems: A guide to methods of inquiry. Newbury
Park, CA: Sage Publication.
Osher Lifelong Learning Institute. (2015). About. Retrieved Apr 17th, 2015 from
http://olli.fsu.edu/About
Pan, B., Hembrooke, H., Joachims, T., Lorigo, L., Gay, G., & Granka, L. (2007). In Google we
trust: Users’ decisions on rank, position, and relevance. Journal of Computer-Mediated
Communication, 12(3), 801-823. doi:10.1111/j.1083-6101.2007.00351.x
Paterson, B. L., Thorne, S. E., Canam, C., & Jillings, C. (2001). Meta-study of qualitative health
research: A practical guide to meta-analysis and meta-synthesis. Thousand Oaks, CA:
Sage Publications, Inc.
Pettigrew, K. E., Fidel, R., & Bruce, H. (2001). Conceptual frameworks in information behavior.
Annual Review of Information Science and Technology, 35(43-78).
Petty, R. E., & Cacioppo, J. T. (1981). The elaboration likelihood model of persuasion. In L.
Berkowitz (Ed.), Advances in experimental social psychology (pp. 123-205). New York,
NY: Springer-Verlag.
Pfeiffer, E. (1975). A short portable mental status questionnaire for the assessment of organic
brain deficit in elderly patients. Journal of the American Geriatrics Society, 23(10), 433441.
Pirolli, P. (1997). Computational models of information scent-following in a very large
browsable text collection. In S. Pemberton (Ed.), Proceedings of the ACM SIGCHI
Conference on Human Factors in Computing Systems (CHI '97) (pp. 3-10). New York,
NY: ACM.
Pirolli, P., & Card, S. (1999). Information foraging. Psychological Review, 106(4), 643-675.
doi:10.1037/0033-295X.106.4.643
Princeton Survey Research Associates. (2002). A matter of trust: What users want from web
sites. Yonkers, NY: Consumer WebWatch. Retrieved from
http://consumersunion.org/wp-content/uploads/2013/05/a-matter-of-trust.pdf

206

Rieh, S. Y. (2010). Credibility and cognitive authority of information. In M. Bates, & M. N.
Maack, (Eds.), Encyclopedia of Library and Information Sciences. 3rd Ed. (pp. 13371344). New York, NY: Taylor and Francis Group, LLC.
Rieh, S. Y., & Belkin, N. J. (1998). Understanding judgment of information quality and
cognitive authority in the WWW. Proceedings of the American Society for Information
Science, 35, 279-289.
Rieh, S. Y., & Danielson, D. R. (2007). Credibility: A multidisciplinary framework. Annual
Review of Information Science and Technology, 41(1), 307-364.
doi:10.1002/aris.2007.1440410114
Rieh, S. Y., & Hilligoss, B. (2008). College students' credibility judgments in the informationseeking process. In M. J. Metzger, & A. J. Flanagin (Eds.), Digital media, youth, and
credibility (pp. 49-71). Cambridge, MA: The MIT Press.
Rieh, S. Y., Jeon, G. Y., Yang, J. Y., & Lampe, C. (2014). Audience-aware credibility: From
understanding audience to establishing credible blogs. In E. Adar, & P. Resnick (Eds.),
Proceedings of the Eighth International Conference on Weblogs and Social Media
(ICWSM '14) (pp. 436–445). Palo Alto, CA: AAAI Press.
Rieh, S. Y., Kim, Y.-M., Yang, J. Y., & St. Jean, B. (2010). A diary study of credibility
assessment in everyday life information activities on the web: Preliminary findings.
Proceedings of the American Society for Information Science and Technology, 47(1), 110. doi:10.1002/meet.14504701182
Robertson-Lang, L., Major, S., & Hemming, H. (2011). An exploration of search patterns and
credibility issues among older adults seeking online health information. Canadian
Journal on Aging, 30(4), 631-645. doi:10.1017/S071498081100050X
Robins, D., & Holmes, J. (2008). Aesthetics and credibility in web site design. Information
Processing & Management, 44(1), 386-399.
Ross, M., Grossmann, I., & Schryer, E. (2014). Contrary to psychological and popular opinion,
there is no compelling evidence that older adults are disproportionately victimized by
consumer fraud. Perspectives on Psychological Science, 9(4), 427-442.
doi:10.1177/1745691614535935
Rowley, J., & Johnson, F. (2013). Understanding trust formation in digital information sources:
The case of Wikipedia. Journal of Information Science, 39(4), 1-15.
doi:10.1177/0165551513477820
Savolainen, R. (1995). Everyday life information seeking: Approaching information seeking in
the context of “way of life”. Library & Information Science Research, 17(3), 259-294.
Savolainen, R. (2011). Judging the quality and credibility of information in Internet discussion
forums. Journal of the American Society for Information Science and Technology, 62(7),
1243-1256. doi:10.1002/asi.21546
207

Savolainen, R. (2012). The structure of argument patterns on a social Q&A site. Journal of the
American Society for Information Science and Technology, 63(12), 2536-2548.
doi:10.1002/asi.22722
Schutt, R. K. (2009). Investigating the social world: The process and practice of research (6th
ed.). Thousand Oaks, CA: Pine Forge Press.
Selwyn, N., Gorard, S., Furlong, J., & Madden, L. (2003). Older adults' use of information and
communications technology in everyday life. Ageing and Society, 23(5), 561-582.
doi:10.1017/S0144686X03001302
Smith, A. (2013). Smartphone ownership: 2013 update. Washington, DC: Pew Internet &
American Life Project.
Smith, A. (2014). Older adults and technology use: Adoption is increasing, but many seniors
remain isolated from digital life. Washington, DC: Pew Internet & American Life
Project.
St. Jean, B., Rieh, S. Y., Yang, J. Y., & Kim, Y.-M. (2011). How content contributors assess and
establish credibility on the web. Proceedings of the American Society for Information
Science and Technology, 48(1), 1-11. doi:10.1002/meet.2011.14504801163
Stanford Persuasive Tech Lab. (2015). Retrieved April 18th, 2015 from
http://captology.stanford.edu
Stvilia, B., Gasser, L., Twidale, M. B., & Smith, L. C. (2007). A framework for information
quality assessment. Journal of the American Society for Information Science and
Technology, 58(12), 1720-1733. doi:10.1002/asi.20652
Stvilia, B., Twidale, M. B., Smith, L. C., & Gasser, L. (2005). Assessing information quality of a
community-based encyclopedia. In F. Naumann, M. Gertz, & S. Mednick (Eds.),
Proceedings of the International Conference on Information Quality (ICIQ '05) (pp. 442454). Cambridge, MA: MIT.
Su, S. S., & Conaway, C. W. (1995). Information and a forgotten minority: Elderly Chinese
immigrants. Library & Information Science Research, 17(1), 69-86. doi:10.1016/07408188(95)90006-3
Sundar, S. S. (1999). Exploring receivers' criteria for perception of print and online news.
Journalism & Mass Communication Quarterly, 76(2), 373-386.
doi:10.1177/107769909907600213
Sundar, S. S. (2008). The MAIN model: A heuristic approach to understanding technology
effects on credibility. In M. J. Metzger, & A. J. Flanagin (Eds.), Digital media, youth,
and credibility (pp. 73-100). Cambridge, MA: The MIT Press.

208

Sundar, S. S., Knobloch-Westerwick, S., & Hastall, M. R. (2007). News cues: Information scent
and cognitive heuristics. Journal of the American Society for Information Science and
Technology, 58(3), 366-378. doi:10.1002/asi.20511
Taha, J., Sharit, J., & Czaja, S. (2009). Use of and satisfaction with sources of health information
among older Internet users and nonusers. Gerontologist, 49(5), 663-673.
doi:10.1093/geront/gnp058
Taraborelli, D. (2008). How the Web is changing the way we trust. In A. Briggle, K. Waelbers,
& P. Brey (Eds.), Current issues in computing and philosophy (pp. 194-204).
Amsterdam, Netherlands: IOS Press.
Tseng, S., & Fogg, B. J. (1999). Credibility and computing technology. Communications of the
ACM, 42(5), 39-44. doi:10.1145/301353.301402
U.S. Census Bureau. (2012). Statistical abstract of the United States: 2012. Washington, DC:
United States Census Bureau.
U.S. Social Security Administration. (2015). Full retirement age. Retrieved June 6, 2015 from
https://faq.ssa.gov/ics/support/KBAnswer.asp?questionID=3733&hitOffset=57+52+9+4
&docID=1938
Wathen, C. N., & Burkell, J. (2002). Believe it or not: Factors influencing credibility on the
Web. Journal of the American Society for Information Science and Technology, 53(2),
134-144. doi:10.1002/asi.10016
Wechsler, D. (1997). WAIS-III: Administration and scoring manual: Wechsler adult intelligence
scale. Psychological Corporation.
Westerwick, A. (2013). Effects of sponsorship, Web site design, and Google ranking on the
credibility of online information. Journal of Computer-Mediated Communication, 18(2),
80-97. doi:10.1111/jcc4.12006
Whitehead Jr., J. L. (1968). Factors of source credibility. Quarterly Journal of Speech, 54(1), 5963. doi:10.1080/00335636809382870
Wicks, D. A. (2004). Older adults and their information seeking. Behavioral & Social Sciences
Librarian, 22(2), 1-26. doi:10.1300/J103v22n02_01
Williamson, K. (1997). The information needs and information-seeking behaviour of older
adults: An Australian study. In P. Vakkari, R. Savolainen, & B. Dervin (Eds.),
Information Seeking in Context: Proceedings of an International Conference on Research
in Information Needs, Seeking and Use in Different Contexts (pp. 337-350). London:
Taylor Graham.
Williamson, K. (1998). Discovered by chance: The role of incidental information acquisition in
an ecological model of information use. Library & Information Science Research, 20(1),
23-40. doi:http://dx.doi.org/10.1016/S0740-8188(98)90004-4
209

Williamson, K., & Asla, T. (2009). Information behavior of people in the fourth age:
Implications for the conceptualization of information literacy. Library and Information
Science Research, 31(2), 76-83. doi:10.1016/j.lisr.2009.01.002
Wilson, P. (1977). Public knowledge, private ignorance: Toward a library and information
policy. Westport, CT: Greenwood Press.
Wilson, P. (1983). Second-hand knowledge: An inquiry into cognitive authority. Westport, CT:
Creenwood Press.
Xie, B. (2011a). Experimenting on the Impact of Learning Methods and Information
Presentation Channels on Older Adults ’ e-Health Literacy. Journal of the American
Society for Information Science and Technology, 62(9), 1797-1807. doi:10.1002/asi
Xie, B. (2011b). Older Adults , e-Health Literacy , and Collaborative Learning : An
Experimental Study. 62, 933-946. doi:10.1002/asi
Xie, B. (2012). Improving older adults' e-health literacy through computer training using NIH
online resources. Library & Information Science Research, 34(1), 63-71.
Yang, J. Y., & Rieh, S. Y. (2013). A dyadic approach to information mediation at work:
Examining credibility and value perceptions. Paper presented at the iConference 2013,
Fort Worth, TX.
Yi, Y. J., Stvilia, B., & Mon, L. (2012). Cultural influences on seeking quality health
information: An exploratory study of the Korean community. Library & Information
Science Research, 34(1), 45-51. doi:10.1016/j.lisr.2011.06.001
Zhang, Y. (2014). Beyond quality and accessibility: Source selection in consumer health
information searching. Journal of the Association for Information Science and
Technology, 65(5), 911-927. doi:10.1002/asi.23023
Zulman, D. M., Kirch, M., Zheng, K., & An, L. C. (2011). Trust in the internet as a health
resource among older adults: Analysis of data from a nationally representative survey.
Journal of Medical Internet Research, 13(1), 1-10. doi:10.2196/jmir.1552

210

BIOGRAPHICAL SKETCH
Wonchan Choi received his Master’s and Bachelor’s degrees in Library and Information Science
(LIS) from Pusan National University in Korea. His research interests include information
behavior, human-computer interaction (HCI), health informatics, longevity informatics, digital
libraries, and bibliometrics. He is also interested in useful methodologies for both qualitative and
quantitative research. As part of his effort to explore methodologies, he has learned various
statistical analysis techniques, pursuing the Graduate Certificate in Measurement and Statistics,
which is granted by Florida State University.

211

