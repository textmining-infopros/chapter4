The Pennsylvania State University
The Graduate School
College of Information Sciences and Technology

A FRAMEWORK FOR EVALUATING ELECTRONIC RESOURCES

A Dissertation in
Information Sciences and Technology
by
Daniel M. Coughlin

 2015 Daniel M. Coughlin

Submitted in Partial Fulfillment
of the Requirements
for the Degree of

Doctor of Philosophy
May 2015

UMI Number: 3715491

All rights reserved
INFORMATION TO ALL USERS
The quality of this reproduction is dependent upon the quality of the copy submitted.
In the unlikely event that the author did not send a complete manuscript
and there are missing pages, these will be noted. Also, if material had to be removed,
a note will indicate the deletion.

UMI 3715491
Published by ProQuest LLC (2015). Copyright in the Dissertation held by the Author.
Microform Edition © ProQuest LLC.
All rights reserved. This work is protected against
unauthorized copying under Title 17, United States Code

ProQuest LLC.
789 East Eisenhower Parkway
P.O. Box 1346
Ann Arbor, MI 48106 - 1346

The dissertation of Daniel M. Coughlin was reviewed and approved* by the following:

Bernard J. Jansen
Professor, Graduate Program Academic Coordinator
College of Information Sciences and Technology
Dissertation Advisor
Chair of Committee

Shawn Clark
Senior Lecturer, College of Information Sciences and Technology

Zhenhui Li
Assistant Professor, College of Information Sciences and Technology

Anne M Hoag
Associate Professor, College of Communications
Carleen Maitland
Interim Associate Dean for Undergraduate and Graduate Studies
Associate Professor, College of Information Sciences and Technology

*Signatures are on file in the Graduate School

ii

ABSTRACT
University libraries can provide access to tens of thousands of journals and spend millions of dollars
annually on electronic resources. With several commercial entities providing these electronic resources, the result
can be silo systems, processes, and measures to manage the access and to evaluate cost and usage of these
resources, making it extremely difficult to provide meaningful analytics for a holistic evaluation. Librarians
responsible for collection management spend much of their time manually aggregating data from various sources
and have little time to invest in the analysis, which is crucial for effective collection management.
Our research leverages a web-analytics approach for three objectives 1) the creation of a process to
evaluate university electronic resources, 2) the creation of a linear regression model to predict usage among these
journals, and 3) the development of a system for evaluation of electronic resources at a large research library.
This web-analytics foundation will enable understanding the value that specific journals provide university
libraries. The first objective is implemented by comparing the impact to the cost, titles, and usage for the set of
journals and by assessing the funding area (e.g., social sciences, arts & humanities, physical & mathematical
sciences, etc.). Overall, the results highlight the benefit of a web-analytics evaluation framework for university
libraries and the impact of classifying titles based on the funding area. By removing the outliers and maintaining
the variance in usage and cost among the funding areas, this analysis illustrates the importance for evaluating
journals by funding area. In the second objective we categorize metrics into two classes, global (e.g., journal
impact factor, Eigenfactor, etc.) that are journal focused and local (e.g., local downloads, local citation rate, etc.)
that are institution dependent. Using 275 journals for our training set, our analysis shows that a combination of
global and local metrics creates the strongest model for predicting full-text downloads. These results demonstrate
the value in creating local metrics for the evaluation of library content collections in order to better inform
purchasing decisions versus relying solely on global metrics. In the third objective, we create a conceptual model,
implement this model (i.e., the system), and validate this implementation using real-world data. The resulting
implementation provides a more sophisticated model of evaluation with a simpler model of implementation than
currently employed by many large research libraries. This model and system architecture is proven to scale for
evaluation of electronic resources at a large research library. The system aggregates several data sources

iii

providing an authoritative repository of information to evaluate journals based on local metrics (i.e. how often an
institution cites a journal, how much an institution pays for a journal, how often a particular journals is
downloaded, etc.) and global metrics such as Impact Factor or Eigenfactor. The combination of these objectives
creates a framework for evaluating electronic resources at scale for a large research library. This framework
provides practical methods to classify and evaluate journals, predict usage, and create automated processes and
systems to aid in this work. This work adds to the research around collection management and continual
improvement within a key component of a research library’s mission to provide access to relevant scholarly
materials.

iv

TABLE OF CONTENTS
LIST OF FIGURES ................................................................................................................................................. vii
LIST OF TABLES ................................................................................................................................................... ix
ACKNOWLEDGEMENTS ..................................................................................................................................... xi
Chapter 1 ....................................................................................................................................................................1
Research Objectives ...................................................................................................................................................2
Chapter 2 ....................................................................................................................................................................5
2.1 Related Work ........................................................................................................................................................9
2.2 Research Question and Hypotheses....................................................................................................................16
2.3 Methods, Data Preparation, & Analysis .............................................................................................................18
2.4 Results ................................................................................................................................................................27
2.5 Discussion ..........................................................................................................................................................32
2.6 Implications ........................................................................................................................................................38
2.7 Conclusion ..........................................................................................................................................................40
Chapter 3 ..................................................................................................................................................................42
3.1 Literature Review ...............................................................................................................................................45
3.2 Research Objective .............................................................................................................................................52
3.3 Methods, Reports, Data Preparation, & Analysis ...............................................................................................53
3.4 Results ................................................................................................................................................................61
3.5 Discussion and Implications ...............................................................................................................................70
3.6 Conclusion ..........................................................................................................................................................73
Chapter 4 ..................................................................................................................................................................75
4.1 Previous Work ....................................................................................................................................................76
4.2 Research Objectives ...........................................................................................................................................81
4.3 Methods, Data Collection, Data Preparation & Analysis ...................................................................................83
4.4 Results ..............................................................................................................................................................101

v

4.5 Discussion and Implications .............................................................................................................................118
4.6 Conclusion ........................................................................................................................................................123
Chapter 5 ................................................................................................................................................................125
5.1 Strengths and Limitations .................................................................................................................................126
5.2 Future Work .....................................................................................................................................................127
References ..............................................................................................................................................................128

vi

LIST OF FIGURES
Figure 1: Listing of vendors and the corresponding compliance for each COUNTER report available.
Report accessed from http://www.projectcounter.org/r4/R4Overview_Dec2013.pdf......................................13
Figure 2: Screenshot of filtered results being displayed on the web page, for the ScienceDirect platform
from Elsevier. ...................................................................................................................................................23
Figure 3: Compares the cumulative expense of the individual funding areas with "All", which ignores
funding area. .....................................................................................................................................................34
Figure 4: Compares the cumulative usage of individual funding areas with "All" category, which ignores
funding area. .....................................................................................................................................................35
Figure 5: Percentage (out of 100) for each funding area for cost, titles, and usage within the ScienceDirect
package for 2012. .............................................................................................................................................36
Figure 6: A display of the funding areas and their representation within the total cost, and content (titles)
of journal with no usage data. ...........................................................................................................................38
Figure 7: This is a snapshot of the 300+ page PDF file listing Web of Science Journal Titles and their
abbreviated Title20 field (accessed via
http://scientific.thomsonreuters.com/imgblast/JCRFullCovlist-2013.pdf). ......................................................59
Figure 8: Residual Plot for global citations, displaying random error and centered on zero, which is
consistent for an assumption of a regression model. ........................................................................................63
Figure 9: Residual Plot for local citations, displaying random error and centered on zero, which is
consistent for an assumption of a regression model. ........................................................................................64
Figure 10: Probability plot with regression line for all metrics at one time, which supports the condition
that the error terms are normally distributed. ...................................................................................................65
Figure 11: Content providers (i.e., Elsevier, Sage, Wiley, etc.) provide full-text download statistics in a
standard format that allows Serials Solutions to aggregate in one database. ....................................................84
Figure 12: Screen capture of the administrative interface on Serials Solutions used to download JR1 data
from various providers (retrieved with credentials from
https://clientcenter.serialssolutions.com/CC/Library/Counter/CounterReportList.aspx?LibraryCode=
PSU). ................................................................................................................................................................86
Figure 13: Thomson Reuters listing of journals and their abbreviated names in the Journal Citation Report
(retrieved from http://scientific.thomsonreuters.com/imgblast/JCRFullCovlist-2013.pdf). .............................92
Figure 14: Data processing flow for full-text downloads for journals to a local database. ......................................98
Figure 15: Data processing flow for Web of Science citation data to a local database. ...........................................98
Figure 16: Data processing flow for Journal Citation Reports to a local database. ................................................100
Figure 17: Data processing flow for financial data for institution into a local database. .......................................101

vii

Figure 18: A display of the data modeled based on many of the spreadsheets from various sources. Many
of these tables exist for the simplicity of query and error checking. ..............................................................103
Figure 19: Creating new Publisher, Provider, and Platform tables from the existing information in the
financial reports. .............................................................................................................................................105
Figure 20: Creating new Journal, Journal Metrics tables from the existing data sources. .....................................108
Figure 21: Linking the funding data into the journal metrics tables.......................................................................109
Figure 22: Entity Relationship diagram to help represent the association between the objects. ............................111
Figure 23: An abstraction of the conceptual model that shows the various data sources being brought
together and benefits this model provides. .....................................................................................................112
Figure 24: System architecture diagram to represent the implementation of the conceptual model from
data collection through implementation. ........................................................................................................113
Figure 25: Web Interface that allows search and filtering of the relational database (interface was created
by the Active Admin Ruby Gem retrieved from http://activeadmin.info/). ...................................................115
Figure 26: Administrative web interface providing a view of the metrics created specific to the institution
(interface was created with Active Admin Ruby Gem http://activeadmin.info/). ..........................................121
Figure 27: Administrative web interface providing a view of the Journal Citation Report information
stored in the database (interface was created with Active Admin Ruby Gem http://activeadmin.info/). .......122

viii

LIST OF TABLES
Table 1: Focus of study and metrics applied to create a framework for evaluating electronic resources at a
research library. .................................................................................................................................................8
Table 2: Listing of usage reports and definitions from COUNTER as of April 2012. Reports not listed as
optional must be provided by provided by vendors to be considered COUNTER compliant. ........................13
Table 3: Funding Area names and their corresponding abbreviation. .....................................................................17
Table 4: Example of how data is displayed on spreadsheet to determine what may be considered a
package deal and what may be considered a line item purchase. ....................................................................20
Table 5: MySQL format for Journal table within database used to import data from spreadsheets for
financial data to link with JR1 reports. ............................................................................................................21
Table 6: MySQL database table used to import the JR1 reports from spreadsheets into the database. ...................22
Table 7: Grouping of items purchased in 2012 and the number of titles those purchased items cost. ....................24
Table 8: Example of cost data on journals showing a journal with a single line item in PO-1 and a
package deal with multiple journals splitting up the cost in PO-2...................................................................25
Table 9: Listing of funding areas and their representative titles, usage, and cost in 2012. .....................................26
Table 10: The bottom third of journals based on usage by funding area in 2012....................................................28
Table 11: Post hoc Analysis for usage by Funding Area. Means that do not share a Grouping letter are
significantly different (p < 0.001) ....................................................................................................................29
Table 12: Post hoc analysis for Cost by Funding Area. Means that do not share a Grouping letter are
significantly different. (p < 0.001) ...................................................................................................................30
Table 13: The bottom third of journals based on annual subscription price paid for the journal in 2012 by
funding area. ....................................................................................................................................................30
Table 14: Post hoc analysis for Usage by Funding Area. Means that do not share a Grouping letter are
significantly different. (p < 0.001) ...................................................................................................................31
Table 15: Post hoc analysis for Cost by Funding Area. Means that do not share a Grouping letter are
significantly different. (p< 0.001) ....................................................................................................................32
Table 16: Breakdown of potential areas for evaluating electronic resources and the goals of the
organization that relate to this areas ................................................................................................................46
Table 17: Listing of the various metrics provided in a Journal Citation Report and what they measure to
provide value (provided by Thomson Reuters, accessed at
http://wokinfo.com/media/pdf/qrc/jcrqrc.pdf). ................................................................................................55
Table 18: Listing of the types of data sources used for this analysis and their definition. ......................................57

ix

Table 19: List of the metrics, a categorization of global or local for this particular institution, and the data
source that supplies these metrics, used as independent variables to model the correlation to
downloads. .......................................................................................................................................................57
Table 20: Listing of the field codes and the contents of those fields within the spreadsheet cell for the
Web of Science spreadsheets for local citation data. .......................................................................................59
Table 21: Snapshot of Journal Citation Reports spreadsheet. .................................................................................60
Table 22: Listing of the linear regression model measures separately for each metric and local institution
downloads. Note: bold figures represent the three independent variables with the strongest
correlation to full-text download. ....................................................................................................................61
Table 23: Summary of the Regression Model created using all the metrics available. ...........................................65
Table 24: Listing of the metrics used and their statistical significance for meaningful addition to a linear
regression model for the sample data 275 journals. Note: bold figures represent metrics with a
statistically significant p-value. .......................................................................................................................66
Table 25: Summary of the Regression Model created using only the statistically significant metrics. ..................66
Table 26: Displays the number of journals tested for accurate range of prediction for downloads and
those that within range of prediction and those outside of the range, and corresponding downloads,
and cost. ...........................................................................................................................................................67
Table 27: A subset of journals that have a higher actual price than predicted price range based on
predicted downloads. Their predicted download range for each journal and the range of price to
offer and potential savings based on the difference between price offer and actual price...............................69
Table 28: Partial listing of a Journal Report 1 that was run on July 15, 2014 to retrieve usage data from
Elsevier Journals between January 1, 2014 and June 30, 2014. ......................................................................88
Table 29: Example output for a source article published with author and citation data from Web of
Science. The rows that are in boldface represent data that is stored locally. ...................................................90
Table 30: Partial listing of Web of Science Journal Citation Report displaying multiple metrics for
journals.. ..........................................................................................................................................................93
Table 31: Partial listing of database purchases in 2012. ..........................................................................................95
Table 32: Partial listing of journals purchased in 2012, the database they belong to, and line item cost of
that journal. ......................................................................................................................................................96

x

ACKNOWLEDGEMENTS
Many people were critical to this research directly and indirectly by supporting me throughout this
process or significant portions of it. First, I want to thank Dr. Jim Jansen for his dedication in advising me
throughout this process. As a non-traditional student, I present my own unique set of management challenges. Dr.
Jansen was attentive to those, and provided a tremendous support structure in mentoring, advising, encouraging,
proof reading, and offering friendship along the way. Without Dr. Jansen, I’d still be talking to people wondering
what topic I should consider for my research, and I’m very grateful to have his leadership and expertise on my
side during this process. Additionally, thank you to Dr. Shawn Clark who was critical to helping me find my way
in my first year of graduate school, and constantly provided encouragement to do what I’m passionate about. He
is a great friend and mentor. Thank you to Dr. Jessie Li who immediately provided wonderful insight and
questions on this research. Thank you to Lisa German and her staff (Jaime Jamison, Amber Hatch, James
Devoss, and Bob Alan) for providing access to data and attending countless meetings to help me unravel the
complexity in electronic resources. I truly hope something I’ve done here can be helpful to those that spend so
much time and energy making sure we have access to these resources. And, thank you to Anne Hoag, who has
been pushing and supporting me as a student since I was an undergraduate (I’ve completely gotten over the time
you destroyed my paper as an undergrad).
As a full-time employee, I would not be able to consider a PhD without the support of my organization. I
thank Mairéad Martin who didn’t just allow me to get a PhD, but also encouraged me to do this and gave me
continual backing, support, and praise throughout all of this. My colleagues (Kurt Baker, Carolyn Cole, Christy
Long, Justin Patterson, Michael Tribone, Jeff Minelli, Joni Barnoff, and Beth Hayes) enabled me to go to class
during the day without any problems which I’m grateful for. Mike Giarlo had countless discussions with me
about all the other topics I considered for my research that may be important to the library and I thank him for
listening and providing thoughtful feedback.
Thanks to Jeff Rimland for always being willing to listen to my ideas, provide encouraging words, and sit
down to help me think things over, proofread some of my work, and pressure me to pick a date for my proposal.
Mark Campbell provided statistical help many times when I was getting started and without that help I may still

xi

be getting started! Thanks to Sue Kelleher and Michelle Hill for their continued patience dealing with me and my
administrative needs. Thanks to Ryan Blanck for helping me step back during my most stressful times and create
a vision for how I would accomplish what I was facing at that point. Thanks to Janine and Matt Gaul for making
sure I laughed a couple times over the past two years and on occasion took a break, and thanks to my brother-inlaw Kevin for continually reminding me to stay the course. I can’t thank my sister Jean enough for her constant
willingness to put everyone else first, including me. Without knowing how much help she would provide my
family I would not have considered grad school at this point in my life. Thanks to my boys, Owen and Liam, for
being patient with dad when I was “always being on the computer,” and thanks to Willa for giving me the
ultimate deadline to complete this. Finally, thanks to my wife Julie, who was around for every high and low
throughout this entire experience, and the past fifteen years, providing seemingly endless support, love, and
encouragement. Thank you for giving me so many reasons to never quit and doing everything else without a
single complaint!

xii

Chapter 1

Introduction
It has become common for research libraries to spend large portions of their budgets on these resources as
journal subscriptions comprise 50-70 percent of the acquisition budgets in most academic libraries (Chung, 2007).
Aside from financial constraints, when libraries only stored physical copies of journals, there were limitations
based on physical space within the library as to how many journals they could subscribe to and subsequently
store. The digital era has alleviated most limitations around space, and arguably eliminated if not mitigated the
barriers to entry around publishing. These factors have given rise to the number of electronic resources available
to libraries. Today, a university library can have access to over 100,000 journal titles from a myriad of content
providers— providing ubiquitous access to a vast amount of research material is a goal that has been realized by
its electronic resources department.
What has become more difficult in this current environment is to understand the true value of the
resources the university library provides access to, what resources provide real value to the community, and what
resources are either underutilized or unnecessary. Given the constantly changing nature of electronic journals and
libraries' attitudes toward them, as well as the inexorable advance of technology, one cannot assume that past
answers apply today (West, Miller, & Wilson, 2011). It is important to create a set of metrics from each
institution can perform their own evaluation in order to truly understand the needs of their research community. It
is imperative that libraries know the content to which they subscribe, and know the value of that content in order
to effectively manage these electronic resources. Managing these collections effectively based on quantitative
metrics has become increasingly difficult because of the environment around electronic resources, specifically the
complexity of subscription models and the sheer amount of data.
The deals libraries have with the providers of this content, in practice, are not simple; Montgomery and
King (Montgomery & King, 2002) point out "subscription" in the electronic world is not a simple payment for the
annual content of a journal title. An electronic subscription often brings with it several years of back files (titles
from previous years). The price models and electronic content vary so radically that it is necessary to define four

1

electronic journal subscription types to try and categorize the levels of complexity (e.g., Individual Subscriptions,
Aggregator Journals, Full-Text Database Journals, and Publishers' Packages) (Montgomery & King, 2002).
In order to provide access to a particular outlet, libraries frequently need to purchase a bundled package of
journals. An additional concern, because of complex packaging offers and large volumes of data to track, is
duplication of content and the unfeasible costs associated with managing various formats, subscriptions, and
indexes (Maple, Wright, & Seeds, 2003). Finally, understanding which content provides value becomes a concern
as “different digital formats, interfaces, pricing structures, and access restrictions complicate our ability to
evaluate journal resources using consistent measures” (Mercer, 2000, para. 2). At the individual level, measuring
value can be difficult. At the university library, with tens of thousands of titles and millions of dollars on the line,
it is extremely complex. In this complex content environment, it becomes increasingly difficult measure the value
of the content collections. The goal of this research is provide a foundation for a framework to evaluate electronic
resources at a university library.
Two global metrics used to evaluate journals are journal citation reports (JCR), and expert perceptions.
However, these overarching methods cannot be a sole resource of analysis for a research library. Coleman states,
journal impact factor does not directly equal journal quality as journal quality is multifaceted; the two primary
methods of journal rankings (JCR and expert perceptions) have shown to be flawed and these limitations provide
a compelling justification to further investigate journal value (Coleman, 2007). As state funds for many public
institutions continue to diminish, libraries need to provide hard numbers to help define their business contribution
(Conyers & Payne, 2011). It is important for libraries to demonstrate the value of continued subscriptions for their
patrons.
This research presents a web analytics methodology that permits libraries to evaluate their paid content
collections via usage. Showing the use of these resources quantifies their value as the value only exists if titles are
used (Montgomery & King, 2002). By pinpointing the lesser-used resources we can begin identifying journals
that are not providing value.
Research Objectives
This research will evaluate different techniques that can be used to evaluate electronic resources. The

2

university library spends over $10 million annually on subscriptions and does not have the ability to
systematically track the value of these subscriptions. There are disparate systems for financial data, usage data,
subscription data and title listings, and citation data. It is a difficult task to bring all of this data together not only
because of the inconsistency between the systems but also because of the amount of data we have and the
changing environment this data exists in. The aim of this research is to only bring this data together but to do so in
ways that allow more thorough analysis for evaluation of electronic resources. The amount of data is over
100,000 titles and subjects within these journals with various levels of access to these titles; and the changing
environment is reflected in the nature of title changes within packages, infrastructure changes, and a digital
environment that is continually evolving.
The work on journals analyzes a subset of the 100,000 titles to understand the impact and value that these
journals have within the electronic content collection. A key motivator is identifying outlying journals for further
evaluation. Librarians do not have time to analyze each title to determine if they are or are not providing value;
however, by providing a method to determine a subset of journals that require further analysis this research can
create a better-formed evaluation of electronic resources. Keeping in mind that a comprehensive review of all
electronic resources was out of scope for this analysis, and the intention was to lay the groundwork for a larger
scale evaluation, the immediate goal was to narrow the aim of the analysis to data that is impactful if not
comprehensive.
This work provides three research objectives. The first objective is to develop a model to evaluate
journals focused on usage, cost, and content. This model will help to identify the journals of both high and low
value. When evaluating journals, we can consider usage in various ways such as a full-text download, a citation,
and/or a publication; for the purpose of this objective usage is considered a full-text download. Cost focuses on
the annual subscription fee for a journal. Content is a measurement based on the funding area the pays for the
journal--it is assumed that if the money to pay for a journal comes from the Arts & Humanities budget then the
journal covers content related to the Arts & Humanities. This work will focus on the quantitative distinction of
usage and cost among journals from different fund areas. Based on this outcome it is apparent the if there is a

3

distinction then journals should compared within fund areas, and if there is no distinction than the fund area of a
journal is a less significant metric to consider in evaluation.
The second objective is to create a model to predict electronic resource usage. This model is created
with a subset of data then, the model is applied to a real world data set. The assumption is that successful full-text
downloads are a foundational measurement of demand and subsequently value of electronic resources, and
because of this predicting downloads can contribute to a model that defines the cost. This type of model can
establish metrics to designate resources that are to be considered outliers; outliers can either be considered a good
deal (library pays less than predicted) or a bad deal (library pays more than predicted). By clarifying the
resources that appear to be a bad deal a library can further evaluate the true value and need to purchase those
resources.
The third objective is to create a system that can automate the work to generate metrics on usage (i.e.,
downloads, citations, authorship) cost, and content for electronic resources. This tool would enable librarians to
spend more time analyzing data rather than collecting it and create a more streamlined process for collection
management of journals. The format of this tool will be a web application that can provide access to any number
of people or institutions for the purpose of evaluation and the ability to repeat these processes on an annual basis
for continued analysis.
These objectives are presented in the form of three distinct but related research projects.

4

Chapter 2
A large research university can spend more than tens of millions of dollars annually on electronic
resources (Furlough, 2012). When libraries stored only physical copies of journals, there were limitations based
on physical space within the library as to how many journals a library could subscribe to and subsequently store.
The digital era has alleviated most limitations around space and arguably mitigated the barriers to entry around
publishing. These factors have given rise to the increase in number of electronic resources available to libraries,
with financial rather than physical constraints being the primary limiting factor. Today, a university library can
have access to more than a 100,000 journal titles from a myriad of content providers. Providing ubiquitous access
to a vast amount of research material is a goal that has been realized by many library electronic resources
departments.
What has become challenging in this environment is to understand the value of the resources to which a
university library provides access. Specifically, what resources provide real value to the community? What
resources are underutilized? What resources are unnecessary? Given the constantly changing nature of electronic
journals and libraries' attitudes toward them, as well as the inexorable advance of technology, one cannot assume
that past answers apply today (West et al., 2011). Instead, it is important to create a set of analytic metrics with
which institutions can perform their own evaluation to understand their specific community’s needs (i.e.,
effectiveness) and also to gauge the process in which they provide these resources (i.e., efficiency). To do this, it
is imperative that libraries know both the content to which they subscribe and the value of that content in order to
effectively manage these electronic resources. Although this may seem obvious to someone not in the library
space, managing these collections effectively and efficiently based on quantitative metrics has become
increasingly difficult because of the electronic resources environment, specifically the complexity of subscription
models and the sheer amount of data.
To provide some simplicity, in this research, we categorize the varied subscription models from content
providers into three subscription general types: aggregations, databases, and packages, defined as


Aggregations – are large collections of journals, sometimes numbering in the thousands from various content
providers on numerous subjects that are purchased through a single provider. Access to individual titles from

5

these deals can be subject to change with or without notice; however, they provide a way to get access to a
large number of titles.


Databases – are search indexes for a set of journals, typically centered on a particular subject. These search
indexes may vary in terms of what they inventory with some indexes containing abstracts and some
containing full-text indexing capability. Access to the search index result set does not necessarily imply
access to the article in the result set, as the level of access varies by database.



Packages – are essentially direct subscriptions to journals that provide access to scholarly articles contained
within those journals. Sometimes these are packages of one (i.e., one journal). Other times, a package may
contain multiple journals. The complexity can come when a specific package or group of packages are sold
together where the library cannot buy a single journal title but rather must purchase the entire package and the
associated titles within that package. So, a package may provide access to only one journal. Packages can also
provide access to a handful of journal titles for a single price without a list price for each journal title.
Note that, throughout the paper, we use the term journal in a broad sense of representing all possible types

of titles, including journal articles, journal notes, conference proceedings, book reviews, etc.
The complexity of understanding these subscription types is how providers manage the subscription
models relative to how libraries may desire to purchase the subscription. By analogy, imagine creating a music
collection from various online music distributors. In this scenario, you may want access to a particular album by
The Beatles and can buy that album individually, but you also want an album by a lesser-known band, say Built to
Spill; however, you cannot buy the Built to Spill album individually. Built to Spill has licensed their album to
only be sold by iTunes, and iTunes sells this album along with a collection of other albums as one set of music
artists. In order to get the album from Built to Spill, you have to buy the set of 10 from iTunes. This situation is
analogous to --on a much smaller scale-- packages (the individual Beatles’ album) and aggregations (the Built to
Spill collection).
Now imagine that once you have bought this collection of independent music artists, you realize this set
of albums also has the album by The Beatles, which would be great if you had known in advance; however, you
have now paid for this album twice resulting in duplication and wasted costs that could be better spent on other

6

albums. This situation is analogous to buying a package from a content provider only to find out that it is also
included in a larger aggregator deal the library has also purchased.
The situation is even more complex. Say you want to purchase another individual album by The Beatles
that has previously unreleased material. Unfortunately you cannot just buy this album, as the provider has decided
to bundle this album in a “box set” and sell it as part of a package with four other albums. Now, you have to pay
for five albums to get the one you would like. This situation is again analogous to package deals with journals.
Finally, considering that now that you have all of this music, the ability to search it becomes increasingly
valuable as the size of your collection grows. The search appliance you buy indexes to musical areas and in some
cases it can index all the songs on an album and in some cases it cannot. In some instances, it returns results that
are outside of your collection just to make you aware that other music outside of your collection exists to meet
your search criteria. While helpful to become aware of an album that you may not have previously known about,
there is confusion at times on what results your searches return. This situation is analogous to databases, where
some of the content is full text and other of the content in the same database is only an abstract.
These situations are similar to how the three subscription models (aggregations, databases, and
packages) can work within electronic resources environment at a major library. At the individual level, as with
our music metaphor, measuring value can be difficult. In this multifaceted content environment of the modern
university research library, with tens of thousands of titles and millions of dollars on the line, it has become
increasingly difficult to measure the value of the content collection, and our methods of evaluation and tools for
evaluation have not kept pace.
The motivation for our research is to address this complex issue by modernizing the methods of and tools
for evaluation. The goal of this paper is to provide a foundation for a framework to evaluate electronic resources
at a university library on a large scale. In this analysis, we begin to build this framework by developing and
analyzing key performance indicator (KPI) metrics on a substantial set of journals that have a sizable impact on
(1) content, (2) titles, and (3) usage at a major research university library. Although we focus on packages in the
research reported in this manuscript, this evaluation approach can be combined with additional metrics for an

7

analysis of databases and aggregations to create a framework for a quantitative decision framework to evaluate
university library electronic resources.
There are at least three areas to examine within electronic resources: 1) cost - how much is spent on these
resources, 2) titles - the number of resources provided for a particular discipline, and 3) usage - how often these
resources are used. We have created a classification based on our own analysis as well as metrics used in prior
studies on electronic resources, subscription type, metrics, and the metric definition (see Table 1). The potential
metric column displays the different metrics related to key performance indicators (KPIs) that may be used to
determine value of that subscription, along with the definition of that metric.
Study Type Subscription Type(s)
(KPIs)
Cost
Packages,
Databases,
Aggregations

Potential Metric

Definition

Cost-per-click
Cost-per-search
Cost-per-title

The number of clicks per download
The number of searches/cost for access
The number of titles within an aggregator

Titles

Packages,
Databases,
Aggregations

Duplicate titles
Unique titles
Number of titles
Area of study
Full-Text Index
Abstract Index

Usage

Packages,
Databases

Number of times this title appears in
collection
Number of unique titles in index
Total number of titles in index
Disciplines covered by index
Access to full-text is provided
Access only to abstracts is provided
How often journal is cited both locally and
globally
Successful full-text-downloads/cost
Comparing cost-per-use with cost of ILL

Citations/Impact
Factor
Cost-per-use
Inter-library loan
(ILL) usage
Table 1: Focus of study and metrics applied to create a framework for evaluating electronic resources at a
research library.
Two global approaches currently used to evaluate the value of electronic resources are journal citation
reports (JCR), and expert perceptions; however, these overarching approaches cannot be the sole resource of
analysis for a research library. Coleman (2007) states that journal impact factor does not directly equal journal
quality, as journal quality is multifaceted. Additionally, the two primary methods of journal rankings, JCR and
expert perceptions, have shown to be flawed, and these limitations provide a compelling justification to further
investigate journal value (Enssle & Wilde, 2002).
This research gives an analysis of journal value, focusing on the management of journals by comparing a
holistic perspective (looking at all journals equally) and a funding area perspective of categorized journals

8

(correlated to the discipline for content of the journal). This comparative approach gives a quantitative
perspective of the impact journal management has on a particular funding area to provide a structure for
evaluating and potentially eliminating superfluous journals.
2.1 Related Work
Is this subscription data useful?
The digitization of scholarly resources has created a benefit for researchers by providing ubiquitous
access to academic content; however, the proliferation of online content has created a difficult landscape for
libraries to navigate in the context of collection management, which is “the set of activities which is intended to
ensure that a library’s internally held and externally provided resources meet the needs of its users, leads to
weeding and acquisition” (Schwartz, 2000, p. 389). The data deluge of the Information Age has created an
overwhelming task for librarians to manage the contents of the collections they preside over. The notion of
weeding out collections that provide little or no value is a primary objective for serving the needs of researchers.
Tight library budgets create an environment that needs to mitigate waste, particularly concerning online
collections, as they compromise a large portion of the budget and are targets for scrutiny. Historically, libraries
have been able to manage collection use through the circulation of content within the library. The ability to
compute the demand of content when accessed from the library is simpler than tracking this information from a
multitude of digital access points. “One of the problems of a ‘library without walls’ is just that — the absence of
physical dividing lines that separate the library from the rest of the world, and that also give us some sense of
being able to control, or at least see, our collections and our users” (Schwartz, 2000, p. 390). Libraries recognize
the need for data collection management in order to make strategic decisions regarding electronic resources;
however, they are not doing this systematically or effectively (Lakos, 2007).
The intricacies within subscriptions quickly become apparent when looking at the sheer number of journal
titles that an academic library at a major research institution has. For example, the library used in our study has
access to over 100,000 titles from subscriptions through journals, databases, and aggregations. This quantity is
important to understand because if all of this information is not used, there could be savings in some areas without
significantly affecting the electronic resource collection. In fact, some of these providers are not particularly

9

selective of the quality of scholarly material they, and, in turn, university libraries provide access to (Inger, 2001).
The potential lack of quality content in host providers leads one to question their worth in some situations and to
suggest they may ultimately do more harm than good as student researchers, having less experience and
knowledge around good scholarly publications, believe higher-quality content exists by just being in a library
content collection. This underscores the need for fewer lower-tiered electronic journals and a greater number of
digital scholarly work because students will choose the convenience of going online for research (Bartsch &
Tydlacka, 2003).
Separate from the amount of data, but of equal importance, is the cost associated with providing access to
all of these journals. The cost goes beyond subscription fees as there are operational costs associated with both
human and technical resources dedicated to providing access to this content (Montgomery, 2000). According to
Tenopir and King (1998), the summary of skyrocketing journal subscriptions is directly correlated to having
online access to journals. There has been a shift in the purchasing paradigm from individual faculty members to
libraries that has reduced campus subscription duplications; however, it also created higher costs assumed by
library budgets rather than individual faculty research budgets (King & Tenopir, 1998).
Complexity on Online Content
Clearly, with access to over 100,000 titles, the complexity based on the collection size alone is evident.
Additionally, monitoring journal usage is problematic because of various types of subscriptions, methods of
access, and tracking information. For example, tracking all the relevant usage data on journals provided by an
aggregator may be difficult because aggregations provide access to a fluctuating number of journals throughout
the year. You may have access to a journal in January and no longer have access in March. The challenge
libraries have gaining access to track journals within aggregate packages is well documented (Chambers & So,
2004; Duranceau, 2002). These various subscriptions models also come with a number of methods to access
content. For example, not all advertised full-text access is always full-text. In some cases, embargo periods take
full-text out of indexes after the article has been printed in an effort to keep value in the print version of a journal
(Blessinger & Olle, 2004). This complexity makes it more straightforward to monitor journal usage that you have
an annual subscription for and to monitor the price of a single subscription rather than determining a method to

10

break down the price of each journal within a package of 10,000 titles from an aggregator or varying levels of
full-text access within a database.
Theoretically, one would be led to believe that accessing digital resources could lead to a more
streamlined process, enabling simple programming scripts to aggregates data for librarian to analyze for cost
effective decisions to be made for electronic resource management; however, the change of access points going
from the physical library to multiple remote servers presents a hurdle that libraries have yet to overcome
(Duranceau, 2002). While the process for tracking circulation data was manual before the process was online, it
was also stewarded by the library and therefore could be amended to meet the workflow needs of the individual
library. Now, content and the data specifying the usage of the content are stored on remote servers by publishers.
Remote access to content that is stored on third party servers presents two stumbling blocks in the form of
standardization and accuracy. Standardization in storing the access logs in the same format on all servers across
all publishers is essential so the same method for retrieving usage statistics can be used no matter who the content
provider. The issue around accuracy is one of inaccurate logs that would lead to erroneous KPIs for journals. In
some cases, publishers lack consistency in how they calculate particular stats, for example, how searches are
logged in a database. Additionally, publishers do not provide information about how they collect their data (Duy
& Vaughan, 2003). Even if publishers did provide methods for how statistics were created, unless there is a
standard for the metrics, it would add yet another distinction to track and analyze.
Perhaps even more troubling is that beyond standardization for counting statistics on resource usage is
that these usage statistics many times lack accuracy. Duy and Vaughan (2006) tracked usage stats locally to
compare with the stats that were provided from publishers and found that in some cases usage was higher for local
stats and in other cases usage was recorded lower for local stats.
It is important for libraries to make sure usage data is accurate to ensure an correct analysis so that
decisions made about collection management for researchers are well-aimed (Duy & Vaughan, 2003). The only
way to ensure this accuracy is to track usage statistics and to compare those local statistics with those of
publishers. This is not suggesting usage errors are made with malicious intent. The environment that exists with
multiple access points, content that exists in multiple publishers, content that is no longer accessible during a

11

calendar year, and the sheer volume of data being accessed result in usage statistics being not easily attainable and
in some cases unattainable. Furthermore, the modes of delivery for these statistics can be numerous and
cumbersome. These various modes include: separate spreadsheets to download for each provider (instead of one
spreadsheet for all titles) requiring manual data curation; copying data from a web site to a spreadsheet or email
attachments; and in the worst scenario, having no data at all.
Standards for Tracking Online Content
Although standards are not implemented with complete coverage throughout electronic journal
publishing, some standards do exist. COUNTER (Counting Online Usage of Networked Electronic Resources) is
a non-profit organization with an international advisory board made up of library and publishing specialists. Since
the initial COUNTER Code of Practice in 2003, this international consortium has been “setting standards that
facilitate the recording and reporting of online usage statistics in a consistent, credible and compatible way”
(Shepherd, 2012, p. 3). COUNTER maintains a list of vendors that are COUNTER compliant for journals and
databases (as well as books and reference works). There are a number of usage reports listed in Table 2 that
display the report name and the description of the data that is found in the report for vendors or publishers to
comply with to help with various types of metrics.

12

Report
Journal Report 1 (JR1)

Description
Number of successful Full-Text Articles Requests by
Month and Journal
Journal Report 1a (JR1a - optional)
Number of Successful Full-Text Article Requests
from an Archive by Month and Journal
Journal Report 2 (JR2)
Access Denied to Full-Text Articles by Month,
Journal and Category
Journal Report 3 (JR3 - optional)
Number of Successful Item Requests by Month,
Journal and Page-Type
Database Report 1 (DB1)
Total Searches, Result Clicks and Record Views by
Month and Database
Database Report 2 (DB2)
Access Denied by Month, Database and Category
Platform Report 1 (formerly Database Report 3 Total Searches, Result Clicks and Record Views by
DB3)
Month and Platform
Journal Report 4 (JR4 - optional)
Total Searches Run by Month and Collection
Table 2: Listing of usage reports and definitions from COUNTER as of April 2012. Reports not listed as
optional must be provided by provided by vendors to be considered COUNTER compliant.
COUNTER’s list of registered vendors provides a chart of vendor name and the type of report they are in
compliance with as shown in Figure 1. Figure 1 gives a snapshot of how few vendors meet all of the COUNTER
compliance reports, as well as an indication that JR1 reports are the most commonly provided.

Figure 1: Listing of vendors and the corresponding compliance for each COUNTER report available.
Report accessed from http://www.projectcounter.org/r4/R4Overview_Dec2013.pdf.
From Figure 1, a vendor may provide one type of standard report while not providing all the reports to
become fully COUNTER compliant. For example, Alexander Street Press Inc. (see row 1 of table in Figure 1)
provides JR1 and DB1 reports but does not provide the JR2 and DB2 reports in order to be considered

13

COUNTER compliant; however, the reports they do provide enable analysis of certain metrics to be done on fulltext article download (JR1). Thus, being COUNTER compliant is an ideal situation for further evaluation, but
there is plenty of value by providing any number of these reports to enable investigation and analysis.
The National Information Standards Organization (NISO), a non-profit organization that develops
standards, and protocols to manage information in our changing digital environment (“Standardized Usage
Statistics Harvesting Initiative (SUSHI) - National Information Standards Organization,” 2013, para. 1), in
conjunction with COUNTER reports, defined the Standardized Usage Statistics Harvesting Initiative (SUSHI).
SUSHI is a web-request framework preconceived around eliminating the manual labor-intensive activities of
collecting COUNTER reports. In short, SUSHI enables organizations to automatically download COUNTER
reports that are stored on the number of publisher servers instead of requiring a librarian to go each publisher web
site, download the various COUNTER reports for each publisher, and then organize and store the reports
somewhere accessible for other librarians to investigate.
An additional standard from which NISO intended to alleviate some of the database access confusion is
OpenURL. In cases where a database provides a search index to abstracts for journal articles but does not provide
access to the full-text of the journal article, OpenURL can help provide access. If a library has access to the fulltext of a journal article through a deal with another content provider, this access can be granted via OpenURL.
OpenURL provides additional metadata in the URL (along with a base URL) to query additional library holdings
and provide direct links to the journals where the full-text of the article is accessible. The base URL in an
OpenURL points to a “link resolver” that is capable of performing a query against the libraries holdings based on
the additional metadata (i.e., ISSN, author, title) provided in the link (McDonald & Van d Velde, 2004).
Metrics and In house Bookkeeping
There are three categories of data that seem to be high priority for making decisions regarding electronic
resources. Those categories are: (1) titles (what do we have access to?), (2) use (how often do we use it?), and (3)
cost (how much do we pay for access to it?). The current methods used to manage electronic journals need to
improve simplicity, affordability, and interoperability with discovery tools (OpenURL, large scale web discovery,
etc.), and integrated library systems (ILS, used to track library assets, and billing) products (West et al., 2011).

14

The University of Illinois at Chicago developed an internal system, Database of Library Licensed Electronic
Resources (DOLLeR), to assist in the management of electronic resources and improve efficiency, particularly
around licenses and subscriptions. The discrepancy in technical expertise and available infrastructure support
create a range in scope and complexity of such systems to assist in the management of electronic resources and
licensing (Stefancu, Bloss, & Lambrecht, 2004).
It is important for libraries to track data internally and to compare this data with that of publishers not
only to ensure the accuracy of data publishers provide but also because any major differences could signal a
problem in a service the libraries provide (Duy & Vaughan, 2003). Internally tracking this information becomes
an even higher priority when considering the most common place for librarians to check for changes in managing
electronic records is with the publisher and technical services (West et al., 2011). Data on cost and usage can be
combined to create important KPIs as to which journals are or are not providing value to the community. These
data points are necessary during a time of shrinking budgets in an effort “to do more with less” (Leon & Kress,
2012). Perhaps the most frequent metric discussed in the literature is usage: how often was this resource
accessed?; how often was this resource searched?; and how often was this resource downloaded?.
Measuring resources usage is invaluable; tracking how often a resource is used and then combining that
information with how much it costs can govern the selection or cancellation of a resource. Increased prices along
with fragmented titles and content that is duplicated among various database are among the motivating factors in
maintaining usage statistics (Medeiros, 2007). Weaknesses exist in measuring usage statistics; for example, the
importance of a journal within a particular discipline cannot be measured simply by how often it is used (Enssle &
Wilde, 2002). One weakness of global data, such as Journal Citation Reports, is the inability to account for a
multidisciplinary approach, as not all disciplines operate the same with respect to citing prior work. Therefore,
there needs to be evaluation by categorization that uses groups to initially define and categorize journals prior to
determining value. This must be done because applying the same methods and measures to evaluate all
disciplines is proven to be an inaccurate measure (Bensman, Smolinsky, & Pudovkin, 2010). Furthermore, some
fields are actually interdependent and comparing even based on subject matter (i.e., management literature is
dependent on social science literature) can be problematic (Neeley, 1981). Even with these caveats, it is generally

15

accepted that usage is a fundamental indicator of determining value within electronic resources (Metz, 1992);
therefore, we would expect that metrics involving usage will influence the significance of these resources.
Understanding the connection between cost, titles, and usage provides an opportunity for evaluating
electronic resources more equitably by comparing resources that are funded by the same area and subsequently
serving a corresponding area. This relationship provides insight to show that users are being affected with usage
statistics, and the audience is being affected based on the funding area. Furthermore, usage based metrics
correlated with funding area provide a means for libraries to measure electronic resource value within these
funding areas. Results from this research can serve a variety of purposes, including recommendations for
eliminating resources that provide little value, recommendations for additional funding in areas that provide high
value, and recommendations for funding in areas that are not equitably represented.
The primary theoretical construct of this research is web analytics, which is the measurement, collection,
analysis, and reporting of Internet data for the purposes of understanding and optimizing Web usage (Jansen,
2009). Although Web analytics cannot measure motivation or satisfaction, it provides a method to evaluate online
behaviors, correlate usage with other available data, and standardize information across data sets (Jansen, 2009),
including financial (Jansen and Molina, 2006; Ortiz-Cordova and Jansen, 2012). One of the contributions from
this research is the creation of advanced web metrics based on combining raw numbers such as JR1 frequencies,
cost data, and funding areas. Creation of these advanced metrics will help evaluate how successful the library is at
meeting the goal of providing access to valuable content. The statistical model used in this research is Analysis of
Variance (ANOVA), used to quantify the variance of the means among categories. In this research it is used to
quantify the variance of usage among funding areas.
2.2 Research Question and Hypotheses
Research Question: Is there a significant difference in the cost and usage among the various funding
areas (i.e., titles)?
The larger research question aims to quantitatively answer if differences in cost and usage affect topical
funding areas. If it is true that there is a significant quantitative difference among these categories, this will
illustrate the need to evaluate journals based on a funding area, as opposed to considering all journals at one time,

16

for a more equitable evaluation; however, if institutional goals are money savings, then a holistic perspective may
be of more significant than user- or funding-area equity. If your institutional goals are to prevent negatively
impacting your users, then maybe looking at funding areas is the better approach. By answering this question, we
can create KPIs to tie the institutional goals to metrics for a more specific evaluation.
Our research site allocates purchases of online content via a mechanism known as topical funding areas,
which is a method of aligning purchases to the various academic areas of the university. To investigate our
research question, we categorize cost and usage into ten funding areas (listed in Table 3) as provided by the
library financial reports to indicate where the funding for each resource comes from. We then compare the cost,
titles, and usage among each of these funding areas. We then gathered cost, title, and usage data for a subset of
electronic resources from a major research university library. While not all universities libraries use an exact
funding method, a similar approach could be implemented with similar funding schemes that seek to align online
content purchases to academic areas.
Funding Area
Abbreviation
Arts & Humanities
A&H
Business
Bus.
Earth and Mineral Sciences
EMS
Education
Ed.
Engineering
Eng.
Life Sciences
Life Sci.
Physical and Mathematical
P&MS
Sciences
8. Social Sciences
Soc. Sci.
9. Nothing
Nothing
10. All
All
Table 3: Funding Area names and their corresponding abbreviation.
Hypothesis 1: Usage is unevenly distributed among the ten funding areas based on the least used third (33%)
1.
2.
3.
4.
5.
6.
7.

titles of all journals.
We define usage as the number successful full-text article downloads from a particular journal in 2012,
which is of primary importance to measure a journal’s demand. This analysis uses an ANOVA test to measure
the variance of usage among the funding areas to determine if the usage is unevenly distributed within the ten
funding areas.
Hypothesis 2: Cost is unevenly distributed among the ten funding areas based on the least used third (33%) of all

17

journals
The cost of these journals is of primary importance in evaluating the journals a library subscribes to on an
annual basis. Ranking the journals in order of usage and then taking the least used third of journals can mitigate
some presumed bias in spending based on funding areas. Additionally, this analysis may offer the ability to find
funding areas that are true outliers and that appear to have very high costs.
Hypothesis 3: Usage is unevenly distributed among the ten funding areas based on the least expensive third
(33%) of all journals.
The importance of usage in evaluating journals has been established; here we analyze if there is a
significant distinction in the usage of the funding areas based on the least expensive journals. Ranking the
journals by price in this analysis normalizes some of the disparity in price among the journals and will provide
insight to see if that price normalization within the funding areas leads to parity in usage among the funding areas.
Hypothesis 4: Titles are unevenly distributed among the ten funding areas based on the least expensive third
(33%) of all journals
Classifying the journals by funding area and ranking in order of most expensive to least expensive within
each funding area should create a level of uniformity within each area of spending. There may be an assumption
that ranking journals by price within each funding area will eliminate any of the parity that exists between the
funding areas. We chose to present results in terms of top third, middle third, and bottom third for ease of
presentation and consideration of space, while still retaining the ranking and prioritizing inherent in the
methodological approach versus discussing thousands of journals. Naturally, in an actual university library
implementation, access to the full ranking of journals would be available.
Data from these four hypotheses will provide a contribution to the broader research question (“Is there a
significant difference in cost and usage among the different funding areas?”). Affirmation of this question will
suggest that an analysis of journals (based on usage or cost) must be done within the classification of funding
areas because there is significant variance of usage and cost among the funding areas.
2.3 Methods, Data Preparation, & Analysis
Data Collection Site

18

Data was collected from a large tier 1 research university with more than half a billion dollars in annual research
funding, 17,000 faculty, and 100,000 students. The library for this university is in the top 10 Association of
Research Libraries (ARL) library investment index (frequently referred to as “ARL ranking”). This library
system consists of 36 separate locations and nearly 5 million cataloged items. In 2012, there were over 4.5
million successful full-text downloads of electronic resources from this library.
Data Collection Cost
The first step was to get a list of the electronic resources the university library purchased in 2012. One
would think that this data is simple to access. It is not; instead, it is one aspect of the complexity of measurement
in the library electric resources domain. That data is housed in a software package called Workflows, which is a
Library Integration System (LIS). The data generated for this study was a list of all active payments for the fiscal
year 2011-2012. Active payments are subscriptions that are paid for based on an annual charge. For this research,
there is no inclusion of one-time payments that may have been made during the 2011-2012 fiscal year. Free
electronic resources are not included, as some back files (journals from previous years) are provided for free, and
those are not specifically included from current year downloads. Publishers have yet to determine how to
provide, or they do not make available, logging data on the publication year for a journal, so it is not possible to
measure journal access by publication year; for example we cannot if distinguish Journal A published in 1998 was
accessed 100 times in 2012 or if Journal A published in 2008 was accessed 500 times in 2012; we can only
distinguish that Journal A was accessed 600 times in 2012.
The fiscal calendar adds another layer of complexity as the university library financial year goes from
July-June, but vendor contracts typically go from January-December. In some ways, this may actually be helpful,
as the library plans their budget for the upcoming year in advance of having to make annual subscription deals.
For example, in July of 2012, the library knows how much money the department should have to spend on
subscriptions in January of 2013. This layer of complexity may make for easier planning than if the budgets lined
up exactly, as it would be hard to decide what journals should be purchased for January of 2013 if the amount of
funds available was not granted until that same time. On the other hand, it does provide confusing analysis when
incorporating the cost of resources as contracts with providers for journals are not stagnant. In this example, a

19

library pays the provider $10,000 for a journal in January of year one, and in January of year two, the price goes
up 5% to $10,500. If the library has no prior knowledge that the journal price is going to increase, then they
cannot accurately plan. Additionally, the advanced knowledge of an increase would have to be more than six
months in order to get budget requests in on time. Even if there is prior knowledge that the cost will go up, there
are only six months of year-one data to analyze for spending in year two. This temporal discrepancy can
complicate a cost analysis.
The list provided for the annual subscription data contains the following information: purchase order ID,
journal name, ISSN, database that indexes the journal, provider (who the journal/database is purchased this from),
cost (annual dollars spent), notes (this will contain information to indicate if the purchase is part of a package or
how many titles are included in the item). For example, Table 4 gives a snapshot of a spreadsheet. The top two
rows, Mechanisms of ageing and development and Ageing research reviews, are examples of how a journal
package may be grouped together. The third row displays a simpler example giving a journal an exact price and
ISSN number to match on for JR1 reports.
Order
ID
PO1234

PO1235

Journal Name

ISSN

Database

Provider

Cost
$247.05

Mechanisms of
ageing and
development

-

ScienceDirect

Elsevier

Ageing research
reviews

-

ScienceDirect

Elsevier

0094-5765 ScienceDirect

Elsevier

Acta astronautica.

Notes
Ageing Purchased
together
Ageing Purchased
together

$4991.57

Table 4: Example of how data is displayed on spreadsheet to determine what may be considered a package
deal and what may be considered a line item purchase.
Each line item in this spreadsheet represents a payment that was made and any number from 1 to 20,000
titles that may be available because of the purchase for that resource. This is how the payment information is
shared for the 3,474 items that represent over 100,000 titles and $10,193,706.96 ($10.2 million) in 2012
subscription purchases.
Data Collection Usage

20

Capturing the usage data from external sources was an additional stream (or streams) of data. The journal
reports that provide information on successful full-text downloads (JR1) are provided by the vendors of those
journals in separate spreadsheets. The next step of data collection was to gather 34 of these spreadsheets (where
available) and link the usage information in these spreadsheets with the cost information from the internal
spreadsheets into a database. Linking this information provides an easier mechanism to sort, search, browse,
aggregate, and view data, and also allow for more robust analytics such as cost-per-use based on these raw
numbers for cost and usage.
In order to link this information --internal data provided from financial reports (See Table 5) and external
data provided from journal usage reports (See Table 6), it was necessary to import the spreadsheets to
corresponding database tables to simplify further manipulation. The parsing of these spreadsheets to insert the
data into a MySQL database is done via the Ruby programming language. Ruby was selected because the Ruby
on Rails web framework provides an easy method to create web pages on top of our data model.
Column Name
Column Type
PO
String
Funding_Area
String
Has_JR1
Boolean
Found_by_issn
Boolean
Name
String
ISSN
String
Platform
String
Publisher
String
Provider
String
Price
Decimal
Cost_per_use
Decimal
Total_download
Integer
Pdf_download
Integer
HTML_download
Integer
Citations
Integer
Authorship
Integer
Notes
String
Titles
String
Table 5: MySQL format for Journal table within database used to import data from spreadsheets for
financial data to link with JR1 reports.
The university library has a business agreement with Serial Solutions for several library services, one of
which is to aggregate a portion of patron’s usage of the library’s electronic resources. Publishers who provide
access to the journals store the usage information in a JR1 format according to the standard set by COUNTER. A

21

librarian with authorization to Serial Solutions can log on to that administrative web site and download any JR1
report that has been obtained from a provider within the past year. Serial Solutions is only able to obtain
information on journals if the provider has formatted the usage statistics into JR1 format; if the data does not
comply with this format, then data is not downloaded. Serial Solutions provided 34 JR1 reports for 44,833
journals in 2012; however, some data was incomplete, only containing usage statistics for half the year. In these
cases, an average was created based on the usage for the number of months that data has been provided and filled
in the remaining months with that average. A Ruby Rake task, a script written in the Ruby programming
language, was developed to parse the 34 spreadsheets and insert it into a database table “JR1” (See Table 6) and
the internally held financial data into a table “Journal.”
Column Name
Column Type
Name
String
Publisher
String
Platform
String
ISSN
String
Total_download
Integer
Pdf_download
Integer
Html_download
Integer
Table 6: MySQL database table used to import the JR1 reports from spreadsheets into the database.
At this point, all 3,474 items paid for and 44,833 JR1 reports for 2012 were stored in separate tables
within the database. Storing the publisher supplied JR1 reports and the locally supplied financial reports in the
database allowed for easier manipulation, faster querying, and more dynamic displaying and filtering of the
information as opposed to a relatively static spreadsheet.
Further work had to be done in order to link the external usage data to the internal financial data. The
method used to link the items the library actively paid for in 2012 and the JR1 reports from 2012 was to loop over
each of the 3,474 items and see if there was a matching item in the JR1 report where a match was determined first
by ISSN, and if that did not provide a match, then journal name was used. Once the financial data was linked to
the usage data, we could start to evaluate both cost and usage based on internal categories (i.e. funding area) and
begin to create more advanced performance metrics such as annual price/annual downloads = average cost per
user (CPU). Furthermore, there was now a web interface (See Figure 2) enabling easy search, browse, and sort
capability of all this information.

22

Figure 2: Screenshot of filtered results being displayed on the web page, for the ScienceDirect platform
from Elsevier.
Data Selection
Rather than investigate the entire online content collection for the library, we chose a substantial subset
that also facilitated rapid verification of the model. The subset of electronic resources used was the 1,074
electronic resources from Elsevier’s ScienceDirect database. This data was chosen because these journals
represent over $2.8 million dollars in annual subscription fees (nearly 30% of all annual subscription fees for this
library), received nearly one million downloads in 2012 (995,126), and have titles funded from all ten areas listed
in Table 3.
In order to find an impactful scope for evaluation, consideration was given to provider, cost, titles (the
number of titles from a provider), and usage or full-text downloads. Having collected all of the data on titles,
usage, and cost, and linking those disparate data sources together created the ability for us to collate that data (See
Table 7) and breakdown the most prolific six providers (in terms of titles and cost); the remaining providers are
grouped together as “Other.” Table 7 shows the line item numbers (the number of purchases made in 2012) and
the estimated total titles from provider. The distinction in these columns (line items and total titles) is that one line
item may account for several titles, and in some cases a line item may account for over 10,000 titles. This
ambiguity creates a layer of complexity in analyzing these items. As mentioned with aggregations providing
access to large numbers of titles, it is difficult to get the total items from a company like EBSCO or ProQuest
because those titles fluctuate. Additionally, the invoice from ProQuest does not contain a listing of all of the
journals purchased; because of the number of titles that may change and the inability to determine the titles an
institution has access to at a given time, it can be extremely complex to evaluate providers like EBSCO and
ProQuest.

23

As a first study, this analysis aimed for a low number of line items with multiple titles while still
maintaining impact from cost, usage, and titles. In order to achieve this simplicity, it required a further
understanding of the relationship between the number of titles and line items. The items that have one title on a
single line provide an easier method to get data specific to that title because the cost of that title is typically listed
right on that line, as opposed to a database that costs $5,000 with 1,000 titles and requires not only an estimation
on cost for each title but also requires additional work to look up the titles that may not be precise. Finally, the
total payment represents the annual payment made to the provider for the subscription content in 2012.
Provider (Invoice from) Line Item Numbers Estimated Total Titles
From Provider

Total Payment

Elsevier

1,081

1,219

$3,158,313.30

EBSCO

1,225

22,378

$1,978,703.80

604

604

$976,894.79

38

12,535

$675,920

Springer-Verlang

2

1,330

$195,342

Wiley-Blackwell

8

775

$48,489.34

516

69,123

$3,160,043.81

3,474

107,964

$10,193,706.96

Harrassowitz
ProQuest

Other
TOTALS

Table 7: Grouping of items purchased in 2012 and the number of titles those purchased items cost.
Elsevier accounts for over 30% (30.9%) of the annual spending on subscriptions and within that,
Elsevier’s ScienceDirect package ($2,835,149) accounts for nearly 90% of the Elsevier subscription fees in 2012
and over 1,000 line items. Based on this preliminary analysis, the decision was made to focus on a subset of data
within Elsevier --the journals that are a part of the ScienceDirect package for this study. The journals purchased
via Elsevier are indexed in the ScienceDirect database, and nearly each title has a corresponding cost. There are
three examples of Elsevier purchase types (See Table 8): 1) PO-1 is a line item with a single cost, 2) PO-2
displays a package deal with many titles being assumed for one price, and 3) PO-3 shows a database of over 100
titles that has a single price. The most common line item in the ScienceDirect package from Elsevier is similar to
PO-1, providing the simplest method of analysis because the title is directly linked to a cost.

24

Packages like the HPS Combination (See Table 8) show how some line items are part of a package deal
where the university spends $N for X journals. In this case, the method used in this study to determine the cost of
each title is to divide the cost proportionally among the listed titles. For example, in the HPS Combination, the
total cost of $1,392.28 is divided by 3 (the number of journals in the package), and each journal assigned a new
price of $464.09 per journal. Elsevier has 19 packages similar to the HPS Combination package. These packages
account for 47 titles, which is a small subset (4.4%) of the ScienceDirect titles from Elsevier. Likewise the
financial impact is similar as these package deals account for $170,326.37 (5.39%).
The Elsevier journals also more precisely match the cost data to the usage data than (perhaps) an
aggregator with thousands of changing titles. In this analysis, more than a majority of the time, the ability to
match ISSN instead of matching with the journal title adds a higher level of precision when linking the JR1
reports to the financial data as 887 of the 1074 (82.6%) Elsevier items purchased in 2012 can be matched via
ISSN. If no match was made either via ISSN or title, the journal was considered to have no usage. This is in
partially because of the matching ISSN and because many of the Elsevier line items are for a single journal and
not for database packages, which in some cases may consist of thousands of titles.
Order
ID
PO-1
PO-2
PO-2
PO-2
PO-3

Journal

ISSN

Database

Provider

Cost

Notes

Social Science &
medicine
Studies in history and
philosophy of
Science. Part A
Studies in history and
philosophy of
Science. Part B
Studies in history and
philosophy of
Science. Part C
MD Consult

02779536

ScienceDirect
Journals
ScienceDirect
Journals

Elsevier

$5,916.25

Elsevier

$1392.28

ScienceDirect
Journals

Elsevier

HPS
Combination

ScienceDirect
Journals

Elsevier

HPS
Combination

HPS
Combination

MD Consult Core
Elsevier
$98,000
139 Titles
Collection
Table 8: Example of cost data on journals showing a journal with a single line item in PO-1 and a package
deal with multiple journals splitting up the cost in PO-2.

25

The overview of funding areas for all the journals within the ScienceDirect database
illustrates a disparity on particular funding areas; however, all funding areas are represented from
the perspective of titles, usage, and cost within the funding areas to analyze the funding areas
separately. Arts and Humanities can be considered an outlier in this data selection because of the
significance in the disparity in journal numbers represented (See Table 9).
Funding Area
Titles
Usage
Cost
All
1074 995,126 $2,832,995
Arts & Humanities
5
1,904
$4,400
Business
59 41,048
$87,772
EMS
122 133,733
$363,560
Education
41 52,946
$45,542
Engineering
133 91,487
$393,861
Life Sciences
212 225,885
$554,908
P&MS
143 148,902
$763,511
Social Sciences
33 24,728
$30,778
No Funding Area
326 274,493
$588,663
Listed
Table 9: Listing of funding areas and their representative titles, usage, and cost in 2012.
Data Categorization and Ranking
Once we selected the Elsevier’s Science direct package, we had to categorize the journals
based on the funding area (See Table 3). The “All” funding area is distinct because it represents
all the journals from the ScienceDirect database without regard to the categorization of funding
area. This was done to see what funding areas (if any) shared means with a more holistic
approach that disregards funding area. For example, if our analysis shows that all of our funding
areas do not have a significant difference from the “All” category, this could imply that the
funding area distinction is not truly providing us with any valuable classification. Conversely, if
the funding area “All” is significantly different from other funding areas, this is further evidence
that any examination of journal price and/or usage should account for this categorization of
funding area. “No Funding Area Listed” indicates journals that are purchased within funding
areas indistinguishable to a particular discipline category like the others (e.g., Arts & Humanities,
Business, Education, etc.).

26

In the first two hypotheses (H1 and H2), the titles are ranked by usage, and the next two
hypotheses (H3 and H4) the titles are ranked by cost. In H1 and H3, the independent variable is
usage, and in H2 and H4, the independent variable is cost; in all four hypotheses the one
dependent variable is titles (i.e., titles within a funding area). This creates a comprehensive view
of the discrepancy of both usage and cost among titles when ranking by either usage or cost. In
each case, the data is ranked appropriately (most expensive or most used to least expensive or
least used) and then divided into thirds: the most expensive (highest 33.3% by cost), middle
expensive (next highest 33.3% by cost), and least expensive (lowest 33.3% by cost), and likewise
for usage/full-text download data (highest 33.3%, next highest 33%, and lowest 33% by usage).
The data used for comparison among the categories was the least expensive and the least used
because it was best demonstrated normalizing the data to the lower third eliminates any of the
variance that may be caused by the more heavily used or more expensively priced journals.
Additionally, ranking by both cost and usage and comparing the variance of cost and usage can
further illustrate any parity that may be created by ranking and comparing the same factor. For
example, some may assume that the variance of mean cost is very little among categories for the
less expensive journals and that the variance of usage may be little among the categories for the
lesser-used journals. The design of this analysis prevents this bias by testing both areas in both
rankings.
There are some journals that we were not able to obtain usage stats for based on the
methods used to link journal usage data and our financial data. If a journal in a funding area did
not have usage data, it was eliminated from bottom third, and we selected a substituting journal
using the next journal on the list in ranked order.
2.4 Results
Once we had our data selected and categorized, we imported the data into Minitab, which
we used to run the one-way analysis of variance (ANOVA) test to compare the differences among
the variance means of funding area categories in usage and cost, ranking separately by both usage

27

and cost. For all ANOVA tests represented, the critical value of p was 0.05. We also used
Tukey’s as the post hoc analysis to uncover where the differences in the categories existed.
H1 (“Usage is unevenly distributed among the ten funding areas based on the least used third
(33%) of all journals.”), and H2 (“Cost is unevenly distributed among the ten funding areas based
on the least used third (33%) of all journals.”) used data from ranking the journals by usage first
(See Table 10).
Funding Area
Titles
Usage Cost
All
340 75,243 $573,262
Arts & Humanities
2
284
$2,068
Business
19
3,868
$16,344
EMS
38
9,523
$71,080
Education
15 11,560
$6,623
Engineering
42
8,880
$88,951
Life Sciences
69 20,174 $121,562
P&MS
44
6805 $134,609
Social Sciences
11
2464
$8,457
No Funding Area
104 22,534 $102,719
Listed
Table 10: The bottom third of journals based on usage by funding area in 2012.
In regard to H1 (“Usage is unevenly distributed among the ten funding areas based on the
least used third (33%) of all journals”), the ANOVA results indicated that there is a significant (p
<0.001) difference among funding areas in usage. The post hoc analysis (Table 11) shows a
significant difference among multiple funding area groups. In fact, the post hoc analysis shows us
that Education is significantly different from all other funding areas in terms of usage among the
bottom third of journals within that funding area. Additionally, Life Sciences do not have a
significantly different mean from two three other funding areas (Earth & Mineral Science, Social
Sciences, and Arts & Humanities). Arts & Humanities (as shown in Table 10) is somewhat of an
outlier to begin with because there are only two journals included in this analysis. Based on this
relatively small sample size from the Arts & Humanities, it is fair to say that Life Sciences are
significantly different from all but two funding areas.
However, this distinction is not necessarily a negative; the mean usage for even the
lowest third used journals is significantly more than the mean usage for the lowest used journals

28

in other funding areas. Education in particular, and, to a lesser extent, Life Sciences, Earth &
Mineral Sciences, and Social Sciences all have a significantly higher mean usage for even their
lowest-used journals when compared with the lowest used journals from the other funding areas.
Physical & Mathematical Sciences (P&MS), outside of Arts & Humanities, represents the lowest
mean usage within their least used journals with an average of 154.66 full-text downloads per
journal in a year. This proves that even when ranking journals by usage and taking the least used
third of journals within each funding area, there is still a significant statistical difference in the
usage of these journals among the funding areas, and thus, H1 is fully supported.
Funding Area
Grouping
Mean Usage
Standard Deviation
Education
A
441.53
196.73
Life Sciences
B
292.38
117.58
Earth & Mineral
B C
250.61
125.71
Science
Social Sciences
B C D
224.00
80.56
All
C
221.30
89.51
No Funding Area
C
216.67
79.08
Listed
Engineering
C D
211.43
56.05
Business
C D
203.58
81.79
P&MS
D
154.66
92.16
Arts & Humanities
B C D
142.00
43.84
Table 11: Post hoc Analysis for usage by Funding Area. Means that do not share a
Grouping letter are significantly different (p < 0.001)
For H2 (“Cost is unevenly distributed among the ten funding areas within the least used
third (33%) of all journals.”), the ANOVA results demonstrate that there is a significant
difference (p<0.001) between the costs of the ten funding areas ranked by usage from each
funding area. The post hoc analysis (Table 12) displays the groups where the significant
difference exists; Physical & Mathematical Science (P&MS) is not significantly different from
Engineering, Earth & Mineral Sciences, and Arts & Humanities; however, Arts & Humanities has
such a small sample size (2 journals) that, practically speaking, it would make sense to say that
P&MS is significantly different from all but 2 funding areas. Additionally, P&MS has the
highest mean cost for journals that are in the lowest third of journals in this funding area by
usage. P&MS was near the lowest means (Table 11) for usage and highest means for cost (Table

29

12) in this evaluation. There were fewer groupings in cost than there existed in usage; however,
there is a significant difference in means, which confirms H2. There is a distinction in both usage
and cost among the funding areas when ranking the journal titles by usage and evaluating the
least used third (33%) of journal titles; however, when ranking the titles by usage, the larger
difference within the funding areas is in journal title usage than journal title cost.
Funding Area
Grouping
Mean Cost
Standard Deviation
P&MS
A
$3,059
3,699
Engineering
A B
$2,118
1,175
Earth & Mineral
A B C
$1,871
1,204
Science
Life Sciences
B C
$1,762
1,207
All
B
$1,686
1,831
Arts & Humanities
A B C
$1,034
552
Nothing
C
$988
913
Business
B C
$860
515
Education
B C
$771
382
Social Sciences
B C
$769
322
Table 12: Post hoc analysis for Cost by Funding Area. Means that do not share a Grouping
letter are significantly different. (p < 0.001)
The following analysis for hypotheses three and four (H3, H4) changes the mechanism
for ranking journals. The journals are still classified by funding area, but then ranked by price
(Table 13), focusing on the least expensive third (33%) of the journals by funding area and
analyzing the differences (if any) based on the funding area classification.
Funding Area
Titles
Usage
Cost
All
340
248,128
$224,807
Arts & Humanities
2
428
$1,044
Business
19
6,352
$12,359
EMS
38
15,857
$42,913
Education
15
6,623
$11,560
Engineering
42
15,667
$54,539
Life Sciences
69
58,143
$60,494
P&MS
44
19,045
$70,301
Social Sciences
11
7,195
$5,585
No Funding Area
104
72,487
$36,590
Listed
Table 13: The bottom third of journals based on annual subscription price paid for the
journal in 2012 by funding area.
Regarding H3 (“Usage is unevenly distributed among the ten funding areas based on the
least expensive third (33%) of all journals.”), the ANOVA results indicate a significant difference

30

(p = 0.021 < .05) among the mean usage for the ten funding areas. The post hoc analysis (see
Table 14) used was the Fisher’s individual error rate to find the differences among the categories.
This analysis exhibited four different groupings among the funding areas and confirms that the
distribution of usage within these funding areas is different and fully supports H3.
Funding Area
Grouping
Mean Usage
Standard Deviation
Life Sciences
A
871.6
1141.5
All
A B
728.8
941.8
Nothing
A B C
725.4
869.7
Social Sciences
A B C D
654.1
551.5
Earth & Mineral
B C D
463.8
333.6
Sciences
Education
A B C D
441.5
196.7
P&MS
C D
434.2
676.6
Engineering
D
400.9
428.5
Business
B C D
334.3
325.0
Arts & Humanities
A B C D
214.0
145.7
Table 14: Post hoc analysis for Usage by Funding Area. Means that do not share a
Grouping letter are significantly different. (p < 0.001)
Regarding H4 (“Cost is unevenly distributed among the 10 funding areas within the least
expensive third (33%) of all journals.”), the ANOVA results indicate there is a significant
difference (p<0.001) between the cost means of the least expensive 33% of journals from the ten
different funding areas. It is interesting to note that even ranking the journals by cost, there is still
a significant difference in cost between the funding areas. The post hoc analysis (see Table 15)
shows a significant difference between Physical & Mathematical Sciences (P&MS) and all other
funding areas in cost. This indicates that even the least expensive journals in Physical &
Mathematical Sciences are significantly different from those of other funding areas. Engineering
and Earth & Mineral Sciences share a grouping with Arts & Humanities, again, though Arts &
Humanities can be considered an outlier because they represent such a small percentage of the
sample in journal titles and annual subscription cost. It is interesting to note that the No Funding
Area Listed category has the lowest mean cost among journals within this classification, which
may indicate that other funding areas could benefit from appearing less expensive if these
journals were appropriately categorized. The difference among these funding areas supports H4,

31

which is that the cost is unevenly distributed among the funding areas within the least expensive
third of journals from those funding areas. There is a distinction in both usage and cost among the
funding areas when ranking the journal titles by cost and evaluating the least expensive third
(33%) of journal titles; however, when ranking the titles by cost, the larger difference within the
funding areas is in journal title cost than journal title usage.
Funding Area
Grouping
Mean Cost
Standard Deviation
P&MS
A
$1,615
757.8
Engineering
B
$1,308
308.1
Earth & Mineral
B
$1,171
525.2
Sciences
Life Sciences
C
$920
344.0
Education
C D
$770
381.9
All
D
$661
305.0
Business
C D
$650
110.0
Arts & Humanities
B C D E
$522
171.7
Social Sciences
D E
507.8
110.3
No Funding Area
E
368.0
213.9
Listed
Table 15: Post hoc analysis for Cost by Funding Area. Means that do not share a Grouping
letter are significantly different. (p< 0.001)
2.5 Discussion
The data confirming the four hypotheses in this paper addresses the broader research
question (“Is there a significant difference in the cost and usage among titles for the various
funding areas?”). Yes, there is a significant difference in cost and usage for these titles among the
various funding areas and when examining the value of these journals this classification (a title’s
funding area) should be considered to meeting the institutional goals in regard to electronic
resources.
Knowing there is a significant statistical difference among the categories, it makes sense
to compare the accumulation of all nine individual categories (Arts & Humanities, Business,
EMS, Education, Engineering, Life Sciences, P&MS, Social Sciences, and No Funding Area
Listed) with the “All” category to see if there is a difference, practically speaking, with the cost
and usage of the least expensive and least used journals. The difference in content (journal titles)

32

is relatively negligible; there are 344 titles in the individual categories and 340 titles in the “All”
category.
For example, we can compare these categorizations to see what makes the most
significant impact if the institutional strategy goal was to save money and reduce expenses on
electronic resources. Would the accumulation of all of the expenses saved by the nine individual
categories be comparable to the money saved when ignoring the funding areas and just eliminated
journals based on being the least expensive or the least used? There is a large distinction based on
how we rank the journals (See Figure 3), but there is a much smaller difference between the
dollars saved when looking at adding up all of the individual funding areas and comparing those
individual funding areas with the method that ignores the funding areas (“All”).
When ranking the journals by least used, the savings would be nearly doubled of that
realized when ranking the journals by least expensive. This would indicate that to save the most
money for the institution (if that were the goal) the best way to do this would be to rank journals
by usage and to eliminate the least used within each category. The “All” category would provide
an inequitable impact on users of various funding areas that it has ignored and would not yield
much in the way of increased savings over an evaluation on individual funding areas.

33

700000

Annual Subscription Dollars

600000
500000
400000
300000
200000
100000
0
Cost By (ranked by usage)
Cost (ranked by expense)
Category Expense
Sum of Individual Category

All

Figure 3: Compares the cumulative expense of the individual funding areas with "All",
which ignores funding area.
As is the case with expense, there is a statistical significant difference among the
categories’ usage when ranking by usage and cost. Examining the significance of the usage
difference between the summation of all nine individual funding areas (Arts & Humanities,
Business, EMS, Education, Engineering, Life Sciences, P&MS, Social Sciences, and No Funding
Area Listed) compared with the “All” funding area (see Figure 4), we can further understand the
practical impact between these methods of comparison. There is little practical difference in
usage between summation of the individual categories and the “All” category that ignores
funding areas completely. Again, the difference in content is negligible; there are 344 titles in the
individual funding areas and 340 titles in the “All” category.
For example, if the institution were trying to determine which method would impact users
the least, then ranking by usage and ignoring “All” may seem to be the logical decision; however;
keep in mind this decision may become completely inequitable among particular users that have a
large number of journals from a particular discipline that is funded by that area where removed

34

without consideration. Additionally, if we consider the significance of the difference in usage
between these two methods (86,000 downloads in individual categories, and 75,000 in the “All”
category), it is less significant. Over the course of the year, there are 11,000 fewer downloads
among 340 journals; this equates to about 32 downloads per journal title that we save by choosing
the “All” method instead of looking at individual categories. Given that there is a significant
statistical difference in usage for the categories, it seems it would be worth looking at usage by
funding area; although, the total impact on users would be larger (86,000 downloads compared to
75,000 downloads), so by analyzing the funding areas individually, the impact would likely be
impartial.

Successful Full-Text Downloads

300000
250000
200000
150000
100000
50000
0
Downloads (ranked by usage)
Downloads (ranked by expense)
Category Usage
Sum of Individual Category

All

Figure 4: Compares the cumulative usage of individual funding areas with "All" category,
which ignores funding area.
Based on the results of this analysis and affirming that these journals should be analyzed
at the individual funding area, it is interesting see to a breakdown of titles, usage, and cost (See
Figure 5) by funding area. Keep in mind that any journals without usage data were omitted from
this chart because we cannot accurately analyze that key component of value for those resources.

35

This visualization helps illustrate why inequity may exist if we do not consider funding area and
happen to be comparing journals from Physical & Mathematical Science (P&MS) with journals
from Arts & Humanities or even Education or Social Sciences. Furthermore, we can see that we
have more journal titles and usage associated with no funding area than we have associated with
any single funding area; this would indicate that there is precision to be gained in our evaluation
if we had an internal funding area to associate with those journal titles and their respective cost
and use.

Percent of Titles/Cost/Usage

35
30
25
20
15
10
5
0

Fund Area
Titles

Cost

Usage

Figure 5: Percentage (out of 100) for each funding area for cost, titles, and usage within the
ScienceDirect package for 2012.
This analysis concludes that there is a significant difference in usage and cost among
titles of various funding areas. The difference in these metrics (usage and cost) among funding
areas indicates that titles should be evaluated based on funding area categorization. For example,
it would not be equitable to compare the usage of titles in Arts & Humanities with the usage of
titles funded by Physical and Mathematical Sciences; evaluating journals without the funding area
classification would be comparing apples to oranges. In order to compare apples to apples, we

36

should compare the usage of journal titles within the funding area they were purchased – the
Physical and Mathematical Science journal titles in one analysis and the Arts & Humanities
journal titles in another analysis.
The focuses of titles analyzed in this model are journals with low usage and cost relative
to other journals in that particular funding area. The primary reason to concentrate on the lower
used and less expensive journals; is that the lesser used journals provide a potential cost savings if
they are used so infrequently that removing them from the collection would not affect library
users. If a library were faced with a need to weed certain journals from a collection, the least-used
journals would provide the first area for investigation. The ultimate goal is to positively inform
collection management and to not negatively impact our users. The least used 340 titles
cumulatively represent less than 8% of usage and over 20% ($573,262) of the annual cost of
journals in the ScienceDirect database. This representation helps depict a scenario in which these
low usage journals would be candidates of further discussion specific to their value to electronic
resources and the potential elimination for cost savings or substitution for more valuable
resources.
In each of these analyses, we eliminated any journals that had no usage data to report.
Nearly 95% of the journals in the ScienceDirect package had this data to create metrics for
evaluation; however, it is worth analyzing what categories have the highest representation of titles
and cost that has no usage data available. These journals provide no ability for any measurement
or evaluation within the community the institution serves, and they should have further
investigation to understand if there is a rational reason these journals do not provide usage data.
The 56 journals that lack this data cost a total $196,786, and we can see (Figure 6) the funding
areas that represent the largest portions of that cost. Additionally, we can examine which funding
areas represent an uneven distribution of titles or content that does not report funding data;
largely, this chart (Figure 6) indicates that the percentage of titles and cost within each funding

37

area are relatively similar. The outlier may be Engineering, which seems to represent nearly twice
the cost of titles without usage data.
35

Percentage

30
25
20
15
10
5
0

Fund Areas
Cost

Titles

Figure 6: A display of the funding areas and their representation within the total cost, and
content (titles) of journal with no usage data.
2.6 Implications
Theoretical Implications
The intention of this research was to provide the foundation for a framework to evaluate
electronic journal resources. There are several layers of complexity behind electronic resources
that create a difficult environment to do a thorough evaluation. This research extends the use of
cost, titles, and usage as metrics to be used not only to determine where there is real value but
also where there is content that does not seem to be providing value. A novel approach to
compare the impact of a holistic (“All” funding area) and the individual funding areas (Arts &
Humanities, Business, Earth & Mineral Sciences, Education, Engineering, Life Sciences, Physical
& Mathematical Sciences, Social Sciences, No Funding Area Listed) is discussed. The model can
be extended in further research to a larger sample of journals, and to databases and aggregations.

38

A study such as this can begin a discussion around what journals are no longer necessary
to include in library collections and what impact various methods of analysis will have on
funding, usage, and content as well as how they pertain to the mission of the institution
(negatively impacting users versus saving money). A further analysis can include metrics around
impact factor and local citations for titles within aggregations as well as journals, and other cost,
titles, and usage metrics. There has been significant work around the number of various metrics
used to evaluate journals such as impact factor, Eigenfactor, h-index variants, and, a more recent
metric, the author affiliation index (this measure relies on the basis that authors from prominent
universities do better research) (Rokach, 2012); this work can help create the foundation for a
model to pursue a more comprehensive method of evaluation. This work quantitatively shows
that any evaluation of journals should consider the funding area or the research area the journal
serves to more equitably distribute the analysis across the fields a university serves.
Practical Implications
One of the primary considerations given to journal selection should be determined based
on how easily we can evaluate the use of the journal. In order to effectively manage our resources
(i.e., titles) there must be knowledge about cost and usage, as with any practical application of
web analysis (Phippen, Sheppard, and Furnell, 2004). The size (both in amount of data and
number of titles) and complexity require that this be done in a streamlined fashion. The only way
to streamline this information is if it is aggregated in a singular way; for example, journals that do
not provide usage data through JR1 reports should not be considered for purchase unless there are
truly extenuating circumstances or unless this knowledge could be used to pressure the publishers
to provide usage data. The goal should be to figure out how to get all journals to report usage
statistics and not one of finding a reason for an exception.
On the internal administration side, consideration should be given to understanding the
items that are purchased without any indication for the purchasing funding area. An
understanding of the funding area and the usage data is something that can obviously benefit

39

libraries as well as providers because they can get a solid understanding of which journals really
do provide value to their customers and that may merit more resources toward publishing,
curating, etc.
Furthermore, the tool developed to perform this evaluation can be enhanced to perform
additional metrics for other subscription types like aggregations and databases to allow for a more
real-time and efficient evaluation of journals that is accessible to librarians and faculty members,
so they can all see the data that is driving these decisions.
2.7 Conclusion
The ideal state to reach for research libraries is one where there is an analytical
framework for each library to evaluate their electronic resources in order to determine value and
efficient use of resources. Libraries need to manage their own journal usage and understand their
own researchers; they cannot rely on results from global metrics (such as impact factor) or studies
that have been done at other universities to determine the selection effective journals at their
university library. This analytics framework also allows libraries to analyze not only the journals
but also the accuracy of various methods and determine what methods illustrate similar value
within journals. The methods presented in this research may lead to a greater credibility for those
methods or at least greater consistency to evaluate journals.
This paper quantitatively illustrated the need to categorize journal evaluation. These
electronic resources provide a tremendous value to the community they serve; the research done
at many large institutions relies on the access provided to these journals. This analysis
quantitatively shows that when analyzing journals, the accuracy and equity can be increased by
categorizing the journals based on the area of research they serve. The high value these resources
provide is precisely why they need to be evaluated and analyzed to accentuate those of particular
importance and to re-evaluate the necessity of those that either are not used or provide no
mechanism for further evaluation. In order to perform this analysis, we began to create an
application to automate some of the processes to integrate the disparate data sources that

40

aggregate information on cost and usage. Future research includes applying our methodology to
the entire dataset of an institution across all publishers, which will require some cross-publisher
leveling and metric alignment on which we are working. We are also going to examine search log
analysis (Jansen, 2008) for insight into article demand and future content. Also, enhancements to
this application to make these processes repeatable based on annual data and collating additional
data points such as citations, impact-factor, cost-per-use, articles, article influence score, etc. are
being explored. An application like this can aid in quantitatively measuring journals that should
be considered outliers based on certain criteria and warrant further investigation to make digital
collection administration of electronic resources a more manageable task.

41

Chapter 3
The increase in spending and the limitation in budget has created an environment within
university collection management that requires an understanding of the value of electronic
content. Leveraging various statistics and metrics, libraries continue attempting to improve their
ability to measure the usage of resources; however, most of these measurements are descriptive
(Coughlin, Campbell, & Jansen, 2013). If libraries could combine the descriptive approach with
other statistics to create a model that would predict value, universities could be better equipped to
enter negotiations with resource providers, understand how much they would be willing to pay
for a resource, debate financing, and defend budgets. Developing this predictive ability for
university libraries is the focus of this research.
Measuring the value of electronic resources (e.g., journal articles, conference articles,
etc., for which we use the term ‘journals’ for brevity) can be difficult from the outset because
there is debate over which metrics illustrate value. For example, a citation is one such metric.
Many find value in a citation because it indicates that an article was not only read but that it was
also considered useful enough to cite; however, citations can be problematic due to normalization
issues. These problems can include conferences or journals requiring (silently or otherwise)
citations from their own publication in order for articles to be published, and/or journals with a
large number of articles are likely to have more citations in a year than a journal with fewer
articles. Journals among different fields also have different citation rates. Additional problems
regarding citations are that they do not tell the whole story; for example, someone may read one
or two articles that lead to an eventual article that arrives at a citation. The discovery of an article
is not accounted for when using citations alone to measure value. Furthermore, the context of a
citation is not available when judging its impact. For instance, an article may be cited as a way to
show what is wrong with a particular method or analysis and yet the citation alone is considered
of equal value to an article that is cited as exemplar.

42

Publishers provide journal metrics, like citations, or related metrics that can be derived
from those provided by publishers. We call these metrics global in the sense that they measure for
the entire distribution of a journal across many institutions. The assumption these metrics make
is that what is valued globally is a reflection of what is valued locally at an individual institution.
This assumption that local will reflect global is an overgeneralization and oversimplification of
what resources will provide value to a local institution.
In addition to global metrics, we propose the development of local metrics, which are
measures concerning online content dependent only to the institution being evaluated. To our
knowledge, there has been limited use of local metrics on any systemic scale beyond simple
counts such as downloads. This situation with a dearth of local metrics exists for good reason – it
has been extremely complicated to determine value at the local level for online content.
Historically, the library has been a steward for aggregating content from many sources to create a
collection of knowledge. Many of the numbers that exist to create local metrics are stored in
disparate systems (i.e., Web of Science, Journal Citation Reports, Journal Reports 1 (JR1),
financial databases) and in different formats (i.e., spreadsheets, SQL databases, PDF documents).
A similar problem exists to measure the value within these resources because the information is
from multiple sources and formats and requires a great deal of curation to understand the value a
journal has at a local institution. The difficulty, scale, and multiple digital formats have created a
complexity to aggregating this data for online content locally, and this issue currently remains
unsolved. Much of the limited prior work that has been done has been descriptive and even this
prior work has relied primarily on global metrics.
The goal of this research is to advance the knowledge within collection management and
create a model to predict journal downloads at an individual library using a combination of global
and local metrics in order to identify the value of journals prior to a purchase or to other content
management decisions. This paper will examine global and local metrics to determine those that
have a strong correlation to local article download numbers. A more comprehensive

43

understanding of the metrics that are strongly linked to local downloads will help provide insight
to the metrics that should be more widely considered when evaluating journals and trying to
determine their value.
Downloads, while problematic because they only show behavior of a user, and lack user
motivation or user satisfaction with what they have “used”, have been an important benchmark to
measure demand and subsequent value within electronic resources (Metz, 1992). Downloads
give a broad perspective on the value a journal may have to a collection of electronic resources
because we know this journal provides articles that are “used” in a measurable way. Presumably,
there is a reason these journals receive a large number of downloads, and there is an assumption
that a percentage of those used downloads either end up becoming referenced articles or
providing insight to an article that leads to a citation. From a collection management perspective,
metrics based on downloads (i.e., cost-per-download) is a de facto standard to analyze some level
of value provided by a journal.
By local usage, we take a broader approach than has typically been addressed in prior
work. Examining local metrics, for example, how often a journal is cited by researchers at an
institution in a year, will provide an appreciation for certain content and how an institution’s local
citation aligns with global citation numbers. Investigating both global metrics and local metrics,
we can determine the power in creating models that combine information from various data
sources and understand the local impact of these global numbers. By creating this model with a
training set of data, we can then apply that model to a real world test data set to evaluate the
accuracy of this model in predicting downloads. The assumption is that by providing a prediction
for downloads, we can then provide an acceptable range of value for that content, which the
library may then use to make decisions such as the price the institution may be willing to pay for
that content. The knowledge created from this model would equip libraries with valuable
information as they negotiate contracts with providers and ensure they meet one of the
institutional goals of providing resources at good value.

44

3.1 Literature Review
Importance of Evaluating the Value of Journals
Prior literature has numerous discussions about the importance of evaluating the
significance of electronic journals. There are two fundamental reasons for these evaluations at
research libraries regarding content management, which are: 1) measurement of the institution’s
collective research output and 2) effective management of the electronic resources to make sure
the needs of the library’s researchers are being met.
This research focuses on the latter journal evaluation with the intention of better
collection management, but, for the sake of comprehensiveness, mentions the former, measuring
the collective quality of research output and providing four examples of measurement as
perceived by journal quality. These examples include: 1) institutional rankings can be based on
the quality of research done by the faculty of that institution, and research quality can be
measured by the perceived journal quality; 2) matriculation decisions may be based on the quality
of research done by faculty members; 3) promotional decisions within the faculty are typically
dependent on quality of publications; and 4) faculty journal submissions can be based on journals
with a perceived prestige (Chan, Chang, & Chang, 2013). There are a number of methods of
extracting meaning from the global metrics provided by commercial companies to libraries, and
algorithms using authorship, citations, or download numbers to create metrics to attempt to
measure journal value. Table 16 provides a categorization that we created linking organizational
goals to potential evaluations and how these evaluations meet the organizational goals. We
consolidate collection management analysis with metrics associated with the areas of content,
cost, and usage. For completeness, we include the institutional research output and evaluations
done to measure that organizational output.

45

Organizational Goal
Collection Management

Areas for evaluation
 Cost
 Content
 Usage
Institutional Research Output
 Rankings
 Student matriculation
 Faculty promotion
 Faculty submissions
Table 16: Breakdown of potential areas for evaluating electronic resources and the goals of
the organization that relate to this areas
In addition to measuring research quality, ranking journals provides a cogent method for
collection management. Collection management is “the set of activities intended to ensure that a
library’s internally held and externally provided resources meet the needs of its users, which leads
to weeding and acquisition” (Schwartz, 2000, p. 389). Collection management is a common
phrase used for the selection, acquisition, and analysis of items related to the needs of an
institution and the patrons (Johnson et al., 2012). Effective collection management practices
require the ability to look beyond the numbers presented by these metrics in order to define the
meaning within these numbers to fully understand the strengths and weaknesses of these metrics
and apply that meaning to their local institution to ensure the needs of the institution are being
met. Whether the reason behind the analysis of journal rankings is to understand the quality of
scholarly output from an institution, to manage cost, content, or usage within electronic resources,
it is clear that a method to determine the value of these journals is needed at an institutional level.
Evaluating Value via Content, Cost, and Usage
Content
The need to have a solid foundation of metrics to assist in collection management has
grown increasingly important in the information age in order to quantify the content an
institution has or should have access to. Not only has the amount of information increased, but
the accessibility and availability of scientific journals both past and present has increased. There
are more peer reviewed articles, unpublished manuscripts, conference papers, etc., and this deluge
of information emphasizes the need for key performance indicators (KPI) to substantiate the

46

importance of the online resources and to prevent the overabundance of information (Oosthuizen
& Fenton, 2013). KPIs “measure performance based on articulated goals” (Jansen, 2009, p. 3).
The ability to filter out electronic resources that limit value or function in serving the
demands of the researchers at an institution is paramount to effective collection management.
Prior to the existence of ubiquitous journal access, libraries used the circulation within their
physical walls to determine the demand and usage of content. This type of circulation
management was much simpler than the current methods that exist to track usage within the many
access points for an online journal. Currently, there is no standard method or set of metrics
existing for libraries to systematically or effectively make strategic recommendations on
electronic resource decisions within collection management (Lakos, 2007).
The sheer volume of journal titles is an important factor to consider because it brings into
question other facets around the content, such as quality, relevance, and usage. As an example of
the amount of content a library may provide access to, the institution in this study manages more
than 100,000 titles. The volume of available scholarly materials has led to questions about
quality, which ultimately questions the need for the library to provide access to these items
(Inger, 2001). Providing access to lower quality resources may also create a proliferation of
inferior work because less experienced students and researchers may not fully comprehend
distinct scholarly publication and believe that access within the library implies a certain standard
of work that is not realistic (Bartsch & Tydlacka, 2003).
Cost
Another emphasis of collection management, other than information abundance, is the
cost associated with these resources. Analysis of electronic resources related to cost considers
the matter of budgets, namely the source of the money to fund resources, price increases, and the
merit for the price increases (Miller-Francisco, 2003). The evaluation of electronic resources and
the significance of providing access to them has also created alternative methods for access, such
as Inter-library loan (ILL), to match the ease and speed of access at a fraction of the cost based on

47

the demand rather than an annual subscription fee (Leon & Kress, 2012). There is an additional
consideration that cost goes beyond the annual subscription dollars. There are costs, both human
resources and technical infrastructure, outside of subscription fees associated with the resource
allocation to consider in order to supply access to these items (Montgomery, 2000). Additionally,
the necessity of reducing and effectively managing content is compelling because of the
conditions surrounding library budgets and the scrutiny that they are under.
Usage
Aside from content and cost, a third need to evaluate within collection management is
usage. Usage is usually measured by successful full-text downloads of an article from a specific
journal and/or provider. Although not a panacea for assessment, downloads remain a popular
metric because of the simplicity in what they represent, namely successful full-text downloads of
articles for a journal in a given timeframe. Additionally, a download can be combined with other
metrics, like cost or content, to create more valuable KPIs for journals that can further assist in
regulating journals within collection management. Having a broad understanding of the journals
researchers are accessing via downloads indicates a perceived level of value and has a significant
impact on the collection management process (Medeiros, 2007).
However, there are some potential biases that exist by evaluating journals on full-text
downloads alone. For instance, some academic domains reference articles in their publications
more often than others. Subjects that require more references for published articles will naturally
provide more article downloads in those subject areas in order to produce those references. This
can skew the number of downloads when evaluating journals from different subject areas.
Nevertheless, full-text downloads are a foundational benchmark to determine value within
electronic resources and have been for years (Metz, 1992).
Evaluating Value via Global Metrics
Bibliometrics are used to measure and analyze citations, which in turn facilitates the
evaluation of journals. There is an inherent assumption in bibliometrics that a citation is a sign of

48

meaningful work. By evaluating these metrics further and by analyzing previous research where
these metrics have been used, we can understand their existing strengths and weaknesses and how
they can be leveraged together to create more meaningful metrics.
Global metrics include measures such as Impact Factor (IF), Eigenfactor, Five Year
Impact Factor (5YIF), and Article Influence Score. They measure varying degrees of citations for
journals and even articles. Impact Factor is the frequency an average article from a journal is
cited in a particular year. The benefit of IF is that it covers article citations for two years. By
covering two years of data, IF normalizes a potential substantial shift for a one-year period.
Another benefit is relevance as updates to journal rankings are maintained annually by ISI to help
to create a time-relevant standard metric (Togia & Tsigilis, 2006). Collection management
departments that annually evaluate journals for selection (and de-selection) have a time-relevant
number to use in their evaluation.
The Five Year Impact Factor provides an even greater degree of normalization of a
potential shift in citation numbers in a one or two year time period.
Eigenfactor, another citation-based metric that tries to eliminate the bias of self-citations,
has an openly available and non-proprietary method and algorithm for creating the score they
distribute to indicate journal value. There is significant meaning behind numbers like IF and
Eigenfactor because they create standard metrics that can be viewed and analyzed by the
collection management team at any institution. This standardization helps foster discussion about
these metrics and a more global understanding of the importance of these numbers and how to
approach them within collection management.
Article Influence Score is another global metric to increase understanding of whether the
value of a whole journal is, in actuality, based on a limited subset of popular articles published by
that journal. For example, does a journal receive a large number of citations simply because it
has a handful of popular articles each year, or do all the articles on average receive a large
number of citations from a single journal (Chan et al., 2013)? However, the value of Article

49

Influence primarily pertains to the importance of journals regarding tenure and research quality,
not collection management. Tenure review could evaluate based on the article level because it
identifies the specificity of output for an article and the subsequent researcher (Fersht, 2009).
These numbers that are based on the entirety of a journal, such as IF, perhaps should be of greater
consequence for collection management than tenure review. For the purpose of collection
management, a broader sense of how frequently any article is downloaded from that journal can
help justify the importance of an annual subscription. At times, these metrics are used to measure
the value of an artifact that they are not intended to evaluate. This does not diminish the value of
the metric, only the value of the implementation. Bibliometrics (IF, Eigenfactor, Article
Influence Score, etc.) provide a level of insight into a journal’s value because we know
researchers are using this work to publish their own research; cited work patently illustrates a
level of influence.
For large research libraries, JCRs only exist for a fraction of the titles they provide access
to, creating an uneven method of evaluation; therefore, by using global bibliometrics for journal
analysis, the evaluation is limited to a fraction of the journal resources that many large libraries
provide access to because only a fraction of these journals are recorded in the JCR. Reviewed
articles accumulate more citations than other articles, and journals that include those article types
can inflate citation counts and subsequently all metrics involving citations (Garfield, 2006;
Roberts, 2011). Some metrics try to devaluate self-citation; however, that does not eliminate the
bias that exists from publications that pressure authors to practice this (Arnold & Fowler, 2011).
Furthermore, some disciplines have more citations and publish more of these article types (e.g.,
review articles) that receive more citations. Multi-disciplinary journals in particular publish
articles with numerous citations (Yin, 2011). As citation numbers vary among disciplines, the
usage in the context of full-text downloads among these areas also fluctuates (Coughlin, Jansen,
& Campbell, 2014). By the same token, some disciplines do not use journals or conferences as
the primary mechanism of scholarly communication; therefore, citation numbers are not a good

50

indicator of the value within those outlets (Togia & Tsigilis, 2006). It is important to consider
these weaknesses or biases, as well as how we can apply and leverage local metrics in cases
where global metrics either do not exist or do not have equitable meaning across disciplines.
Collection management uses bibliometrics to facilitate decisions regarding electronic
resources and which of those resources are worthy of being included in a library’s electronic
resource collection. Currently, the existing strategies used to administer electronic journals need
to enhance their simplicity, affordability, and interoperability with other systems and products
(West et al., 2011). Creating an interoperable environment—where many of these metrics from
disparate systems (both global and local) could be aggregated to create more advanced metrics
and a more simple way to retrieve this data in one place—would elevate the strategies around the
administration of collection management of electronic resources. It would create the ability to
spend more time evaluating these resources, rather than spending so much time collating data.
The caveats that surround these metrics when used alone create reservations to using any single
metric or even group of metrics from a single report to evaluate journals and/or predict the value
they will serve to the local constituency. The fact that some metrics do not exist for all journals
makes it difficult to equitably analyze them, which necessitates creating metrics that can be
applied to local institutions. By understanding the limitations and strengths of these metrics, one
can more accurately apply meaning to these metrics within the resources their library provides
and can be more effective within collection management.
The results of these evaluations and prior works make it clear that these global metrics
are useful, but these global metrics could provide even further benefits if combined with local
institutional metrics. Journal rankings suppress some widely accepted and useful measurements
for performance; this indicates the need for a more broadly focused investigation to advise the
management of these resources and capture a multitude of qualities that cannot be diminished to a
single number (Rafols, Leydesdorff, O’Hare, Nightingale, & Stirling, 2012). It is necessary to
create a reliable, open, and interoperable network of this information to promote new and

51

enhanced metrics for evaluating scientific journals (Lane, 2010). Enhanced metrics, based on
combining global and local metrics, will help collection management better evaluate journals at
the necessary scale that exists in today’s digital age with the efficiency that is required in a time
of rising costs. Based on this review of literature and obvious need, we present the following
research objectives.
3.2 Research Objective
Research Objective: Predict the usage of a journal at an institution using global and local
metrics to inform institutional decisions concerning the value of an electronic resource and guide
collection management decisions.
This research objective aims to quantitatively measure the correlation between journal
metrics and the number of downloads for that particular journal at a given institution. If there is a
correlation between these metrics and full-text downloads, then this correlation will allow library
collection managers to predict the value of a particular journal prior to purchase based on an
acceptable cost-per-download model.
The global numbers such as Impact Factor, Eigenfactor, Article Influence Score, etc.
come from the JCR, and because of this, it is important to understand which of these metrics has
the strongest correlation to local downloads. Determining local metrics can create a great deal of
work and, therefore, additional cost of human resources and time. A full understanding of the
correlation between local metrics and future journal downloads can help inform how much effort
should be put in generating these local metrics and which ones. It is important to understand if
the expenditure of generating a local metric is worth the effort.
By creating regression models to define the correlation between full-text downloads and
other independent variables, it is possible to see the independent variable’s impact in this
correlation. We can create a model that includes both global metrics (i.e., Total Citations, Impact
Factor, Total Articles, etc.) and local metrics (e.g., internal citation metric – number of citations
by local institution authors for articles published in given year, and annual subscription cost);

52

thus, can we create statistically significant stronger models than just creating models from one
data source by combining metrics from multiple sources that have influence both globally and
locally? We posit the strongest models are not from either global metrics or local metrics.
Instead, using global metrics and local metrics enables us to create the strongest models to predict
downloads.
The research institution being analyzed had purchased 3,400 line items in 2012 that cost
more than $10 million dollars, and of those 3,400 line items, there are 1,510 items that represent
journals with all the global metrics we are using for evaluation. For example, not every journal
has a record in a JCR and subsequently does not have an Impact Factor, Article Influence Score,
etc. Also, not every journal has usage data formatted to the JR1 standard, so we could not
evaluate every journal purchased in 2012; however, we have a significant portion of the annual
financial spending and full-text downloads to make this data meaningful. These 1,510 journals
represent over 40% of the annual subscription fees for this library and more than 1.5 million
downloads in 2012. In order to create our regression models, we used a random sample of 275 of
the 1,510 journals for the training data. Once the regression model was created, we applied this
model to remaining 1,235 journals to see how well this model predicted downloads on a realworld test dataset.
3.3 Methods, Reports, Data Preparation, & Analysis
Methods
The theoretical construct of this research is web analytics, which is the measurement,
collection, analysis, and reporting of Internet data for the purposes of understanding and
optimizing Web usage (Jansen & Rieh, 2010). Although Web analytics cannot directly measure
motivation or satisfaction, it provides a method to evaluate online behaviors, correlate usage with
other available data, and standardize information across data sets and can inform financial
decisions (Ortiz-Cordova & Jansen, 2012). One of the contributions from this research is the
correlation of many advanced global metrics with local metrics to predict downloads of journal

53

articles at a local institution. Understanding the relationship these advanced metrics have on
downloads will assist in determining the demand of these journals at an institution and will aid
collection management practices in understanding the local value of journals.
The statistical model used in this research is multiple linear regression. Multiple linear
regression is a statistical analysis that allows multiple factors (i.e., independent variables), which
in this case are local citation counts, Eigenfactor, Impact Factor, Article Influence Score, Cited
Half-Life, Immediacy Index, 5 Year Impact Factor, Total Articles, and Global Citations, to be
considered to estimate the effect of the independent variables on the dependent variable, in this
case, full-text downloads (Sykes, 1993). This analysis uses multiple linear regression to create a
model for predicting downloads at an institution. This analysis focuses on creating the strongest
model with the smallest number of independent variables.
Data, Metrics, and Reports
The significance of journal evaluation has led to the creation of several reports and
scoring metrics to assist in understanding journal rankings. Some reports offer global numbers,
providing information on journals in totality, for example, how often a journal is cited in a given
year. Others provide information based on usage, for instance, how often a journal was
downloaded in a given year at a particular institution. All global metrics are vendor provided
(i.e., ISI, Web of Knowledge providing Journal Impact Factor), and local metrics can either be
provided by vendors (e.g., a provider giving information on how frequently a journal was
downloaded at their institution) or by institutional departments (e.g., the finance department
providing cost data). In this study, data are coalesced from four different types of reports to
further examine journal value: JCR, Web of Science, JR1, and Annual Financial data.
Journal Citation Reports (JCR) are part of ISI Web of Knowledge offering an organized
document that contains meaningful metrics on over 10,000 journals from both Science (8,000
journals) and Social Science (2,900 journals). The current year publication provides this
information for the previous year (i.e., 2014 JCR supplies information for journals in 2013).

54

JCR tabulate citation and article counts from “virtually all specialties in the sciences, social
sciences, and technology fields” (Journal Citation Reports, 2012, p. 1). The JCR provides
numerous metrics (see Table 17) to help appraise the value of a journal. So, the JCR is an
aggregation of metrics used to assist collection managers.
Metric Name
Total Cites

Measurement
Total number of times that each journal has been cited by all
journals included in the database within the current JCR year.
Impact Factor (IF)
The frequency an average article from a journal is cited in a
particular year.
Five Year Impact Factor The average number of times articles from the journal published
(FYIF)
in the last five years has been cited in the JCR year.
Immediacy Index
The frequency the average article from a journal is cited within
the same year it is published.
Article Counts
The number of articles published in a journal in a particular year.
Cited Half-life
Identifies the number of years from the current year that account
for half of the cited references from articles published by a
journal in the current year.
Eigenfactor Score
Uses the current JCR year citations to citable items from the five
previous years. Eigenfactor assigns a greater weight to citations
coming from influential journals, allowing these journals to exert
greater influence in determination of the rank of any journal they
reference. Eigenfactor does not count self-citations, the sum of
all Eigenfactor scores is 100; each journal’s Eigenfactor score is
a percentage of this total.
Article Influence Score
The relative importance of the journal on a per-article basis. This
is the journal Eigenfactor Score divided by the fraction of
articles published by the journal. The mean Article Influence
Score is 1.00, a score greater than 1.00 indicates above average
influence; a score lower than 1.00 indicates articles in that
journal have a below average influence.
Table 17: Listing of the various metrics provided in a Journal Citation Report and what they
measure to provide value (provided by Thomson Reuters, accessed at
http://wokinfo.com/media/pdf/qrc/jcrqrc.pdf).
The JR1 is a standard format for reporting journal downloads created by Counting Online
Usage of Networked Electronic Resources (COUNTER), which is a non-profit consortium with
an international steering committee of specialists from the library and publishing fields. Since
2003, this organization has been making it possible to record and report usage stats to allow for
dissemination of this information in a consistent and systematic way (Shepherd, 2012).
COUNTER is responsible for provisioning the standards of many important reports for various
electronic resource types (i.e., databases, journals).

55

Among the tools provided by Thomson Reuters in their suite of citation data within Web
of Science is a download of their database based on criteria provided to filter information in or
out. These filters enhance the tremendous value in database access with rich information because
the global data can become local; for example, filtering source articles by a particular institution
creates a report for a given year on the articles written by that institution. Now, this report is
providing meaningful information relevant to that particular institution. One local report by the
JR1 is the number of successful full-text article requests (downloads) by month and journal for a
particular institution. This local download report provides a view into the demand for a particular
journal at a particular institution.
We can aggregate information from a JR1 report and cost data (i.e., cost-per-download)
to create more robust metrics (Blecic, Wiberley Jr., Fiscella, Bahnmaier-Blaszczak, & Lowery,
2013). Cost-per-download alludes to the next data point used for evaluating journals, and that is
financial data to understand how much these resources cost at an institution. The ability to get
access to these reports can vary among institutions; however, institutions separately negotiate
prices for electronic resources based on factors such as the number of subscriptions purchased
from a particular provider.
Data Preparation
Analysis for this research required coalescing data from the aforementioned disparate
reports that represent the sources of data, three of which are externally sourced (see Table 18) and
various metrics that are provided or derived from these data sources (see Table 19). Full-text
downloads are the number of successful downloads of an article from a specific journal in a
month at a particular institution. Cost refers to the annual subscription price of the journal for the
institution. Local citations refer to the number of times a journal was cited in a year when authors
from this institution published articles. So, this metric does not refer to the number of times
authors published in this journal but rather the number of times published authors from the
institution cited this journal provided from Web of Science.

56

Name

Sourced

Finance Reports

Internal

Definition

Spreadsheet listing prices for 3,400+ electronic
resources purchased (for this research in 2012).
Journal Report 1 (JR1)
External Spreadsheet listing the successful full-text
downloads for each journal by provider.
Web of Science
External Spreadsheet listing published articles for this
institution and corresponding cited articles.
Journal Citation Reports (JCR) External Spreadsheet listing the journal name and the
corresponding global metrics (i.e. Impact Factor,
Eigenfactor, Article Influence Score, etc.).
Table 18: Listing of the types of data sources used for this analysis and their definition.
Metric Name
Metric Type
Data Source
Full-Text Downloads
Local
Journal Report 1 (JR1)
Cost
Local
Finance Reports
Local Citations
Local
Web of Science
Total Cites
Global
Journal Citation Reports (JCR)
Impact Factor (IF)
Global
Journal Citation Reports (JCR)
Five Year Impact Factor Global
Journal Citation Reports (JCR)
(FYIF)
Immediacy Index
Global
Journal Citation Reports (JCR)
Article Counts
Global
Journal Citation Reports (JCR)
Cited Half-life
Global
Journal Citation Reports (JCR)
Eigenfactor Score
Global
Journal Citation Reports (JCR)
Article Influence Score
Global
Journal Citation Reports (JCR)
Table 19: List of the metrics, a categorization of global or local for this particular
institution, and the data source that supplies these metrics, used as independent variables to
model the correlation to downloads.
For example, the financial reports, which represent the cost metric in Table 4, are
provided in a spreadsheet from the finance department containing annual subscription data for
electronic resource purchases made in 2012 with the subsequent information, including purchase
order id (PO-ID), journal name, ISSN, database, provider, and cost. The purpose of the financial
spreadsheet is to provide the local cost metric listed in Table 4 for each journal at the institution.
For this research, data was migrated into a database using a Ruby Rake task, a script
written in the Ruby programming language. In the process of migrating the spreadsheets into the
database, a unique identifier was created to link the financial data with other journal metrics.
The university library has a business agreement with Serial Solutions, a company that
provides services for electronic resource management, and one of these services is to aggregate a

57

portion of patron’s downloads of the library’s electronic resources. Publishers who provide
access to the journals store the downloaded information in the JR1 format according to
COUNTER standards. This JR1 represents the local download metric listed in Table 4. A
librarian with authorization to Serial Solutions can log on to that administrative website and
download any JR1 report that has been obtained from a provider within the past year. Serial
Solutions is only able to obtain information about journals if the provider has formatted the
download statistics in accordance to the JR1 standard. If the data does not comply with this
format, the data is not downloaded. For this research, in the cases where data may be incomplete
for a journal for the entire year, an average was created based on the number of downloads for the
months that data has been provided, and the average was used for the remaining months.
Similarly to migrating the financial data, a Ruby Rake task was developed to parse the JR1
spreadsheets and insert it into a database.
Arranging the data from Web of Science required more sophisticated parsing techniques
in order to link this data with other data sources. We were able to download data in a spreadsheet
that represented all the publications authored by researchers from this institution according to
Web of Science. From these publications, we can derive the local citation metrics referenced
above in Table 4. This spreadsheet had over 540,000 lines to represent nearly 5,000 articles
published by this institution in 2012. This data provides various fields for each article that is
represented and some of these fields vary in number or length based on the article --for example,
all the journals that are cited for a publication are represented in each record; obviously the
number of citations for an article will never be a consistent number as one article may cite 20
articles and another article may only cite 7 articles. Each field is preceded with a two-letter code
to identify contents of the field (see Table 20) in the spreadsheet that is provided from the Web of
Science download.

58

Field Code Field Contents
AU
Author of the article.
SO
The journal name representing where the article was published.
CR
Listing of first author, journal, year for each cited reference listed in the article.
NR
Indicates the end of the cited references (CR) for this article.
ER
End of record for this article.
EF
End of the file to be processed.
Table 20: Listing of the field codes and the contents of those fields within the spreadsheet
cell for the Web of Science spreadsheets for local citation data.
The result of parsing this spreadsheet from Web of Science was to find the source articles
that this institution published in and the journals used for citations based on those publications.
There was an additional level of complexity initially involved in linking the cited journals
because they are represented by an abbreviated title (referred to as Title20). Because cited
journals have an abbreviated name and our other reports have full journal names, linking by
journal name directly was not successful; however, Web of Science provides a PDF listing of
journal Titles and their corresponding Title20 field (see Figure 7).

Figure 7: This is a snapshot of the 300+ page PDF file listing Web of Science Journal Titles
and their abbreviated Title20 field (accessed via
http://scientific.thomsonreuters.com/imgblast/JCRFullCovlist-2013.pdf).
For example, AAOHN JOURNAL has a corresponding AAOHN J Title20. Using optical
character recognition software (OCR) of the PDF, it was then possible to extract over 11,000
journals listed in this Web of Science PDF and insert them into a database that provides the
ability to link the journal name for cited journals that was abbreviated with the Title20 field
provided in the Web of Science citation spreadsheets and the journal’s title.
Compiling the Web of Science citation data provided an appropriate segue to integrating
the Journal Citation Reports from Thomson Reuters. It is only possible to download the Journal
Citation Reports for 500 journals at one time into a spreadsheet. Due to this constraint, it

59

required 17 downloads and spreadsheets to obtain the JCRs for the 8,000+ journals for which
Thomson Reuters furnishes this information (see Table 21).
Imp. 5-Year
Cited EigenAbbr.
Total Facto Impact Immed Article Half- factor
Journal Title ISSN
Cites r
Factor . Index Count Life
Score
4OR-Q
J 1619OPER RES 4500
270 0.73 1.137
0.059 17
5.4
0.0016
0891AAOHN J
0162
474 0.856 0.977
0
6.8
0.0008
0149AAPG BULL 1423
7081 1.768 2.455
0.413 92
>10.0 0.0063
1550AAPS J
7416
2779 4.386 5.714
0.604 91
4.7
0.0082
AAPS
PHARMSCIT 1530ECH
9932
2606 1.584 1.906
0.171 164
5.3
0.0059
Table 21: Snapshot of Journal Citation Reports spreadsheet.

Article
Influence
Score
0.635
0.216
1.019
1.537
0.418

Each spreadsheet was parsed separately and linked into the database by ISSN or (if that
was not provided) by Abbreviated Journal Title (the same value as Thomson Reuters Title20 field
in the PDF displayed in Figure 7).
The collection and coalescence of this data from disparate sources eventually provided us
with a database of information on these journals to begin our analysis. This database now has
local categorization for the journals based on funding sources in 2012, download data to represent
successful full-text downloads for these journals in 2012, citation data depicting the number of
times authors from this institution cited these journals in their published articles in 2012, and
finally their corresponding global metrics in 2012, such as Total Cites, Impact Factor, 5-Year
Impact Factor, Immediacy Index, Articles, Cited Half-Life, Eigenfactor, and Article Influence.
The database now contained 1,510 journal line items that contained the global bibliometrics
wanted for evaluation, and the local metrics for evaluation (such as subscription cost, full-text
downloads, and local citations). We exported this data to an Excel Spreadsheet, imported the data
into Minitab, and used the random selection tool in Minitab to randomly select 275 rows from our
data and being our regression analysis.

60

Minitab 16.2 was used for all statistical calculations.
3.4 Results
In order to get an understanding of the relationship between the individual metrics (both
global and local as indicated in Table 19) and total downloads at an institution, we ran separate
single linear regression models that compare total downloads and the respective metrics listed in
Table 22 (i.e., Article Influence Score, Cited Half-Life, Eigenfactor, etc.) on the 275 journal
training data set. Table 22 indicates S, the average distance that the observed values of total
downloads fall from the regression line; R2 informs us how well the model explains the variance
of the response (i.e., downloads). Table 22 also presents R2 (adjusted), a modified version of R2
to account for how much the predictor variable improves the model based on the number of
predictors in the model. Finally, Table 22 shows R2 (predictability), which is how well a
regression model predicts responses for new observations rather than just the original data set
(Frost, 2013).
Metric

Number of S
Journals
Influence 275
1332

R2

R2 (adjusted)

R2
(predictability)
-.3%

Article
3%
3%
Score
Cited Half-Life
275
1351
.3%
-.1%
-1%
Eigenfactor
275
895
56%
56%
54%
Immediacy Index
275
890
3%
3%
-1%
Impact Factor
275
804
4%
4%
2%
5
Year
Impact 275
805
4%
4%
2%
Factor
Total Articles
275
1110
33%
32%
30%
Global Citations
275
879
58%
58%
55%
Local Citations
275
827
63%
62%
60%
Table 22: Listing of the linear regression model measures separately for each metric and
local institution downloads. Note: bold figures represent the three independent variables
with the strongest correlation to full-text download.
Table 22 illustrates the various strengths of each model (i.e., R2, R2 (adjusted) and R2
(predictability)). Each metric listed in column 1 represents a separate model, and the strength of
that model (indicated by the three R2 values) indicates the strength of the correlation between that
particular single metric and downloads. By singling this metrics out in this way, we can see that

61

these metrics have varying levels of correlation with downloads based on the R2 value; for
example, local citations, global citations and Eigenfactor all create regression models with an R2
over 56% and even have predictability over 54% with local citations having the highest
predictability number at 60%.
However, some of these metrics create equally weak models. In this instance, Cited
Half-Life and Immediacy Index have a R2 of .3% and 3% respectively and a negative ability to
predict downloads based on these metrics alone. These metrics (Cited Half-Life and Immediacy
Index) have very little ability to explain the variance of downloads among journals; whereas,
local citations, global citations, and Eigenfactor have a much stronger ability to explain the
variance of downloads.
The assumptions for the residual plots in both the global citations (see Figure 8) and the
local citations (see Figure 9) display random and unpredictable error that is consistent with an
assumption of a regression model (i.e., one should not be able to predict the error for any given
observation). The residuals are centered on zero, and the model is correct, on average, for fitted
values.

62

Figure 8: Residual Plot for global citations, displaying random error and centered on zero,
which is consistent for an assumption of a regression model.

63

Figure 9: Residual Plot for local citations, displaying random error and centered on zero,
which is consistent for an assumption of a regression model.
Now, if we were to create a model based on combined multiple metrics using multiple
linear regression, which metrics would be meaningful to add to this model? Again, using the
training data set of 275 journals, we can run a linear regression model with all the metrics at one
time to create an overall regression model. The normal probability plot (see Figure 10) of the
residuals is approximately linear, and this supports the condition that the error terms are normally
distributed; although, there are some outliers that could be considered for removal if we wanted to
further strengthen the model.

64

Figure 10: Probability plot with regression line for all metrics at one time, which supports
the condition that the error terms are normally distributed.
The summary of the regression model using all metrics available (see Table 23) indicates
a model that is stronger in explaining variance (R2), how much the additional predictor variables
improve the model (R2 adjusted), and how well this model predicts downloads based on new
observations (R2 predictability). The regression equation is a mathematical formula that can be
used to predict the outcome--in this case downloads--and based on the predictor variables.
Field
S
R2
R2 (adjusted)
R2
(predictability)
Regression
Equation

Value
658
77%
76%
73%

89.205 + 15.8377 Citations - 0.00495926 Total Cites - 160.003 Impact Factor
+ 254.964 Five Year Impact Factor -139.04 Immediacy Index + 0.846012
Total Articles - 14.9955 Cited Half Life + 19474.8 Eigenfactor - 223.299
Article Influence Score
Table 23: Summary of the Regression Model created using all the metrics available.

65

However, while this model is stronger than the individual models, this still does not tell
us which of these metrics are statistically significant meaningful additions to our regression
model. Looking at the coefficient table (see Table 24), there is a listing of the metrics, associated
p-value, and the coefficient (the multiplier) for that metric.
Metric
P-Value
Coefficient
Article Influence Score
.062
-223
Cited Half-Life
.288
-15
Eigenfactor
.000
19,475
Immediacy Index
.233
-139
Impact Factor
.035
-160
5 Year Impact Factor
.006
255
Total Articles
.000
.8
Global Citations
.606
0
Local Citations
.000
16
Table 24: Listing of the metrics used and their statistical significance for meaningful
addition to a linear regression model for the sample data 275 journals. Note: bold figures
represent metrics with a statistically significant p-value.
We consider a p-value of .05 or less to be statistically significant. Using this p-value to
represent a meaningful addition to the model, the following metrics are considered significant:
Eigenfactor, 5 Year Impact Factor, Total Articles, and Local Citations, and Impact Factor.
A regression model using only the statistically meaningful metrics is listed in Table 25
(Local Citations, Total Articles, Impact Factor, Five Year Impact Factor, and Eigenfactor). The
summary of this model shows that the model lost little to no strength by eliminating four metrics
(Article Influence Score, Cited Half-Life, Immediacy Index, Global Citations). This simpler
model is equally as strong and even more predictive by 1% (see Table 23).
Field
Value
S
660
2
R
77%
R2 (adjusted)
76%
R2
74%
(predictability)
Regression
16.6861 + 16.1885 Citations - 121.229 Impact Factor +111.129 Five Year
Equation
Impact Factor + 1.00529 Total Articles + 14925.5 Eigenfactor
Table 25: Summary of the Regression Model created using only the statistically significant
metrics.

66

Based on an analysis of these regression models, the simplest equation that also had the highest
R2 value for predictability is presented in Table 25. This equation included five metrics. Four of
these metrics are global (Total Articles, Impact Factor, Five year Impact Factor, and Eigenfactor),
and one metric is local (Local Citations) and has an R2 predictability value of 74%. This model
was created based on a training data set of 275 journals. We then used the developed equation on
a test set to predict a range of downloads for 1,235 journals. We considered the model accurate if
the predicted number of downloads fell within plus or minus of the S value (664), the average
distance the observed value falls from the regression line. The results of this model’s accuracy
for prediction are displayed below (see Table 26), as well as the number of downloads and cost
those journals accounted for in 2012 for this institution.
Number of Journals

Overall Number of Cost of Journals
Downloads
538,905 (43%)
$2,253,780 (72%)

Journals within
1000 (81%)
Range of Predicted
Downloads
Journals out of
235 (19%)
727,553 (57%)
$870,765 (28%)
Range of Predicted
Downloads
Total
1235
1,266,458
$3,124,545
Table 26: Displays the number of journals tested for accurate range of prediction for
downloads and those that within range of prediction and those outside of the range, and
corresponding downloads, and cost.
The R2 predictability value for this model was 74%. When applied to a real-world data set, this is
accurate because this model was able to successfully predict downloads within range where the
range was the result of the regression equation plus/minus the S value of actual downloads 81%
of the time, so, our model performed better than expected.
In our initial research objective, we stated that a successful model would predict a range
of downloads within 70% of the journals tested, and these results exceed those expectations of a
successful model. There were eight journals that could be considered outliers within the number
of downloads they represented for their journals that this model was unable to accurately predict.
Those eight journals that we could not predict full-text downloads for represented 322,183 (44%)

67

of all full-text downloads for the 235 journals that were not predicted within range. So, if we
removed these outliers, the predictability of our model would be even higher, and the percentage
of downloads would be consistent with the number of journals.
An accurate model that can predict usage can subsequently be used to create a reasonable
price range for libraries to pay for journals or determine value of these journals. Based on the
metrics developed in this research, we can create a predictable download range, where the range
is the plus/minus average observed error in predicting downloads. Using this downloads
prediction range, we can create a price range based on the average cost-per-download in 2012 for
the 1235 journals predicted ($2.47). Using this range of payment the institution is willing to pay,
we then find the outliers that are priced above this price range. Those that have an actual price
above the predicted price range can be singled out for price adjustment. Then, we can compare
the actual price and the minimum and maximum price range to create a range of potential savings
(See Table 27).

Actual
Price

Name
Journal of Applied
Polymer Science
$20,361
Chemical Physics
Letters
$16,501

Predicted
Download
Range
3,430 – 4, 750
1,245 – 2,565

Thin Solid Films
Journal of Cellular
Physiology
Journal of Cellular
Biochemistry
Journal of Crystal
Growth

$15,600

3,351 -4,671

$15,399

1,340 -2,660

$14,828

1,168 – 2,488

$14,446

513 – 1,833

Surface Science
Molecular
Microbiology

$11,214

2,535 - 3,855

$11,181

860 – 2,180

$10,800

3,033 – 4,353

$9,769

574 – 1,894

Neuroscience
Journal
Neuroscience

of

Min
Price
Offer
$8,47
1
$3,07
3
$8,27
6
$3,31
0
$2,88
4
$1,26
7
$6,26
1
$2,12
3
$7,49
1
$1,41
8

Max
Price
Offer

Min
Potenti
al
Savings

Max
Potential
Savings

$11,731

$8,629

$11,889

$6,334

$10,167

$13,427

$11,536

$4063

$7,323

$6,571

$8,827

$12,088

$6,145

$8,683

$11,944

$4,527

$9,918

$13,179

$9,521

$1,692

$4,953

$5,384

$5,797

$9,057

$10,751

$48

$3,309

$4,678

$5,090

$8,350

68

Actual
Name
Price
Research
Surface
&
Coatings
Technology
$9,226
Developmental
Dynamics
$8,457
Statistics
in
Medicine
$8,131
Electrophoresis
Cell and Tissue
Research
The Science of the
Total Environment
Nuclear
Engineering and
Design
International
Journal of Heat
and Mass Transfer

Predicted
Download
Range

1,274 – 2,594
599 – 1,919
1,676 – 2,996

Min
Price
Offer

$3,14
7
$1,47
8
$4,14
0
$2,20
3

$8,108

892 – 2,212

$7,786

62 -1,382

$7,577

1,580 – 2, 900

$152
$3,90
2

$7,441

245 -1,565

$605

Max
Price
Offer

Min
Potenti
al
Savings

Max
Potential
Savings

$6,407

$2,819

$6,079

$4,739

$3,718

$6,978

$7,401

$730

$3,990

$5,464

$2,644

$5,904

$3,413

$4,373

$7,633

$7,163

$413

$3,674

$3,866

$3,575

$6,835

$3,40
$7,329
1,379 – 2,699
6
$6,667
$662
$3,922
$1,99
Marine Biology
$7,003
808 – 2,128
6
$5,257
$1,746
$5,006
Table 27: A subset of journals that have a higher actual price than predicted price range
based on predicted downloads. Their predicted download range for each journal and the
range of price to offer and potential savings based on the difference between price offer and
actual price.
For instance, the average journal costs $2.47 per download. The regression model
developed predicts The Journal of Applied Polymer Science should have a range of 3,430–4, 750
downloads. Based on this range and the average cost per journal, the library can calculate the
range they would be willing to pay in order for this journal to maintain an average cost-per-use of
$2.47. Based on these numbers, the library may be willing to pay in the range of $8,471$11,731.
This dollar range is created by multiplying the low number in the predicted download range and
average cost-per-download as the low point the library would pay (Min Price Offer) and the high
number in the predicted download range for the price the library would pay is calculated by
multiplying the high number in the predicted download range and average cost-per-download
(Max Price Offer). This calculation creates dollar figures that allow the library to approach

69

providers with when negotiating purchase deals and contracts with an understanding of the value
of these journals at their home institution. In this example alone, the institution could have saved
anywhere from $8,629 (Min. Potential Savings)-$11, 889 (Max Potential Savings) on the
purchase price of this single journal or the explored alternate methods of acquiring this journal.
Using this model to calculate a range in downloads and subsequent range in spending, we
can derive the number of journals that fall outside of this range on the expensive side and the
potential savings that could be gained if this range was used as a basis for payment.
Out of 1,235 journals analyzed, 264 (21%) were above the high price ceiling in the range
created by predicted downloads and average cost-per-download. By focusing on cost savings for
or alternate methods of acquiring these journals, the institution could see savings in the $496,183$1,356,929 range per year. The range in savings is based on using either the lowest dollar point
in the range the library should pay compared with the actual cost the institution paid or the
highest dollar point in the range the institution should be willing to pay with the actual cost paid.
Even at the lowest point in the range, there is savings of nearly $500,000 per year.
3.5 Discussion and Implications
JCRs have several metrics designed to provide insight into how valuable a journal will be
at an institution. We consider these metrics global metrics because they are providing
information on the journal’s totality (i.e., how often a journal was cited by any journal from an
author at any institution) rather than the providing data on a journal’s impact at a specific
institution. Metrics dealing with a particular institution are termed local metrics. The comparison
of global metrics alone with the combination of global and local metrics is significant because it
illustrates the distinction between global and local metrics, and the importance for libraries to
calculate their own local metrics.
Taking a broader approach to collection management and understanding how these global
metrics relate to local full-text downloads illustrates the meaningfulness of specific metrics and
their correlation or predictive ability to full-text downloads. The strongest single metric to model

70

local downloads of a journal is Local Citation, how often a journal is cited by authors within that
institution. This implies that there is more value in creating and monitoring local metrics to
predict user behavior than relying on those metrics that are globally created and monitored.
We also show the predictive capability with both a single-regression model and multiplelinear regression model that used Local Citation rate as a meaningful variable in predicting fulltext downloads. There is clearly a strong correlation between these local numbers (citations and
downloads), but what was surprising is the ineffectiveness of many other metrics at predicting
these downloads at an institution; for example, when looking at global metrics, there is a good
deal of work in trying to normalize citations for a particular journal based on self-citations or
normalization based on the number of articles that are published in a year. Yet, the raw number
of global citations still produces a stronger model to correlate to the number of downloads at a
local institution than either of these metrics (e.g., Impact Factor, Five Year Impact Factor) that
have been normalized for equity.
However, these metrics do not need to live in a vacuum, and there may be advantages
that can be gained by combining these metrics to create stronger models. When evaluating a
regression model, it is important to keep the model as simple as possible because the cost in
obtaining additional metrics may result in a significant expense. JCRs provide all of these global
metrics together so the additional cost in obtaining these metrics is negligible. The resource
strain, aside from obtaining JCRs, is in the digitization of this data and programming required to
make it possible to link with the journals at a particular institution. Once the link is made
between reports and a single metric within the report, it is trivial to link the additional metrics in
the report.
Creating a simple model is still a goal, but creating a strong model is the ultimate goal
and adding an additional metric does not create the complexity in this case that it may create in
other regression models. What is particularly interesting is that global citations alone created a
relatively strong model to correlate journal downloads; however, when creating a regression

71

model with multiple independent variables, global citations became less meaningful and local
citations remained essential. This indicates the importance for institutions to create their own
metrics to rely on for predicting journal value. Eigenfactor also remained a meaningful metric in
this predictive model for local full-text downloads, illustrating that the best way to analyze these
resources is to combine both global and local metrics. Conversely some of these metrics are not
helpful in predicting downloads either alone or in a multiple regression model, and in this case,
they have little value relative to correlating downloads at a local institution.
The ability to create a strong model to correlate downloads is helpful to understand the
most meaningful metrics in this context. A model that can accurately predict downloads within a
range provides the capability to create an acceptable price-per-download that an institution may
be willing to pay. Based on this model and the ability to predict downloads, a library can enter a
contract with an estimated range of how much they would be willing to pay for a journal or a
dollar figure on the value of that journal to the institution. Journals that fall outside of that range
may do so because they are good deals or because they are bad deals. In either case, from a
collection management view, this provides administrators with a powerful metrics to leverage
when considering purchasing journals; for example, if a library pays $10,000 for a journal, and
articles from that journal are downloaded 7,000 times a year, then it makes sense to pay for that
journal on an annual basis rather than on a per-article basis.
The model developed to create the ability to predict value has inherent weaknesses. JCRs
only exist for a little over 10,000 scientific journals, and this is only a fraction of the journals that
the libraries provide access. This creates an issue because not all journals can be evaluated with
that model. Furthermore, JR1 reports exist for a larger number of journals than JCR; however,
this is yet another layer of abstraction required to create a prediction model that relies on third
party resources. Additionally, there is a large overhead involved in aggregating this data from
various resources, writing scripts to parse the data, and data modeling to successfully organize the
data for evaluation.

72

However, the benefit to the over-head involved in programming, data modeling, and
aggregating the data is that this process can easily be repeated each year after the initial
investment. The volume of journals in a library collection to manage can be over 100,000, so this
model creates an ability to quickly take the number of journals to pay particular attention to a
more manageable number while still providing an ability to make a financial impact. This model
also creates local metrics that may not otherwise exist and that illustrates the correlation these
local metrics have with journal value at a local institution. They can also be used to create other
key metrics such as cost-per-use. Finally, this model aggregates all the data sources and
simplifies the ability to provide a number of people to access one place to analyze data and
reference for further discussion regarding collection management.
This work required a great deal of programming to create scripts for automating tasks that
have typically taken individual librarians at this local institution weeks of data entry work. The
initial overhead in programming is significant; however, once the program is written, it can be
applied to information for additional data for the upcoming years. For future work, automating
this process and creating an application that makes this data viewable for multiple people to
analyze for collection management decisions could add tremendous value to the collection
management team and provide more time for data analysis rather than data aggregation. A
contribution of this application would be to not qualitatively make final decisions in regards to
collection management but rather highlight the electronic resources that are worthy of discussion
and further evaluation. This would create a more manageable number of journals to analyze.
3.6 Conclusion
Collection management for electronic resources is an extremely complex job. There are
tens of thousands of journals, a multitude of providers, and various platforms to manage; these
factors all contribute to the complexity. The information needed to analyze these resources
resides in a number of systems, and many of these systems are remote and have various formats
to access them. Financial data is typically stored locally and is the one that perhaps offers the

73

most flexibility because internal staff can offer to output the data in a variety of formats. The
most obfuscated data is that which is provided from third parties for usage statistics, such as
Journal Citation Reports and Web of Science metrics. This data comes in spreadsheets, PDF
files, and more spreadsheets. The outside data from third party vendors have value, but the real
value is when data can be combined with local numbers to create a true understanding of what
resources are providing value to the institution and to better serve the mission of the library and-more specifically in this case--collection management departments.
The work done in this research shows the disparity of these global metrics and their
correlation to downloads when looked at singularly; in fact, the most powerful metric correlating
to downloads is a local metric—the number of citations a journal received at the institution in that
year. One thing that perhaps was not emphasized enough is the rigorous amount of legwork done
to bring all of this data together. There are plenty of existing data format standards that would
allow the coalescence of these disparate systems in a more real-time and seamless manner.
Mechanisms should be created to allow this information to flow more freely. Evaluation of
electronic resources typically provides many numbers to consider; any of these numbers alone tell
part of the story and leave much to guess work; however, when these numbers are combined, they
can create a model that can financially benefit to libraries. Providers of this data should be
willing to help make this data less complex and create an environment where fiscal responsibility
is something providers facilitate rather than obfuscate. It would make sense to ensure customers
are spending their money wisely, which ensures they can continue to spend it.

74

Chapter 4
Providing access to scholarly data is one of a research library’s primary goals and must
be maintained even with a dramatic increase in budget allocation; however, a primary concern
becomes ensuring that the scholarly materials provided through these electronic resources are
meeting library patrons’ needs and are ultimately meeting a research library’ primary mission and
are enabling further research. The digitization of academic journals has proliferated the number of
journals that exist, and hence the number of journals a research library provides access to, and
institutions continually see a dramatic increase in the percentage of their budget allocated to
electronic resources (Chung, 2007). This proliferation makes it difficult for libraries to know
which journals provide what level of value to the library patrons. In order to get an understanding
of the value these journals provide, it is necessary to evaluate these journals systematically,
within a standard set of processes and automation when possible. Unfortunately, the data
necessary to perform this type of evaluation exists on many servers, in various formats, and can
become outdated, which is why performing this systematic evaluation can become convoluted.
Research libraries brokering ubiquitous access to scholarly data, in some cases, provide
access to more than 100,000 titles. Much of the complexity related to access is concealed from
library users. Access gateways are hidden, content is duplicated, and subscription costs are widely
unknown to the user because of the complexity that exists within big package deals. Meanwhile,
annual subscription costs to access online journals and conference manuscripts can exceed ten
million dollars annually (Furlough, 2012). Due to rising costs and shrinking budgets in a time of
economic concern, it is important for large research libraries to be able to adequately discern the
journals that provide value to their patrons and those that do not to help stabilize the financial
concerns (Collins, 2011). Providing relevant scholarly information is part of the libraries’
mission; they must have a better grasp of their electronic resources’ value in order to fulfill that
mission; however, determining the value of these electronic resources is a complex problem with
much of that complexity attributed to the content providers, and less exploration on effective

75

attempts of research libraries to manage their electronic resources (Stefancu et al., 2004);
however, there has been little discussion concerning how libraries can develop their own
mechanisms to address or, at least, alleviate some of the complexity existing in the current
electronic resource climate.
This research’s context is based on the understanding that a research library’s critical
function is to provide relevant scholarly materials but that there is difficulty in deciphering the
electronic resources that do or do not provide value to the library. Can we create a model to be
implemented as a system to alleviate some of the complexity of evaluating electronic resources?
Is there a way to automate some of the analysis tasks to scale used to evaluate the electronic
resources for a large research library’s content collection? Can we aggregate several disparate
data sources together in a single authoritative system to provide collection managers a simpler
mechanism to work with? These are some of the questions that motivate our research.
4.1 Previous Work
The digitization of electronic resources has made scholarly content more accessible to
library patrons as they can now access these resources nearly anywhere with an Internet
connection; however, digitization has also given way to a more fragmented environment for
evaluating the demand of these resources. Prior to subscription resources being made available
online, the library managed the demand of resources through on premise content circulation.
Although one may assume that journal digitization would lead to a more formulaic process of
creating a more robust set of metrics, this has not been the case in the current electronic resource
environment. Providing ubiquitous access removes the library’s ability to see the users and the
use of particular collections (Schwartz, 2000); however, the deluge of electronic content the
library provides access to and the increase in electronic resource budget costs has created the
need for libraries to collect data that enables them to make calculated electronic resources
investment decisions (Lakos, 2007). The portion of acquisition budgets in most academic libraries
has gone up (in some cases) reaching as high as 70% of the budget for journal subscriptions

76

(Chung, 2007). The cost of providing these resources has risen in research libraries; yet, there is
not a clear method to evaluate how those costs are providing value to these research libraries and
the researchers who rely on access to electronic resources.
Areas of Evaluation for Electronic Content in Libraries
Although, no clear method exists for a systematic way to evaluate electronic resources at
a large research library, there have been studies to help define metrics to assist in such an
evaluation. The data points measured create metrics for evaluating electronic resources fall within
one of three areas: cost, content, and/or usage (Coughlin, Campbell, & Jansen, 2014). Cost is
relatively apparent as it typically relates to the amount that a library pays for a subscription;
however, there are complex pricing models involving big package deals that may require
evaluating the worth in the complexity (Chambers & So, 2004). For example, does it make sense
for a library to purchase bundled packages of journals instead of just a pricing model where a
library only pays for a single journal at a given price? The notion of the “package deal” implies
that there is a cost savings by purchasing in bulk, and this may very well be true. An analysis of
these deals will focus on the value of the package--are all the journals purchased providing some
level of worth to the institution?
Content analysis typically focuses on either content duplication (perhaps caused by
complex big package deals) or proving that journals from different disciplines require a different
analysis (Bensman et al., 2010). The latter evaluation considers how journals are used in each
discipline and the frequency they publish, cite, etc., which requires journal examination to be
performed on a discipline-by-discipline basis in order to compare “apples to apples.”
Usage of journals as a metric for assessment illustrates that usage proves the efficacy of
one journal compared to another (Metz, 1992); however, usage may be measured with a couple of
metrics; in some cases, usage is considered full-text downloads of articles from a journal, or
usage can be considered citing a journal in a publication. Nevertheless, usage is a standard metric
used to consider the value of a journal.

77

Difficulty of Data Access
Combining metrics from these multiple areas (content, cost, and usage) is difficult
because the sources of the information and the format the information can be stored in varies. The
provenance for each area of evaluation is different, and depending on the type of data within that
area (cost, content, usage), there may be another source of data. For example, if we are looking at
usage for a journal and define usage as citations, then that information can be found in Web of
Science; however, if we are considering full-text downloads as the canonical method to define
usage, then that information is stored on any number of the content provider servers. Cost data
may be found at a local institution; however, this may provide a layer in complexity if the
institution does not want to provide that data. Cost data may also be found on the web site of a
content provider. Tracking content studies can be intricate because the number of journals
provided access to could be immense (Duranceau, 2002). Instances exist where content is
provided from a number of sources via package deals. Also, there may be some package deals
that do not provide an accurate assessment of the content they provide. For example, access to a
database of journals may include access to 10,000 journals; however, the accessible journals can
change on a daily basis. The list of journals that a database provides access to may not be kept up
to date. This data is located at several different points and in different formats; a large number of
electronic resources create a difficult environment for producing a system to help evaluate a large
research library’s electronic resources.
Directly related to the heterogeneous data sources is a lack of standardization or a lack of
implementation where standards exist. Standards regarding data formatting would alleviate the
existing issue of needing to manage electronic resources’ evaluations differently based on the
different formats of different journals. Counting Online Usage of Networked Electronic
Resources (COUNTER) is a non-profit consortium with a mission to create and improve
standards within electronic resources (Shepherd, 2012); however, having a standard only has
value based on the level of implementation. For example, Journal Report1 (JR1) is a reporting

78

standard that provides full-text downloads for a journal in a given month, and is widely adopted
by content providers. This JR1 is one such standard created by COUNTER. This adoption rate
puts a premium on this information because it creates a metric that can be shared among libraries,
and the data is stored in a consistent manner (Togia & Tsigilis, 2006). This consistency enables a
programmatic solution to scaling journal usage; however, these standards are less readily
available or do not exist for other types of data. No method exists to programmatically find
journals accessible from a large database--that must be done manually and does not scale to a
large evaluation. Considering citations as a metric of usage, there is no standard method to
aggregate this data for an institution. Because no standard exists, many libraries rely on numbers
provided to them by vendors. Lack of standards or the inability to implement existing standards
can compound the complexity created by data being stored in many places. A system that would
evaluate electronic resources at the scale necessary to be useful at a large research library would
require leveraging standards. The advantage to these standards would provide a programmatic
mechanism to deal with the amount of information these institutions handle for electronic
resources.
An example of data that collection management departments value, but lack a standard
format for sophisticated querying that would benefit an evaluation by a large research library, is
Thomson Reuters Journal Citation Report (JCR). Thomson Reuters provides a JCR; this report
distributes several citation-based metrics for over 10,000 journals. This information supplies
libraries and collection managers several metrics on journal citation evaluation holistically; this
citation information is not specific to any particular institution. There is little debate that these
reports provide value and a level of relevance for a journal’s worth. However, the global worth of
a journal is not completely predictive of the local worth a journal may have (Coughlin & Jansen,
2014), and because these numbers lack such foresight, it is important for local institutions to
create their method of evaluation. Generating more complex metrics that are related to the distinct
value an institution has for a journal will create more accurate analysis and will prevent a scenario

79

where an important journal is “weeded out” without properly evaluating its value to the native
institution. The key is to turn these various data points into knowledge on resources so an
informed decision can be made; however, relying on metrics from other sources to determine
local value can become a risky proposition (Enssle & Wilde, 2002). There is a good deal of
research and prior studies on the correlation between these global metrics, but there is less prior
work on the correlation between local metrics and global metrics. How does a global metric relate
to a particular institution? Additionally, the caveats that do exist lead one to believe that to find
the value of electronic resources at a local institution, one must consider local factors. Journal
rankings suppress some widely accepted and useful measurements for performance; this indicates
the need for a more broadly focused investigation to advise the management of these resources
and to capture a multitude of qualities that cannot be reduced to a single number (Rafols et al.,
2012). The JCRs are one example of how a system that includes these metrics in a standard
format could benefit a large research library’s evaluation of electronic resources.
Metrics for Evaluation
The issues that are brought up in previous studies--disparate data sources, complex
subscription models, scalability, lack of standardization--have created an environment with biases
or weaknesses. These weaknesses among existing metrics and disciplines have led researchers to
begin creating new metrics, enhancements to existing metrics, and ways to understand the
correlation between these metrics. For example, Modified Impact Factor is a metric created to
account for the disputes that exist among disciplines (Iftikhar, Masood, & Song, 2012).
Additional studies have shown the correlation between certain citation-based metrics or
bibliometrics; popular bibliometrics include Impact Factor, the frequency an average article from
a journal is cited in a particular year, and Eigenfactor, a similar metric to Impact Factor that does
not include self-citations to account for presumed bias of publications. The similarity among
these metrics adds a level of consistency to the numbers because in some cases, these metrics are
based off of similar factors (Chang, McAleer, & Oxley, 2010). This consistency among these

80

numbers illustrates their large-scale value; however, there is still an issue of understanding their
local value. This lack of understanding creates a need for local metrics to evaluate journal
resources. This need persists to ensure a level of accuracy; at times numbers provided by vendors
have been found to be inaccurate or irrelevant (Duy & Vaughan, 2003). The subject of relevance
is prevalent because an institution cannot assume that all institutions will have equal metrics (Duy
& Vaughan, 2006). Libraries need a model to systematically evaluate journals on an annual basis
that will allow collection management to highlight the outliers within electronic resources. A
system that can highlight the outliers will provide a more manageable number of journals to
perform a more precise analysis.
The key to create a system that could more effectively and efficiently manage electronics
resources evaluations at the scale necessary for a large research library is to create a model that
accurately defines the existing resource types or entities, and the relationship among these
entities. Defining the entities and their relationships would facilitate implementing a model to
develop a system to fulfill this need.
4.2 Research Objectives
Currently there is no way to systematically evaluate electronic resources at major
research libraries at scale. The deluge of information in a library’s online content and the
separation of where this data exists are problematic for those trying to evaluate these resources.
Librarians in collection management spend most of their time manually aggregating this data
leaving little time to invest in the analysis. If there existed a process and an automated system to
highlight the journals requiring further analysis as well as those outlier journals (either for good
or bad) requiring less analysis, this would improve work efficiency and save resources; with less
journals to analyze and less time spent aggregating the data, there would be additional time for
analyzing a much more manageable scale due to the decreased number of journals. In order to do,
this we must systematically demonstrate a conceptual model, demonstrate an implementation of
that conceptual model, and validate the system, process, and benefits.

81

Research Objective 1: Create a conceptual model for electronic resources to develop artifacts
that facilitate implementing a robust system to evaluate electronic resources at scale for a large
research library.
The goal of creating a conceptual model is to help understand the entities that exist in this
domain and the relationships between them. This conceptual model is unconstrained by
implementation characteristics, agnostic to the specifics around technological decisions and
independent of design and implementation concerns; however, we will discuss the definitions of
the vague terms used within this space and expound on their relationships to eliminate confusion
frequently created by the ambiguity of databases, aggregators, journals, package deals, etc. The
primary artifact created from this objective will be an entity-relationship (ER) diagram.
Research Objective 2: Take the resulting conceptual model and map that into the physical
design and implementation of the library electronic collection evaluation system.
We will manually develop a system based on examination of the model with the
technology of our choice. Implementing the conceptual model will create a platform based on
requirements to coherently evaluate resources at scale in a contemporary collection management
department for a large research university library. The output of this objective will be both a
physical data model to represent our implementation of the conceptual model and a web front-end
that enables a thorough analysis of these resources based on the business needs of a research
library. Our output for this objective will include a system architecture chart, and taking the
architecture from this concept and illustrating how it is implemented as a potential solution.
Research objective 3: The third objective is to validate the implementation of the conceptual
model and processes using real world data from a major research library.
This objective focuses on the system, the validation of which is used for additional
analysis. For example, what are the strengths of this system, and how can it be used to evaluate
electronic resources in a large research library? In one instance, we explore the effectiveness of
this system to corroborate the necessity of evaluating electronic resources based on their area of

82

discipline and to further promote standards. In another example, we explore the use of this system
to forecast usage in a large research library as another mechanism to validate the benefits of this
model and subsequent implementation.
4.3 Methods, Data Collection, Data Preparation & Analysis
Methods
This research is based on a web analytics framework to evaluate electronic resources
used by faculty, staff, and students at a large research library. Web analytics is the measurement,
collection, analysis, and reporting of Internet data for the purposes of understanding and
optimizing Web usage (Jansen, 2009). Web analytics do not measure user motivation or user
satisfaction; however, this method is used to analyze online interactions and behaviors that are
directly related to usage or demand and are linked with other available data to standardize
information across data sources to inform financial decisions (Ortiz-Cordova & Jansen, 2012).
We have adapted web analytic techniques from the ecommerce domain to research libraries by
focusing on the strength of web analytics that is measuring a particular journal’s usage and value
compared to other journals’ usage. Web analytics is not just the measurement of demand;
through a variety of additional metrics and measurements, one can understand the relationship
between the user and the organization (Phippen, Sheppard, & Furnell, 2004).
One this research’s contributions is a systematic process for aggregating many of the
sources of web logs into one place to create advanced metrics in one location to accommodate the
needs of those involved with collection management at a large research library. Tools created
from Web analytics provide a basis for making strategic decisions; the use of a tool based on
these metrics can be laced with organizational goals and internal decision-making lens (Nakatani
& Chuang, 2011).
Data Collection
The data collection for this research discusses the methods used to obtain data from the
various data sources of particular value. This section discusses the complexity involved in the

83

tedious tasks of aggregating data on full-text journal downloads, citations at a local research
library for a given year, Thomson Reuters JCRs, and internal financial data collection.
Data Collection: Journal Report1
Effectively creating a system used to evaluate journals on more than one criterion
invariably requires bringing data from several sources and coalescing those data sources. The
basic metrics we explore for this system are full-text downloads (i.e., how often articles from this
journal are downloaded annually), citations (i.e., how often authors from a specific institution cite
these journals annually), authorship (i.e., how often authors from a specific institution publish in a
journal annually), cost (i.e., how much a specific institution pays for a subscription annually), and
citation metrics that are created based on global evaluations of these journals (i.e., Impact Factor,
Eigenfactor, Article Influence Score for a journal).
Full-text downloads are supplied by each content provider in a standard form called a
Journal Report1 (JR1). In order to simplify the number of content providers, a library needs to
download these reports from, which Serials Solutions does stores them on the institution’s servers
(see Figure 11).

Figure 11: Content providers (i.e., Elsevier, Sage, Wiley, etc.) provide full-text download
statistics in a standard format that allows Serials Solutions to aggregate in one database.
Serials Solutions aggregates all of the JR1s and provides a convenient mechanism for
collection managers by offering one interface (see Figure 12) to download JR1s for content

84

providers that adhere to this standard for reporting full-text download usage. Figure 12 provides
an interface to sort JR1s by vendor, view the date range of the report to signify when these fulltext downloads are being reported, and view the upload date to know how long the report has
been up there (and subsequently how much longer it will be up there because they are removed
after a year). The final column in Figure 12 provides links to delete, download, or get more
metadata about a particular JR1. The common operation is to download the JR1 reports one-byone clicking the download button.
At this point, the data collection procedure for full-text usage has aggregated all the JR1s
for content providers in the form of an individual spreadsheet for each content provider, but there
is still no way to compare data from separate providers or even the same provider without
manually comparing lines in a spreadsheet; furthermore, there is no way to systematically create
more advanced metrics to look at cost-per-download for a specific journal or even more broadly
at the content provider level (i.e., there are this many total downloads from this content provider,
and this content provider is paid this much money).

85

Figure 12: Screen capture of the administrative interface on Serials Solutions used to download JR1 data from various providers
(retrieved with credentials from
https://clientcenter.serialssolutions.com/CC/Library/Counter/CounterReportList.aspx?LibraryCode=PSU).

86

The Serials Solutions interface provides the ability to view reports based on vendor; the
vendor provides access to the journal, and many of these vendors provide access to content from
multiple publishers. In order to create a system that allows collection management to evaluate the
journal usage from all vendors for all journals provided by that vendor, one must download the
JR1 separately from each vendor on the Serials Solutions interface. Each of these reports (see

Table 28) provides the access data for the current year and after the following year; these reports
are expunged from the Serials Solutions database. Table 28 is an example of a JR1 that provides
information on full-text downloads month-by-month and breaks downloads into PDF and HTML
(web page) for each journal listed and gives a total number of downloads for each journal
displayed during the time period covered by the report. Typically, as is the case in Table 28, each
JR1 provides full-text download information on multiple journals provided by a single publisher,
and they are grouped by platform. For example, Elsevier provides many of its journals in the
ScienceDirect platform as listed in this table; however, if this library had journal subscriptions in
additional platforms provided by Elsevier, those platforms would be listed here. The final column
worth mentioning, Print ISSN, is the identification number for that journal.
This restriction necessitates libraries to store this full-text download information locally
in order to do an analysis over periods of one or two years.

87

Journal Report 1 (R4)

Number of Successful Full-Text Article Requests by Month and Journal

Account: Research Library Name
Period covered by Report:
2014-01-01 to 2014-06-30
Date run:
7/15/14
Journal

Publisher

Platform

Print ISSN

Total for all Journals

Reporting
Period Total

HTML

PDF

Jan-2014

657490

356160

301246

95814

AASRI Procedia

Elsevier

ScienceDirect

2212-6716

5

0

5

2

Academic Pediatrics

Elsevier

ScienceDirect

1876-2859

4

3

1

3

Academic Radiology

Elsevier

ScienceDirect

1076-6332

0

0

0

0

Accident Analysis & Prevention

Elsevier

ScienceDirect

Jan-75

1058

628

430

97

Accident and Emergency Nursing

Elsevier

ScienceDirect

0965-2302

66

32

34

15

Accounting Forum
Accounting, Management and Information
Technologies

Elsevier

ScienceDirect

0155-9982

3

2

1

0

Elsevier

ScienceDirect

0959-8022

6

3

3

3

Accounting, Organizations and Society

Elsevier

ScienceDirect

0361-3682

182

81

101

15

Acta Astronautica

Elsevier

ScienceDirect

0094-5765

385

214

171

54

Acta Biomaterialia

Elsevier

ScienceDirect

1742-7061

969

619

350

250

Acta Haematologica Polonica

Elsevier

ScienceDirect

Jan-14

1

1

0

0

Acta Histochemica

Elsevier

ScienceDirect

0065-1281

1

1

0

0

Acta Materialia

Elsevier

ScienceDirect

1359-6454

4309

2376

1933

943

Acta Metallurgica

Elsevier

ScienceDirect

Jan-60

381

0

381

64

Acta Metallurgica et Materialia
Acta Metallurgica Sinica (English Letters)

Elsevier
Elsevier

ScienceDirect
ScienceDirect

0956-7151
1006-7191

137
1

0
0

137
1

17
0

Table 28: Partial listing of a Journal Report 1 that was run on July 15, 2014 to retrieve usage data from Elsevier Journals between
January 1, 2014 and June 30, 2014.

88

Data Collection: Web of Science
Thomson Reuters provides data on journals both specific to the institution in Web of
Science and also metrics that can be applied to a smaller subset of journals in the form of Journal
Citation Reports. Through Web of Science, Thomson Reuters provides an interface to download
citation data for articles with authorship from someone within a specific institution. The citation
data not only contains authorship information for an article but also contains the referenced works
for a published article (see Table 29) enabling an evaluation of how often a journal is published
or cited in order to create a published work. Table 29 has no column headings because there is
no real consistency of the values in each column. They are labeled or coded by the values each
cell. For example, the cell that begins with ‘AU’ signifies the Author of the article, and ‘C1’ is
the home institution for the published author. The citations for an article are exposed with ‘CR’
and each subsequent row is considered a citation for the article until reaching the cell with ‘NR.’
Finally, the information for a particular article is complete indicated by ‘ER’ that stands for End
of Record.
Web of Science provides a great deal of data in their citation report, but for the purpose
of evaluating the journal’s publication and citation count, the only metrics considered are source
articles and citation records.

89

PT J
AU Kociolek
MG
Hoermann
O
AF Kociolek
Martin G.
Hoermann
Olivia
TI SYNTHESIS OF 1
2-BENZISOXAZOLE 2-OXIDES
SO SYNTHETIC COMMUNICATIONS
LA English
DT Article
C1 Pennsyivania State
Erie
Sch Sci
Behrend Coll
MG (reprint
RP Kociolek
author)
Penn State Erie
EM mgk5@psu.edu
CR Aggarwal R
2006 TETRAHEDRON LETT
BIRD CW
1993 TETRAHEDRON
Boulton A. J.
1986 J CHEM SOC P1
J CHEM SOC CHEM
BOULTON AJ
1980 COMM
Boulton A. J.
1987 J CHEM SOC PERK T
Chen DJ
2001 SYNTHETIC COMMUN
Chiari G.
1982 ACTA CRYSTALLOGR B
Das B
2005 SYNTHESIS-STUTTGART
Das B
2004 TETRAHEDRON LETT
Iranpoor N
2002 SYNTHETIC COMMUN
Jadhav VK
2000 SYNTHETIC COMMUN
Kotali A.
2008 MOLBANK
Kotali A.
2008 MOLBANK
Kotali A
2010 CURR ORG SYNTH
LIU KC
1980 J ORG CHEM
Mendelsohn BA
2009 ORG LETT
MORIARTY RM
1986 SYNTHETIC COMMUN
Touaux B
1998 HETEROATOM CHEM
J HETEROCYCLIC
TSIAMIS C
1985 CHEM
TSOUNGAS PG
1988 MAGN RESON CHEM
TSOUNGAS PG
1987 TETRAHEDRON
NR 21
ER
Table 29: Example output for a source article published with author and citation data from
Web of Science. The rows that are in boldface represent data that is stored locally.

90

The data collected from Web of Science includes 4,937 records of published articles in
2012 and 197,149 citations in those publications for the institution analyzed. This information is
stored locally, resulting in a spreadsheet with over 500,000 rows.
Data Collection: Journal Citation Reports
In addition to the data for a specific institution, Thomson Reuters creates Journal Citation
Reports (JCR) (see Table 30) that curate a number of metrics and assemble them in a single
annual report. This data is based on these journals’ global usage and represents a more holistic,
rather than individualistic, view of a journal’s influence. Table 30 lists a set of journals as they
would appear in a JCR. The initial column is the abbreviated title, or title20, the ISSN is the
identifier for the journal, and the various metrics provided by JCR such as Total Cites the journal
received in a given year, Impact Factor, Five Year Impact Factor, etc.
Thomson Reuters throttles the number of JCRs that can be downloaded at a single time to
500, and, because of this limitation, it required 17 separate downloads to collect all the 8,451
journals with JCR data. Once downloaded, these spreadsheets were locally stored for further data
processing and analysis.
Thomson Reuters, in coordination with publishing Journal Citation Reports, prints a
listing (see Figure 13) of all the journals listed in these reports with their journal title and an
abbreviated version of the title referred to as title20. Figure 13 displays a list of the journals in a
JCR and the corresponding journal title, title20 (abbreviated title), the publishing company, and if
the journal is considered Science or Social Science.
The title20 field is used for the names of the journals in the Journal Citation Report and
the Web of Science citation listing. In the Journal Citation Report, the title20 field is the only way
to identify the journal, and in the Web of Science citation data the, cited works listing for a source
article lists the journal names using the same title20 abbreviation.

91

Figure 13: Thomson Reuters listing of journals and their abbreviated names in the Journal
Citation Report (retrieved from
http://scientific.thomsonreuters.com/imgblast/JCRFullCovlist-2013.pdf).
This data is openly accessible on the Internet to be locally downloaded into a PDF file.
This information is useful because it provides a list of journals in the JCR, which provides a
mapping of the journal name and the title20. The mapping of journal names to abbreviated names
is valuable for later data analysis as it links many of these data sources together.

92

Abbreviated Journal
Title
4OR-Q J OPER RES
AAOHN J
AAPG BULL
AAPS J
AAPS
PHARMSCITECH
AATCC REV
ABDOM IMAGING
ABH MATH SEM
HAMBURG
ABSTR APPL ANAL
ACAD EMERG MED
ACAD MED
ACAD PEDIATR
ACAD RADIOL
ACCOUNT RES
ACCOUNTS CHEM
RES
ACCREDIT QUAL
ASSUR

ISSN
16194500
08910162
01491423
15507416
15309932
15328813
09428925
00255858
10853375
10696563
10402446
18762859
10766332
08989621
Jan-42
09491775

Total
Cites

Impact
Factor

5-Year Impact
Factor

Immediacy
Index
0.059

Articl
es

Cited HalfLife

Eigenfactor
Score

Article Influence
Score

17

5.4

0.00162

0.635

0

6.8

0.00083

0.216

0.0063

1.019

270

0.73

1.137

474

0.856

0.977

7081

1.768

2.455

0.413

92

2779

4.386

5.714

0.604

91

4.7

0.00827

1.537

2606

1.584

1.906

0.171

164

5.3

0.00591

0.418

205

0.354

0.297

0.067

30

8.3

0.00024

0.065

2345

1.905

1.861

0.304

125

6.6

0.0045

0.458

399

0.568

0.461

0

10

0.0009

0.68

1244

1.102

1.183

0.349

811

2.3

0.00389

0.358

6020

1.757

2.425

0.445

191

6.9

0.01575

0.905

8646

3.292

3.284

0.795

200

7.5

0.0206

1.189

569

2.328

3.017

0.46

63

2.9

0.00365

1.174

3876

1.914

2.068

0.48

196

5.9

0.00999

0.635

159

0.756

0.087

23

6.4

0.0004

42112

20.833

24.633

5.295

207

6.5

0.10832

7.951

671

1.132

0.885

0.19

63

6.1

0.00118

0.19

>10.0

>10.0

Table 30: Partial listing of Web of Science Journal Citation Report displaying multiple metrics for journals..

93

Data Collection: Financial Data
There are two methods of getting financial data as a means to understand a journal’s
annual subscription fees. The first method is to simply find the pricing sheet of a journal on its
webpage; the second method is to find out a precise dollar amount paid for that year from the
library’s financial department. If the intention is to create institution-specific metrics for
collection management and to understand a resource’s real value, then finding the exact funding
amount improves the accuracy and subsequent value of the metrics being created that include
journal pricing. Fortunately, we had cooperation from the financial department of the library, and
they provided us with three sets of data. They provided us with: 1) a spreadsheet of databases and
the annual subscription paid for databases in 2012 (see Table 31), 2) a spreadsheet of all annual
purchases made in 2012, which included the individual purchase prices as well as package price
for journals (see Table 32), and 3) a spreadsheet with the fund area that was used with each
purchase order. Table 31 lists the databases purchased during this year. A database is a synonym
for what JR1 refers to as a platform. The additional information in this spreadsheet includes the
annual cost of the database, the number of titles that are included in this database (if available),
and the provider of the database. Table 32 has similar information to Table 31 (cost and
provider); however, Table 32 displays the information for each journal purchased in 2012 and
the ISSN, the database that the journal is considered to be a part of, and finally the order ID. The
order ID is an internal number for tracking subscription purchases that allows us to figure out the
fund area for each order ID.
The data from several sources was localized, and we started processing data for more
critical analysis, creating an ability to perform more advance metrics based on the conglomeration
of these data points rather than of just a single data point.

94

Database
Business Source Premier
CINAHL
Communications and Mass Media
Criminal Justice Abstracts
EconLit
Film & Television Literature Index
Historical Abstracts (EBSCO)
History of science, technology, and medicine
LGBT Life with Full Text
Library Literature & Information Science
Peace Research Abstracts
Race Relations Abstracts
Readers' Guide Retrospective: 1890-1982 (H.W. Wilson)
Single Journals
Wildlife & Ecology Studies Worldwide
Women's Studies International
ATLA Religion Database with ATLASerials
Ecological Society of America Publications
Healthcare Standards Directory
Electrochemical Society Digital Library
MD Consult Core Collection
Reaxys
ScienceDirect Journals
Engineering Village
INSPEC
Endocrine Society Journals
Arab-Israeli Relations, 1917-1970: The Middle East Online Series 1

Provider
EBSCO
EBSCO
EBSCO
EBSCO
EBSCO
EBSCO
EBSCO
EBSCO
EBSCO
EBSCO
EBSCO
EBSCO
EBSCO
EBSCO
EBSCO
EBSCO
EBSCO via CIC
Ecological Society of America via EBSCO
ECRI Institute
Electrochemical Society via EBSCO
Elsevier
Elsevier
Elsevier
Elsevier Engineering
Elsevier Engineering
Endocrine Society
Gale

Number of Titles
9381
58
Not Provided
Not Provided
Not Provided
Not Provided
Not Provided
Not Provided
316
Not Provided
Not Provided
Not Provided
Not Provided
19
Not Provided
Not Provided
247
5
Not Provided
14
139
Not Provided
1077
Not Provided
Not Provided
4
Not Provided

Cost
$54,221.00
$6,830.00
$2,000.00
$5,240.00
$6,544.00
$1,276.00
$26,448.00
$4,953.00
$8,626.00
$2,690.00
$1,910.00
$1,533.00
$688.00
$8,493.32
$9,338.00
$10,650.00
$17,301.00
$1,364.00
$3,935.00
$5,831.00
$98,000.00
$58,023.00
$2,839,073.03
$87,566.00
$75,867.00
$9,743.00
$530.45

Table 31: Partial listing of database purchases in 2012.

95

Order
ID
PO3909
PO3911
PO3915
PO3925
PO3926
PO3928
PO3929
PO3942
PO3951
PO4012
PO4076
PO4237
PO4273

Journal

ISSN

Database
Provider
Physics of metals and metallography [business
SpringerLink Contemporary
Springer via
record]
0031-918X (1997 - Present)
Harrassowitz
SpringerLink Contemporary
Springer via
Russian metallurgy [business record]
0036-0295 (1997 - Present)
Harrassowitz
Human nature: an interdisciplinary biosocial
SpringerLink Contemporary
Springer via
perspective [business record]
1045-6767 (1997 - Present)
Harrassowitz
SpringerLink Contemporary
Springer via
Atlantic economic journal.
0197-4254
(1997 - Present)
Harrassowitz
Review of world economics =
SpringerLink Contemporary
Springer via
1610-2878
Weltwirtschaftliches Archiv / IfW.
(1997 - Present)
Harrassowitz
SpringerLink Contemporary
Springer via
Physics of particles and nuclei.
1063-7796
(1997 - Present)
Harrassowitz
SpringerLink Contemporary
Springer via
Russian journal of physical chemistry. A.
0036-0244
(1997 - Present)
Harrassowitz
SpringerLink Contemporary
Springer via
Behavior research methods.
1554-351X
(1997 - Present)
Harrassowitz
SpringerLink Contemporary
Springer via
Experimental mechanics.
0014-4851
(1997 - Present)
Harrassowitz
Izvestiya. Physics of the solid earth [business
SpringerLink Contemporary
Springer via
record]
1069-3513 (1997 - Present)
Harrassowitz
SpringerLink Contemporary
Springer via
Management international review.
0938-8249
(1997 - Present)
Harrassowitz
SpringerLink Contemporary
Springer via
Pal├ñontologische Zeitschrift.
0031-0220
(1997 - Present)
Harrassowitz
SpringerLink Contemporary
Springer via
Symbiosis.
0334-5114
(1997 - Present)
Harrassowitz
Table 32: Partial listing of journals purchased in 2012, the database they belong to, and line item cost of that journal.

Cost
$5,754.92
$3,194.63
$437.40
$291.72
$216.78
$6,083.54
$6,428.20
$364.42
$987.01
$1,794.26
$533.81
$476.39
$977.21

96

Data Processing & Analysis
Once the data has been aggregated, the next step is to process that data into a single
database of information. This requires parsing and processing all of the information that has been
gathered by the several data sources. The following sections discuss the methods used to process,
parse, and analyze the data collected from JR1, Web of Science, JCR, and the finance department.
Data Processing & Analysis: Journal Report1
Once the full-text downloads for content providers had been accumulated into 31 separate
spreadsheets documenting journals’ usage in 2012, we developed a script to process these
spreadsheets, parsing out relevant information and inputting that data into a local database (see

Figure 14). Figure 14 shows downloading the JR1s from a single source at Serials Solutions
into a separate spreadsheet for each provider because the JR1s are created individually by each
provider spreadsheet. Those spreadsheets are then parsed and their data is imported into a single
table in the database.
In some cases, data may be missing for a particular month or months. When this
happened, we averaged downloads-per-month based on available data and averaged it over the
year. The local database adds flexibility to the query with more sophisticated techniques than a
spreadsheet and creates the ability to quickly filter out potentially unnecessary results. For
example, a query could be written to compare the usage of two separate journals and only view
information that may have been previously concealed within two separate spreadsheets.

97

Figure 14: Data processing flow for full-text downloads for journals to a local database.
Data Processing & Analysis: Web of Science
The biggest obstacle to overcome in processing the Web of Science data from several
local spreadsheets to parse into meaningful metrics is primarily the amount of low-relevance data
needing to be discarded. Each field in the spreadsheet contains a code to identify the data that is
stored in that cell within the spreadsheet; by mapping this information, it is possible to
programmatically process these spreadsheets. These programmatic processes (see Figure 15) are
necessary because the volume of data being analyzed (over 500,000 lines of data) would take
weeks to manually examine without incorporating the other data points being integrated into the
system that enable more advanced metrics beyond raw citation counts.

Figure 15: Data processing flow for Web of Science citation data to a local database.

98

Figure 15 illustrates downloading the citation information from a single source at Web of
Science into several different spreadsheets because Thomson Reuters does not allow more than
500 records to be downloaded at a time, and a large research library’s prolific annual publication
rate among its researchers will require several downloads to get the information for all of the
publications in a given year. Once those spreadsheets have been downloaded, the process splits
the records up from citations and authored records. For example, the published article is imported
to the source table, and all of the citations from that published article are imported to the citation
table.
Data Processing & Analysis: Journal Citation Reports
Processing the JCR data required two separate steps, one of which was vastly different
than previous data processing done to create this system. The only systematic technique to get the
journal names and abbreviated names (i.e., title, and title20) was by using optical character
recognition (OCR) software. Thomson-Reuters provides the PDF of names and abbreviated
names, but this file format can only be parsed with OCR software as opposed to parsing the data
in a spreadsheet as previously done. Again, the amount of data being appraised, in this case the
number of journals recorded in this PDF is 11,076, requires a programmatic approach (see

Figure 16). Figure 16 illustrates, at the top, the process of downloading the PDF of journals in
a JCR and using OCR to import those journal names from the PDF into a local database.
Once the journal names and abbreviations have been imported to a local database, the
import process for the JCR data is similar to that for Web of Science and JR1 data, with the
exception that we can link JCR data with the journal name (title) and abbreviated name (title20).
This is important because the Web of Science citation data provides titles in both formats (title,
and title20) making the ability to link these fields by the OCR software critical to ultimately bring
these metrics together. The bottom of Figure 16 depicts the process for downloaded the JCR into

99

separate spreadsheets based on the journals listed in the PDF, and then importing that data from
the spreadsheets into a local database.

Figure 16: Data processing flow for Journal Citation Reports to a local database.
Data Processing & Analysis: Financial Data
The biggest advantage to getting financial data is perhaps the biggest disadvantage: the
data can be received in almost any format depending on the finance department’s accounting
software. The lack of standardization can be overcome by a desire to provide accommodating
requirements from a local administrator. Processing financial information was done in three
phases (see Figure 17) in order to get both database and journal subscription payment
information, and then finally to process the funds used for purchasing these subscriptions into a
local database. Figure 17 demonstrates the processing of the financial information stored about
journals into the line item table in the database, then parsing the funding data for each order ID
and linking the fund area into a funds table in the database, and finally processing the financial

100

information from the annual purchases made on databases into another separate table in the
database.
The critical step in this data processing was to link the purchase orders in the journal and
database subscription information (in the first two steps) with the fund areas (imported in the
third step) to be able to create database queries that would enable an administrator to see what
journals or databases are purchased with what funds and create more meaningful metrics than
currently exist.

Figure 17: Data processing flow for financial data for institution into a local database.

4.4 Results
Research Objective 1
In order to effectively create a conceptual model of research objective 1, the initial action
is to start defining data sources and the data entities provided from those sources. The disparate
sources of data include Serials Solutions’ JR1s, Web of Science citation data, JCRs, and

101

internally held financial data. In this instance, Serials Solutions aggregates the usage data for all
of the journals and databases in one administrative interface; however, this is merely a
convenience mechanism to obtain the data; if the data was not provided by Serial Solutions, then
library administrators could obtain usage data in the form of JR1 reports for each individual
subscription the library has access to. This convenience does not affect the conceptual model or
the relationships that exist; it only facilitates the process to get the usage data. Similarly, in data
collection and processing, we have downloaded data from several data sources and imported them
into a local database to create an entity model (see Figure 18) that can be leveraged to run
advanced queries that were not possible with spreadsheets or data existing on content provider
servers.

102

Figure 18: A display of the data modeled based on many of the spreadsheets from various
sources. Many of these tables exist for the simplicity of query and error checking.

Figure 18 clarifies the entities that exist for evaluating these resources, but it has not yet
created the relationships between these entities. The Journal Report 1 table lists all of the full-text
download data for all the journals that this is available. The Source and Citation tables list the
data for the authored publications and the articles they cited respectively. The name and area for
each fund is listed in the Fund table; Journal Citation Report aggregates all the data from the
JCRs. Line items and database tables record the information on databases and journals
respectively from the internal financial department.

103

This data listing is advantageous, as the data is now all in one location and format;
however, there is more to be gained by creating links between these entities. These links will give
us a strengthened conceptual model because it will further define the relationships and turn these
disparate data sources that have been localized into a relational database. The first step in linking
this information is to create three separate tables to represent the Publisher, Provider, and
Platform that we subscribe to annually (see Figure 19). The publisher creates the content, the
provider supplies the contract and ultimately who is paid the subscription fee, and the platform
represents the groupings that journals are sold under. In some cases, publisher and provider are
the same, and in some cases, a journal is not sold as part of a platform and is sold singularly.

104

Figure 19: Creating new Publisher, Provider, and Platform tables from the existing
information in the financial reports.

Figure 19 demonstrates moving the data from the existing tables that replicated the
journal and database spreadsheets into a more relational model. This more relational model
eliminates data duplication for the publishers, platforms, and providers; also, this makes it

105

possible to query or filter results in a system based on any of these criteria (platform, publisher, or
provider).
The power and complexity of enhancing the data model is in building the relationships
between the existing entities. When building these relationships, it was helpful to re-imagine the
existing entities as well to improve efficiencies (i.e., storing data in only one entity). For example,
instead of an entity to represent the line items purchases for a fiscal year, it becomes more
scalable to represent these in two separate entities (see Figure 20). In Figure 20, the separation
and aggregation of data becomes more apparent. For example, one entity, journals, represents the
unique information about a journal that identifies the journal and rarely changes (i.e., title, title20,
ISSN). The second entity represents information about the journal that changes with more
frequency, which a model will want to more accurately update on an annual basis. For example,
some fields are less likely to change such as platform or provider, but others fields that largely
represent metrics to evaluate the journal such as Impact Factor and Article Influence Score will
change annually; this system can track that evolution. Many of the meaningful statistics are now
recorded in the Journal Metrics table (see Figure 20) aggregating the usage data from JR1
reports, global metrics on JCR, the citation data from Web of Science for the institution, and the
financial data paid for the subscription (see Figure 21). This model allows for the ability to do a
year-to-year comparison, as well as to compare journals within a particular year. For instance, it
would be simple to look at journals that only have a high cost-per-use or low cost-per-use, or to
view journals that are cited a certain number of times and those that an institution does not
publish. The iterations of queries are vast because this model brings so many entities together and
creates relationships that do not exist in the silo system initially designed by data collection and
analysis alone (see Figure 18).
In Figure 20, the metric linked from the Web of Science citation data is simply the raw
citations and authorship numbers; however, this model retains the relationship back to both the

106

Source and Citation data for potential future metrics that can be created with more complex
investigations about bibliometrics. In addition to having this data in one place for quick
comparison, there is an element of comprehensiveness that may not exist in other data stores of
this information. That thoroughness is to store this data as it is updated on an annual basis.
Instead of requiring a download of these data sources for prior years to see emerging trends, this
model will enable that type of analysis more simply.

107

Figure 20: Creating new Journal, Journal Metrics tables from the existing data sources.

108

Figure 21: Linking the funding data into the Journal Metrics tables.

109

Figure 21 displays how the funding information is split into the necessary names and areas and
then recorded with the purchase order for a journal. This record of the funding area tied directly
into the journal subscription allows filtering journal evaluation statistics by funding area.
The conceptual entity relationship model created by linking the entities together based on
their associations in the data collection and analysis phase is capable of more complex queries
and ultimately more complex metrics to evaluate electronic resources (see Figure 22). In Figure
22, the representations are that a publisher creates the content and a provider provides the content
published. Content is published in either a journal or a database, and a journal may or may not
exist within a database. A journal should always have a provider and a publisher; however, a
journal may not always have a platform, and these relationships may change on an annual basis.
The potential changing relationships are why Figure 22 splits out the metrics for a journal
because those also change annually. Journals potentially change titles on an annual basis; if this is
the case, it would be recorded as a new journal to track as the ER diagram Figure 22 illustrates
by breaking out those annual metrics from title, title20, and ISSN. Finally the funding area is tied
into the annual journal metrics table as well because where the funding comes from may also
change on an annual basis.
This ER diagram enables a more robust set of queries on electronic resources than exists
today. For example, this model can return data based on broad questions (i.e., “How many
journals exist in the business fund area?”) and specific metrics (i.e., “How many journals cost
over $3.50 per download in 2012?” or “How many times was Nature cited by publications from
this institution in 2012?”). This provides both a broad view of electronic resources at an
institution and enables more advanced evaluations of specific resources. This model does not
automate the evaluation of journals, but rather, it enables a more automated mechanism to
identify outliers based on specific criteria. Identifying outliers more simply enables those in
collection management to spend more time evaluating these outliers on a case-by-case basis.

110

Figure 22: Entity Relationship diagram to help represent the association between the
objects.
Research Objective 2
This conceptual model facilitates implementing a robust evaluation system for electronic
resources at scale for a large research library. This model does this by homogenizing valuable
sources of information that were previously heterogeneous (see Figure 23). For example,
financial subscription costs and journal usage data that are no longer segregated can create more
informative compound metrics (i.e., cost-per-use). This model recognizes the importance of
automation in scaling to a large research library because without the ability to automate importing
the data and linking the data, there would be no realistic method to aggregate this information and

111

create metrics from multiple data sources. Another technique required for automation is the
consideration that this data will be refreshed on an annual basis and that the model must account
for yearly updates in order to provide accurate metrics as well as the capability of a temporal
analysis. An ancillary benefit of this model is the establishment of an authoritative data source for
this information to use as a standard for evaluating electronic resources.

Figure 23: An abstraction of the conceptual model that shows the various data sources
being brought together and benefits this model provides.

Figure 23 explains the various data sources and types (e.g., Web of Science, Journal
Report1, Journal Citation Report, Financial data) that are aggregated in this conceptual model.
The aggregation of this data allows for more informative compound metrics, automating the
data aggregation, de-duplication of data, an annual update of this information, and an
authoritative source of this information that can be shared. These benefits allow collection
managers more time to spend analyzing electronic resources and less time parsing information

112

from these many data sources.
Now that we have created a conceptual model representing the entities and their
relationships, we will develop a system architecture (see Figure 24) to represent our conceptual
model and implement the design, as set forth in research objective 2. Based on an implementation
of the system architecture, we will create a web front-end that will enable new analysis
capabilities.

Figure 24: System architecture diagram to represent the implementation of the conceptual
model from data collection through implementation.
In Figure 24, internal and external data sources represent the several sources for data
collection, such as, content providers’ JR1 full-text download reports, Web of Science, JCR, and
financial data. These data sources reside in various areas, in a number of formats, and because of
these numerous formats one of the more complex portions of the system is the parsing engine.
The parsing engine in Figure 24 is a series of scripts that takes data files and stores them in a
standard database that can be queried using the structured query language (SQL). At this point,
the parsing engine is specific for each data source and is customized based on assumptions made
in how the data will be formatted based on the data source. The parsing engine knows how to take
the custom data files and turn them into a more standard database.
Now that the parsing engine has created a standard data store in a local database, Figure

24 displays the linking engine, to create new entities from the existing entities and to cement

113

relationships that previously did not exist. This is another series of scripts that are customized
based on the structure of a local database and the relational database system that is being created.
For example, some relationships in the newly formed database may need to be created before
other relationships. Ultimately when the relational database is created, this will be the canonical
data source.
Once the relational database is created in Figure 24, there are a number of web
application frameworks that have default search, sort, and filter web interfaces to interact with the
data stored in the database (see Figure 25). Any of these frameworks would provide an
authoritative source for this information to be viewed, shared, and analyzed by any number of
users via the web as Figure 25 displays the collection managers interacting with a web site to
view this data.

114

Figure 25: Web interface that allows search and filtering of the relational database
(interface was created by the Active Admin Ruby Gem retrieved from
http://activeadmin.info/).

115

Figure 25 is a web interface that allows complex search queries to be run on any of the
fields we implemented from the conceptual model. There are dropdowns for providers, platforms,
fund areas, etc. that allow a user to filter the results of their analysis based any of those fields (i.e.,
only show journals from the Elsevier ScienceDirect filters based on specified provider and
platform).
Research Objective 3
The third research objective is to validate the implementation of the conceptual model
and processes. Based on the conceptual model, we have a system that represents an
implementation of this model. This system evaluated electronic content using real world data
from a major research library. The intention of this evaluation was to validate the system by: 1)
qualitatively determining the distinction of usage and cost of electronic resources between
different disciplines, and 2) creating a model to predict the annual usage of journals in order to
develop an acceptable cost range to pay for an annual subscription.
In order to evaluate the distinction in usage and cost among different disciplines, we
analyzed these two variables (usage, and cost) based on funding area. The underlying assumption
is that based on the funding area (i.e., Arts & Humanities, Business, Engineering, Social
Sciences, etc.), we could determine if those electronic resources had content that corresponded to
the discipline of the funding area. This system was able to validate the distinction in the usage
and cost of electronic resources based on funding area, and which substantiates the notion that
hypothesizes that in order to evaluate electronic journals, it must be done on a discipline-bydiscipline basis. There are varying factors, such as usage and cost that make comparing electronic
resources across disciplines inequitable. An inequitable evaluation can lead to an analysis that
inappropriately designates impact to a resource that is unwarranted or vice versa. This not only
validates using the system to evaluate electronic resources, it also validates the assumption that
disciplines should be evaluated separately. An additional benefit of this system is that this
examination was also able to detail the cost of the journals that were not reporting standard usage

116

results and that this information can equip collection managers with data that aims to enforce
standards are met so the library can systematically evaluate these resources (Coughlin et al.,
2014). In this instance, the system validated a method for evaluating journals (by discipline) and
substantiated using standards with a quantitative metric given that the was $196,786 annually
spent on electronic resource subscriptions that can not be evaluated because they do not meet
reporting standards for usage data.
Another validation of this system was to predict annual full-text downloads of a journal
using this system. Aggregating several of these data points together facilitated creating a
predictive regression model that was able to accurately guess a range of usage for particular
journals based other existing metrics. We used this system to organize a number of meaningful
journal statistics both global (i.e., Impact Factor, Eigenfactor) and local (i.e., number of times a
journal was cited at a particular institution in a given year). Using this system enabled us to
create a predictive model based on several of these independent variables, both global and local,
at one time that were historically disjointed. This predictive model was able to accurately forecast
a range of usage for electronic resources in a given year. Knowing the range of usage for a given
year made it possible to create an acceptable price range to pay for a journal based on that
predicted usage; for example, if an institution will not pay more than $3.00 per download, then
this system enables the creation of a model to predict the usage of a journal and combines the
prediction for usage with the cost of a journal. An accurate usage prediction and subscription cost
may resolve issues that may arise from bargaining with content providers or publishers, or at the
very least, help inform negotiations to prevent an institution from paying more than the
acceptable range. The institution assessed could see savings in the $496,183 - $1,356,929 range
per year by using this system to develop complex prediction formulas and associated ranges for
acceptable subscription fees (Coughlin & Jansen, 2014). The results of this system validated our
model because we aggregated many data sources (i.e., Web of Science, financial data, Journal
Citation Reports, etc.) and data points (i.e., Impact Factor, local citation numbers, etc.) that were

117

meaningful, and through this aggregation, we verified the significance with further analysis; for
example, we proved the importance of evaluating journals based on their discipline, we
confirmed the need for standards in order to scale this system for a large research library, and we
verified the significance of created complex metrics generate new models for predicting usage
and informing decisions in collection management.
4.5 Discussion and Implications
The advantages of this model and subsequent implementation are: 1) a greater
understanding of metrics, 2) the ability to create new metrics, and 3) a common place to view the
data. This model is able to aggregate data from several sources, which enables immediately
creating new metrics and generating a side-by-side view with other data that is relevant to
evaluate journals (see Figure 26). Figure 26 demonstrates how a user can see cost, cost-peruse, and total downloads for a number of journals at one time and can sort by these fields as well.
For example, by merging the cost of electronic resources with the full-text download
numbers for an electronic resource, this system can create a cost-per-use metric. To look at either
cost or download numbers in a vacuum would create an incomplete view of a journal’s value.
This system provides an understanding of specific metrics that are created here and global metrics
that are provided from other sources to evaluate journals (see Figure 26). Figure 27 illustrates
the benefit to having these global metrics (Impact Factor, Eigenfactor, etc.) with metrics such as
funding area. The combination of data from figures Figure 16 and Figure 27 empower a user
to see these many metrics listed at one time on one screen. Having a web interface provides a
mechanism to easily share this information at any time. Anyone given authorization access to the
system and who has an Internet connection can view the data and provide feedback. Additionally,
this interface creates a canonical data source for those who are evaluating journals to view. A
common problem in evaluating journals becomes wondering what data each person involved in
the evaluation is looking at and where that data came from. Instead of sifting through emails and

118

attachments and wondering who has the most up-to-date information, this system provides an
established data source that can be agreed upon for evaluation.
Limitations that exist within this system are generated from the complexity of the current
environment that exists within electronic resources. For example, because there are so many data
sources, the parsing engine may have to be updated annually to account for modifications that are
made to imported data that is not standardized. If the PDF for the journal listing of JCRs changes,
there will need to be corresponding efforts to change the script that runs the OCR on that PDF.
The data this system provides is limited to what outside sources are willing to provide; there is
not usage data for all the journals we provide access for, and the system is dependent on that data
to be accurate from vendors with no true way to determine the data’s validity. JCRs exist for over
10,000 journals, but this is just a fraction of the journals we provide access to. Information on
databases that the library subscribes to annually is limited because content providers do not
update the journals in a database package in real time. For example, a database may lose access to
a journal, but the system has no real way of knowing that access has been systematically lost.
Based on the strengths the system has, naturally it is good for certain types of analysis.
For example, this system would allow for the analysis of usage and the amount of money spent on
journals based on funding area and presumably area of discipline the journal serves. It is
commonly considered that evaluating journals in different disciplines is evaluating “apples to
oranges.” Filtering journals based on funding area generates a more appropriate evaluation of
journals, and this system allows for that capability, ultimately creating a more equitable analysis.
Another strength of this model is the ability to analyze the strength of different metrics and their
relation to each other. For example, are there specific metrics that relate to full-text downloads
more strongly than to other metrics? A thorough understanding of how the metrics relate to each
other or even are predictive in nature can provide insight into how this system can further add to
the evaluation process. This system and the capabilities it provides enable these types of analysis;
this capability is validated by the results of these studies in two previous evaluations (Coughlin et

119

al., 2014; Coughlin & Jansen, 2014). In the first analysis, we were able to compare the usage and
cost of journals--and even more specifically, journals from a particular provider and platform-based on funding area. In the second analysis, we were able to compare metrics to see those that
had stronger correlations to full-text downloads. Then, based on that assessment, we could create
a model to predict full-text downloads for journals. This model enables collection managers to
have a better sense of how frequently journals will be used in a given year and subsequently a
cost range a collection manager may be willing to pay based on the range of usage.

120

Figure 26: Administrative web interface providing a view of the metrics created specific to the institution (interface was created with
Active Admin Ruby Gem http://activeadmin.info/).

121

Figure 27: Administrative web interface providing a view of the Journal Citation Report information stored in the database (interface
was created with Active Admin Ruby Gem http://activeadmin.info/).

122

4.6 Conclusion
The current climate of collection management is complex for several reasons. The
reasons for complication include big package deals (Blecic et al., 2013), metrics that exist in a
number of places (if at all), and the amount of data that exists to perform an equitable evaluation.
Some of these complexities could be alleviated with a system that could remove or simplify areas
of the complexity in order to scale an evaluation needed at a large research university. For
example, the complication around subscription packages cannot be solved by technology alone;
however, equipped with the proper analytics, collection managers could enhance their position at
the negation table and improve the current situation. The conceptual model provided in this
research provides an example of how the existing entities and their relationships could be created
in order to simplify some of this complexity. This model is perhaps the most difficult outcome to
create because it requires a deep understanding of the problem existing in electronic resources
and it offers a general view to create a more simplified version of the problem. This more
simplified view enables more complex analytics by combining information from disparate
sources, for instance, cost and full-text downloads to create cost-per-use.
The implementation of this model provides a look at the more challenging aspects of
implementation, which are the parsing engine and linking engine. The parsing engine presents a
mechanism to automate the aggregation of this data on an annual basis; however, because this
parsing engine relies on outside data there may require future modifications. In the case of JR1
(full-text download) data, we are dealing with standards. So, any modifications should be
relatively minor. Where standard formats do not exist, libraries should push content providers to
create and follow standards. As libraries become more equipped with data on electronic
resources, they can make standards and reporting for the purposes of rich analytics a demand. The
complexity of the linking engine is due to the intricacies that exist within the associations for each
entity. There would likely be additions or modifications to this engine as it matures to enhance

123

the system, but this is something that is based on the complexity of user requirements in a system
more than dealing with the complexity of the data the system is analyzing.
The work done here shows it is possible for a model to be created and implemented to
evaluate electronic resources on a large scale. As proof of this, we have two separate evaluations
of electronic resources using an implementation of this system. The information provided in by
this system can contribute to a more robust evaluation of electronic resources. This evaluation is
intended to show those resources that do not provide value and may be candidates for weeding
out of the collection. These candidates potentially selected for weeding out are considered outliers
as most electronic resources provide value and should remain in the collection. This system will
provide librarians the ability to illustrate that value as well. Illustrating the great value that many
of these resources provide is of significant importance as well to ensure funding of these
resources. Establishing sustainable evaluation and payment models is in the best interest of both
the library and content providers.

124

Chapter 5
Ultimately, this research creates a foundation for a framework to evaluate electronic
resources at a large research library. A large research library may spend millions of dollars on
these resources and must create mechanisms to understand the value these resources provide.
The different systems and sources of data used to evaluate electronic resources such as financial
data, bibliometrics, and usage data create a complex ecosystem for a large-scale comprehensive,
examination. There intricacies that exist in coalescing this data create a time consuming process
that leaves little room for evaluation. The ability to automate many portions of this would reduce
the time necessary to bring this information together, creating additional time for evaluation.
Additionally, the ability to quantitatively find electronic resource outliers narrows the scope that
is required for evaluation.
The first research objective developed a model to evaluate journals focusing on usage,
cost and content. This model definitively demonstrates the comparison of these journals must be
done on the basis of similar disciplines or there is a risk of partisan view. The second objective
created a model for predicting electronic resource usage based on several metrics. Based on this
prediction, a library can be more prepared to understand the value a journal will provide at their
specific institution and have a better informed decision on the annual subscription fee. In
addition, this work substantiates the need for libraries to create metrics that are specific to their
institution. Administrators, librarians, and collection managers cannot rely solely on metrics that
are provided to them based on global figures, because these numbers may not illustrate the value
at a local institution. The third research objective proved that a system could be developed to
automate considerable portions of this work and provide value in creating advanced metrics for
evaluating electronic resources. This system was used to create an application that was used in
creating the evaluations for the first two goals of this research.
This research study contributes to the literature on the evaluation of electronic resources.
Annual subscription prices and spending budgets cannot increase in perpetuity. A healthy

125

ecosystem for content providers, publishers, and libraries will involve the ability to honestly
assess the value these resources provide in order to maintain access to valuable scholarly
materials for the advancement of research. This work constitutes methods to examine these
resources, metrics to use for assessment, and a tool that alleviates much of the effort involved in
the processes required for these evaluations to create a framework for the evaluation of electronic
resources at a large research library.
5.1 Strengths and Limitations
A limitation of this work is that we currently only have data for one year. However, the
data we have is quite meaningful, in particular the metrics that are specific to this institution (i.e.
citations, articles authored, full-text-downloads, etc.) and compound metrics created by
combining two metrics (i.e. combining cost and use to get cost-per-use) that otherwise would not
exist. It may be fair to wish for a data set that spans multiple years, but fortunately the
automation and models created in this work will create an environment to analyze data over
multiple years. There is no consistent method to evaluate aggregator packages because the
content providers do no provide real-time updates as to the content (i.e. journal titles) they
provide access to. This may be an example of content that should be considered “as needed” by
the library based on other metrics we are now able to evaluate for fund areas. If the library
cannot accurately examine the value of the resource according to best practices and standards,
then we have shown the value to be considered questionable. Certainly, it would be great to have
a more thorough evaluation of these resources; fortunately this model provides the ability to
determine the value of other resources within those areas (i.e. fund areas) and ultimately
determine the necessity of these packages. Metrics based on web analytics cannot determine user
motivation or satisfaction. However, with web analytics, we are able to determine a widely held
standard for determining some level of value and we can abstract that value to many different
types of usage (i.e. authorship, citation, searches, downloads, etc.). Web analytics as a method
provides strengths and weaknesses. We can abstract those strengths to leverage the various types

126

of usage depending on the category of evaluation to understand a sense of value within that
resource. By using web analytics it creates a natural framework to develop an application on the
Web to help create complex metrics to evaluate electronic resources that can be shared among
those involved in the selection process for these resources (Coughlin & Jansen, 2014). This
creates an authoritative source of data to evaluate these journals that otherwise would not exist.
Having one source of data can eliminate confusion and provide a more seamless method to
evaluate journals.
5.2 Future Work
Collecting additional years of data will provide additional avenues for future theoretical
work. One piece of analysis that can be done is a temporal analysis of this data. Constructing an
analysis over time will provide insight into other emerging trends within usage; some global
bibliometrics are done over two and five year spans, and over time we can do the same type of
normalization with this data set that pertains to the local institution and it’s users specifically.
Adding additional years to the data set collection may also improve accuracy, and find outliers
that were non-existent with a single year of data. There could be more analysis done on emerging
patterns for authors or journals based on specific local bibliometrics (i.e. do specific fund areas
have a closer correlation to global metrics like Impact Factor). On the more practical perspective
real time analytics of aggregator packages can be done to include the most up to date content.
This would likely involve the effort of a consortium to convince content providers that it is in
their best interest to provide real time analytics. As is the case with all electronic resources, if
they are providing value it should be relatively easy to show this value in either full-text
downloads, citation numbers, authorship, etc. If the value of the content is hidden from all of
these metrics, perhaps there is no value to show.

127

References
Arnold, D. N., & Fowler, K. K. (2011). Nefarious Numbers. American Mathematical Society,
58(3), 434–437.
Bartsch, R. A, & Tydlacka, B. L. (2003). Student perceptions (and the reality) of percentage of
journal articles found through full-text databases. Research Strategies, 19(2), 128–134.
Bensman, S. J., Smolinsky, L. J., & Pudovkin, A. I. (2010). Mean citation rate per article in
mathematics journals: Differences from the scientific model. Journal of the American
Society for Information Science and Technology, 61(7), 1440–1463.
Blecic, D. D., Wiberley Jr., S. E., Fiscella, J. B., Bahnmaier-Blaszczak, S., & Lowery, R. (2013).
Deal or no deal? Evaluating big deals and their journals. College & Research Libraries,
74(2), 178–194.
Blessinger, K., & Olle, M. (2004). Content analysis of the leading general academic databases.
Library Collections, Acquisitions, and Technical Services, 28(3), 335–346.
Chambers, M. B., & So, S. (2004). Full-text aggregator database vendors and journal publishers:
A study of a complex relationship. Serials Review, 30(3), 183–193.
Chan, K. C., Chang, C.-H., & Chang, Y. (2013). Ranking of finance journals: Some Google
Scholar citation perspectives. Journal of Empirical Finance, 21(March), 241–250.
Chang, C. L., McAleer, M., & Oxley, L. (2010). Journal Impact Factor versus Eigenfactor and
Article Influence. KIER Working Papers (737). Kyoto University, Institute of Economic
Research.
Chung, H. K. (2007). Evaluating academic journals using Impact Factor and Local Citation
Score. The Journal of Academic Librarianship, 33(3), 393–402.
Coleman, A. (2007). Assessing the value of a journal beyond the Impact Factor. Journal of the
American Society for Information Science and Technology, 58(8), 1148–1161.
Collins, T. (2011). The Current Budget Environment and its Impact on Libraries, Publishers and
Vendors. Journal of Library Administration, 52(1), 18–35.
Conyers, A., & Payne, P. (2011). Library performance measurement in the digital age. In
University Libraries and Digital Learning Environments (pp. 201–214). Ashgate
Publishing.
Coughlin, D. M., Campbell, M. C., & Jansen, B. J. (2013). Measuring the Value of Library
Content Collections. Proceedings of the American Society for Information Science and
Technology, 50(1), 1–13.

128

Coughlin, D. M., Campbell, M. C., & Jansen, B. J. (2014). A web analytics approach for
appraising electronic resources in academic libraries. Journal of the American Society for
Information Science and Technology. Manuscript accepted for publication.
Coughlin, D. M., & Jansen, B. J. (2014). Modeling journal bibliometrics to predict downloads
and inform purchase decisions at university research libraries. Journal of the American
Society for Information Science and Technology. Manupscript submitted for publication.
Coughlin, D. M., & Jansen, B. J. (2014). A Systemic Approach to the Evaluation of Electronic
Resources Management at a Major Research University Library: From Concept to
Implementation to Assessment. Journal of the American Society for Information Science
and Technology. Manupscript submitted for publication.
Duranceau, E. F. (2002). Electronic Journal Forum. Serials Review, 28(01), 49–52.
Duy, J., & Vaughan, L. (2003). Usage data for electronic resources: Vendor-provided statistics.
The Journal of Academic Librarianship, 29(1), 16–22.
Duy, J., & Vaughan, L. (2006). Can electronic journal usage data replace citation data as a
measure of journal use? An empirical examination. The Journal of Academic Librarianship,
32(5), 512–517.
Enssle, H. R., & Wilde, M. L. (2002). So you have to cancel journals? Statistics that help. Library
Collections, Acquisitions, and Technical Services, 26(3), 259–281.
Fersht, A. (2009). The most influential journals: Impact Factor and Eigenfactor. Proceedings of
the National Academy of Sciences of the United States of America, 106(17), 6883–6884.
Frost, J. (2013). Applied Regression Analysis: How to Present and Use the Results to Avoid
Costly Mistakes, part 1 - Adventures in Statistics | Minitab. The Minitab Blog. Retrieved
May 03, 2014, from http://blog.minitab.com/blog/adventures-in-statistics/appliedregression-analysis-how-to-present-and-use-the-results-to-avoid-costly-mistakes-part-1
Furlough, M. J. (2012). Opening access to research: From concepts to actions. Paper presented at
Open Access Week. The Pennsylvania State University, University Park, PA.
Garfield, E. (2006). The history and meaning of the journal Impact Factor. JAMA, 295(1), 90–93.
Iftikhar, M., Masood, S., & Song, T. T. (2012). Modified Impact Factor (MIF) at specialty level:
A way forward. Procedia - Social and Behavioral Sciences, 69, 631–640.
Inger, S. (2001). The importance of aggregators. Learned Publishing, 14(4), 287–290.
Jansen, B. J. (2009). Understanding User-Web Interactions via Web Analytics. Synthesis Lectures
on Information Concepts, Retrieval, and Services, 1(1), 1–102.
Jansen, B. J., & Rieh, S. Y. (2010). The Seventeen Theoretical Constructs of Information
Searching and Information Retrieval. Journal of the American Society for Information
Science and Technology, 61(8), 1517–1534.

129

Johnson, B. S., Evensen, G., Gelfand, J., Sipe, L., Zilper, N., Fronty, J., Schmolling, R. (2012).
Key Issues for e-Resource Collection Development : A Guide for Libraries. International
Federation of Library Associations and Institutions.
Journal Citation Reports. (2012). Retrieved from http://wokinfo.com/media/pdf/qrc/jcrqrc.pdf 1–
4.
King, D. W., & Tenopir, C. (1998). Designing electronic journals with 30 years of lessons from
print. Journal of Electronic Publishing, 4(2), 1–29.
Lakos, A. (2007). Evidence-based library management: The leadership challenge. Libraries and
the Academy, 7(4), 431–450.
Lane, J. (2010). Let’s make science metrics more scientific. Nature, 464(7288), 488–489.
Leon, L., & Kress, N. (2012). Looking at resource sharing costs. Interlending & Document
Supply, 40(2), 81–87.
Maple, A., Wright, C., & Seeds, R. (2003). Analysis of format duplication in an academic library
collection. Library Collections, Acquisitions, and Technical Services, 27(4), 425–442.
McDonald, J., & Van d Velde, E. F. (2004). The Lure of Linking. Library Journal. Retrieved
October 23, 2013.
Medeiros, N. (2007). Usage statistics of e-serials. Usage statistics of e-serials. Binghampton, NY:
The Haworth Press.
Mercer, L. S. (2000). Measuring the use and value of electronic journals and books. Issues in
Science and Technology Librarianship, 25(Winter).
Metz, P. (1992). Thirteen steps to avoiding bad luck in a serials cancellation project. The Journal
of Academic Librarianship, 18(2), 76–82.
Miller-Francisco, E. (2003). Managing electronic resources in a time of shrinking budgets.
Library Collections, Acquisitions, and Technical Services, 27(4), 507–512.
Montgomery, C. H. (2000). Measuring the impact of an electronic journal collection on library
costs: A framework and preliminary observations. D-Lib Magazine, 6(10), 37–52.
Montgomery, C. H., & King, D. W. (2002). Comparing Library and User Related Costs of Print
and Electronic Journal Collections A First Step Towards a Comprehensive Analysis. D-Lib
Magazine, 8(10), 1–17.
Nakatani, K., & Chuang, T. T. (2011). A web analytics tool selection method: An analytical
hierarchy process approach. Internet Research, 21(2), 171–186.
Neeley, J. D. (1981). The management and social science literatures: An interdisciplinary crosscitation analysis. Journal of the American Society for Information Science and Technology,
32(3), 217-223.

130

Oosthuizen, J. C., & Fenton, J. E. (2013). Alternatives to the Impact Factor. The Surgeon :
Journal of the Royal Colleges of Surgeons of Edinburgh and Ireland, 8–12.
Ortiz-Cordova, A., & Jansen, B. J. (2012). Classifying web search queries to dentify high revenue
generating customers. Journal of the American Society for Information Science and
Technology, 63(7), 1426–1441.
Phippen, A., Sheppard, L., & Furnell, S. (2004). A practical evaluation of Web analytics. Internet
Research, 14(4), 284–293.
Rafols, I., Leydesdorff, L., O’Hare, A., Nightingale, P., & Stirling, A. (2012). How journal
rankings can suppress interdisciplinary research: A comparison between Innovation Studies
and Business & Management. Research Policy, 41(7), 1262–1282.
Roberts, W. C. (2011). Piercing the Impact Factor and promoting the Eigenfactor. The American
Journal of Cardiology, 108(6), 896–8.
Rokach, L. (2012). Applying the Publication Power Approach to Artificial. Journal of the
American Society for Information Science and Technology, 63(6), 1270–1277.
Schwartz, C. (2000). Digital libraries: An overview. The Journal of Academic Librarianship,
26(6), 385–393.
Shepherd, P. T. (2012). The COUNTER Code of Practice for e-Resources: Release 4 Copyright:
Counter Online Metrics, 1–29.
Standardized Usage Statistics Harvesting Initiative (SUSHI) - National Information Standards
Organization. (2013). National Information Standards Organization. Retrieved from
http://www.niso.org/workrooms/sushi
Stefancu, M., Bloss, A., & Lambrecht, J. (2004). All about DOLLeR: Managing electronic
resources at the University of Illinois at Chicago Library. Serials Review, 30(3), 194–205.
Sykes, A. O. (1993). Introduction to regression analysis. An Introduction to Regression Analysis
Coase lecture. Law School, University of Chicago, 20(1) 1-33.
Togia, A., & Tsigilis, N. (2006). Impact Factor and education journals: A critical examination and
analysis. International Journal of Educational Research, 45(6), 362–379.
West, W. L., Miller, H. S., & Wilson, K. (2011). Electronic journals: Cataloging and management
practices in academic libraries. Serials Review, 37(4), 267–274.
Yin, C. (2011). Do Impact Factor , h-index and Eigenfactor of chemical engineering journals
correlate well with each other and indicate the journals’ influence and prestige? Current
Science, 100(5), 648–653.

131

Daniel M. Coughlin Vita
Education
May 2015

Doctor of Philosophy (Ph.D.)
College of Information Sciences & Technology
The Pennsylvania State University, University Park, PA

August 2004

Master of Science (M.S.) Computers and Information Technology
School of Engineering and Applied Science
The University of Pennsylvania, Philadelphia, PA

May 1999

Bachelor of Arts (B.A) Telecommunications
College of Communications
The Pennsylvania State University, University Park, PA

Experience

Director—Information Technology Services (ITS), Pennsylvania State University, University Park,
PA. December 2012 – Present
Team Lead Applications & Repository Services, DLT—ITS, Pennsylvania State University,
University Park, PA. January 2010—November 2012
Programmer/Analyst, ITS, Pennsylvania State University, University Park, PA. January 2005—
December 2009
Web Developer, Law School, The University of Pennsylvania, Philadelphia, PA. August 2002—
December 2004

Papers (selected)

Coughlin, D., Jansen, B. J. (2015). A Web Analytics Approach for Appraising Electronic Resources in
Academic Libraries. Journal of the American Society for Information Sciences and Technology [Accepted for publication].
Coughlin, D., Jansen, B.J. (2015). Modeling Journal Bibliometrics to Predict Downloads and Inform Purchase
Decisions at University Research Libraries. Journal of the American Society for Information Sciences and Technology
[Submitted for Publication].
Coughlin, D., Campbell, M., & Jansen, B. J. (2013). Measuring the Value of Library Content Collections.
Proceedings for Association for Information Science and Technology, Montreal, Canda.
Hswe, P., Giarlo, M.J., Belden, M., Clair, K., Coughlin, D., Klimczyk, L. (2012). Building a community of
curatorial practice at Penn State: A case study. Journal of Digital Information, 13(1).
https://journals.tdl.org/jodi/article/view/5874

Presentations (selected)

Hswe, P., Coughlin, D. (2014). ScholarSphere. Yale University, Sterling Memorial Library. New Haven, CT.
Coughlin, D. (2014). Intro to Hydra, ScholarSphere (and Sufia). Committee on Insitutional Cooperation (CIC)
IT Data Storage Working Group, Chicago, IL.
Coughlin, D., Giarlo, M. (2013). Architecting ScholarSphere. Code4Lib, Chicago IL.
Belden, M., Clair, K., Coughlin, D., Giarlo, M., Hswe, P., Klimczyk, L. (2011). Building a Community of
Practice: The Curation Architecture Prototype Services (CAPS) Project at Penn State, Open Repositories.
Austin, TX.

