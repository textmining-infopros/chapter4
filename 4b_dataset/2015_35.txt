CONVENIENCE TO THE CATALOGER OR CONVENIENCE TO THE USER? :
AN EXPLORATORY STUDY OF CATALOGERS' JUDGMENT
Richard Lee Hasenyager, Jr., B.S., M.L.S.

Dissertation Prepared for the Degree of
DOCTOR OF PHILOSOPHY

UNIVERSITY OF NORTH TEXAS
May 2015

APPROVED:
Shawne D. Miksa, Committee Co-Chair
Oksana L. Zavalina, Committee Co-Chair
June Abbas, Committee Member
Barbara Schultz-Jones, Committee Member
Suliman Hawamdeh, Chair of the Department
of Library and Information Sciences
Herman Totten, Dean of the College of
Information
Mark Wardell, Dean of the Toulouse Graduate
School

ProQuest Number: 10032270

All rights reserved
INFORMATION TO ALL USERS
The quality of this reproduction is dependent upon the quality of the copy submitted.
In the unlikely event that the author did not send a complete manuscript
and there are missing pages, these will be noted. Also, if material had to be removed,
a note will indicate the deletion.

ProQuest 10032270
Published by ProQuest LLC (2016). Copyright of the Dissertation is held by the Author.
All rights reserved.
This work is protected against unauthorized copying under Title 17, United States Code
Microform Edition © ProQuest LLC.
ProQuest LLC.
789 East Eisenhower Parkway
P.O. Box 1346
Ann Arbor, MI 48106 - 1346

Hasenyager, Richard Lee, Jr. Convenience to the cataloger or convenience to the user?
An exploratory study of catalogers' judgment. Doctor of Philosophy (Information Science),
May 2015, 207 pp., 63 tables, 12 figures, references, 149 titles.
This mixed-method study explored cataloger’s judgment through the presence of text as
entered by catalogers for the 11 electronic resource items during the National Libraries test for
Resource Description and Access (RDA). Although the literature discusses cataloger’s judgment
and suggests that cataloging practice based on new cataloging code RDA will more heavily rely
on cataloger’s judgment, the topic of cataloger’s judgment in RDA cataloging was not formally
studied. The purpose of this study was to study the differences and similarities in the MARC
records created as a part of the RDA National Test and to determine if the theory of bounded
rationality could explain cataloger’s judgment based on the constructs of cognitive and
temporal limits. This goal was addressed through a content analysis of the MARC records and
various statistical tests (Pearson’s Chi-square, Fisher’s Exact, and Cramer’s V). Analysis of 217
MARC records was performed on seven elements of the bibliographic record. This study found
that there were both similarities and differences among the various groups of participants, and
there are indications that both support and refute the assertion that catalogers make decisions
based on the constructs of time and cognitive ability. Future research is needed to be able to
determine if bounded rationality is able to explain cataloger’s judgment; however, there are
indicators that both support and refute this assertion. The findings from this research have
implications for the cataloging community through the provision of training opportunities for
catalogers, evaluating workflows, ensuring the proper indexing of bibliographic records for
discovery, and recommended edits to RDA.

Copyright 2015
by
Richard L. Hasenyager, Jr.

ii

ACKNOWLEDGEMENTS
I have been fortunate to have had so many people guiding and encouraging me
throughout this journey. I would first like to thank my wonderful dissertation committee
collectively; you have made me a better researcher. Barbara, June and Oksana, thank you for
the time you have given me to listen to my questions and giving advice in addition to asking me
questions, testing my assumptions, and guiding me through this process. Shawne, thank you for
posing great questions and challenging me, but most of all I appreciate your willingness to drop
everything to answer my questions or to check up on me. I consider you a great mentor, and I
have learned a great deal from you over the past nine years.
I thank the Library of Congress and ALA Publishing for granting me access to the data
and resources needed to complete this study.
My greatest pleasure was to be a part of the second doctoral cohort supported by a
grant from the Laura Bush 21st Century Librarian Program of the Institute of Museum and
Library Services (IMLS), and my thanks goes to the IMLS for this opportunity. My cohort
members provided me with support and long lasting friendships. To all of you, you have my
sincerest appreciation.
Finally, I would like to acknowledge my family and loved ones. You have been a constant
support, even when you nagged me to finish. Without all of you, this would not have been
possible. I am most thankful for my parents for instilling a love of learning and always
encouraging me to do my best.

iii

TABLE OF CONTENTS
ACKNOWLEDGEMENTS ...................................................................................................................iii
LIST OF TABLES .............................................................................................................................. viii
LIST OF FIGURES .............................................................................................................................. xi
1. INTRODUCTION ........................................................................................................................... 1
Problem Statement ..................................................................................................................... 2
Significance of the Study ............................................................................................................. 3
Research Questions ..................................................................................................................... 5
Background.................................................................................................................................. 5
Bounded Rationality ................................................................................................................ 5
Cataloger's Judgment .............................................................................................................. 7
Electronic Resources................................................................................................................ 8
RDA National Test .................................................................................................................. 10
Definition of terms .................................................................................................................... 16
Delimitations and limitations .................................................................................................... 17
Summary ................................................................................................................................... 19
2. LITERATURE REVIEW ................................................................................................................. 20
Introduction............................................................................................................................... 20
Bounded Rationality .................................................................................................................. 21
Cataloger's Judgment ................................................................................................................ 27
Cataloging Principles ................................................................................................................. 31
Functional Requirements of Bibliographical Records ............................................................... 37
iv

Functions of a Catalog ............................................................................................................... 44
AACR2 ........................................................................................................................................ 46
RDA ............................................................................................................................................ 50
RDA National Test ..................................................................................................................... 56
Implementation of RDA Worldwide .......................................................................................... 62
History of Non-Print Materials .................................................................................................. 65
Cataloging of Electronic Resources ........................................................................................... 68
Conclusion ................................................................................................................................. 77
3. METHODOLOGY ........................................................................................................................ 78
Introduction............................................................................................................................... 78
Sample ....................................................................................................................................... 78
Research Approach/Design ....................................................................................................... 79
Research Questions and Methods to Analyze....................................................................... 80
Data Collection .......................................................................................................................... 82
Data Analysis ............................................................................................................................. 83
Findings for Item P .................................................................................................................... 89
Conclusion ................................................................................................................................. 94
4. DATA ANALYSIS ......................................................................................................................... 95
Introduction............................................................................................................................... 95
Sample Statistics........................................................................................................................ 96
Recording Names .................................................................................................................... 103
Recording Titles ....................................................................................................................... 117
v

Titles (245 $a, 245 $b, and 246 $a) ..................................................................................... 118
Recording the Statement of Responsibility............................................................................. 132
Recording Publishing Information........................................................................................... 135
Recording Extent ..................................................................................................................... 141
Physical Descriptions ........................................................................................................... 141
Content, Media and Carrier Types and Characteristics (33x).............................................. 146
Notes ....................................................................................................................................... 151
General Notes (500) ............................................................................................................ 151
Notes in Other Fields (538 and 588).................................................................................... 155
Electronic Location and Access (856 $u) ................................................................................. 159
Summary of p-value with Significance .................................................................................... 161
Conclusion ............................................................................................................................... 165
5. FINDINGS ................................................................................................................................. 166
Introduction............................................................................................................................. 166
Restatement of the Problem ................................................................................................... 166
Major Findings ......................................................................................................................... 167
Research Question 1 ............................................................................................................ 168
Research Question 2 ............................................................................................................ 170
Further Discussion ................................................................................................................... 172
Names .................................................................................................................................. 173
Statement of Responsibility ................................................................................................ 173
Publication Information ....................................................................................................... 173
vi

Extent ................................................................................................................................... 174
Notes.................................................................................................................................... 174
Electronic Location and Access............................................................................................ 175
Future Research ...................................................................................................................... 175
Replicate Current Study ....................................................................................................... 176
Organizational Policy Based on Cataloger's Judgment ....................................................... 177
Study on Preferred Sources of Information ........................................................................ 178
Implications and Recommendations for the Cataloging Community ..................................... 178
Training ................................................................................................................................ 178
Workflows............................................................................................................................ 179
Indexing ............................................................................................................................... 179
Further Edits to RDA ............................................................................................................ 180
Conclusion ............................................................................................................................... 181
APPENDICES ................................................................................................................................ 183
REFERENCES ................................................................................................................................ 191

vii

LIST OF TABLES
Table 1.1 U.S. RDA Test Survey Categories................................................................................ 15
Table 2.1 Structure of RDA .......................................................................................................... 52
Table 3.1 Record Titles and Counts Used in the Study ................................................................ 79
Table 3.2 Marc Fields and the Type of Data Included in the Field .............................................. 86
Table 3.3 Field Count Report for Item P ...................................................................................... 90
Table 3.4 Data Sample for Title and Statement of Responsibility of Item P................................ 91
Table 3.5 Data Sample for Publication, Distribution, etc. of Item P ............................................ 92
Table 3.6 Data Sample for Physical Description of Item P .......................................................... 93
Table 4.1 Records Submitted by Test Participants ....................................................................... 96
Table 4.2 Descriptive Statistics for Categorical Groups ............................................................... 98
Table 4.3 Categorical Group Crosstabs ...................................................................................... 100
Table 4.4 Item MARC Record Distribution ............................................................................... 102
Table 4.5 Chi-Square p-value/Fisher’s Exact Results for Names............................................... 109
Table 4.6 Cramer’s Ѵ Results for Names ................................................................................... 110
Table 4.7 Authority Headings Used in the MARC 110/710 Field for Item V............................ 112
Table 4.8 Corporate Names Used in Describing Item Y ............................................................ 113
Table 4.9 Relator Terms Used in MARC 1xx and 7xx Subfield $e, Listed by Occurrence ....... 115
Table 4.10 Chi-Square p-value/Fisher’s Exact results for corporate names ............................... 116
Table 4.11 Cramer’s Ѵ results for corporate names ................................................................... 117
Table 4.12 Entries for Preferred Titles (130) .............................................................................. 118
Table 4.13 Text Entered and Frequencies of the Text Entered in the 245 $a and $b and 246 $a
Subfields ......................................................................................................................... 122

viii

Table 4.14 Text Entered into MARC 245 $a and $b for Items V, W, X, and Y ........................ 125
Table 4.15 Text Entered into MARC 245 $a for Items V, W, X, and Y .................................... 126
Table 4.16 Text Entered into MARC 245 $b for Items V, W, X, and Y .................................... 127
Table 4.17 Text Entered into MARC 246 for Items V, W, X, and Y ......................................... 128
Table 4.18 Summary of Variations within the 245 and 246 Fields and Subfields ..................... 130
Table 4.19 Chi-Square p-value/Fisher’s Exact Results for Titles............................................... 131
Table 4.20 Cramer’s Ѵ Results for Titles ................................................................................... 131
Table 4.21 Variations in the Text Entered into the MARC 245 $c Subfield .............................. 132
Table 4.22 Items Q and W Variations in Text Entered in MARC 245 $c .................................. 133
Table 4.23 Chi-Square p-value/Fisher’s Exact Results for Statement of Responsibility ........... 134
Table 4.24 Cramer’s Ѵ Results for Statement of Responsibility ............................................... 134
Table 4.25 Place of Publication Entries for Item I...................................................................... 136
Table 4.26 Publisher Names Entered for Item M ....................................................................... 138
Table 4.27 Types of Information Included in the Date of Publication (MARC 260 $c) ............ 139
Table 4.28 Chi-Square p-value/Fisher’s Exact results for publication information ................... 140
Table 4.29 Cramer’s Ѵ results for publication information ....................................................... 141
Table 4.30 Text Entered for Physical Description (MARC 300 $a) for Item Q ......................... 143
Table 4.31 Text Entered for Other Physical Details (MARC 300 $b) for Item H...................... 144
Table 4.32 Chi-Square p-value/Fisher’s Exact Results for Extent of Item................................. 145
Table 4.33 Cramer’s Ѵ Results for Extent of Item ..................................................................... 146
Table 4.34 Text Entered for Content Type (MARC 336 $a) for All Items ................................ 147
Table 4.35 Text Entered for Media Type (MARC 337 $a) for All Items ................................... 148
Table 4.36 Text Entered for Content Type (MARC 338 $a) for All Items ................................ 149

ix

Table 4.37 Chi-Square p-value/Fisher’s Exact results for Content, Media, and Carrier Types . 150
Table 4.38 Cramer’s Ѵ Results for Content, Media, and Carrier types ..................................... 150
Table 4.39 Note Types and the Frequency of Types Utilized in MARC 500 $a........................ 152
Table 4.40 Chi-Square p-value/Fisher’s Exact Results for the General Notes Field.................. 154
Table 4.41 Cramer’s Ѵ Results for the General Notes Field ...................................................... 155
Table 4.42 Chi-Square p-value/Fisher’s Exact Results for Other Notes Fields ......................... 158
Table 4.43 Cramer’s Ѵ Results for Other Notes Fields.............................................................. 158
Table 4.44 Frequency of URLs Entered in MARC 856 $U for Item V...................................... 159
Table 4.45 Chi-Square p-value/Fisher’s Exact Results for Electronic Location and Access ..... 160
Table 4.46 Cramer’s Ѵ Results for Electronic Location and Access ......................................... 160
Table 4.47 Frequency of Expected Results for Job Title ............................................................ 162
Table 4.48 Frequency of Expected Results for Total Years of Cataloging Experience ............. 163
Table 4.49 Frequency of Expected Results for Total Years of Cataloging Experience ER ....... 163
Table 4.50 Frequency of Expected Results for Total Years Cataloging Experience.................. 164
Table 4.51 Frequency of Expected Results for Total Years Cataloging Experience ER ........... 165
Table A.1 List of Common Original Set (COS) Items ............................................................... 185
Table B.1 Record Creator Profile (RCP) .................................................................................... 187
Table B.2 Common Original Set (COS) ..................................................................................... 188
Table B.3 Institutional Questionnaire (IQ) ................................................................................. 189

x

LIST OF FIGURES

Figure 2.1: The function of a catalog. Cutter, C. A. (1904).......................................................... 32
Figure 2.2: Group 2 Entities and “Primary” Relationships........................................................... 39
Figure 2.3: Group 2 Entities and "Responsibility" Relationships................................................. 41
Figure 2.4: Group 3 Entities and "Subject" Relationships. ........................................................... 42
Figure 4.1: Screenshot of the homepage for Item V ................................................................... 111
Figure 4.2: Item V, Homepage of Our Science – Research Directory ....................................... 119
Figure 4.3: Item W, Homepage of Abstracts Database – National Criminal Justice Reference
Center Service ................................................................................................................. 120
Figure 4.4: Item X, Homepage of Welcome to the United Nations: It’s Your World................ 120
Figure 4.5: Item Y, Homepage of CSA ...................................................................................... 121
Figure 4.6: Publisher Information Listed on Cover of Item J ..................................................... 137
Figure 4.7: Publisher Information as Listed on the Verso of the Title Page .............................. 137
Figure 4.8: Rule for Equipment or System Requirements .......................................................... 156

xi

CHAPTER 1
INTRODUCTION
Even though the term "cataloger's judgment" is a more contemporary one, the concept
has been around since Charles Cutter published Rules for a Dictionary Catalog in 1876. Cutter
states,
The convenience of the public is always to be set before the ease of the cataloger. In
most cases, they coincide. A plain rule without expectations is not only easy for us to
carry out, but easy for the public to understand and work by. But strict consistency in a
rule and uniformity in its application sometimes lead to practices which clash with the
public's habitual way of looking at things. When these habits are general and deeply
rooted, it is unwise for the cataloger to ignore them, even if they demand a sacrifice of a
system and simplicity. (1904, p.6)
With this, Cutter grants permission for catalogers to deviate from the rules to create a catalog
that is easy for the users to access the materials they need. This concept led to the birth of
cataloger's judgment.
Since this time, others have described this "convenience" to the user through the use of
terms such as choice, judgment, interpretation, etc. (Dunkin, 1969; Snow, 2011). In each of
these cases, they discuss how the user must be at the forefront of thought while describing the
surrogates of the bibliographic record catalogers create. Dunkin states, "[t]here is seldom just
one "true" answer" (1969, p. 8) and that the cataloging rules adopted stress that judgment is
critical to the role of the cataloger. This has not always been the case. Jewett believed that
cataloging rules should be all encompassing and should leave little to judgment. However, with
the adoption of the new cataloging standard, Resource Description and Access (RDA), it is
evident that the broader, long-standing cataloging community has abandoned Jewett's thought
in favor of convenience of the user through applying cataloger's judgment. This is exemplified

1

throughout the new cataloging rules because they provide options in which catalogers exercise
judgment (Cronin, 2011). Cataloger's judgment is a decision-making process of determining
what information to include in the bibliographic record. It takes education, experience, and
knowledge in order to determine what information should be included (Snow, 2011). This
decision-making process has similarities to economic-based decision-making theories,
specifically, bounded rationality. Bounded rationality is a theory that states people make
decisions based on time and their cognitive abilities (Simon, 1955).
This study will explore the relationship of bounded rationality to cataloger's judgment in
a test of RDA conducted by the Library of Congress (LC), National Library of Medicine (NLM),
National Agricultural Library (NAL), and the other members of the formal RDA test group.
Problem Statement
Released in 2010, RDA is a replacement for the second edition of the Anglo-American
Cataloguing Rules (AACR2). Since then, few studies have been conducted by other researchers
regarding the use of RDA; however, none of these studies have been focused on the decisions
made by librarians and others that create the Machine-Readable Cataloging (MARC) record and
others that use the MAchine Readable Cataloging (MARC 21) format to communicate
bibliographic and related information using the new set of rules. RDA is based on the Functional
Requirements for Bibliographic Records (FRBR) and the Functional Requirements for Authority
Data (FRAD), which are conceptual models created by the International Federation of Library
Associations (IFLA) Study Group on FRBR under the direction of the Standing Committee of the
IFLA Section on Cataloging and the Working Group on Functional Requirements and Numbering

2

of Authority Records (FRANAR), falling under both the Cataloguing and Indexing Sections of
IFLA.
In 2010, three national libraries, the Library of Congress (LC), the National Library of
Medicine (NLM) and the National Agricultural Library (NAL), tested these new standards
utilizing the electronic version of the rules (i.e., RDA Toolkit). They selected 23 other test
partners to create bibliographic records using RDA, and to provide feedback on their
experiences using the new standard. Twenty-six organizations involved in the test group
created 10,570 original cataloging records. Each group was to catalog 25 common items using
both AACR2 and RDA resulting in the creation of 50 records per test organization (Library of
Congress, 2011). In addition to this set of records, each organization was to submit records for
at least another 25 items of their choosing. Participants were then to submit a survey for each
record they created and include information such as who created the record and their overall
experience with the process (e.g., time spent creating the bibliographic record, etc.).
The data to be analyzed for this study comes from the common set of MARC records
created by the formal test group of the RDA national test. As stated above, the testing
procedures for the formal test of RDA required each cataloging organization to construct
original cataloging records for the same set of items; this created a great opportunity to study
the similarities and differences in how cataloging staff interpret cataloging rules.

Significance of the Study
One of the stated goals of RDA is to provide better guidance in the creation of
bibliographic records for electronic resources, and it is for this reason this study was limited to
only the MARC records for electronic resources created in the RDA National Test. The reported
3

dissertation provides the library community with a formal study relating to the analysis of
cataloger's judgment by cataloging staff of those participating in the National test by the
national libraries. One outcome of this study was to provide a better understanding of how
catalogers interpret cataloging rules through professional practice; ultimately, to provide the
cataloging community with significant information to identify the greatest variances of such
practices. The data gathered during the National test provided the researcher with a rich test
bed in which to study cataloger’s judgment when creating records for electronic resources.
This study is important to the future of cataloging. In 2012, the Library of Congress
embarked on a new initiative called the Bibliographic Framework (BIBFRAME) to serve as a
replacement for MARC. Although development work is underway to transition from MARC in
favor of a new encoding scheme, the fact remains that the goal is not to abandon library
catalogs. Instead the focus is to develop a new method that will better meet the needs of today
users (Kroeger, 2013). According to Van Ballegooie and Borie (2014), the power of BIBFRAME
will be in leveraging linked data and the practice of cataloging will change significantly if or
when BIBFRAME is implemented. If the system goal of the BIBFRAME project and the greater
community is to move from MARC to another schema, this leads to the question of, why
conduct a current practice that utilizes MARC? The answer is this study is about cataloger's
judgment, and since the most widely used standard in the US for cataloging is MARC, then it
only makes sense to study what catalogers use in order to determine how they define
cataloger's judgment through practice.

4

Research Questions
This study attempts to answer how librarians interpret cataloging rules through the lens
of bounded rationality and cataloger's judgment. The study explores the following questions in
order to understand cataloger's judgment and its relation to practice.
RQ1 How did catalogers participating in the Resource Description and Access (RDA) National
Test exercise cataloger's judgment as they created RDA-based MARC records for
electronic resources?
1a: What are the similarities and differences of the records?
1b: To what extent can the differences in text entered in the records be explained by
differences in characteristics of the catalogers (e.g., level of position, experience,
prior course work and/or training, etc.)?
RQ2 How can cataloger’s judgment be explained through the lens of Bounded Rationality?
2a: How can cataloger's judgment be predicted using the constructs of bounded
rationality?

Background
Bounded Rationality
This study is not about the "right" decisions; rather it is a study about the outcomes of
decisions made by various catalogers analyzing the same information entities. In order to
explain these phenomena, a closer look at judgment theory, decision-making, and bounded
rationality will be necessary to understand the behavior of catalogers as they create
bibliographic records.
5

Koehler & Harvey (2004) define judgment "as a set of evaluative and inferential
processes that people have at their disposal and can draw on in the process of making
decisions" (p. xv). For catalogers, this would include the rules, standards, and local practices
they refer to in making their decisions. Koehler & Harvey (2004) describe decision making as a
broad activity where a person takes in the account of "social, emotional and cultural influences"
and weighs the risks to determine the outcome (p. xv). Over (2004) asserts that one must also
look at rationality when describing judgment and decision-making. Rationality applies when
one chooses to follow or deviate from traditional rules through conscious decision-making. In
bounded rationality, there are limits to decision-making by the constraints of time and
cognition.
Classical judgment theory is rooted in the thought of absolution, meaning individuals do
not stop making decisions until they have reached an omnipotence of the idea they are
considering. It is assumed that those making decisions have an unbounded amount of time and
possess all of the information required to determine the best outcome (Simon, 1955). Simon
(1955) has described this as optimal choice. Since many individuals do not have the ability to
spend an unbounded amount of time or resources to make decisions, many contemporary
researchers are in favor of the ideals of descriptive decision-making theory and have
abandoned the classical view of judgment theory.
Decision-making theory has two major divisions, normative and descriptive. Normative
decision making-theory seeks to describe the highest level of decision-making through the
optimal use of all faculties that lead one to making the one right judgment. Normative decisionmaking is similar to classical judgment theory, while descriptive decision-making theory, on the
6

other hand, attempts to describe how individuals and/or organizations make decisions through
normal limitations (Johnson & Kruse, 2009). Normative theories of decision-making are
unattainable in everyday life. For this reason, a descriptive theory of decision-making is most
suited for this type of study, and H. A. Simon's theory of bounded rationality is the best match
for this study of cataloger's judgment.
Although bounded rationality is a theory that describes economic behaviors, it has also
found acceptance in the field of library and information science. The theory has been used to
describe the information seeking of individuals (Marchionini, 1995; Higgins, 1999; Agosto,
2002), the use of information in user decisions (Hines, 2009), the decisions in collection
development (Chu, 1994), and others. Buczynski (2005), Monsourian, Ford, Webber & Madden,
(2008), Hines (2009), and Holt (2010) used the idea of satisficing, which is a blend of satisfying
and sufficing used to formulate a decision and a primary concept in bounded rationality, to
explain various aspects of library user behavior. Further exploration of bounded rationality will
be discussed in the review of literature in Chapter 2.

Cataloger's Judgment
In a review of the literature, there is no mention of formal research on the topic of
cataloger's judgment. However it is often mentioned in the literature relating to subject
analysis and classification, cataloging education and training, and cataloging quality;
additionally a few articles are devoted solely to cataloger's judgment. The articles written on
the specific topic describe anecdotal evidence or are opinion pieces on the struggles of practice.

7

Many describe cataloger's judgment as "common sense" (Ferris, 2008, p. 179); however,
it also involves a series of interpretations of rules, standards, and needs of users. For one
organization, the interpretation of a rule may be different than another based on the type of
institution, the purpose of the acquisition, or local cataloging workflows and rules. Some see
these inconsistencies between records as a series of mistakes or as evidence that one record is
better than the other; however, these varied records may both be correct (Santamauro and
Adams, 2006).
The variances between records often create chaos for cataloging students as well as
novice and experienced catalogers. Miksa (2008) describes how her cataloging students are
always requesting the "correct" answer to cataloging examples. Her reply is "it depends" along
with various interpretations of the rules, which leaves her students with a level of anxiety in
their quest for meaning (p. 21). For novice and experienced librarians, the challenge is that they
may lack training or enough coursework to prepare them for the tasks they are to complete. A
strong understanding of cataloging principles and standards is required in order to apply them
to create bibliographic records. It is through this maze of chaos that order arises, which then
enables users to discover the resources they need (Santamauro & Adams, 2006). In other
words, it is through the application of cataloger's judgment that users are able to find, identify,
select, and acquire the information they need.

Electronic Resources
The discussion of the infusion of electronic resources into library catalogs has greatly
increased over the past two decades. The majority of articles written on the topic of electronic
8

resources are specifically on the actual cataloging processes, E-Serials, digital library collections,
and eBooks. In the articles specific to cataloging, the literature points to the difficulties in
cataloging such assets. The international cataloging community has recognized this challenge,
and in an effort to create a better environment for cataloging electronic resources, the Joint
Steering Committee for the Development of RDA (JSC) has included this problem as one of the
goals of RDA. RDA is a paradigm shift that is not only a change in rules, but also provides a new
approach to describing resources of all types. This is accomplished through providing a “…set of
guidelines and instructions on resource description and access covering all types of content and
media" (Joint Steering Committee for the Development of RDA, 2009, RDA - Resource
Description and Access: A Prospectus, para. 2) including non-print resources found in library
catalogs.
To gain a better understanding of the history of electronic resource cataloging, one
needs to investigate the history of non-print materials first. Non-print materials include
anything that are not considered to be books including, but not limited to maps, still images,
sound recordings, motion pictures, and electronic resources. Even though these items may
contain the printed word, they are not monographs. The recorded history of non-print
materials begins with the inclusion maps in libraries in 1800 , followed by still images in 1889,
sound recordings around 1900, and motion pictures in 1935 (Weihs, 2011). The post-World War
II era marked some very important changes in the cataloging of non-print materials due to an
increase in the number of audio-visual materials purchased by school libraries to update their
collections, to meet new instructional standards, and through a large appropriation of federal
funds to school libraries (Weihs, 2011).
9

Weihs (2011) describes these events as having a pivotal role in the revision of AACR.
AACR provided rules for cataloging non-print items in Part III. These rules did not follow the
same convention found for books and did not meet the needs of catalogers attempting to
describe audio-visual materials. She contends that through further committee work to provide
better audio-visual cataloging standards, the broader cataloging community agreed with the
need for a revision of AACR in order to describe all items available for inclusion into the library
catalog. This process paralleled the challenge of revising AACR2 and creating a new standard,
RDA, in order to meet the cataloging needs of electronic resources.

RDA National Test
A three month formal national test of RDA was conducted October 2010 through
December 2010. During this period of time, test participants created bibliographic records to
submit to the national libraries for analysis, which was conducted in January of 2011. For the
three months prior to the test, participating organizations accessed the RDA Toolkit to practice
using the new tool and to determine workflows.
The reason for the test was based on the concerns of the Library of Congress Working
Group on the Future of Bibliographic Control, with the result that the Library of Congress (LC),
National Library of Medicine (NLM), and National Agricultural Library (NAL) conducted a formal
test of RDA. The stated outcome of this study by the national libraries was to “assure the
operational, technical, and economic feasibility of RDA" (Library of Congress, About the U.S.
National Libraries Test Plan for RDA, para. 1). The test included the three national libraries and
the broader U.S. library community.
10

The U.S. National Libraries RDA Test Coordinating Committee determined who would be
participating in the national test and limited the group to a manageable size. In all, there were
over 90 applicants; however, only 23 test partners in addition to the three national libraries
received an invitation to participate. This group comprises “representatives from all types and
sizes of libraries - - national, government, academic, public, school, special; archives; museums;
book vendors; system developers; library schools; and consortia” (Library of Congress, U.S.
National Libraries RDA Test Partners, para. 2). These test partners used a variety of library
automation systems, encoding schemas (MARC, MODS, etc.), types of materials cataloged, and
a variety of locally adopted cataloging rules (AACR2, AMIM, DACS, etc.) used by the chosen
organizations.
The selection of test partners was based on the limited funds that the national libraries
had for such a study. They had determined that they would choose 20 test partners; however,
the response was great, so they increased the number of testing organizations to 26, which
included a broad spectrum of organizations that create bibliographic records (U.S. RDA Test
Coordinating Committee, 2011). It is important to note that the researcher was a formal
participant in the RDA National Test, representing school libraries. In addition to the formal test
partners, they also allowed participation by informal testers; however, the only data considered
for this study is the common set of original cataloging records from the formal test group.
In this test, the U.S. National Libraries RDA Test Coordinating Committee developed a list
of eight factors that would be evaluated by the formal test. These factors were:
•

Record creation

•

Record use
11

•

Training documents and needs

•

Use of the RDA Toolkit/RDA content

•

Systems and metadata

•

Technical feasibility (later merged with systems and metadata)

•

Local operations

•

Cost and benefits (U.S. RDA Test Coordinating Committee, 2011, p. 29)
The coordinating committee had also created some assumptions about the formal test.

These assumptions included that the data and results would be shared with the library
community and the testing would wait until a final version of the RDA Toolkit was formally
released. The National Libraries also determined which elements of RDA were considered core;
however, participating organizations were encouraged to follow their own cataloging
workflows and requirements as long as the core elements were represented (U.S. RDA Test
Coordinating Committee, 2011).
The U.S. RDA Test Coordinating Committee (2011) modeled their test design after that
of a similar test for adoption of the CONSER Standard Record. In the RDA test, participating
organizations were to create original catalog records for 25 common items for all testers, an
additional set of at least 25 items of their choosing, the creation of 5 common copy items, and
the optional creation of additional copy items. Two records were created for each original
catalog record; one following AACR2 (or their current standard practiced) and one utilizing RDA.
Testers were to use their local cataloging practices and were only to provide the descriptive
elements of the bibliographic record without any subject analysis or classification; however,

12

testing partners were to provide authority work if that was a normal workflow for their
institution. Each tester was to complete a survey for each bibliographic record they submitted.
The coordinating committee selected the surrogates to be cataloged and packaged the
common original record set (COS) that included print and non-print materials including
monographs, serials, integrated resources, audio-visual materials, electronic resources, etc. The
goal was to engage the testers in creating records of a broad spectrum of resources. The
committee provided electronic surrogates for the 25 common items through a password
protected Basecamp site, an electronic content management system similar to Joomla or
Moodle. Each of the 25 items was given a generic title so as not to skew the testing results.
In addition to the 25 COS items to be cataloged by the formal testers, they were also to
create at least 25 additional items of their choosing, also known as the extra original set (EOS),
and copy catalog five (5) common items (CCS). Testers could also submit additional copy
records if they chose to, referred to as the extra copy set (ECS). The only set of records to be
considered for this study is those from the COS.
One of the greatest challenges the coordinating committee faced was how testers
would submit their records. They had determined that those using Online Computer Library
Center (OCLC) metadata services would submit their records through their typical workflows,
and non-OCLC users would either email their records or upload them via FTP to the LC. This was
not a factor for this dissertation study since all of the records were able to be retrieved from
the LC website.
Once a bibliographic record was created, testers were instructed to complete and
submit a survey on each record. Using an online survey service, SurveyMonkey, the committee
13

had created four (4) different record surveys for each of the four types of records that an
institution could submit (COS, CCS, EOS, and ECS). The surveys included both quantitative and
qualitative data such as to capture the amount of time it took to complete the bibliographic
record, any problems encountered, and how long RDA record creators have been cataloging
that type of resource. Test participants were also asked to complete a record creator profile
(RCP), which provided descriptive data as to their level of cataloging experience, education, and
training. Each test organization was also to submit an institutional questionnaire (IQ) and a
record use survey (RU) that would provide information on the users perceptions of the RDA
records created for the test. The survey data relevant for this study come from RCP, COS, and
IQ surveys (Appendix B).
Once the records and surveys were collected, the coordinating committee analyzed the
data. The coordinating committee had determined they would conduct an in-depth study of the
COS records since these were common to all testers and they had the surrogates for them. The
LC staff created benchmark records to compare to the submitted records with various options.
They identified differences between records to determine the needs for future RDA training.
The committee also analyzed the data from the surveys. Table 1.1 provides a breakdown of the
8509 survey responses by the type of survey, its abbreviation, the number of surveys collected,
and the number of testing institutions that did not submit any survey questions for a particular
survey.

14

Table 1.1
U.S. RDA Test Survey Categories
Survey Title
Institutional Questionnaire*
Record Creator Profile
Record Use Survey**
Common Original Set
Extra Original Set
Common Copy Set***
Extra Copy Set ***
Informal Testers Questionnaire

Abbreviation

Number of Surveys

IQ
RCP
RU
COS
EOS
CCS
ECS
IT

29
219
163
1200
5908
111
801
80

Institutions that did not
respond
0
3
4
0
1
7
7
Not Applicable

* The GSLIS Group submitted four IQ surveys.
**Some testing institutions did not have users that could be surveyed.
*** Work on copy records was optional.
In order to make sense of the surveys, the coordinating committee had to clean up
some of the data. This included normalizing the data to strip responses of wording when they
had requested the numeric value of minutes. A small number of surveys were determined to be
invalid, and they chose to exclude this data. In the findings of the final report released by the
U.S. Coordinating Committee (2011), they stated that they had considered running a more
rigorous approach to analysis; however, they did not have the time to prepare tools for a more
rigorous analysis, such as a codebook for open-ended questions.
In January 2011, once the formal test period was completed, the national libraries
analyzed the data to determine whether to adopt RDA. At the conclusion of the analysis, the
national libraries shared their findings with the broader cataloging community and announced
that they would indeed adopt RDA. The adoption of RDA by the national libraries and the wider
cataloging community occurred in March 2013.

15

Definition of terms
For the purpose of this study, the following terms have been defined:
•

Cataloger's Judgment - The decisions catalogers make while creating bibliographic
records for information entities to be included in the library catalog. These decisions not
only include the information that appears in the record, but also where it may exist in
the record or whether or not it is left out entirely. These decisions are based on the level
of education, training, and “practice” a cataloger engages in the process of applying
rules to practice even though there may not be an exact rule to meet every scenario in
order to meet their user’s need. This definition of cataloger's judgment is adapted from
Snow (2011).

•

Electronic Resources - Any content made available by a library to its patrons via
electronic means including computers, smart phones, eReaders, etc. Electronic
resources include E-Monographs (including eBooks), E-Serials, websites, integrated
resources, streaming video, etc.

•

Indexing – Instructions made within a database system that inform which parts of the
data will be analyzed and available to be searched and retrieved by users. In the case of
MARC records found in library systems, not all fields of the MARC record are mapped to
be searched and displayed to library users. Indexing action may be automatically set by
a vendor or individually set by a library institution.

•

Non-Print Materials - A broad class of materials included in library collections that
include maps, still images, sound records, motion pictures, and electronic resources.
Non-print materials and non-book materials are synonymous (Weihs, 2011).
16

•

Satisficing - A concept that is very similar to satisfying as it is a blend of satisfy and
suffice, as those who make decisions do so by making an adequate decision opposed to
an optimal one (Gigerenzer & Selten, 2001; Simon, 1972)

Delimitations and limitations
This study only focused on the common set of electronic resource RDA records created
by the formal test group. One of the purposes of RDA is to allow for better cataloging of
electronic resources, including E-Monographs, E-Serials, streaming video, and integrating
resources. In an effort to curtail the amount of data, this study only included those records that
conform to the RDA standard and not those created using AACR2. The purpose of limiting this
study was to look closer at the decisions made by catalogers in their practice of applying a new
set of cataloging rules for electronic resources, one of the reasons RDA was created.
One important caveat of this study was that RDA was in its infancy, and some would
argue it still is, and for this reason very few catalogers had been exposed to applying the
standard. For this reason, it is very possible that because of the participants’ limited exposure
they had difficulty applying the new set of rules to the practice they had known in the past. RDA
was designed to help the cataloger think differently about cataloging and the information we
provide to users, and for some test participants they may have found the rules difficult to apply
or considered the standard to be ambiguous. Anecdotally, in various informal conversations
between the researcher and catalogers, they have suggested that catalogers’ practice using the
prior standard, AACR2, was very much based on the structure of MARC and not necessarily the
cataloging rules themselves. It is very possible that participants would continue their previous
17

practice in the new standard. The intent of this study was not to confirm this assumption;
however, the data analysis does suggest support for the assumption.
The purpose of the RDA national test was to understand the cataloging of the
descriptive elements of the bibliographic record. For this reason, only the descriptive elements
of the record are included in this study. This study does not include subject analysis or
classification since they were not included in the National test. Although in addition to
bibliographic records, many authority records were created, not all institutions participated in
this part of the test. It is for this reason that this study does not analyze this type of data.
There are several limitations in this dissertation. First, the data is representative of only
the 26 cataloging agencies involved in the formal test group. Furthermore, the test participants
themselves were self-selected since not all catalogers from each institution could opt-in or optout of participating. The 26 test partners represent a broad spectrum of the types of libraries
and other cataloging organizations; however, for some types of institutions, such as school
libraries, only one K - 12 organization was chosen from this very large group of libraries.
Second, the surveys submitted by participants were self-reported. Individual participants
completed a survey about their experience after completing each catalog record whether the
record was created using AACR2 or RDA. This self-reporting may skew the results of the
participant surveys due to self-bias.
Another limitation in this study is that not all catalogers submitted a survey for every
record they created. Although 1,200 surveys were returned, there were 1,509 COS records
submitted. This number is higher than the 1,300 records anticipated since some testing
organizations submitted more than one record for an information resource. This includes the
18

group of LIS students that participated in the testing from several universities. As a result, any
record that did not have the corresponding data associated with it was removed from this
study.
The final limitation is the size of the sample which included all records created by RDA
test participants for all 11 electronic resources in the common set of test resources. Although
this sample may be small, the data is rich. Because of the limited number of information
resources being studied, there were difficulties in determining significance levels due to the
granular level at which this study intended to explore.

Summary
This study investigated cataloger's judgment during the RDA National Test by examining
the MARC records and surveys created and submitted by the test participants for the electronic
resources included in the common original set of test items. Analysis of the data was completed
by utilizing descriptive statistics for quantitative survey information, content analysis of the
notes fields, and the application of a regression analysis of the MARC records.

19

CHAPTER 2
LITERATURE REVIEW
Introduction
Full rationality requires unlimited cognitive capabilities. Fully rational man is a mythical
hero who knows the solutions to all mathematical problems and can immediately
perform all computations, regardless of how difficult they are. Human beings are in
reality very different. Their cognitive capabilities are quite limited. For this reason alone,
the decision-making behavior of human beings cannot conform to the ideal of full
rationality. (Selten, 2001, p. 14)
Individuals are constantly tasked with needing to make decisions in their quest to solve
problems. They may ask themselves, "What is the best route to take to work?"; "How much
money should I save for my child's education?"; "Should library organizations adopt RDA?"
Some have argued that for every decision one makes, the decision is fully rational. This was the
belief prior to the early 1950's and was the classical or traditional thought of judgment and
decision-making theories.
In the quest to help explain the phenomena of cataloger's judgment, a variety of
theories were investigated to determine the one that would most relate to the practice.
Judgment theory was first considered and later abandoned as it seemed to assume too much of
the decision-maker. It is now thought that decisions are made based on the information and
time they have, and this is the reality of the human decision-maker and the foundation for
bounded rationality (Gigerenzer & Selten, 2001).
It is through bounded rationality that people and organizations make their decisions. In
the library cataloging community, decisions are made through the creation and revision of
principles, standards, and processes. This chapter is organized from the most philosophical
20

aspects of cataloging, down to a more practical application of cataloging electronic resources;
from the abstract to the concrete.

Bounded Rationality
Good, fast, cheap... pick two (Wolf & Grodzinsky, 2006), you cannot have it all, and
cataloger's know this all too well. This quote can be used to describe a workflow process
relating to the quality, efficiency and economics in decision-making. One is able to have a high
level of quality along with high efficiency, but it will be expensive; efficient and inexpensive, but
not of high quality; or of high quality and inexpensive, but not efficient. This economic view of
decision-making parallels that of bounded rationality. There are constraints that limit the
decisions of individuals. Simon (1955) refers to these constraints as bounded by time and
cognitive limits.
Bounded rationality provides an explanation of how individuals and organizations work
to solve problems through making judgments or decisions. H. A. Simon is credited for creating
an economic theory of bounded rationality where individuals work within the constraints of
time and knowledge to reject and ultimately accept possible outcomes until they are satisfied
with a workable decision. Although bounded rationality has its roots in economics, a search of
the literature has found the information science community has accepted the theory as a way
to explain how people make decisions.
Through the exploration of the literature on decision-making, bounded rationality
stands out because it explains the decision-making process. Other, more classical, theories
assumed no limits, as the decision-maker was omnipotent about the decision to be made and
21

there is no room for mistakes. The decision-maker would conduct all mathematical calculations
of the various plausible outcomes and then choose the best, also known as optimization
(Simon, 1956). Instead, Simon (1955) believed that people are more accustomed to making a
decision that is "good" instead of "best." In order to find the "best" result would entail greater
cognitive abilities and time. The terms "aspiration levels" and "satisficing" describe this
decision-making process.
The aspiration level is the level at which an individual will be satisfied with any one
decision. Aspiration levels are not static since they move either up or down as a person
continues through the decision-making process. Satisficing is a Scottish term that is very similar
to satisfying as it is a blend of satisfy and suffice, as those who make decisions do so by making
an adequate decision opposed to an optimal one (Gigerenzer & Selten, 2001; Simon, 1972).
Simon (1955, 1972) explained the process of aspiration levels and satisficing through the
analysis of chess players. In the game of chess, a good player anticipates the future moves of
both himself and his opponent. In classical decision-making theories, the player would need to
calculate every possible future move of both himself and his opponent before moving a chess
piece. This is an example of optimization. This process is far too lengthy and the chance of
missing a predicted move increases along the continuum of increasingly possible outcomes. For
this reason, the chess player determines which moves would benefit him the most within the
time he is comfortable in giving to the task. This is known as setting an aspiration level. The
player begins to analyze the moves, and based on the difficulty of the task and his cognitive
level, the aspiration levels move up or down. At some point in time, the player finally decides
on the move he will take, and executes it through satisfying behaviors based on his aspiration
22

level (Simon, 1955; Simon, 1972). The concepts of aspiration levels and satisficing haves also
been described in the library and information science literature.
The use of bounded rationality emerged in the library and information science field in
the late 1980's, but it began to be more widely accepted in the 21st century. Overall, there has
been little research conducted on the topic. However, this decision-making theory has been
used to describe elements of information seeking behavior within the reference interview (Chu,
1994), federated searching (Buczynski, 2005) collection development (Schwartz, 1989), and
ending the decision making process (Prabha, et al. 2007; Watt, 2010). In these studies, bounded
rationality has been used to explain aspiration levels and satisficing.
Chu (1994) describes the reference interview as a conversation in the discovery of
information of library users. The user comes to the interview with a predetermined aspiration
level and with the assistance of the librarian, works to find the information or resources
needed. "Sometimes when librarians and [users] don't understand each other, it becomes a
process of expanding the bounds of rationality until the librarian's fuzzy set of satisfactory
answers and the student's fuzzy set of acceptable answers overlap" (p. 459). This ‘fuzzy set’ is
the question that the user is looking to have answered. In order for the librarian to assist the
user, the aspiration level of the student will be negotiated, altered, decreased and/or expanded
throughout the interview. Rarely are all possible resources or scenarios considered during these
types of interviews. Instead, a subset of possible choices was used. Users do decide at what
level they will be satisfied with their results; their aspiration level is based on their knowledge
and limitations of the topic. The research has not discussed the time and spatial constraints on
aspiration levels; however, this would be worth further investigation in future research.
23

The most popular topic in the literature relating to bounded rationality is that of
satisficing, or stopping a search when finding something is found to be "good enough" (Agosto,
2002; Chu, 1994). Zach (2005) describes satisficing as not collecting all knowledge, but
collecting enough knowledge. Chu (1994) relates bounded rationality to the library and
information science (LIS) field by comparing it to the reference interview used when academic
library users come with questions that are described as "fuzzy sets." These sets are information
needs that are more than just factual information, but instead rely on broader resources to
back up inferences. Chu states that even though the librarian attempts to find the one right
answer, there is often more than one correct answer, and the user will decide when the answer
is "good enough" for his/her needs, and this decision is dependent upon the user's level of
rationality.
Buczynski (2005) describes satisficing as a way to understand why users do not use
better resources to discover information. He states that users tend to forgo specialized
electronic collections such as digital libraries in favor of Google or other search engines in order
to work within time constraints, and he calls for the use of federated searching to allow for
greater access to specialized collections. Through federated searching, the individual will
reduce the number of ways to search, increasing their boundaries through redundant searches
among multiple resources. Connaway, et al. (2011) agrees "convenience is central to
information-seeking behaviors" (p. 186) and those users tend to gravitate to resources that
they feel are more accessible and easier to use.
Within the LIS literature, one of the tenets of satisficing is the discussion of limits.
According to Simon (1957), the limits for decision makers are of time and cognitive constraints.
24

These limits are identified as stopping points for information-seekers throughout the literature
(Hines, 2009). Time is one of the most common stopping rules described. Prabha, et al. (2007)
states, that both faculty and students work within time constraints to complete either their
coursework or projects. In a study of information seeking behaviors of young adult females, it
was found that physical constraints should be added to the list of limits. Agosto (2002)
unexpectedly found that searches might also terminate based on physical discomfort (pain, eye
strain, etc.). One could argue that these physical constraints are also a factor of time since as
time passes, these limitations are more pronounced. Mansourian et al. (2008) agree with the
limits described by Agosto and have incorporated bounded rationality into a model of searching
behavior.
The other stopping rule explained by Simon is that of cognitive constraints. The LIS
literature agrees with this constraint as information seekers often find themselves with
information, textual, and outcome overload. In each of these scenarios, the user finds the
search for information at times to be overwhelming (Agosto, 2002). Other studies found that
some resources were not considered by information seekers due to the lack of knowledge, or
because they did not see the need to use resources different from the ones they typically rely
on (Buczynski, 2005; Zach, 2005). Mansourian et al. (2008) identified that searchers often
demonstrate remorse over ending a search knowing that there could be more information out
there to find. Chu (2004) does provide some insight to the stopping behaviors as it relates to
not having access to all of the information a user may need. This is related to the fact that the
user may have a higher aspiration level to begin with and then is disappointed when he/she
needs to lower the level. The research in the LIS literature does point to a variety of factors in
25

information seeking; however, there is no discussion of the ability to access information or the
information seeking behaviors of people with disabilities.
In some cases, the user relies on heuristics, or "rules of thumb," in order to guide their
decisions. This was the case with senior arts administrators in a study relating to their
information-seeking processes. Zach (2005) found that "they rely heavily on direct personal
experience to fill their information-seeking needs" (p. 32). They view these "rules of thumb" as
important devices in seeking of information. Watt (2011) found that parliamentary staffers rely
heavily on heuristics as a means to search "fast and frugal" (p. 445). This allows them to find
information with the swiftest speed and at the lowest cost, even though the information they
find may not be of the highest quality.
Although bounded rationality has been used to describe the decisions in the
information-seeking process, the literature has pointed to greater research in the use of
bounded rationality in the library community. Hines (2009) suggests considering heuristics in
the planning, implementation, and evaluation of library services. She also believes that
satisficing is a valid part of decision-making and it should be embraced, and "[f]urther research
into the application of these theories into concrete situations is also welcome" (p.85). Other
researchers mentioned a greater need for research conducted on the information seeking of
individuals and how they apply satisficing (Mansourian & Ford, 2007; Warwick & Rimmer,
2009).

26

Cataloger's Judgment
Cataloger's judgment is another area of library science that could benefit from taking a
closer look at bounded rationality. The literature on cataloger's judgment is very limited in
scope. There are only a few articles devoted solely to this topic (Santamauro & Adams, 2006;
Intner, 1998); however, some authors have included cataloger's judgment within broader topics
such as cataloging education (Clare, 1950; Elrod, 2008; Miksa, 2008; Tauber, 1953), cataloging
quality (Harmon, 1996; Paiste, 2003; Snow, 2011), ethics in cataloging (Bair, 2005, Ferris, 2008),
the role of cataloging personnel (Cox & Myers, 2010; Fain, Brown, & Faix, 2008; Rider, 1996;
Wakimoto, 2009), and how cataloger's were encouraged to exercise cataloger's judgment
during the RDA National Test (Cronin, 2011). One of the goals of RDA was to "make it easier for
catalogers to exercise their judgment, rather than depend solely on its instructions" (Intner,
2006, p. 1). For this reason, it is anticipated that there will be much more written about
cataloger's judgment as libraries begin to implement RDA.
Cataloger's judgment has been described as applying common sense to the cataloging
rules; however, common sense is defined as "the use of sound and practical judgment that any
reasonable person, devoid of specialized training, would apply given the specifics of the
situation in hand" (Dinur, 2011, p. 697). Since cataloger's judgment is an activity of applying
specialized training, education and experience, to a set of established rules (Santamauro &
Adams, 2006), common sense is often overruled. For this reason, many practitioners and
students have trouble in applying rules that are, at times, ambiguous (Intner, 2006).
Santamauro and Adams (2006) describe cataloger's judgment as "applying tenets of
information management to order and provide access to texts" (p. 14) in accordance with local
27

policies to "meet end-user needs according to cataloging principles" (p. 13). Ferris' (2008)
definition is aligned to the one offered by Santamauro and Adams by stating that cataloger's
judgment is "the level of expertise attained by each cataloger after years of having interpreted
and applied the principles of bibliographic control" (p. 179). Snow (2011) takes this definition
even further by stating that "one could also argue that cataloger's judgment is not solely about
the level of expertise, but rather the cataloger's ability to utilize that expertise to make
informed cataloging decisions" (p. 4). She continues her argument that the rules do not cover
all possible scenarios that a cataloger may face when cataloging an information resource, and
that "judgment is usually developed and refined over time as the cataloger gains more
experience cataloging information objects and navigating various cataloging tools" (p. 4).
Cataloger's judgment is necessary because the current set of rules cannot be applied
universally without a cataloger's interpretation due to the number of inconstancies that may be
encountered and the audience for which the information resources are intended. Ferris (2008)
states, "the value of catalogers' judgment is that it supports the idea that one size does NOT fit
all in applying the rules of bibliographic control" (p. 179).
The history of cataloger's judgment found in the literature supports the struggles of
library educators in trying to prepare future catalogers by training them to understand
cataloger's judgment. There was a philosophical split among library educators about whether to
teach to the cataloging rules in a laboratory setting or to teach only the cataloging principles
(Tauber, 1953). Boughton was a believer of teaching the "objectives of cataloging and the
problems to be solved" (p. 32). However, Boughton was in the minority (Tauber, 1953).
Humeston (1951) was undecided about whether to teach to the rules or the principles but did
28

believe the rules "leave too much to the judgment of the student -- the student, who, with little
basis for judgment wants a rule" (p. 41). Almost 60 years and many rules later, the similar
problems still seem to occur. Miksa (2008) writes that students often display frustration when
the cataloging rules do not guide them to the one "right" answer (p.21).
Other library educators make connections between cataloger's judgments along with
cataloging quality. Elrod (2008) notes that students lack the skills needed to make decisions,
and they "lack a basis in the principles in cataloging, which should have been a part of their
professional education for librarianship" (p. 5) thus leading to a reduction of quality in library
catalogs.
The discussion in the literature relating to the behaviors of catalogers often turns to that
of cataloging quality. Although the topic of quality is different from cataloger's judgment, there
are some similarities between the two which make the topic of quality worth discussion.
Ruschoff (1995) states there are two facets to cataloging. First is the quality of
cataloging and how it relates to the accepted standards and practices, and how the cataloging
process should be completed so that there is no need to revise work that has already been
performed. The second aspect of quality is related to the user, and how catalogs align to user
tasks. Quality has been defined as the absence of typographical errors (Beall, 2005; Beall &
Kafadar, 2004; Mann, 1991), maintaining authority control (Bade, 2002), the avoidance of
duplicate records (Norgard, Berger, Buckland, & Plaunt, 1993), and the level of information
provided in the bibliographic data, often referred to as record enhancement (Hanson &
Schalow, 1999; Shedenhelm & Burk, 2001). The quality of the library catalog reflects the
decisions made by catalogers.
29

The topic of quality catalogs also includes the types of information included in the
bibliographic record as it relates to ethical practice. Bair (2005) describes catalogers as "gate
keepers for information and architects of the information infrastructure to provide fair and
equitable access to relevant, appropriate, and uncensored information in a timely manner and
free of personal or cultural bias" (p. 22). It is their decisions in creating subject headings and
recording descriptive information and other access points that allow users to find the
information objects they need. To achieve these goals, well-practiced and educated catalogers
need to act without bias in describing information resources.
Professional catalogers are not the only cataloging staff members that are expected to
exercise judgment. The literature points to an increase in the number of paraprofessional
cataloging staff that are responsible for copy cataloging, the process of editing bibliographic
records created by another cataloging agency, and engaging in the creation of original catalog
records (Cox & Myers, 2010; Rider, 1996). Some libraries have decided to "cross-train" other
library professionals to create or edit bibliographic records as well. Reference librarians bring
their subject expertise and their knowledge of information-seeking behavior of their users to
create records with more relevance (Fain, et al., 2004). Fait et al. (2004) express the benefits of
using reference librarians in decreasing the cataloging backlog by providing the reference
librarians with an increased knowledge of the structure of metadata; however, the literature
does not state how effective these practices are in creating quality bibliographic records with
sound judgment and would benefit with further research.
"Cataloging is an art, not a science. No rules can take the place of experience and good
judgment, but some of the results of experience may be best indicated by rules" (Cutter, 1904,
30

p. 6). These are the last words of the Preface to the 4th and final edition of the Rules for a
Dictionary Catalog by Charles A. Cutter (1904) and these words underline the problem for this
study. The cataloging community can argue whether cataloging is an art or a science, but the
one thing it can agree upon is that judgment serves an important role and it will only be more
so with the adoption of RDA.
Cataloging Principles
Tillett (2009) states, "The idea of stating [cataloging] principles is to build cataloger's
judgment in deciding how to describe or provide access to bibliographic resources" (p. 4). The
principles will guide the cataloger in preparing the bibliographic record for an item to be
included in the library catalog through the use of various rules and encoding standards (AACR2,
RDA, MARC21, etc.). These sets of codes have been adopted by libraries worldwide and allow
for the sharing of bibliographic records. However, in order to ensure effectively that the codes
and rules are in alignment, it is necessary to have a set of principles that will provide the
necessary guidance in the creation of the codes and rules. The first set of internationally
recognized cataloging principles was adopted in 1961. The Statement of Principles, more
commonly known as the Paris Principles, was an international effort to provide a framework to
guide the creation of cataloging rules.
The Paris Principles were influenced largely by Charles Cutter's work and the theoretical
foundations assembled by Seymour Lubetsky (Spanhoff, 2002). The use of Cutter's objectives or
functions of a catalog (Figure 1) provide catalogers with a goal of what an effective catalog
should be able to do in assisting users.

31

Figure 2.1: The function of a catalog. Cutter, C. A. (1904). Rules for a printed dictionary
catalogue (4th Edition). Washington, D.C.: Government Printing Office.
Prior to 1961, there were no internationally agreed upon principles for cataloging.
According to Buizza (2004), one of the reasons behind the creation of the internationally
accepted principles was to allow for the greater sharing of bibliographic information from
libraries around the world. These principles were not for the purpose of providing a framework
for authority files, but for the “author and title catalogue” with the following aspects: (1)
"finding and collocating functions (the latter twice over: for checking which works of an author
and which editions of a work are in the library); structure (at least one main entry per book,
with added entries and references), [and (2)] devices (uniform headings both main and added)”
(p. 118). Spanhoff (2002) agreed with Buizza and provided more enlightenment to the overall
purpose of principles by stating that principles would provide a framework for the catalog and
how the objectives of a catalog can be achieved.
Section 1 of the Paris Principles presents the scope of the principles and how they are
limited to the “choice and form of headings and entry words” (ICCP, 1961, Sec 1). Section 2
describes the library catalog as a vessel and it should be able to state whether a library holds a
32

particular book. This is accomplished by searching for the author, title, variant title, or a
combination of them as well as being able to determine the edition of each book. Sections 3 –
12 describe the use of main entries, added entries and references within the bibliographic
record such as a uniform heading that includes the author(s), corporate body, or a title.
These principles were created for large library catalogs for describing “books and other
library materials having similar characteristics’” (Creider, 2009, p. 584). It is also important to
keep in mind that the Paris Principles were created during a technical environment in which
“individual typewritten catalogue cards” (Buizza, 2004, p. 119) were considered the best
method of cataloging. At the time, most libraries only had books represented in their library
catalogs; however, records for other types of materials such as maps were starting to be
included (Guerrini, 2009). As time progressed, the number of different types of electronic
resources and other non-print materials increased and catalogers struggled with how to
appropriately record these types of resources.
Over time, it became necessary to reexamine the principles, rules, codes, etc., in order
to evaluate whether or not they are still relevant. In the 1980's many libraries began adopting
library automation systems that required printed cards to be converted into electronic
bibliographic records. The library community found that the sharing of electronic records would
facilitate a swifter transition from print to electronic. These changes in cataloging provided the
need to review the processes and rules used in the development of electronic cataloging
records. The Conference on the Conceptual Foundations of Descriptive Cataloging in 1987
began these discussions. The purpose of this meeting was to discuss ways in which library
automation would allow for increased level of cooperation among libraries (Spanhoff, 2002).
33

The transition from the printed catalog card to the electronic bibliographic record is only
the beginning of the effects the electronic age has had on library organizations. During the
1990's, libraries began to incorporate electronic resources into their library collections.
Although the Paris Principles provides a footnote describing that they are also for the use of
other types of materials beyond books, the principles still did not meet the needs many
required to describe electronic resources and other non-print materials. A study was
commissioned by International Federation of Library Associations (IFLA) in 1990 in part to reexamine functions and to provide a new framework to display bibliographic records. This study
is known as the Functional Requirements for Bibliographic Records (FRBR). The review was
prompted by economic concerns that would allow for the greater sharing of bibliographic data
and to meet the needs of the digital world (IFLA, 1998). This study led to the creation of the
FRBR entity-relationship model and an update to the Paris Principles and the creation of RDA.
According to Spanoff (2002), the growth of the Internet was the greatest threat to
cataloging due to the increasing number of electronic resources (computer software, electronic
databases, digital files, etc.) available to users. She further discusses how the search engines are
sophisticated and easy to use, and that they are in competition with library catalogs. The
cataloging community responded by calling for an update to the Paris Principles.
In 2001, there was a call by Natalia Kasparova, a librarian at the Russian State Library in
Moscow, and also a member of the IFLA’s Cataloguing Section, for the review of the Paris
Principles in order to meet the needs of the time. This call was met with agreement from others
in attendance and resulted in the evaluation of the current principles. Through the reexamination, it was found that
34

The objectives of the new principles should be formulated in clear expressions, easily
understood and valid everywhere for all types of resources; should make it possible to
work in the web; and be consistent with other rules. They had to mirror the relational
structure of the catalog, and to address primarily the various resources found in libraries
and the electronic ones to which libraries provide access via the Internet, or other
connecting modes. The text also has to cover both descriptive and subject cataloging, to
pay great attention to authority records, and to deal not only with the collection of one
library but virtually with the collections of all libraries and including the features of
digital collections (Guerrini, 2009, p. 723-724).
These new expectations of the principles were in alignment with FRBR, which will be discussed
in more detail in the next section of Chapter 2. A further look at the ever increasing number of
types of resources in the universe opposed to the printed items of the 1960’s, this review
should account for the emergence of technology in how users search for information (Guerrini,
2009).
Work on a new set of principles began in 2003 by the IFLA Meeting of Experts on
International Cataloguing Code (IME ICC). Meetings of the global bibliographic community took
place prior to the IFLA conferences in Frankfurt, Germany; Buenos Aires, Argentina; Cairo,
Egypt; Seoul, South Korea; and Pretoria, South Africa. In all, representatives from eighty-one
countries were in attendance throughout the five-year review of the new principles (Creider,
2009; Guerini, 2009).
The original plan was simply to update the Paris Principles to reflect the cataloging
needs of the time; however, IMEICC realized that the standard needed to be expanded and
broadened in scope. Terminology needed to reflect the number of different resources found in
library catalogs and the increase of electronic resources available to users. For example, the
term “bibliographic resources” would replace “books” and align the International Cataloging

35

Principles (ICP) with FRBR and FRAD so that systems would provide tools that are more efficient
for future users (Guerrini, 2009).
The official name change from Statement of Principles, known as the Paris Principles, to
the Statement of International Cataloging Principles (ICP) within the revision, emphasizes the
international scope of the work. The revision of Paris Principles into the ICP resulted in two
important outcomes (1) new terminology and (2) attention to the user. For example, creator
replaces author and defining the terms work, expression, manifestation and item in alignment
with FRBR (Creider, 2009). The ICP makes it clear in the introduction of the principles that “[t]he
first principle [of the nine] is to serve the convenience of catalogue users” (IFLA, 2009). The
other eight principles are common usage, representation, accuracy, sufficiency and necessity,
significance, economy, consistency and standardization, and integration. Each of the other eight
general principles also guide the creation of cataloguing rules to allow for cataloger's judgment
in order to meet the needs of the information consumers.
Although created over a century ago, the library community still believes that the
functions have relevance today, albeit with some modifications. Cutter's objectives were
created for print materials and modern libraries offer many different types of materials
(electronic, audiovisual, etc.). It is for this reason that one should think of the term ‘book’ to be
synonymous with the term ‘work’ to recognize all types of formats found in catalogs (Taylor,
2006). The process of updating the cataloging principles was in response to this need, resulting
in multiple revisions of the ICP prior to its final acceptance. In 2009, the IME ICC released the
ICP, which replaced the only set of principles, the Paris Principles that have been accepted
internationally and will continue to focus on the needs of the user (Bianchini & Guerrini, 2009).
36

Functional Requirements of Bibliographical Records
In 1997, the IFLA Section on Cataloguing approved the Study Group on the Functional
Requirements for Bibliographic Records document entitled Functional Requirements for
Bibliographic Records: Final Report – 1998. This report was the result of the study resolution
passed at the IFLA Universal Bibliographic Control and International MARC Programme and the
IFLA Division of Bibliographic Control’s Stockholm Seminar on Bibliographic Records in 1990.
The charge of the study was to determine the functions of a bibliographic record and how it will
meet user needs (IFLA, 2009) and how to best promote Universal Bibliographic Control (UBC) to
reduce cataloging costs through the reduction of the duplication of the work done by many
organizations (Madison, 2005). According to the IFLA (2009) report, the study was needed due
to changes in the technology in the bibliographic world including automation, databases, the
continued sharing of bibliographic data due to reducing cataloging costs and the inclusion of
electronic resources into the publishing world. It was also determined that user expectations
and needs had grown and the current practices were not necessarily meeting all of these needs,
including how to address the wide variety of materials represented in the bibliographic
universe. This resulted in the examination of the minimal level record, and elements such as the
descriptive portions, access points, annotations, etc. The most significant findings of FRBR for
this dissertation is the shift from describing elements in isolation to describing elements that
share relationships between Works, Expressions, Manifestations, and Items through the use of
an entity-relationship conceptual model.

37

The study group determined early in the process that the entity-relationship (E-R) model
would be used to develop FRBR. Questions were raised as to why other models were not
considered. Madison (2005) reflected that
The E-R approach is one of several approaches popular in logical database design. This
approach differs from some other approaches in that it begins with an abstract or
conceptual schema of neither the domain nor universe question. The universe is
characterized in terms of the entities in it and the relationships that hold among them.
As such the conceptual schema is not restricted by the capabilities of any particular
database system and is independent of any particular record definition. By virtue of its
unrestrictive and independent nature, it is perceived as providing a unified view of the
data to be modeled. It is perceived as being more easily understood, more stable, and
easier to design than a schema conditioned by assumptions pertaining to what
constitutes a bibliographic record or by storage and efficiency consideration. (p. 29)
Madison believed that this statement provided the bibliographic community the assurance that
this model was the correct one to pursue. This led to the acceptance of the FRBR model by
many community members.
One of the reasons driving the creation of a new model for bibliographic records was
the evolving concept of the user. FRBR has defined four user tasks for the bibliographic record
to support: a user in finding, identifying, selecting, and acquiring (or obtaining) information. The
FRBR user tasks are expanded from the user tasks identified in Charles Cutter's Objects for a
catalog that were also the building blocks for the Paris Principles (Madison, 2005). The catalog
needs to assist users in finding the materials they are looking for; to confirm they have found
the correct item through identification; selecting the appropriate resource based on their
needs; and assisting the user in obtaining the item through available access points (loan,
purchase, etc.). The “user tasks reinforce the traditional objectives of a catalog, as described by
Cutter in 1876 to enable a user to find and to collocate works” (Tillett, 2004, p. 5).

38

The FRBR model is made of three groups of entities and relationships between them.
Group 1, Entities and Primary Relationships describes the nature the entities Work, Expression,
Manifestation, and Item. Figure 2 demonstrates the relationships between these four entity
types. A work is realized through an expression, the expression is embodied in a manifestation,
and the manifestation is exemplified in an item.

Figure 2.2: Group 2 Entities and “Primary” Relationships. IFLA Study Group on the Functional
Requirements for Bibliographic Records (IFLA) (1998). Functional Requirements for Bibliographic
Records: Final Report. Retrieved from http://archive.ifla.org/VII/s13/frbr/frbr1.htm#3
As noted by Boeuf (2005), understanding Group 1 entities can be difficult due to the
abstract nature of the term Work. He further concludes that the term Item is exemplified by the
“physical carrier of a text” (p. 3); the Manifestation is the edition of that physical carrier; the
expression is the actual text of the Manifestation; but the Work is still a Work. He argues that
the term Work is so abstract that it will have a different meaning to different people and that
the model should have forgone the term Work. Adamich (2007) describes the term work to
39

imply that it is the idea of the entity and the expression is that idea recorded in some form by a
creator who then produces a Manifestation such as edition, translation, performance, etc., that
is then put into a tangible item which the library can then shelve or find electronically for a user
to use.
Group 2 entities are the entities involved in the creation of the Work, Expression,
Manifestation or Item. These entities are either persons or corporate bodies. Figure 2.3
provides a visual of how these are related to the Group 1 entities. The Work is created by, the
Expression is realized by, the Manifestation is produced by, and the Item is owned by a person
or corporate entity.

40

Figure 2.3: Group 2 Entities and "Responsibility" Relationships . IFLA Study Group on the
Functional Requirements for Bibliographic Records (IFLA) (1998). Functional Requirements for
Bibliographic Records: Final Report. Retrieved from
http://archive.ifla.org/VII/s13/frbr/frbr1.htm#3
Group 3 entities are the relationships between the Work and the subjects. These
entities assist the user in understanding what the Work is about. Figure 2.4 shows how the
Work could have a subject of any of the Group 1 entities, Group 2 entities, or a concept, object,
event, or place.

41

Figure 2.4: Group 3 Entities and "Subject" Relationships. IFLA Study Group on the Functional
Requirements for Bibliographic Records (IFLA) (1998). Functional Requirements for Bibliographic
Records: Final Report. Retrieved from http://archive.ifla.org/VII/s13/frbr/frbr1.htm#3
It is important to note that FRBR is a model for bibliographic records and it does not
reflect a model for authority data. The Functional Requirements for Authority Data (FRAD) is
the model that has been created to describe authority data. FRAD was originally known as the
Functional Requirements for Authority Records (FRAR); however, the name was changed to
reflect that the focus was on the data and not the record (Veve, 2009).
42

FRAD also has a set of user tasks which include Find, Identify, Contextualize, and Justify.
In order for a database to be successful in implementing FRAD, it must be able to meet all of
these user needs. The user must be able to locate the entity; confirm the entity as the one
being searched; know that the entity is the one they are looking for by matching it with variant
names or to be able to clarify the relationship between it and another entity; and to provide the
proof that the entity is the entity the cataloger says it is (Patton, 2005). Delsey (2005) mentions
more similarities between the two models such as the ability of FRAD to characterize authority
records in the same manner that FRBR characterizes the bibliographic universe, and that there
are similarities between the different types of entities described within the models (i.e.,
persons, corporate bodies, objects, concepts, events and places). When comparing the two
models, there is a definite relationship between them.
Tillett (2005) summarizes FRBR and FRAD when discussing the benefits of adopting
these two models worldwide using technology
The benefits for end-users are enormous for future ‘one-stop-shopping’ to search all
potential sources of information through intelligent systems like the Semantic Web, and
that system would have improved precision of searches and better clustering of related
entities through cataloging, authority control, and the implementation of the FRBR
concepts… We are moving to an era when we have the ability to share and re-use
bibliographic descriptions created anywhere in the world and tie the bibliographic
descriptions with real access, so users can obtain the resources they want (p. 201-202).
If this outcome is realized through the creation of these models, then the intent of the
resolutions at the Stockholm Seminar would, at the very least, be met. The re-writing of rules,
codes and principles resulted with the acceptance of these models, especially with FRBR.

43

Functions of a Catalog
Online catalogs have been described as inefficient and as barriers to those searching for
information. Wakimoto (2009) asserts that the catalog faults are due to Integrated Library
System (ILS) vendors that have been unresponsive to the changes in user needs. The catalog
provides users with quality resources that are not always available using search engines such as
Google. Wakimoto states, “[p]roviding appropriate and context-sensitive resources for the local
users is what makes the catalog viable and valuable” (p. 412). This is accomplished by providing
enriched content through the access of hidden resources such as rare materials, archives and
manuscripts; electronic resources such as eBooks and E-Serials; and digital collections such as
institutional repositories (Wakimoto, 2009). The provision of additional resources allows users
greater access to materials. Many catalogs do not allow this to happen due to how resources
are indexed. It is through catalogs that are designed to meet the conceptual model of FRBR and
the records created using RDA that data will be to be allowed to be used in ways that have been
limited in the past. The power of the data is not in the data itself, but in the relationships
present in the metadata. The current state of most library catalogs does not capitalize on this
possibility.
The ICP and the FRBR model both recognize Cutter's objects and provide guidance as to
the purpose of the catalog and the user tasks. As discussed previously, FRBR acknowledges the
user by defining four general user tasks: find, identify, select, and obtain (IFLA, 2009). The ICP
uses "acquire" instead of "obtain" and also provides a fifth task, navigate (IFLA, 2009). These
user tasks guide both catalogers and systems designers as to how a catalog should maximize
information retrieval for its users.
44

Members in the cataloging community believe that use of the FRBR model in library
catalogs will increase the collocation of titles in the catalog because the model has a greater
emphasis on bibliographic relationships (Kemp, 2008). The Online Computer Library Center
(OCLC) recognized the need to apply the FRBR model into a new program that fulfills the intent
behind the model. Hickey and O’Neill (2005) discuss how OCLC’s WorldCat categorizes
information entities at the Manifestation level; when a user searches for an item in the catalog,
the result set provides the user with bibliographic records based on the Manifestation. In FRBR,
the initial level of display should be the Work (IFLA, 2009). OCLC has released an algorithm that
will allow for bibliographic records to be grouped by “collapsing a large number of expressions
into a single work mak[ing] the display of large numbers of records much more
comprehensible” (Hickey & O’Neill, 2005, p. 250).
There are projects underway that are attempting to realize the FRBR model in the
catalog interface. These include prototypes such as OCLC’s Fiction Finder; automation system
vendors such as VTLS, Innovative Interfaces, Ex Libris, and Portia; and digital libraries and
institutional repositories (Salaba & Zhang, 2007; Dickey, 2008). Overall, the development of
these types of products has been slow, and many automation system vendors have yet to
release a product that has a fully realized FRBRized catalog. However, there are some examples
of next generation catalogs going live that do attempt to realize the FRBR model such as the XC
project at the University of Rochester, E-Matrix at North Carolina State University, and Virtura
Integrated Library System by Visionary Technology in Library Solutions (VTLS). Salaba and Zhang
(2008) state that the benefits of a FRBRized catalog will have the most beneficial effect on
works of fiction, music, and serials. These materials typically have a higher number of
45

Manifestations. OCLC recognized the effect of a FRBRized catalog with fiction titles which lead
them to create Fiction Finder, their prototype of a FRBRized catalog.
FRBR provides a model of data that will allow users to access resources of all types in
new ways. However, the previous cataloging standard, the Anglo-American Cataloging Rules,
2nd Edition (AACR2) was a barrier to successful implementation of the FRBR model, which was
one of the reasons why RDA was created.
AACR2
“The Paris Principles [is] the most relevant theoretical reference framework in the
history of cataloging in the second half of the twentieth century; it was taken as the basis for
the codes developed worldwide from the mid-sixties, beginning with the 1967 AACR” (Guerrini,
2009, p. 723). In 1974, there was a need to review AACR, the cataloging rules of the time. The
new standards needed to incorporate additional rules for non-book materials. The new text
was called simply AACR2 and was released in 1978 as an international collaborative project
between the American, British, Canadian, and Australian library communities.
The introduction to AACR2 states the intended use of the rules is for libraries and for all
types of materials found in libraries, but not necessarily for use in special libraries except as a
resource for the development of their own rules. The previous set of rules, AACR, was released
as two separate publications, one for the British community and the other for the North
American community. The introduction also provided the cataloger with the scope of the rules,
descriptive cataloging and access points, and that it is a shared text with one publication for
both the British and North American communities (AACR2r, 2005).

46

AACR2 has two parts, Description and Headings and Uniform Titles and References. Part
I, Description, was based on the General International Standard Bibliographic Description
(ISBD(G)) which is a standard that was created to facilitate the world-wide sharing of
bibliographic data among libraries. Part I consists of thirteen chapters:
1. General Rules for Description
2. Books, Pamphlets, and Printed Sheets
3. Cartographic Materials
4. Manuscripts
5. Music
6. Sound Recordings
7. Motion Pictures and Videorecordings
8. General Materials
9. Electronic Resources
10. Three-Dimensional Artifacts and Realia
11. Microforms
12. Continuing Resources
13. Analysis
Chapter 1 provides rules that the cataloger will refer back to when using chapters 2 - 12.
This includes such tasks as determining the chief source of information, punctuation, level of
description, and the general rules for the title and statement of responsibility; edition; material
type; publication information; physical description; series; notes; and the standard number and
terms of availability. Chapters 2 - 12 provide rules for cataloging specific types of materials
47

based primarily on their format. Each chapter begins with a scope note that provides the
information of what types of entities are to be cataloged using the rules found in that chapter
and then continues with more specific rules that follow the structure of chapter 1. Chapter 13
assists the cataloger in describing those entities that are a combination of multiple parts, which
makes up a single bibliographic heading (AACR2r, 2005).
Part II of the AACR2 contains six chapters for Headings, Uniform Titles, and References:
21. Choice of Access Points
22. Headings for Persons
23. Geographic Names
24. Headings for Corporate Bodies
25. Uniform Titles
26. References
Chapter 21 provides direction in how to create access points in the bibliographic record
based on type of work. Chapters 22 - 25 provide the cataloger direction in creating headings for
persons, geographical names, corporate bodies, and uniform titles. Chapter 26 provides rules
for creating reference statements (see and see also) for the bibliographic record (AACR2r,
2005).
Much has been written about the efficiency of the rules found in AACR2. Various
authors have pointed to detailed issues with the General Material Designation (GMD), serials,
uniform headings, the level of difficulty in using the rules, and the rules’ inability to provide
guidance in cataloging new types of materials that are acquired into libraries, especially
Internet resources (Taylor, 1999; Weihs, 2011).
48

Prior to the 1997 IFLA conference, a three day meeting was convened to discuss
cataloging principles and the future of AACR2 (Tillett, 1998). This conference was the beginning
of the creation of a new set of cataloging rules, RDA, to be based on the FRBR conceptual model
(Guerrini, 2009). The format of the conference focused on exploring a variety of cataloging
issues through a worldwide contribution of presented papers on topics related to the problems
with AACR2 such as the previous principles, rules, processes, etc. The speakers conveyed that
the old rules do not meet the needs of the bibliographic community, and the effect of new
technologies had progressed, creating a mandate for such a change. During the conference,
seven challenges were highlighted as areas of concern under the current rules:
(1) The need for a greater number of access points in library catalogs and the need to
allow additional means to deal with multiple authors. This issue relates directly to the "Rule of
Three,” which will be later defined in this chapter.
(2) The new set of principles must match the technology available at this time and in the
future.
(3) The principles and rules must be easy to understand and to teach and be extended
to new types of media that are unknown today.
(4) The new set of rules must be compatible with current principles and rules by being
backwards compliant in order to ease adoption and contain costs.
(5) Increase the use of shared bibliographic records between libraries to maximize
cooperative efforts between libraries.

49

(6) Solve the dilemma of the GMD, which can reflect a content type or a carrier type.
This is especially true of electronic resources that currently can be classified as multiple GMDs.
Finally,
(7) eliminate the special case-based rules found in Part 2 (Tillett, 1998).
Participants at the International Conference on the Principles and Future Development
of AACR2 in 1997 determined that a revision was necessary. AACR2, released in 1978, was too
old to meet the current and future technological needs of catalogers and information
consumers. Moore (2006) points out that card catalogs were still the norm in 1978, but library
catalogs have progressed into databases with the ability to index and search large amounts of
metadata at one time. The branch of thought of limiting the amount of data found on the
catalog card conflicts with the expectation of providing robust data for today’s electronic
catalogs. Attempting to accommodate the current needs, revisions to AACR2 were attempted;
however, the revisions still did not meet the demands of cataloging emergent types of
materials. Moore notes that “[i]t is becoming increasingly difficult to retrofit the old rules.
Difficult issues include: the nature of authorship, the nature of the ‘work;’ bibliographic
relationships, seriality; and the description of new types of media” (p. 14). New types of media,
mostly those found in the digital environment, are one of the reasons why a revision to AACR2
was necessary.

RDA
It was not until 2004 that the Joint Steering Committee (JSC) for the Revision of AACR
began working on what was to be called AACR3. In 2005, the working title was changed to
50

Resource Description and Access (RDA) due to the cataloging community's rejection of the first
drafts of AACR3. According to Kraus (2007), the name change was needed
because RDA’s intent to become compatible with international standards; ‘Cataloguing’
was replaced with ‘Resource Description,’ a term embraced by other metadata
producing communities; and ‘Access’ points towards the goal of creating a flexible
framework for describing all analog and digital resources… (p. 66)
RDA was intended to be used by all types of libraries and other organizations that have a need
to create data to describe their collections (Moore, 2006).
According to Carr (2007), the creation of RDA is based on the framework of FRBR’s
conceptual model and adopts the same terminology and user tasks. RDA is also aligned with the
ICP, which instructs catalogers to base their cataloging records on the Manifestation level of
FRBR and that “a new bibliographic record should be created for each physical format” (p. 284).
RDA is not a metadata schema and will not replace encoding standard such as MARC, MODS,
Dublin Core, etc. Instead, it is a content standard (Oliver, 2007). RDA does not provide guidance
for classification or constructing subject headings (Moore, 2006). It defines the rules for the
preparation of reliable descriptions of items included in a library catalog. Such descriptions
define the attributes associated with the item being described, keeping in mind that the item
could be in one of many different types of formats (including print, multimedia, electronic).
Integrated library system (ILS) vendors have been working with the creators of RDA to ensure
RDA-based records will work within their systems (Carr, 2007).
RDA and AACR2 are guided by the cataloguing principles (Paris Principles and ICP).
However, RDA is designed to meet the needs of a technological or digital environment. RDA
covers a broad range of materials that are not covered in AACR2 in order to meet the objectives
that the standard needs to “provide a consistent, flexible and extensible framework for both
51

the technical and content description of resources” (Carr, 2007, p. 284). Carr (2007) mentions
that MARC records created using RDA will be compatible with those created using the rules
from AACR2.
RDA is comprised of four parts with a total of ten sections and 32 chapters plus an
introduction, 13 appendices, and a glossary (Table 2.1).
Table 2.1
Structure of RDA
Introduction
Section 1: Recording attributes of
manifestation and item

Section 2: Recording attributes of
work and expression
Section 3: Recording attributes of
person, family, and corporate body

Section 4: Recording attributes of
concept, object, event, and place

Section 5: Recording primary
relationships between work,
expression, manifestation, and item
Section 6: Recording relationships to
persons, families, and corporate
bodies

Chapter 1: General guidelines on recording attributes of manifestations and
items
Chapter 2: Identifying manifestations and items
Chapter 3: Describing carriers
Chapter 4: Providing acquisition and access information
Chapter 5: General guidelines on recording attributes of work and expression
Chapter 6: Identifying works and expressions
Chapter 7: Describing content
Chapter 8: General guidelines on recording attributes of persons, families,
and corporate bodies
Chapter 9: Identifying persons
Chapter 10: Identifying families
Chapter 11: Identifying corporate bodies
Chapter 12: General guidelines on recording attributes of concepts, objects,
events, and places*
Chapter 13: Identifying concepts*
Chapter 14: Identifying objects*
Chapter 15: Identifying events*
Chapter 16: Identifying places
Chapter 17: General guidelines on recording primary relationships between a
work, expression, manifestation, and item
Chapter 18: General guidelines on recording relationships to persons,
families, and corporate bodies associated with a resource
Chapter 19: Persons, families, and corporate bodies associated with a work
Chapter 20: Persons, families, and corporate bodies associated with an
expression
Chapter 21: Persons, families, and corporate bodies associated with a
manifestation
Chapter 22: Persons, families, and corporate bodies associated with an item

* Placeholder - Chapter not yet written

(table continues)
52

Table 2.1 (continued).
Section 7: Recording the subject of a
work
Section 8: Recording relationships
between works, expressions,
manifestations, and items

Section 9: Recording relationships
between persons, families, and
corporate bodies
Section 10: Recording relationships
between concepts, objects, events,
and places

Chapter 23: General guidelines on recording the subject of a work*
Chapter 24: General guidelines on recording relationships between works,
expressions, manifestations, and items
Chapter 25: Related works
Chapter 26: Related expressions
Chapter 27: Related manifestations
Chapter 28: Related items
Chapter 29: General guidelines on recording relationships between persons,
families, and corporate bodies
Chapter 30: Related Persons
Chapter 31: Related families
Chapter 32: Related corporate bodies
Chapter 33: General guidelines on recording relationships between concepts,
objects, events, and places*
Chapter 34: Related concepts*
Chapter 35: Related objects*
Chapter 36: Related events*
Chapter 37: Related places*
Appendices A - M
Glossary

* Placeholder - Chapter not yet written

RDA Toolkit (2015)

Although there are some similarities between AACR2 and RDA, there are a number of
differences in the rules. In AACR2, the cataloger starts with the format of the entity to begin the
description of the resources and then moves to the elements of the description. In RDA, the
goal of its organization is to begin with smaller pieces of the entity and go from general to
specific. This difference is very noticeable when looking at the RDA.
Transcription of the chief source of information is another change from AACR2 to RDA.
RDA no longer uses the term ‘chief source,’ instead it uses ‘preferred source.’ This is to allow
the cataloger to determine the best place to pull the data from in order to describe the item.
RDA allows organizations to either transcribe the information as it is recorded on the item or
53

use local rules that may still conform to AACR2. An example of this would be transcribing
capitalization in the title or correcting misspellings that occur on the preferred source. This
reflects a what-you-see-is-what-you-get approach to transcription (Carr, 2007).
The use of the ‘Rule of 3” is no longer a mandate with RDA. In AACR2, the cataloger is
limited to the number of names it would associate with the creation of an information object.
AACR2r (2005) rule 1.1F5 states,
If a single statement of responsibility names more than three persons or corporate
bodies performing the same function, or with the same degree of responsibility, omit all
but the first of each group of such persons or bodies. Indicate the omission by the mark
of omission (…) and add et al. (or its equivalent in a nonroman script) in square brackets.
In RDA, catalogers are allowed the freedom to determine if all creators of the work will receive
credit no matter how many were involved in the creation of the work. This new practice allows
for greater access to information resources through a wider list of names of those that had part
in creating the intellectual content contained in the information entity.
In AACR2, the use of Latin abbreviations is used when there were more than three
people that could be listed in the statement of responsibility. RDA has reduced the number of
Latin abbreviations. Instead, the cataloger may use all of the names or terms such as "and 5
others” instead of [et al.]. There are many other differences between the abbreviations in RDA
and those found in AACR2. For example, in AACR2, the use of ‘ill.’ is used to describe if there
were illustrations in the book; however, in the new standard, the cataloger should spell the
word ‘illustrations’ out in its entirety. Another set of abbreviations that will change are those
used to describe publisher information. In AACR2, [s.I.] was used to describe that the place of
publication was not known. In RDA, the phrase [Place of publication unknown] is used instead
54

(AACR2r, 2005; Carr, 2007; RDA Toolkit, 2015). Two of the goals of RDA are to allow greater
convenience to the user, and the ability to share bibliographic data with non-library
communities. The elimination of abbreviations is one way of achieving this goal.
Although there has been much praise surrounding RDA, there has been much discussion
that RDA may not meet the needs of the bibliographic community. Coyle and Hillmann (2007)
state that RDA is still too deeply based on AACR2 and that the changes do not go far enough.
Others feel that RDA goes too far and that the cost of change will prohibit institutions from
adopting the new standard or that the current standards serve the community well (Intner,
2006; Kraus, 2007). Gorman (2007) provides several reasons as to why RDA does not meet the
needs of the library community. One of the main issues he has with RDA is the term
“guidelines” instead of “rules.” This change in language promotes a greater use of cataloger’s
judgment in the interpretation of the new standard. He fears there is a lack of structure to the
new guidelines that prohibit a logical progression through the completing of a bibliographic
record. Gorman also notes that the guidelines have partially abandoned the ISBD(G) which
provided standardization for bibliographic records world-wide. He also finds that the rules
confuse the reader and do not make them easier for the user; however, he provides no data to
support his claims. Gorman is not alone in his assessment of RDA. Intner (2008) agrees with
many of his arguments including problems of the structure, vocabulary and lack of good
examples as issues with RDA.
Aside from the issues with RDA as it relates to the content and structure of the rules,
Hillman (2006) finds the goals of RDA to be overambitious. They are to allow for the cataloging
of the digital world, but in order to catalog the digital world one must assume that there is
55

some level of stability within digital resources. However, that is not the case; digital resources
can appear one day and either be modified or completely gone the next.

RDA National Test
RDA is the new cataloging code for the international cataloging community; however,
many in the U.S. cataloging community had reservations in the implementation of the rules. For
this reason, the three national libraries vowed to conduct a test of the new code in order "to
assure the operational, technical, and economic feasibility of RDA" (Library of Congress, n.d.,
para. 1). The national libraries collaborated to create a coordinating committee that would
determine how to conduct such a test, which should participate, create a timeline for testing,
and evaluate the results. In the end, they choose twenty-three (23) additional test partners that
included libraries and non-library organizations such as museums and archives, book vendors,
etc. The official testing period of the National Test of RDA occurred from July 1, 2010 December 31, 2010. The first three months of the testing period were devoted to training and
practice for testers using the RDA Toolkit. The second half of the testing involved the actual
creation of descriptive bibliographic records for the coordinating committee to review and
evaluate to determine whether they would recommend the implementation of RDA as the new
cataloging code.
The formal test contained 25 common items that each testing institution used to create
bibliographic records in their current set of cataloging rules (i.e., AACR2) and again using RDA.
Partner organizations determined which type of encoding schema that they would choose to
create these records (MARC, MODs, etc.). The common set of items to be cataloged included
56

monographs, audio-visual materials, serials, and integrating resources. A portion of these
materials can also be classified as electronic resources. Each institution would assign two
different staff members to each item to be cataloged. One would catalog the item using the
current rules of the institution and the other person would catalog the item using RDA. If the
organization performed authority work in their normal workflow, then the testers should also
apply this to the test; however, if it is not a normal part of the organizational workflow, then
the tester was not to perform such work. In addition to the 25 common set items, each partner
institution would also create bibliographic records for at least 25 additional items. Each partner
was to determine which items to include; however, they were to include a variety of formats
and types of resources they normally catalog including maps, kits, music scores, realia, etc.
(Library of Congress, 2009b)
Once bibliographic records were created, individual testers were to complete a survey
that provided additional data on the process that they had taken to create the record. The
survey included questions on the amount of time it took to create the cataloging record, ease
of using the cataloging standard, challenges, and other resources used to create the record. In
addition to submitting a survey for each RDA or AACR2 record created, each institution was to
"solicit feedback from their internal end users about the RDA records they create[d]" (Library of
Congress, 2009b, number 7).
On June 20, 2011, the national libraries released a report with a conclusion that RDA
would be adopted by the national libraries no sooner than January 2013. The adoption was
implemented soon after the proposed date – in March 2013. The committee did find that RDA
does meet some of the goals, but not all, and for this reason is delaying implementation until
57

revisions can be made to RDA and additional training materials are created. The coordinating
committee provided a list of tasks that needed to be addressed prior to the adoption. The
broad categories are listed below:
•

Rewrite the RDA instructions in clear, unambiguous, plain English

•

Define process for updating RDA in the online environment

•

Improve functionality of the RDA Toolkit

•

Develop full RDA record examples in MARC and other encoding schemas

•

Announce completion of the Registered RDA Element Sets and Vocabularies.

•

Ensure registry is well described and in synchronization with RDA rules

•

Demonstrate credible progress towards a replacement for MARC

•

Ensure and facilitate community involvement

•

Lead and coordinate RDA training

•

Solicit demonstrations of prototype input and discovery systems that use the RDA
element set (including relationships)

(U.S. RDA Test Coordinating Committee, 2011, p. 3-4)
Many of these recommendations were also uncovered as problems in a special issue of
Cataloging & Classification Quarterly (Hall-Ellis & Ellett, 2011).
Some of the testers of RDA for the national libraries have since authored articles
describing their experiences throughout the test in a special issue of Cataloging & Classification
Quarterly. In the special issue the authors discuss the testing, RDA rules, encoding standards,
RDA Toolkit, the education and training, their users, cataloger's judgment, and their
recommendations to implement RDA (Hall-Ellis & Ellett, 2011).
58

Many describe their organization's design and purpose in testing RDA. Most of the
participants included only a portion of their cataloging staff; however, at the University of
Chicago, the decision was for all cataloging professionals to participate. This decision was made
based on their initial discovery that RDA would at one time be adopted and if so, additional
training beyond the test would cause additional training costs and time (Cronin, 2011). The
University of Chicago not only submitted bibliographic records, but they also performed
authority work and submitted the authority records for the bibliographic items (Cronin, 2011);
however, this was not true of all test partners since this was not in their normal workflows
(Bloss, 2011). None of the authors expressed that they had second thoughts about participating
in the study; however, McCutcheon (2011) does question the purpose of the test. One of the
outcomes of the test was to determine the difference in time it takes to create an RDA record
as opposed to creating one using AACR2. McCutheon's feelings were that the focus of the test
should have concentrated on the usefulness of the record instead of the amount of time
required to create records.
Each of the articles in a 2011 special issue of Cataloging & Classification Quarterly
devoted to RDA National Test described the new rules found in RDA, and how they would affect
workflows and decisions. Bloss (2011) commented that the new rules lacked clarity and the
vocabulary was not easy to understand. The elimination of abbreviations was discussed by
several of the authors who also described how cataloging personnel did not like the additional
time it took to enter the full spellings; however, some were able to create shortcuts to insert
data (Cronin, 2011; Shieh, 2011). Format and the elimination of the GMD were discussed as
well. Bloss (2011) noted that there was a de-emphasis on format and type of materials that
59

allows for greater flexibility in the rules. However, the term "unmediated" used for the media
type is difficult to understand (Bloss, 2011; Cronin, 2011; McCutcheon, 2011). Cronin (2011)
described another major shift from AACR2 to RDA in the elimination of the "Rule of Three." The
new rules allow a cataloger to decide if they want to include all names in the statement of
responsibility and create additional entries for the individuals. Overall, Cronin (2011) agrees
with the move away from the "Rule of Three".
Several of the authors discussed the effectiveness of using an encoding schema to
create a bibliographic record. Of the RDA national test participants who submitted articles
published in this special issue, all but one used MARC as the encoding standard. Bloss (2011)
conveyed that the new rules were created for the MARC record creator and the rules are
friendly for this environment. The rules will allow for backward compatibility to AACR2 for
those institutions that choose to incorporate the new standard without converting records
created under AACR2 to RDA. It is for this reason that one does not see much difference
between RDA and AACR2 (McCutcheon, 2011). The one author that discussed testing while
using a non-MARC encoding schema advocated for better training documents needed for nonMARC standards, and that it was difficult to test RDA rules implementation in non-MARC
standards without these additional materials (Wacher et al., 2011).
The RDA Toolkit is the container through which the RDA rules are accessible. The toolkit
provides not only the rules, but also workflows and examples. It was only available
electronically for the test and many noted how difficult it was to navigate (Shieh, 2011).
However, some consulted the workflows that were added to the toolkit and mentioned how
extremely important these were to overcome the difficulties in navigation (Young & Bross,
60

2011). There is also consensus that more examples should be included to provide catalogers
with additional guidance (Biella & Lerner, 2011; McCutcheon, 2011).
Testing organizations found a variety of sources to consult when educating and training
individual participants. They used the LC webinars, consulted Library of Congress Policy
Statements, created local documentation, and consulted training materials developed by others
outside of the formal test (McCutcheon, 2011; Shieh, 2011). Bloss (2011) and Cronin (2011)
agree that catalogers need to have a good understanding of FRBR and less so of FRAD in order
to appropriately apply the new rules. Bloss (2011), who worked with library and information
science graduate students, noted that there is a need to continue to teach AACR2 in graduate
programs, but also a need to expose students to RDA and the use of the RDA Toolkit.
Cronin (2011) states that applying cataloger’s judgment in RDA is greater than it had
been when utilizing AACR2. Most of the authors in this special issue briefly mentioned or
referred to cataloger's judgment directly or indirectly (Biella & Lerner, 2011). This is mostly due
to the number of options RDA provides to a cataloger (Cronin, 2011). In one article, satisficing
behavior was noted as catalogers searched for a decision that was "good - enough" (Cronin,
2011, p. 637).
There were a number of authors that described how the new rules would affect users.
The elimination of many abbreviations will aid users in understanding the data in a more
meaningful way (McCutcheon, 2011)and creating additional access points by eliminating the
"Rule of Three" will allow users to find related items (Cronin, 2011). However, Biella & Lerner
(2011) state that RDA does not meet the needs of an "international" community as
demonstrated through the cataloging of works written in Hebrew. They contend RDA is still for
61

an English speaking cataloging world and that RDA does not meet the needs of other languages,
specifically those that are recorded in a different script. RDA is now available in Chinese,
German, French, Korean, and additional translations in other languages are currently under
development (Luo, Zhoa & Qi, 2014).
Although some remarked that creating a cataloging record with RDA took more time
(McCutcheon, 2011; Young & Bross, 2011), the thought was that in time, the speed would be
increased through practice (McCutcheon, 2011) or with shortcuts (Shieh, 2011). McCutcheon
(2011) also argued that RDA relied too much on ISBD punctuation rules. The authors in the
special issue agreed that the testing had been able to provide them with greater information to
assist their organization in determining how they may adopt RDA in the future. Not every
author stated whether they supported the adoption of RDA; however, several did state that
they were in favor of adoption with modifications (Bloss, 2011; Cronin, 2011). This was the
same recommendation made by the U.S. RDA Test Coordinating Committee.

Implementation of RDA Worldwide
Up to this point the review of the literature on RDA has focused on the United States;
however, it is important to understand that RDA is an international standard, and other
countries have been involved in a variety of activities related to the implementation of RDA.
The RDA Toolkit is a joint publishing project of the American Library Association, the Canadian
Library Association, and the Chartered Institute of Library and Information Professionals
(United Kingdom) (RDA Toolkit, 2015).

62

Although not a national library, the decisions of LC have had a profound effect on the
cataloging standards worldwide. For example, the National Library of Israel decided to adopt
RDA as the new standard for several reasons, but Goldsmith and Adler (2014) stated that their
previous practice of following American cataloging practices by accepting copy cataloging from
LC played a major role in the decision to adopt RDA.
International discussions have started to emerge in the literature regarding RDA,
including a special issue published in Cataloging & Classification Quarterly in 2014. The articles
presented in journals have provided a broader context in the challenges and successes that
various countries have encountered as they are in the mist of implementation of RDA at various
stages.
Several themes have emerged throughout the articles relating to the challenges such as
language and translation barriers, costs of implementation, moving away from old practices,
legacy data, and the need for additional training. In China, Luo, Zhao & Qi (2014) discussed the
challenges of overcoming the language barrier of RDA and how it can be effectively translated
into Chinese. The challenge of translating into other languages other than English was also
confirmed by librarians in Latvia (Goldberga, Kreislere, Sauka, Stürmane &Virbule, 2014).
Cost was another barrier that has been mentioned by the global cataloging community.
The steep costs associated with a change in standards included training, subscription costs for
RDA Toolkit, and the cost of integrating legacy data with data created in RDA (Luo, et al., 2014).
Such costs will have an impact on library organizations because they may not have direct access
to the RDA Toolkit (Acedera, 2014), the ability to be ready for what seems to be eminent, a

63

post-MARC environment, and delayed training for more rural libraries (Luo, et. al., 2014; Choi,
Yusof &Ibrahim, 2014).
The topic of training is not unique to the U.S. as it was the strongest theme throughout
literature that discussed RDA with a global perspective. In Iran, a survey was conducted to
determine the perceptions of cataloger knowledge of RDA. It was found that catalogers had a
higher level of knowledge in a self-assessment, but after more detailed questions it was
determined that they did not have as much familiarity as first thought (Pazooki, Zeinolabedini &
Arastoopoor, 2014). For this reason, it was determined that greater training opportunities were
needed. In China (Luo, et al., 2014), many libraries have held onto local traditions of cataloging
and the examples are not clear, so training will be necessary to ensure the proper adoption of
the new standard. The most significant statement regarding an implied need for training is
based on a statement about cataloger's judgment in Singapore by Choi, Yusof, and Ibrahim.
They state,
Staff noticed that with RDA, there were many more instances requiring a cataloger’s
judgment, alternatives, and optional additions/omissions. It takes time for staff to
practice applying a cataloger’s judgment and to familiarize themselves with it. It is not
easy to make decisions to embrace RDA and at the same time, fulfill NLB’s internal
cataloging requirements. (2014, p. 619)
This statement has implications for the training of those that employ cataloger’s judgment so
that fidelity to the rules can be maintained.
With these challenges, come opportunities. As an international community, catalogers
have embraced the testing of RDA (Luo, 2014; Behrens, Frodl, & Polak-Bennemann, 2014; Choi,
et al., 2014). In Canada, where most training occurred face-to-face prior to adoption, webinars

64

have brought together a community of catalogers that, in the past, were very isolated (Cross,
Andrews, Grover, Oliver, and Riva, 2014).

History of Non-Print Materials
Non-print items began to appear in library catalogs around 1800 with maps, then still
images, sound recordings, and later followed by motion pictures. It was not until the Post
World War II Era that an influx of a variety of audio-visual materials started to appear in
libraries in mass quantities (Horn, 1955; Weihs, 2011). The thrust of this started to occur in the
1940’s and 1950’s as school libraries began collecting various audio-visual materials (e.g.,
filmstrips, slides, records, etc.) (Intner, 2006). Universities began creating special collections of
non-print materials around this same time (Horn, 1955). By the mid-1960's, the library
community had difficulty cataloging these materials. The standards did not fully address the
needs of the librarian for these newly introduced types of formats. The lack of audio-visual
standards stilted efforts to share records electronically to ease the workload on librarians
(DAVI, 1968). Weihs (2011) confirmed this in the recollection of her work from the early 1960's
through the beginning of the 21st century.
In 1957, the Soviet Union launched Sputnik into space. This single act caused the
American government to increase federal funding in the areas of science, math, and foreign
languages. This funding increased the collections of existing school libraries and created new
ones where they did not exist before causing an influx of materials to be ordered and cataloged
including audio-visual materials (Weihs, 2011). This great increase in resources for school

65

libraries provided additional need for a change in cataloging rules for both print and non-print
materials.
In 1968, a new standard was released for the cataloging of audio-visual materials. The
National Education Association’s Department of Audiovisual Instruction (DAVI) published
Standards for Cataloging, Coding and Scheduling Education Media. These standards provided
school librarians with rules to catalog audio-visual materials for the description, access, and
retrieval of non-print materials. However, the broader context of the need for standards was as
stated in the standards.
In the larger centers, computer technology is being used for both cataloging and
scheduling. The possibility of sharing, by means of the computer, a national pool of
information about materials and the potential exchange of information among local,
regional, and national resource centers are especially intriguing. The use of computer
technology for these purposes, however, requires the development of national accepted
guidelines or standards for catalog information and computer in-put. (DAVI, 1968, iii)
DAVI was instrumental in providing a new set of standards to the school library community.
However, the broader library community continued to struggle with the cataloging of non-print
materials. The DAVI standards were created outside of any guidance from AACR and only briefly
mention AACR in a footnote (Weihs, 1972).
According to Weihs (2011), AACR did not provide sufficient rules in describing non-print
resources in Part III. AACR lacked guidance in describing various formats of materials, and she
concludes that there was not enough time and energy devoted to the writing of Part III since it
was completed in only two weeks. This lack of attention to non-print materials in AACR left the
cataloging community at odds in describing non-print resources, which resulted in the
formation of committees to come up with solutions.

66

Up until the late 1960's, most libraries segregated their non-print catalogs from print.
For many libraries, they kept non-print materials from the hands of the typical library user and
only allowed access to a chosen few. As more non-print materials became available, librarians
began to allow these materials to be circulated to a larger number of users, which led to the
idea of combining catalogs or "to create an omni-catalog" that would integrate typical print
collections with non-print (Weihs, 1972, p. 307). Various committees met after the release of
AACR tasked with finding solutions. Within time, it was decided that a revision to AACR was
necessary to embody all of the changes that were recommended for the cataloging of non-print
materials.
Rose and Duncan (1976) mentioned the cataloging community was preparing yet again
for a change in cataloging standards. The AACR2, based on the International Standard
Bibliographic Description (ISBD) was under development and would be released and adopted in
1978. Once AACR2 was released, a few libraries began implementing the new standards for
their organizations; however, it took LC until 1981 to accept and embrace the new cataloging
rules. A part of the delay in adoption from the Library of Congress was how to mitigate the
changes in cataloging for non-print materials (Weihs, 2011). Others however, had decided to
move forward knowing that AACR2 would offer additional guidance for the cataloging of print
and non-print materials.
AACR2 did not solve all of the problems in cataloging non-print items. As new formats
emerged and electronic resources were introduced into library catalogs, it was more apparent
that additional changes were needed. Many of the revisions of AACR2 were in part an attempt
to resolve long standing issues in the treatment of non-print materials. However, problems with
67

the general media designations, the "Rule of Three" in the assigning of the main entry, and
media form subdivisions became evident, as more resources were available electronically
(Weihs, 2011). Non-print materials often are of mixed media, such as describing an MP3 file
that is both an electronic resource and a sound recording. Another issue with describing nonprint materials is who is responsible for the intellectual content of the work. More than one
individual creates a motion picture; however, the "Rule of Three" restricts listing all of those
responsible for creating the intellectual content. This leads to difficulty in defining the main
entry or main access point for non-print materials. Some prefer a title entry and others try to
determine who has the greatest influence in creating the intellectual work. Finally, librarians
reported that users need additional ways to search for certain types of media and eliminate
others (Wiehs, 2011). These challenges are factors that have led the cataloging community to
declare that change was needed in the way they describe emerging types of materials available
in catalogs.
Cataloging of Electronic Resources
Electronic resources first appeared in OCLC's WorldCat in 1975 with only one resource
type. By the end of 1979, only ten additional electronic resources appeared. From 1980 - 1989,
this collection grew by almost 30,000 records in WorldCat. In the next ten years, the number of
these types of resources grew by almost 120,000 items. The exponential growth in the number
of electronic resources continued in 2000s and 2010s. For example, from 2000 - 2005, there
was an estimated additional 1,000,000 electronic resources added to WorldCat (Lavoie,
Connaway, & O'Neill, 2007). These numbers provide a startling picture as to the acceptance of
including electronic resources into library collections. Cataloging of electronic resources began
68

primarily with cataloging computer software in the late 1960s (Weiss, 2003), followed by
computer files in the 1970s (Weiss, 2003), integrating resources and E-Serials in the 1990s
(Zhao, 2006), and finally eBooks (Bothmann, 2004; Wu & Mitchell, 2010).
According to Weiss (2003), the library community began creating rules for the
cataloging of electronic resources in the early 1970s, and these rules first appeared in AACR2
released in 1978. This first set of international cataloging rules for computer media give the
general material designation (GMD) machine-readable data files (MRDF). The new GMD
included any type of media that included data that could be read by a computer or similar type
of machine. It did not specifically include software programs. The chief source of information
for these types of materials would be what is found within the internal file without consultation
of any outside labels or containers. The physical carrier for the MRDF was not considered as a
part of the bibliographic record since the rules concentrated on the intellectual work and not
on the type of media itself. A new bibliographic record would only be created if there was a
change to the intellectual content. Catalogers recorded the technical requirements needed to
read or view the MRDF as a note in the area called mode of use.
As microcomputers began to emerge, libraries began offering software to their users
and this created additional challenges. New standards to deal with new types of media were
presented to the cataloging community that was outside of AACR2. One set of standards
created by Intner (1985) were used to describe computer software. These standards relied
more on the physical format of the material and allowed the cataloger to use any labels or
containers along with the internal information to determine the chief source of information.
"[I]n the note area, 'mode of access' was changed to 'system requirements' suggesting the
69

software was not going to be transferred from one carrier or operating system to another"
(Weiss, 2003, p. 173). This change from AACR2 placed additional importance on the form of
material than it did previously.
Due to the changes in cataloging of electronic resources, there was a call for a revision
of AACR2. In 1988, the second edition of Anglo American Cataloging Rules underwent a revision
(AACR2r) and within it provided a broader definition of electronic resources as the GMD was
changed to "computer file" which allowed the description for both computer files and software.
The chief source of information for computer files still remained the internal source; however,
this would now be termed as the "title screen" and a note of the source would be provided as
well, which was consistent with the cataloging of other types of materials (Weiss, 2003). Other
changes in AACR2r offered greater importance of format and carrier in the description.
Previously, a change in format did not constitute a new bibliographic record unless there was a
significant change to the intellectual content. In the revised set of rules, a change to the format
or type of operating system used to read the information was considered to be a new
manifestation and required a new record (Weiss, 2003).
As computer software evolved, the need for addition revisions to the standards were
needed to describe new types of electronic resources such as interactive media (video discs)
and the inclusion of links to the World Wide Web in the MARC record. These links offered
remote access to information, and created additional changes. In 1997, the ISBD (ER) was
created to provide additional guidance and a new GMD, "electronic resources" (Weiss, 2003).
The ISBD (ER) led to amendments to the AACR2r which were published in 2002. This set
of amendments cast a wider net over all types of electronic media whether it were accessed
70

directly, remotely, or through interactive media, which allowed for one set of rules to describe
all such resources. There was also a change from the chief source of information to the
resource itself, which includes the internal content as well as the physical resource itself and
any labels (Weiss, 2003).
Beyond the discussion of cataloging standards, the literature also reflects challenges of
whether to catalog certain types of web resources, cooperative cataloging of electronic
resources, and the managing the workflows of cataloging eBooks.
Porter and Bayard (1999) struggled with including web sites in the library catalog. Prior
to inclusion, subject librarians at the University of Notre Dame created web pages as
pathfinders to websites that users could consult for research. This caused users to not only find
resources in the library catalog, but also have to find the subject-specific web pages for their
research discipline. To solve this problem the authors sought to investigate workflows and
determine the amount of time to catalog web sites and to create policies and guidelines to
determine the selection of web sites to be included in the library catalog. They found that "web
sites are inherently problematic for libraries because there are no standards of any kind to
guide the creation of these resources" (p. 391). Libraries are determined to provide barrier free
access to resources, including various free web resources such as online technical reports,
websites, eBooks, and databases for their users. However, catalogers struggle with the amount
of time it takes to catalog these types of materials as well as the issue that URLs are dynamic
(Brown & Meagher, 2008). Benerjee (1998) provided three reasons why cataloging electronic
resources are difficult: (1) They are inherently more difficult than print items since they cannot
be held and inspected as other types of resources; (2) Electronic resources are unstable as they
71

are easily changed, edited, or modified without notice; (3) Print and digital resources relate
differently to the library catalog. Benerjee (1998) described this through the way users search
and browse for materials. With printed material, users will find a resource in the catalog and
then browse the shelves for similar items; however, with electronic resources, this type of
browsing is limited to what is found in the catalog. In Brown and Meagher's (2008) study, the
researchers found that the enhanced web content to the library catalog was worth the time
invested, and there is a need to expand the cataloging of these materials in spite of the
dynamic nature of URLs and maintaining these web links in the library catalog.
Managing the workflow of eBooks and other online resources cooperatively has been
described in the literature as a way to combine resources to reduce workflows and increase
productivity (Chen et al., 2004; Preston, 2011). OhioLINK and the Illinois Library Computer
Systems Organization (ILCSO) both work to provide services to member libraries in providing
users greater access to materials. OhioLINK is a statewide organization that provides a
consortium catalog for 88 college and university libraries and the state library in Ohio.
Together, they share the workload of creating bibliographic records for electronic resources.
Through sharing this work, each member library benefits from the expertise of the other
members. Together, they have designed policies to allow continuity between the member
cataloging agencies. Through their work, they make six suggestions in how others could benefit
from a similar partnership.
1. Communicate expectations for member libraries and the level of work to be performed
by each entity

72

2. Designate at least one expert that will explore various bibliographic tools and then train
others on the use of the preferred tool
3. Institute cataloging standards that are flexible enough to meet the needs of member
libraries and their users
4. Explore methods to obtain records to copy to increase efficiency
5. Be open to change, and accept new workflows as they present themselves that allow
for increased efficiency
6. Record processes and procedures to allow for greater participation in the future
(Preston, 2011).
The ILCSO consortium is similar to that of OhioLINK in that it provides a union catalog to
colleges and universities and the state library although they also include high school and other
non-academic libraries. Naun and Braxton (2005), discuss their participation in the cooperative
cataloging process in their state. Much of their work aligns to that of OhioLINK's, but they
stressed the need to make sure the MARC records created for the catalog meet the needs of
the users and the member libraries. In 2003, ILCSO created a task force to investigate the best
way to increase access to e-Journals and eBooks through the library catalog. What they found
was that "they are dealing with a volatile set of unstable resources which change names,
contents, providers, and URLs with alarming frequency and thereby require repeated revisions"
(Chen et al., 2004, p. 174). For this reason, many libraries do not include all possible resources
into their library catalog. This was also to be found true by Parks and Wang (2005). The reality
of electronic resources is that they are difficult to catalog, but there is value in providing them

73

to users. The use of cooperative consortia cataloging is one solution to share the work and to
reduce costs.
The final major theme in the literature relating to the cataloging of electronic resources
was the discussion of managing and cataloging eBooks. Although the first eBook emerged in
1971, they were not readily adopted by libraries until the end of the 1990s when eBook readers
began to appear on the market (Wu & Mitchell, 2010). EBooks may be acquired in a number of
ways such as purchased or sold on a subscription basis. Some may allow unlimited
simultaneous users or be restricted to one user per copy owned.
Although the literature points to eBook use in school and public libraries, most of the
literature on cataloging eBooks is presented by academic libraries. The earliest work describes
the practice of how one should catalog eBooks (Bothmann, 2004). Since then, libraries have
struggled to provide access to this new type of material (Simpson, Lungren, & Barr, 2007) which
involved uploading large batches of eBook MARC records (Sanchez et al., 2006). Dinkelman and
Stacy-Bates (2007) state that users need multiple access points to find eBook collections. They
argue that there needs to be a link directing users to web pages that provide a list of eBook
offerings or to search the catalog limited to eBooks. Although their perspective is different from
many cataloging professionals, they do discuss the convenience to the user and how libraries
need to provide additional means to obtain the materials users need. Simpson et al. (2007)
provide a different perspective. They argue that eBooks should be linked from the print,
bibliographic record. This would allow users to discover any print materials and electronic
resources for the same title at once. It would also reduce the number of bibliographic records in
the catalog and provide a more FRBR-like feel to the catalog.
74

One of the largest challenges for cataloging departments is keeping up with the
acquisition of new electronic resources. Sanchez et al. (2006) describe the challenges in batch
loading a large number of NetLibrary eBook records. They were well aware of the issues other
libraries have had in working with these vendor-supplied records. Some of the problems
included incorrect subject headings, duplicate records, and problems with authority control.
In 2009, the Program for Cooperative Cataloging (PCC) came out with a new standard
that called for a provider-neutral record for eBooks. This standard was to simplify the
cataloging of eBooks by providing a single record for a resource regardless of the provider. This
standard does seem to help; however, it does complicate things when a library owns the same
title offered through several different vendors. The solution to this problem has been either to
add another 856 MARC tag or to create a second bibliographic record (Wu & Mitchell, 2010).
Each call for a change to AACR2 has resulted in a modification in how catalogers
describe electronic resources. The inclusion of electronic continuing resources and eBooks has
only provided additional challenges to cataloging. It is partially due to the changing nature of
electronic resources and that AACR2r 2002 continued to be based on the printed card catalog
(McCracken, 2007). McCracken (2007) states, “One of the major goals of RDA is to simplify the
code and make it more consistent/less redundant. RDA will include guidelines that describe
analog and digital materials...” (p. 271). It is for this reason describing electronic resources is
expected to be made easier through the adoption of RDA.
Since the development and release of RDA, the topic of how to handle the workflow of
electronic resources has emerged. Topics include methods to provide access to electronic
resources, digital rights management, and coordination of acquisitions and cataloging. Libraries
75

have had a difficult time determining how to represent electronic resources for their users.
Sapon-White (2014) states that some make content available through the online public access
catalogs (OPACs) while others chose to provide users with content options through various
webpages not linked or represented in the OPAC including, but not limited to, topical lists and
bookshelves hosted by third party vendors (Shorten, 2006). There are benefits and challenges
for both of these models. For example, utilizing the OPAC as the one place where all electronic
resources are accessed could make it easier for users to access just one website. However,
since many electronic resources are purchased as packages, this requires batch uploading of
MARC records, which may include uploading poor vendor supplied MARC records (SaponWhite, 2014). Utilizing A-Z lists, topical lists of links, or a vendor bookshelf allow users to narrow
their search to discover resources by topic or collection; however, often times, it does require
users to spend more time searching individual collections (Hinton, 2002).
There are several reasons why libraries have not included MARC records for all of the
online content they purchase or subscribe to in the library catalog. The most prevalent reasons
are because the record quality varies from vendor to vendor and some of the MARC records
received require more editing than time permits (Sapon-White, 2014); there are challenges to
the batch loading of MARC records into the ILS (Martin, Dzierba, Fields, & Roe, 2011); and
finding solutions in how to remove access through the OPAC when licensing to the content
expires (Chen, Colgan, Greene, Lowe, & Winke, 2004). Boydston and Leysen have recognized
these challenges and confirmed through their research; libraries are opting to use non-MARC
local collections to manage electronic resources in order to find more efficient workflows
(2014).
76

Although workflows were discussed in the literature, decisions about which specific
MARC fields should be represented in a bibliographic record for electronic resources has been
ignored.

Conclusion
The literature states that providing access to materials for their users is an important
function of a library. Principles, rules, and guidelines provide catalogers the tools necessary to
be able to do this effectively. However, it takes education and experience for catalogers to
determine the level of cataloger’s judgment they will exercise throughout this process.
Bounded rationality helps to explain this phenomena; catalogers work within time and
cognitive constraints in order to engage in the decision-making process.
The inclusion of electronic resources in library catalogs has made the decision-making
process even more difficult. The literature states that AACR2 did not provide enough guidance
to catalogers in describing these types of entities, and this is one of the reasons for the creation
of RDA. The RDA National Test provided the library community with an opportunity to evaluate
the new set of rules. The outcome of the test was that the national libraries adopted RDA in
March 2013.
The RDA National test provides a rich set of data to study, and it was the original
intention of the national libraries to share this data for further research. The LIS literature does
indicate research on cataloging quality; however, the literature is almost void of any research
on the concept of cataloger’s judgment.

77

CHAPTER 3
METHODOLOGY
Introduction
This chapter defines and explains the methodology used in this exploratory study. The
study utilized both qualitative and quantitative methods. Mixed method studies have received
a wider acceptance in recent years in LIS and other social science fields (Fidel, 2008). Mixed
method research is “research in which the investigator collects and analyzes data, integrates
the findings, and draws inferences using both qualitative and quantitative approaches or
methods in a single study or program of inquiry” (Tashakkori & Creswell, 2007, p. 4). One of the
reasons researchers use mixed method research is for the triangulation of data to ensure
accuracy and validity within a study that utilizes various data collection methods (Fidel, 2008).
This study employed the use of descriptive statistics (mean, mode, Chi-square, etc.), content
analysis, and regression analysis to analyze the data.

Sample
The study included the analysis of the electronic resource MARC records created using
RDA and the survey responses for those items from the common original cataloging records
prepared by the formal testers involved in the RDA National Test. The purpose for limiting this
sample to only electronic resources is based on one of the goals of RDA, to provide rules that
will be able to describe those resources found in the digital environment (McCracken, 2007).
Therefore, this study focused on all of the electronic resource items cataloged as part of the
RDA National Test (44% of 25 test items). These 11 electronic resources were described by 329
78

or 41.6% of the 791 total common original set of records submitted for the formal RDA National
Test (see Table 3.1). Of the 329 possible records, 112 were eliminated from the study due to
missing common original set (COS) and the record creator profile (RCP) surveys. To ensure the
most comprehensive data analysis, as well as meaningful comparisons for the 11 records
studied, only those records for which a COS survey and RCP survey could be traced back to the
bibliographic record were included in the study. This resulted in 217 viable records that were
further analyzed. Table 3.1 illustrates the sample of items reviewed and selected in this study.
Table 3.1
Record Titles and Counts Used in the Study
Item

Title

Material Type

H
I
J

Americans with Disabilities
Benjamin Button
Reconciling Scientific Approaches
for Organic Farming Research
Criterion
Utley
San Diego
Acupuncture
Our Science – Research Directory
NCJRS
UN
Proquest

E-Monograph
E-Monograph
E-Monograph

M
N
O
Q
V
W
X
Y
Total

E-Serial
E-Serial
E-Serial
Streaming Video
Integrating E-Resource
Integrating E-Resource
Integrating E-Resource
Integrating E-Resource

Total Number
Created
35
34
31

Number Studied

28
29
29
30
27
30
29
27
329

17
18
21
22
19
18
19
19
217

20
22
22

Research Approach/Design
A mixed method study was conducted to answer the research questions as stated in
Chapter 1 and below. The qualitative and quantitative research methods were used to analyze
an existing data source from the Library of Congress (LC), National Medical Library (NML) and

79

the National Agricultural Library (NAL). This data was collected through their formal test of a
new international cataloging standard, Resource Description and Access (RDA).
The RDA National test was described in both Chapters 1 and 2, and was used by the
three national libraries to determine if they should adopt this new standard. The test involved
23 other partners (26 testing organizations in total) tasked to create bibliographic records and
complete a survey after they had completed a bibliographic record. This resulted in the creation
of 10,570 records, and of those, 791 were RDA records for the 25 common set of items each
organization was to create. This study analyzed a sample of the data collected by the RDA Test
Coordinating Committee. The sample being evaluated to answer the research questions are the
RDA common set of electronic resource records created using MARC.
Prior to beginning the study, the MARC records for one of the non-electronic resource
items (Item P) in the RDA National Test was used as a pilot to test and refine the research
procedure. The details of the examination of Item P, a DVD video of March of the Penguins, are
presented later in this chapter.

Research Questions and Methods to Analyze
The following research questions were used to guide the study:
RQ1 How did catalogers participating in the Resource Description and Access (RDA) National
Test exercise cataloger’s judgment as they created RDA-based MARC records for
electronic resources?
1a: What are the similarities and differences of the records?

80

1b: To what extent can the differences in text entered in the records be explained by
differences in characteristics of the catalogers (e.g., level of position, experience,
prior course work and/or training, etc.)?
RQ2 How can cataloger’s judgment be explained through the lens of Bounded Rationality?
2a: How can cataloger’s judgment be predicted using the constructs of bounded
rationality?
To answer the first question and the two sub-questions, a variety of methods was used
to analyze the data. Specifically, the analyses completed for each sub-question were then used
to answer the broader main question. Sub question 1a was answered by reviewing the
submitted record data and recording observations of the actual text. Descriptive statistics were
collected on the frequency of text presence and absence.
Sub-question 1b required conducting a content analysis using the MARC records and the
RDA rules as found in the RDA Toolkit during the time of the test. Chi-square test for association
was used to determine if the categorical groups (job title, years of experience in cataloging and
years of experience cataloging electronic resources, minutes spent creating MARC records, and
minutes spent consulting others) were statistically independent. Cramer’s Ѵ was used to
determine the magnitude of the associations. Descriptive statistics were used to describe the
MARC records and categorical groups to assist in answering the sub question.
A binomial logistic regression was used to find the relationship between a dependent
variable (e.g., the presence of text in the 100 $a MARC field) and the categorical groups. The
analysis was completed by analyzing the constraints of cognitive limits (job titles as were selfreported on the RCP survey), as well as the time spent in the creation of MARC records.
81

Data Collection
This study included the analysis of the RDA COS electronic resource MARC records and
survey responses (Appendix B) for the records created by the test participants. The Library of
Congress (LC) provided the data for this study. The MARC records are available to anyone
through the LC website (http://www.loc.gov/catdir/cpso/RDAtest/rdatestrecords.html), and
the participant survey responses were acquired through a request to LC. As a participant in the
National Test, I knew who to contact and where the data records would be archived. The
request stated that the survey data would be used in a study on cataloger’s judgment. Susan
Morris from LC provided an original copy of the Access database at the ALA Annual Conference
in June 2011. LC stated the data in the Access database was downloaded by the survey
instrument and the data file had not been edited in any way.
Once the data was received, a small sample of bibliographic records and accompanying
RCP surveys for Item P, a non-electronic audiovisual resource, COS item, was reviewed. Item P
was not one of the items that was to be considered in the formal study, but was selected to
provide additional information regarding the amount and breadth of data that was available to
study. A review of Item P revealed that only six (20.6%) of the surveys were submitted for the
29 RDA MARC records created for this one item. The COS and RCP surveys were extremely
important since they provided information to determine the categorical groups of:
•

Job title

•

Years of experience in cataloging

•

Years of experience cataloging the resource type

•

Number of minutes spent creating the cataloging record
82

•

Number of minutes spent consulting others (this would be the amount of time the
participant consulted others about creating the MARC record)
Further discussion of the preliminary findings for Item P will be discussed later in this

chapter.
Data Analysis
Since the data collection is based on existing data, no additional data collection was
needed. A variety of statistical analysis techniques were used to extract further information
about the phenomena of cataloger’s judgment. A mixed methods approach including content
analysis, descriptive statistics, chi-square test, and logistic regression assisted in bringing
meaning to the data.
Prior to analyzing the MARC records, a free MARC editing utility called MarcEdit was
used to extract the MARC data into an Excel spreadsheet. The steps used to complete the task
are listed below:
1. Download MARC Records from
http://www.loc.gov/catdir/cpso/RDAtest/rdatestrecords.html
2. Unzip File and change file type from .bib to .mrc
3. Import into Marc-Edit 5.7 and perform the following functions
a. Select the records to export into a new file for the item needing analysis
b. Go to File  Select Individual Title to Make...  Import File  Select
appropriate titles  Click Retain Clicked Items  Export Selected
c. Save the File as ItemName.mrc
d. Run Field Count Report
83

e. Create a .txt file to import into MARC Tools Using the following
i. #NORMAL:#false
ii. #MARC:#false
iii. 000
iv. 001
v. 003
vi. 100
vii. Add all other tags in the field count report
viii. Name this file ItemName_ImportSettings.txt
4. Create a spreadsheet of all MARC fields in the selected records by performing the
following functions in MARC Tools component of MarcEdit
a. Tools  Batch Process Records – Export Tab Delimited  Choose your source
information (ItemName.mrc) and destination file name (ItemName.csv)  Click
Next  Click settings  Load Settings and import ItemName_ImportSettings.txt
 Click MARC  Export
5. Open .csv spreadsheet in Excel and view data
It is important to note that for repeatable MARC fields and subfields, the text will appear in only
one cell with a semicolon used to differentiate between repeated field information. Therefore if
there were two 500 $a notes, they would appear in the same spreadsheet cell as “note 1;note
2” (e.g., “Serial No. 111-95.”;Viewed November 4,2010.).
Once the MARC data was extracted and placed into Excel, and non-electronic resource
records were removed, MARC records were matched with the surveys submitted for the
84

specific records. In each MARC record the cataloger included information as to who created the
record. The VLOOKUP function in Excel, which looks up a value in one column of an Excel
spreadsheet and populates it in another, was used to match the MARC record data with COS
and RCP survey data. Five data points from these two surveys were pulled into the worksheet
with the MARC data.
1. Job title (RCP)
2. Years of experience in cataloging (RCP)
3. Years of experience cataloging the resource type (COS)
4. Number of minutes spent creating the cataloging record (COS)
5. Number of minutes spent consulting others (COS)
Once all of the data was placed in one worksheet, a frequency analysis of the fields was
conducted to determine which fields of the electronic resource records were used most often.
The basis of determination of the fields included in this study was based on this list. All fields
prior to the 100 field were removed from consideration since most of them are system
supplied or were used to identify which cataloger created the record. Therefore, they were not
part of the descriptive cataloging that the test was assessing. The two lists of fields were also
compared to two different studies. The first was the MARC Content Designation Utilization
study that investigated frequently used fields in the OCLC catalog (Eklund, Miksa, Moen,
Snyder, and Polyakov, 2009). The second was a study on the implications on cataloging quality
in which participants ranked various fields that they determined to be important when
evaluating the quality of a MARC record (Schultz-Jones, Snow, Miksa, & Hasenyager, 2012).
Both of these studies were consulted to support the final list of fields to be included in this
85

investigation. The fields that were studied in this test were 100, 110, 130, 245, 246, 260, 300,
336, 337, 338, 500, 538, 588, 700, 710 and 856. Table 3.2 provides the MARC field number and
the description of the type of information included in each of these fields.
Table 3.2
Marc Fields and the Type of Data Included in the Field
MARC
Field
100
110
130
245
246
260
300
336
337
338
500
538
588
700
710
856

Description
Personal Name (Main Entry)
Corporate Name (Main Entry)
Uniform Title (Main Entry)
Title Statement
Varying Form of title
Publication, Distribution, etc. (Imprint)
Physical Description
Content Type
Media Type
Carrier Type
General Note
System Details Note
Source of Description Note
Personal Name (Added Entry)
Corporate Name (Added Entry)
Electronic Location & Access
Since the national test concentrated on descriptive analysis, the subject fields, 6xx, were

not included in this study.
Due to the richness of the data, and this being an exploratory study, the only fields in
which content analysis was performed are the note fields (500, 538, and 588). These fields were
selected in order to determine the types of notes that were represented by the data entered by
the test participants.

86

A content analysis of the General Note MARC field 500 $a was performed. Content
analysis is “a research technique for making replicable and valid inferences from texts (or other
meaningful matter) to the contexts of their use” (Krippendorff, 2004, p. 18) and is used to
identify a person’s perspective on a topic or to “characterize the communications of [a] group”
(White & Marsh, 2006 p. 29). It requires the creation of a codebook for the researcher to
indicate the themes of the data being analyzed. Once the codebook was created, and the
researcher had coded the 500 $a field, the reliability of the coding was tested. This was
completed through a process termed “intercoder reliability.” Intercoder reliability is a process
in which at least one other coder performs the same test to determine if there is agreement.
According to Neuendorf (2002), this process is essential to ensure reliability and a Cohen’s
kappa of .80 or greater is recommended in order to get an acceptable reliability of 80% or
higher.
For this study one other coder was used to validate the coded data for the General Note
500 $a field. The person performing this work is a faculty at a university in Illinois who’s
teaching and research expertise is cataloging. The only portion of the data that was analyzed
using content analysis was 500 $a; 100% of the data was coded by both the investigator and the
outside expert. The value of Cohen’s kappa was .911 which exceeds the minimum acceptable
level of agreement.
Once the coding and reliability was confirmed, Chi-square and regression testing were
performed using the statistical software program SPSS. In some cases, the assumptions for the
Chi-square test were not met due to the small sample size. In these cases a Fisher’s Exact Test
was performed to determine if the categorical variables were statistically independent. These
87

tests assisted in answering Q1. According to Kotrlik, Williams, and Jabor (2011), while the pvalue is not the only way to demonstrate significance, the Cramer’s Ѵ is able to “ judge the
magnitude of the differences between or among groups, which increases the researcher’s
capability to compare current research results to previous research and judge the practical
significance of the results derived” (p. 134).
To determine a significant result for Chi-square or Fisher’s Exact the p-value must equal
or be less than .050 (Krippendorff, 2007). To determine a Cramer’s Ѵ effect size, values
between .00 and under .10 have a negligible association, .10 and under .20 have a weak
association, .20 and under .40 have a moderate association, .40 and under .60 have a relatively
strong association, .60 and under .80 have a strong association, and .80 and under 1.00 have a
very strong association (Rea & Parker, 1992).
Once the associations had been tested, a binomial logistic regression, or logistic
regression for brevity, was performed. Originally, it was intended that a series of T-tests, multifactor ANOVAs, and factor analysis would be conducted; however, due to the sample size the
regression test was selected instead.
The logistic regression is a predictive test to determine if there is any probability that an
occurrence will appear in a certain category. For the purpose of this test, each group would be
tested to predict the probability that a cataloger would enter text or leave the field empty. This
type of analysis provides a prediction model to determine if bounded rationality can apply to
this test.
Finally, descriptive statistics was used to describe how the MARC records created by
various individuals compare to each other, in addition to the inferential statistics methods
88

performed. The occurrences of specific MARC fields and subfields as submitted by the testers
were computed and then applied to the descriptive statistical analysis. The descriptive analysis
provided a way to describe the data while the inferential statistics allowed for investigating the
content of the data that was entered into the MARC record.
With the triangulation of data utilizing various research methods, it was possible to
answer the research questions as stated above.

Findings for Item P
Prior to applying the methodology outlined above, Item P, a DVD of the March of the
Penguins, was analyzed to better understand the data to be studied and to test the proposed
methodology. The main purpose was to determine if the data was rich enough to conduct a
study on cataloger’s judgment.
A total of 29 MARC records were submitted for Item P. The MARC record information
revealed many differences among the catalogers in their cataloging of the descriptive elements
of Item P. Table 3.3 demonstrates the variances found in the MARC field and subfield counts for
Item P. Each cataloger included a 245 field and 300 field; however, there was a difference in the
subfields ($) they chose to include. Seven catalogers did not include a $c, Statement of
Responsibility, and only one person included a $b, Remainder of Title, for the 245 field. In the
260 field, one cataloger did not enter any information for any subfield. A further look at the
260, Publishing Information, revealed that some catalogers entered multiple subfields $a, Place
of Publication, and $b, Publisher Name. There were two entries for $f, Manufacturer, and $e,
Place of Manufacture. For the 300 tag, Physical Description, there was a bit more consistency in
89

the description of this data as it relates to the number of occurrences for the subfields.
Although this provided some information about Item P and how cataloger’s judgment
manifested their decisions, it did not provide a complete picture for these MARC fields.
Table 3.3
Field Count Report for Item P
Field
245

260

300

Subfield
$a
$b
$c
$a
$b
$c
$e
$f
$a
$b
$c

Field/Subfield Description

Total

Title Statement
Title
Statement of responsibility, etc.
Remainder of title
Publication, Distribution, etc.
Place of publication, distribution, etc.
Name of publisher, distributor, etc.
Date of publication, distribution, etc.
Place of Manufacture
Manufacturer
Physical Description
Extent
Other physical details
Dimensions

29
29
1
23
28
35
42
28
2
2
29
29
29
29

The next three tables (3.4, 3.5, and 3.6) provide additional information regarding the
decisions made by catalogers. These tables provide the data input, evidence of rule usage, and
comments for the 245, 260, and 300 fields for three random records submitted for Item P.
Since test participant did not record the rule sequences used, the evidence of rule usage was
determined by the investigator by comparing the text with the RDA rules. Due to the fact that
the participant did not provide this data, there is no way to be completely certain that the rules
referenced were used by the cataloger. It was however, one way to look at cataloger’s
judgment.
The sample of the title and statement of responsibility for Item P reveals that there are
differences between the three records. All three catalogers recorded the same information for
90

the title; however, they decided not to transcribe the title as allowed in RDA (i.e. MARCH OF
THE PENGUINS). However, the statement of responsibilities differs between all three. For
example, it appears that the third cataloger chose to use the container as the preferred source
of information, and the other two used the label on the DVD. This led to additional variations.
There are other differences in the application of cataloger’s judgment as seen in the “Evidence
of Rule Use” in Table 3.4 which provides the data entered into the MARC record, evidence of
rule usage, and researcher comments for three of the MARC records submitted for Item P.
Table 3.4
Data Sample for Title and Statement of Responsibility of Item P
245
00$aMarch of the penguins /$cWarner
Independent Pictures ; and National
Geographic Feature Films.

00$aMarch of the penguins /$c[presented
by] Warner Independent Pictures and
National Geographic Feature Films.

00$aMarch of the penguins /$cas told by
Morgan Freeman ; Warner Independent
Pictures and National Geographic Feature
Films present a Bonne Pioche production
in association with Wild Bunch.

Evidence of Rule
Use
2.3.1.1
2.3.2.1
2.3.2.2
2.2.2.3
2.3.2 .1
2.3.2.7
2.4.2.1
2.4.2.2
2.4.2.3
2.3.1.1
2.3.2.1
2.3.2.2
2.2.2.3
2.3.2 .1
2.3.2.7
2.4.2.1
2.4.2.2
2.4.2.3
2.4.1.5
2.4.1.7
2.3.1.1
2.3.2.1
2.3.2.2
2.2.2.3
2.3.2 .1
2.3.2.7
2.4.2.1
2.4.2.2
2.4.2.3

91

Comments
Preferred source of Information =
DVD

Preferred source of Information =
DVD

Preferred source of information =
Container (front and back)

Table 3.5 is a set of sample data for Item P relating to the publication, distribution, and
other characteristics of the information entity. There was partial agreement with the place of
publication as well as one person stating that it was not identified. There was no agreement
within $b or $c of the MARC record. However, this lack of agreement seems to be more related
to the level of specificity of how the publication data was described. The first entity below did
appear not to follow the rule for publication/copyright date, but appears to blend the set of
rules together. Table 3.5 provides the rule use and other comments by the researcher for the
data that was entered by catalogers.
Table 3.5
Data Sample for Publication, Distribution, etc. of Item P
260
\\$aBurbank, CA :$bWarner Home Video,
$cc[2005?]
\\$a[Place of publication not identified]
:$bWarner Independent Pictures,$c[2005],
Â©2005.
\\$aBurbank, CA :$bWarner Independent
Pictures :$bNational Geographic Feature
Films :$bWarner Home Video
[distributor],$cÂ©2007.

Evidence of Rule
Use
2.8.2.3
2.8.4.3
2.8.6.6
1.9.2.3
2.8.2.6
2.8.4.3
2.8.6.6
1.9.2.3
2.11.1.3
2.8.2.3
2.8.4.3
2.8.4.4
2.8.4.5
2.8.6.3
2.11.1.3

Comments
$c c[2005?] does not follow the
convention it is a blend between
2.8.6.6/1.9.2.3/2.11.1.3
Chooses the option of including a
publication date from the
copyright date
Includes the role of Warner Home
Video

The sample of physical description data in Table 3.6 shows how catalogers encoded
their interpretations of the RDA rules for this part of the MARC record. It appears the first
record below did not consult rule 7.22.1.3, which instructs the cataloger to follow the
abbreviations in the RDA Toolkit Appendix for minutes. Other variations include the order of
92

the sound and color attributes, and the utilization of the Library of Congress Policy Statements
(LCPS) for the transcription of the dimensions. Specifically, the U.S. uses inches instead of
centimeters for videodiscs.
LCPSs were included in the RDA Toolkit for the duration of the test (U.S. RDA Test
Coordinating Committee, 2011); however, these particular documents were not accessible
through the RDA National Test archive that was posted online. Based on the text entered in the
300 field, it could be assumed that these policy statements and other LCPSs published by LC
could have affected the application of RDA by test participants.
Table 3.6
Data Sample for Physical Description of Item P
300
\\$a1 videodisc (80 minutes) :$bsound,
color ;$c4 ¾ in.

\\$a1 videodisc (80 min.) :$bcolor, Dolby
digital stero and mono ;$c4 ¾ in.

\\$a1 videodisc (80 min.) :$bsound, color
;$c12 cm

Evidence of Rule
Use
3.4.1.3
7.18.1.3
7.17.3.1
3.5.1.4
LCPS 3.4.1.4.4
3.4.1.3
7.17.3.1
3.16.2.3
7.22.1.3
3.4.1.4
LCPS 3.4.1.4.4
3.4.1.3
7.18.1.3
7.17.3.1
3.5.1.3
3.5.1.4.4

Comments
Used LCPS for diameter of disc

stereo misspelled
Used LCPS for diameter of disc

“sound” is not found by itself in
the rules

Although this was an extremely small sample of what could be uncovered, the variances
were noted and it was determined that the MARC records from the RDA National Test were
worthy of further research. The remaining data that was made available for this study from the

93

RDA National Test was extensive, and even without statistically significant results, the data
provides greater insight into the decision making process of catalogers.
Conclusion
This chapter reviews methodology utilized throughout the study. It describes the
research approach, sample, data collection, and the preliminary data that was considered
before formal study began. A mixed method research approach allowed for the study of
cataloger’s judgment as manifested through the creation of MARC records during the RDA
National Test. Together, these steps and processes assisted in answering the research
questions.

94

CHAPTER 4
DATA ANALYSIS
Introduction
In this study, 217 cataloging records produced by 79 different individuals were analyzed
to determine if the theory of bounded rationality, which proposes that individuals make
judgments within the constructs of cognitive and time constraints is able to explain any
significance in cataloger’s judgment. No two records submitted had identical text entered for
every field, which resulted in 217 unique records. This chapter discusses observations made
from analyzing the data and statistical findings. There were an additional 112 records created
for the electronic resource items; however, they were removed from this study since they did
have corresponding survey data that would allow for the analysis required.
Due to the small sample size per record, the assumptions for the Chi-square Test of
Independence were met 28.49% of the time. For this reason, the Fisher’s Exact test was used
for 71.51% of the data. In addition to describing the associations between variables using the
Chi-square or Fisher’s Exact, the Cramer’s V is also reported. This test evaluates the effect size
when evaluating the relationships between groups.
Throughout this chapter, various RDA rules are referred to for each of the fields studied.
The rules that are referenced are the set of rules that were published in the RDA Toolkit at the
time of the RDA National Test. Many revisions have been made since the time of the test and
some rule numbers have changed.

95

Sample Statistics
The data studied consisted of a sample of 217 MARC records created by 78 unique
individuals that submitted between one and ten MARC records for the test (Table 4.1). The
MARC record data was acquired freely through the LC website; however, a formal request to
the LC was required to obtain the survey data.
Table 4.1
Records Submitted by Test Participants
Number of Records Submitted
1 Record Submitted
2 Records Submitted
3 Records Submitted
4 Records Submitted
5 Records Submitted
6 Records Submitted
7 Records Submitted
8 Records Submitted
9 Records Submitted
10 Records Submitted
Total Unique Participants

Number of Participants
19
13
30
8
5
0
1
1
0
1
78

The RDA National Test required each test participant that submitted at least one MARC
record to complete two surveys. The first was a profile survey that provided general
information regarding their job title and total number of years cataloging. Participants only
needed to submit this survey once. The second survey provided information about the resource
they cataloged. Some of questions included asked participants how long it took them to create
the catalog record, how many minutes they consulted with others, and the amount of years
they have spent creating cataloging records for the type of resource being described.
Participants needed to submit one of these surveys for each record they completed for the test.

96

The categorical groups used in this study were created based on the respondents’
answers (Table 4.2). A great majority (87.57%) of the respondents describe their job title as a
librarian, followed by 7.37% paraprofessionals, 5.53% for both students and other.
A majority of the participants, 55.3%, stated they had between 6 – 22 years total
experience in cataloging, 22.5% stated they had more than 22 years of experience, 17.97% of
the respondents reported they had 3 – 6 years of experience, and 4.15% reported 0 – 3 years of
experience. However, the makeup of these groups was very different when they responded to
the question about the number of years cataloging electronic resources. The majority of
respondents, 67.74%, stated they had 0 – 3 years of experience, 16.59% reported having 3 – 6
years of experience, 16.59% stated having 6 – 22 years of experience, and 1.84% reported
having more than 22 years of experience.

97

Table 4.2
Descriptive Statistics for Categorical Groups
Group

N

%

Job Title
Librarian
177
Paraprofessional
16
Student
12
Other
12
Total Number of Years Cataloging
0-3 Years of Experience
9
3-6 Years of Experience
39
6-22 Years of Experience
120
22+ Years of Experience
49
Years of Experience Cataloging Electronic
Resources
0-3 Years of Experience (ER Only)
147
3-5 Years of Experience (ER Only)
30
6-22 Years of Experience (ER Only)
36
22+ Years of Experience (ER Only)
4
Number of Minutes Creating Test MARC Record
0-30 Minutes
36
31-60 Minutes
74
61-90 Minutes
49
91-120 Minutes
26
121-300 Minutes
32
Number of Minutes Consulting Others
0 Minutes
122
1-30 Minutes
73
31+ Minutes
22

81.57%
7.37%
5.53%
5.53%
4.15%
17.97%
55.30%
22.58%
67.74%
13.82%
16.59%
1.84%
16.59%
34.10%
22.58%
11.98%
14.75%
56.22%
33.64%
10.14%

Further analysis of these groups was made by comparing the crosstabs of the groups.
The crosstabs is a matrix of the frequency distributions of the categorical variables. The
crosstabs are important to review since they provide insight into those that took part in
creating the electronic resource records.
The most interesting findings in the crosstabs were between job title and years
of experience cataloging and then job title and years of experience cataloging electronic
98

resources. The crosstabs display that there are 107 records created by librarians that have
between 6 and 22 years of experience cataloging, but there is a large difference with those that
have far less experience cataloging electronic resources. There were only 21 records created by
librarians with 6 - 22 years of experience cataloging electronic resources and 118 records
created by librarians with 0 – 3 three years of experience cataloging electronic resources. Due
to the size of the output from SPSS, summaries of the data have been created instead of
sharing the result of each test. Anyone requesting to view the complete output may contact the
researcher directly. Table 4.3 provides a summary of the crosstab data.

99

Table 4.3
Categorical Group Crosstabs

Paraprofessional

Student

0 - 3 Years

3 - 6 Years

6 - 22 Years

22+ Years

0 - 3 Years

3 - 6 Years

6+ Years

0 - 30 Minutes

31 - 60 Minutes

61 - 90 Minutes

91 - 120 Minutes

121 - 300 Minutes

Did Not Consult

1 - 30 Minutes

31+ Minutes

Number of
Minutes Spent
Consulting
Others

Other

Number of Minutes
Spent Creating Cataloging
Record

Librarian
Total Years of
Experience
Cataloging

Job Title

Job Title

Experience
Cataloging
Electronic
Resources

Experience
Cataloging
Electronic
Resources

Total Years of
Experience
Cataloging

Librarian

-

-

-

-

1

24

107

45

118

21

38

32

63

42

22

18

104

60

13

Other

-

-

-

-

0

7

3

2

8

3

1

0

3

2

2

5

7

3

2

Paraprofessional

-

-

-

-

0

5

10

1

10

6

0

3

5

5

0

3

4

10

2

Student

-

-

-

-

8

3

0

1

11

0

1

1

3

0

2

6

7

0

5

0 - 3 Years

1

0

0

8

-

-

-

-

9

0

0

2

6

0

0

1

3

0

6

3 - 6 Years

24

7

5

3

-

-

-

-

27

12

0

2

15

15

9

4

18

14

7

6 - 22 Years

107

3

10

0

-

-

-

-

81

14

25

28

37

37

27

14

65

48

7

22+ Years

45

2

1

1

-

-

-

-

30

4

15

4

22

22

13

7

36

11

2

0 - 3 Years

118

8

10

11

9

30

27

81

-

-

-

26

47

33

15

26

78

50

19

3 - 6 Years

21

3

6

0

0

4

12

14

-

-

-

3

14

8

2

3

17

11

2

6+ Years

38

1

0

1

0

15

0

25

-

-

-

7

13

8

9

3

27

12

1

(table continues)

Table 4.3 (continued).

0 - 30 Minutes

32

0

3

1

2

2

28

4

Experience
Cataloging
Electronic
Resources
26
3
7

31 - 60 Minutes

63

3

5

3

0

15

37

22

47

14

61 - 90 Minutes

42

2

5

0

0

9

27

13

33

91 - 120 Minutes

22

2

0

2

1

4

14

7

121 - 300
Minutes
Did Not Consult

18

5

3

6

6

9

14

104

7

4

7

3

18

1 - 30 Minutes

60

3

10

0

0

31+ Minutes

13

2

2

5

6

Number of Minutes
Spent Creating
Cataloging Record

Job Title

Number of
Minutes
Spent
Consulting
Others

-

-

-

-

-

Number of
Minutes Spent
Consulting
Others
27
5
4

13

-

-

-

-

-

52

21

1

8

8

-

-

-

-

-

21

25

3

15

2

9

-

-

-

-

-

14

9

3

3

26

3

3

-

-

-

-

-

8

13

11

65

36

78

17

27

27

52

21

14

8

-

-

-

14

48

11

50

11

12

5

21

25

9

13

-

-

-

7

7

2

19

2

1

4

1

3

3

11

-

-

-

Total Years of
Experience
Cataloging

101

Number of Minutes
Spent Creating Cataloging
Record

The MARC record file downloaded from the LC website consisted of 329 MARC records
for the 11 items that were analyzed for this test. The items included three E-Monographs, three
E-Serials, one streaming video, and four Integrating Resources. The 112 records that were
removed from the analysis were done so due to missing data in the MARC 040 field that was
required to trace the entry back to a specific cataloger or because the cataloger did not
complete the required RCP and/or COS surveys (Table 4.4).
Table 4.4
Item MARC Record Distribution
Item

Title

Material Type

Total Number of
Records Created

H
I
J

Americans with Disabilities
Benjamin Button
Reconciling Scientific
Approaches for Organic
Farming Research
Criterion
Utley
San Diego
Acupuncture

E-Monograph
E-Monograph
E-Monograph
E-Serial
E-Serial
E-Serial
Streaming
Video
Integrating EResource
Integrating EResource
Integrating EResource
Integrating EResource

M
N
O
Q
V
W

Our Science – Research
Directory
NCJRS

X

UN

Y

Proquest

Total

Percentage of
Total Analyzed

35
34
31

Number of
Records
Analyzed
20
22
22

28
29
29
30

17
18
21
22

61%
62%
72%
73%

27

19

70%

30

18

60%

29

19

66%

27

19

70%

329

217

66%

57%
65%
71%

The data collected from the RDA National Test surveys and the bibliographic records in
MARC format were analyzed using Excel spreadsheets and the statistical program, SPSS 22. The
following discussion provides the results of the analysis performed on survey data and the
bibliographic records.

Recording Names
It was noted that in reviewing the records, that there were some differences in how
personal names were reflected in the test records. Often times, catalogers consult the LC
authority database, or a comparable authority database, when creating records. However,
there are times when name authority files are lacking for individual, corporate, or family
names. In these cases the cataloger is left to consult the item itself for additional information or
to seek other sources that may assist in determining the appropriate entry of information.
It is important to note that the directions to participants in the RDA National Test were
as follows:
If your institution normally uses templates to create the basic record, you may use
templates to create the bibliographic records. If your institution normally creates
authority records, create authority records from scratch for each access point in the
record, based on the same rules you used to catalog the record. Do not search for an
existing heading. Do not submit any of these headings to the National Authority File as
these are only artificial test records. Authority records may be saved in the OCLC save
file. Do not assign subject headings or classification for any of the titles in the common
original set. Do not use the single record approach when cataloging the common
original set even if your institution normally uses a single record approach for material
issued in print and online. (Library of Congress, 2009a, p. 3)
Although participants were not to consult a database for the headings, many did use the
correct headings. Alternatively, it is possible that the system used to create the records
provided options to consult previous authority work.

Personal Names (MARC 100 & 700 Fields)
Within the RDA National Test, there were a number of variances among the participants
in how personal names were constructed in the 100 and/or 700 MARC fields. Through the
103

informal analysis of the MARC data, it was found that some records had existing authority
records in the LOC Name Authority Headings and others did not. Three major areas of
disagreement were found within the data that included variations of field data due to not
following authority records, variation based on no authority record, and inconsistencies in
recording a family name.
In item I, an E-Monograph, participants could have consulted the LC Authorities for the
author F. Scott Fitzgerald; however, in analyzing the 100 field for this item, the researcher
noticed variances that five of the 22 participants (22.7%) did so. The other participants entered
a variety of text that did not adhere to the authority file. Below is the authorized heading and
then the variations of what participants entered:
Bibliographic MARC record entry based on authorized heading for Francis Scott Fitzgerald:
100 1 _ $a Fitzgerald, F. Scott $q (Francis Scott), $d 1896-1940, $e author.
Variations entered by participants:
100 1 _ $a Fitzgerald, F. Scott $q (Francis Scott)
100 1 _ $a Fitzgerald, F. Scott $q (Francis Scott), $d 1896-1940.
100 1 _ $a Fitzgerald, F. Scott, $d 1896-1940, $e author.
100 1 _ $a Fitzgerald, F. Scott $q (Francis Scott Key), $d 1896-1940, $e author.
100 1 _ $a Fitzgerald, Francis Scott $q (Francis Scott Key) $d 1896-1940.
100 1 _ $a Fitzgerald, Francis Scott, $q (Francis Scott Key) $d 1896-1949, $e author.
100 1 _ $a Fitzgerald, Francis Scott, $e author.

104

Although there were differences in how Francis Scott Fitzgerald was recorded, there was
agreement among all participants that he was chiefly responsible for the intellectual content of
the work and did include him in the 100 MARC field.
Variances occurred in Item J, an E-Monograph, as well; however for this item, there was
no LOC name authority record for the author. On both the cover and the title page, the author’s
name was printed as “Ton Baars.” RDA rule 9.2.2 instructs catalogers to use the preferred
source of information as defined in RDA rule 2.2. Since this is an electronic version of a paperbased document, the preferred source should be the title page.
In the case of Item J, the title page had the author’s name as “Ton Baars.” Below are
some of the variants that appeared in the records created by the test partners:
100 1 _ $a Baars, T.
100 1 _ $a Baars, T. $q (Anthonie)
100 1 _ $a Baars, Ton
100 1 _ $a Baars, Ton, $d 1956100 1 _ $a Baars, Ton, $d 1956, $e author.
At first, it was confusing why someone creating this record would not use Ton Baars, as
what was listed on the title page, when the LOC authority file was not located, but after further
review of the resource, it was noted that the author was listed on the verso of the title page as
“T. Baars.” However, if a cataloger used the verso of the title page, then he/she did not do as
instructed under RDA 2.2.

105

Upon further investigation, the Virtual International Authority Files (VIAF), an
international directory of authority files, listed three authority files by four different
agencies that have completed authority work on the personal name Ton Baars.
Below are the results found in VIAF:
•

Baars, T. (Anthonie), 1956- (National Library of the Netherlands)

•

Baars, T. (International Standard Name Identifier (ISNI) and German National Library)

•

Baars, Anthonie (1956-). (NUKAT Center of Warsaw University Library)

The third large area of disagreement in recording names was that of a family name.
Family names are not as common as personal names in cataloging; however, the test did
include one such item. Item N, a family newsletter, was the resource that some of the test
participants felt should have the family name being recorded as the creator or at least partially
responsible for the creation of the newsletter.
In reviewing the records, eight out of the 18 (44.4%) participants did record some
variance of the family in the 100 MARC field, six out of 18 (33.3%) recorded the family name in
the 700 field, two out of 18 (11.1%) listed the family name as a corporate name in the 710, and
finally, two out of the 18 did not record any information in any 1xx or 7xx fields.
Text represented in 100 Field for Item N:
Utley
Utley (Family : Jackson, Tenn.),*
Utley (Family, author. Utley, John Allen),**
Utley (Family : Jackson, Tenn.), author***
106

Utley (Family : Utley, John Allen)
Utley (Family)
Utley (Family), author
*Two participants listed this entry
** Also included Utley, Jackie, editor. In 700
*** Also included Utley, Jackie, editor of compilation.

Text represented in 700 for item N (two participants listed two 700 fields):
Utley (Family)
Utley (Family : Jackson, Tenn.)
Utley (Family) author
Utley Family, issuing body
Text represented in 710 for item N:
Utley (Family : United States) descendants
Utley (Family : United States) issuing body
During the test, RDA addressed recording family names beginning with Rule 10.2 and
directs catalogers to use the preferred name for the family, which then refers the cataloger to
consult RDA 2.2.2. It appears that those catalogers that created the records that included a
family name, did choose Utley as the preferred name. However, there was some disagreement
whether or not they should include the place associated with the family name (RDA 10.5) or the
relator term (RDA 18.5.1.3).

107

After examining the MARC records qualitatively, statistical analysis was performed. A
majority of the data for the 100/700 fields did not meet the assumption for the Chi-Square test
and for those that did not meet the assumption the Fishers Exact p value is reported instead.
Associations were examined between categorical variables and the text that was
recorded in the MARC records. The text was categorized into two groups; the presence of text
and no text present. There was a statistical association for 100 $e and total years of experience
cataloging, p = 0.039 and a Cramer’s Ѵ = .216, a moderate effect size; and the number of
minutes consulting others, p < .001 and a Cramer’s Ѵ = .257, a moderate effect size. There was
also a statistical association for 100 $q and years of experience cataloging ER, p = 0.031 and a
Cramer’s Ѵ = .197, a weak effect size. There was also a statistical association for 700 $e and
minutes consulting others, p = 0.048 and a Cramer’s Ѵ = .168, a weak effect size, and a
statistical association for 700 $q and years of experience cataloging ER, p = 0.025 and a
Cramer’s Ѵ = .208, a moderate effect size. There was no statistical association between the
other variables. Tables 4.5 and 4.6 provide a summary of the results of these tests.

108

Table 4.5
Chi-Square p-value/Fisher’s Exact Results for Names
Chi-square p-value/Fisher’s Exact
Years of
Experience
Cataloging
.090*
.063
.199*
.039*
1.000
.229
.492*
1.000
.412
.745
.261
.221
1.000
.097
.581

Years of
Experience
Cataloging ER
.104*
1.000
.084*
.592*
1.000
.031
.491*
.805
.696
.743
.185
1.000
.323
.025
.210

Minutes
Spent
Creating
Record
.260*
.717
.393
.121*
.267
.673
.278*
.710
.515
.917
.811
.659
.659
.811
.604

Minutes
Consulting
Others
.108*
.679
.438*
.001*
.438
.904
.395*
.126
.893
.048*
.630
.438
.438
.630
1.000

Data Field
Job Title
Field_100$a
.199
Field_100$c
.199
Field_100$d
.056
Field_100$e
.117
Field_100$g
1.000
Field_100$q
.267
Field_700$a
.241*
Field_700$c
1.000
Field_700$d
.580
Field_700$e
.096
Field_700$i
.230
Field_700$l
1.000
Field_700$p
1.000
Field_700$q
.230
Field_700$t
.580
The shaded cells indicate significance.
*Variable met Chi-square assumption; therefore, the Chi-square is reported. The p-values are based on all cases
with valid data.

109

Table 4.6
Cramer’s Ѵ Results for Names
Cramer’s Ѵ

Data Field
Field_100$a
Field_100$c
Field_100$d
Field_100$e
Field_100$g
Field_100$q
Field_700$a
Field_700$c
Field_700$d
Field_700$e
Field_700$i
Field_700$l
Field_700$p
Field_700$q
Field_700$t

Job Title
.149
.056
.197
.160
.032
.129
.139
.073
.070
.145
.129
.032
.032
.129
.070

Years of
Experience
Cataloging
.173
.166
.147
.216
.061
.153
.105
.035
.086
.055
.124
.145
.061
.172
.067

Years of
Experience
Cataloging ER
.145
.082
.155
.069
.047
.197
.081
.030
.043
.055
.127
.047
.143
.208
.107

Minutes
Spent
Creating
Record
.156
.102
.133
.183
.164
.112
.153
.116
.126
.075
.109
.126
.126
.109
.117

Minutes
Consulting
Others
.143
.085
.087
.257
.096
.041
.093
.152
.030
.168
.060
.096
.096
.060
.018

Cramer’s V – Effect Size Categories
.000 - .099 (Negligible Association)
.100 - .199 (Weak Association)
.200 - .399 (Moderate Association)

Corporate Names (MARC 110 & 710 Fields)
Test participants determined that several of the items had corporate names associated
with the resource. As with personal names, there were variances among how participants
would record the corporate names for the same items, including those that have LOC
authorized headings. There were two additional types of variances that occurred that are worth
pointing out for corporate names.

110

The first is how the corporate name for Item V was recorded by various participants.
Item V was an online directory for the Center for Cancer Research. Participants appeared to
have difficulty in determining the corporate name based on the preferred source of information
(RDA 11.2.2, which then refers the cataloger to 2.2.2), which in this case is the home page for
the directory. Some considered Center for Cancer Research as the preferred name and others
determined it was the National Cancer Institute.
Figure 4.1 is a screenshot of the archived version of the homepage for Item V as it
appeared on May 27, 2010. This is the iteration of the web site as close to the test date that
could be discovered by the researcher. Figure 4.1 shows that the two organizational names that
appear more prominent than any others are Center for Cancer Research (CCR) and the National
Cancer Institute.

Figure 4.1: Screenshot of the homepage for Item V

111

When consulting the authority files for each of these, two viable choices appeared
based on which corporate name was chosen as the preferred name (1) Center for Cancer
Research (National Cancer Institute (U.S.)) and (2) National Cancer Institute (U.S.).
This is further complicated by which field the participants would choose to enter this
information. A majority, or 63.2%, determined that this information should be represented in
the 110 field and the remaining determined it was most appropriate to record the information
in the 710 field. Table 4.7 provides a breakdown of how the corporate name was determined
for the nineteen records submitted for Item V.
Table 4.7
Authority Headings Used in the MARC 110/710 Field for Item V
LOC Name Authority Heading
Center for Cancer Research (National Cancer Institute (U.S.))
National Cancer Institute (U.S.)
Included both headings

110 Field
8
4
0

710 Field
4
2
1

The second example of discord among those cataloging the test items concerned Item Y,
the CSA Illumina database. With this resource, the general consensus was that there was no
preferred name; however, participants used a variety of authorized and unauthorized headings
to describe the corporate names that had some responsibility in the intellectual content. Two
participants determined there was a preferred name and two did not list any corporate name in
either the 110 or 710 fields. Table 4.8 provides the headings and their use for Item Y.

112

Table 4.8
Corporate Names Used in Describing Item Y
Heading Used by Test Participants

LOC Authorized Heading

110

710

No

-

1

Yes*, but updated heading
ProQuest CSA (Firm) is preferred
No

-

1

-

1

CSA (Firm)

No

-

8

CSA (Firm : Cambridge Information Group)

No

-

1

Proquest

No

1

2

Proquest (Firm)

Yes

1

12

Proquest CSA (Firm)

Yes

-

2

Cambridge Scientific Abstracts (Firm)
Cambridge Scientific Abstracts, Inc.
CSA

*Instructions in the LOC Authorities states, “THIS 1XX FIELD CANNOT BE USED UNDER RDA UNTIL THIS RECORD
HAS BEEN REVIEWED AND/OR UPDATED”

A majority of the participants that included a corporate name did so by including the
LOC authorized heading, Proquest (Firm). The second most common used heading was CSA
(Firm), which is not an LOC authorized heading. Based on what was entered, it is obvious that
the LOC authorized headings were inconsistently used as a determining factor when choosing
the corporate name for Item Y.

Relator Terms in Names
Subfield $e for the 1xx and 7xx fields provides users with information relating to the
relationship between a name (personal, corporate or family) and that of the resource being
described. This subfield is not and was not considered “Core” at the time of the test; however,
it is often useful for users to know if the name is associated with authorship, issuing,
publication, production, etc. With the understanding that the Functional Requirements for
Bibliographic Records (FRBR) model is built upon relationships between works, expressions,
113

manifestations, and items, and RDA is partly based on FRBR, building relationships through
relators is becoming increasingly more important.
During the RDA National Test, a list of relator terms were listed in Appendix I of RDA.
The directions state to utilize this list unless the terms listed in Appendix I are not appropriate
or specific enough for the relationship being described.
Analysis of the data shows that out of the 217 records, there were 317 instances were a
participant entered a 1xx and/or 7xx field into their record. However, only 147 (46.4%) of these
entries had a relator term applied in subfield $e. Table 4.9 provides a greater breakdown of the
relator terms by field.

114

Table 4.9
Relator Terms Used in MARC 1xx and 7xx Subfield $e, Listed by Occurrence
Relator Term

Term Listed
in Appendix
I

author

Yes

MARC Code
List for
Relators
(LOC)
Yes

100

110

700

710

30

4

2

1

compiler

Yes

Yes

-

2

-

1

online information system

No

No

-

1

-

1

sponsoring institution

No

No

-

1

-

1

corporate sponsor

No

No

-

-

-

1

degree granting institution

Yes

Yes

-

-

-

1

descendants

No

No

-

-

-

1

hosting research institution

No

No

-

-

-

1

awarding institution

No

No

-

-

-

1

originator

No

Yes

-

-

-

1

production company

Yes

Yes

-

-

-

1

host institution

Yes

Yes

-

-

-

2

other

No

Yes

-

-

-

2

sponsoring body

Yes

-

1

-

6

publisher

No

No: Use
Sponsor
Yes

-

-

1

7

issuing body

Yes

Yes

-

9

2

35

contributing editor

No

No

-

-

1

-

dedicatee

Yes

Yes

-

-

1

-

dissertation committee

No

No

-

-

3

-

editor

Yes

Yes

1

-

8

-

editor of compilation

Yes

Yes

-

-

1

-

editor-in-chief

No

No

-

-

1

-

lecture series planner

No

No

-

-

2

-

lecturer

No

No

1

-

-

-

on-screen presenter

Yes

No*

1

-

1

-

performer

Yes

Yes

-

-

1

-

presenter

Yes

Yes

2

-

1

-

speaker

Yes

Yes

2

-

3

-

website designer

No

No

-

-

1

-

37

18

29

63

*on-screen presenter is not acceptable, but onscreen presenter is
In Table 4.9 (above), 119 (54.84%) of the entries for subfield $e did appear in Appendix I
of RDA. The LC also maintains the MARC Code Lists for Relators, as an alternative or in addition
115

to the RDA list of relator terms. Since this element of RDA is optional, it is possible for a variety
of answers to be based on the catalog agency’s local practices, which is often an example of a
formalized version of cataloger’s judgment, or an individual’s cataloger’s judgment.
Associations were examined between categorical variables and the text that was
recorded in the MARC records. The text was categorized into two groups; the presence of text
and no text present. There was a statistical association for 110 $a, Corporate Name, and total
years of experience cataloging, p = 0.022 and a Cramer’s Ѵ = .210, a moderate effect size. There
was also a statistical association for 110 $e, Relator Term, and years of experience cataloging
ER, p = 0.006 and a Cramer’s Ѵ = .232, a moderate effect size; as well as the minutes spent
creating the MARC record, p = .022 and a Cramer’s Ѵ = .229, a moderate effect size. There was
no statistical association between the other variables. Table 4.10and 4.11 provides a summary
of the results of these tests.
Table 4.10
Chi-Square p-value/Fisher’s Exact results for corporate names
Chi-square p-value/Fisher’s Exact
Years of
Experience
Cataloging
.022*
.729
.069
.095
.569
.834*

Years of
Experience
Cataloging ER
.059*
.548
.006
.545*
.635
.610*

Minutes
Spent
Creating
Record
.181*
.677
.022
.600*
.513
.929*

Minutes
Consulting
Others
.290*
.958*
.865*
.728*
.123*
.097*

Data Field
Job Title
Field_110$a
.273
Field_110$b
1.000
Field_110$e
.822
Field_710$a
.765
Field_710$b
.168
Field_710$e
.412
The shaded cells indicate significance.
*Variable met Chi-square assumption; therefore, the Chi-square is reported. The p-values are based on all cases
with valid data.

116

Table 4.11
Cramer’s Ѵ results for corporate names
Cramer’s Ѵ

Data Field
Field_110$a
Field_110$b
Field_110$e
Field_710$a
Field_710$b
Field_710$e

Job Title
.144
.044
.102
.071
.167
.114

Years of
Experience
Cataloging
.210
.074
.170
.170
.103
.063

Years of
Experience
Cataloging ER
.161
.083
.232
.075
.072
.067

Minutes
Spent
Creating
Record
.170
.104
.229
.113
.124
.063

Minutes
Consulting
Others
.107
.020
.037
.054
.139
.147

Cramer’s V – Effect Size Categories
.000 - .099 (Negligible Association)
.100 - .199 (Weak Association)
.200 - .399 (Moderate Association)

Recording Titles
Preferred Title (MARC 130 Field)
The preferred title, formerly known as the uniform title, allows users to discover various
manifestations of a work. The preferred title is to be used for translations or new editions that
have revised content and title changes. The preferred title is determined to be the title and the
language in which the resource was originally printed (RDA 6.2, 2.2., and 6.2.2).
In previous practice (AACR2), in the case where the title was the authorized access
point, MARC field 245, then the MARC 130 field should be completed. This would be for works
in which there is not a name associated with the creator of the work. In the test there were two
such electronic resources in which some test participants adhered to this rule. The two
resources were Items M and O, both of which were online journals.
117

In each of these resources, seven individuals determined a 130 MARC field was
necessary. For Item O, six of the seven used the exact same text to describe the preferred title,
but there was less agreement with Item M. For M, there were four variations of text with
“Criterion (Maharashtra, India)” receiving the most acceptance. Table 4.12 provides the detail
of what was entered in the 130 field for both items.
Table 4.12
Entries for Preferred Titles (130)
MARC 130 Field Text

Criterion (2010)

Number of
Participants
using Text
2

The Criterion (Maharashtra, India)

1

Criterion (India)

1

Criterion (Maharashtra, India)

3

Journal of San Diego history (Online)

6

The Journal of San Diego History (Online)

1

Only one of the participants deemed that the editor was the authorized access point
and included the name in the 100 fields “Bite Vishwanath, editor.”

Titles (MARC 245 $a, 245 $b, and 246 $a Fields)
The title was a core element in the RDA National Test. When entering the title into
MARC, the cataloger does so by placing the title in subfield $a in the 245 field. If there is a
remainder title, or a subtitle, it is entered into subfield $b of the 245 field. There are times
when variations of the title that were different enough from the title entered into the 245 field
were entered into the 246 field (varying form of title).

118

In the study, for E-Monographs, E-Serials and the streaming videos there was much
agreement among the participants as to what should be entered for the titles. In contrast, there
was less agreement with the Integrating Resources (Items V, W, X, and Y). According to the RDA
Toolkit at the time of the test, an integrating resource is defined as “a resource that is added to
or changed by means of updates that do not remain discrete but are integrated into the whole.
An integrating resource may be tangible (e.g., a loose-leaf manual that is updated by means of
replacement pages) or intangible (e.g., a Web site that is updated either continuously or on a
cyclical basis).” To investigate this further, the websites were located using
http://web.archive.org in order to find a version of the website as close to the test date as
possible, without going beyond the end date of the test (Figures 4.2, 4.3, 4.4 and 4.5).

Figure 4.2: Item V, Homepage of Our Science – Research Directory

119

Figure 4.3: Item W, Homepage of Abstracts Database – National Criminal Justice Reference
Center Service

Figure 4.4: Item X, Homepage of Welcome to the United Nations: It’s Your World

120

Figure 4.5: Item Y, Homepage of CSA
Each of the integrating resource items studied had 19 records, except for Item W, which had 18.
Table 4.13 provides all of the text that each participant entered for 245 $a and $b and 246 $a
simultaneously. Item V had 13 variations, Item W and X had 15, Item X had 12, and Item Y had
11 for a total of 54 variations across all four items. These numbers suggest that there was very
little agreement for the 75 records that were created.

121

Table 4.13
Text Entered and Frequencies of the Text Entered in the 245 $a and $b and 246 $a Subfields

Item
V
V
V

245$a and $b

V
V
V
V

$a Our science $b research directory
$a Our science $b research directory
$a Our science $b research directory /
Center for Cancer Research
$a Our Science $b
$a Our science – research directory $b
$a Our science – research directory $b
$a Our science – research directory $b

V
V

$a Our science – research directory $b
$a Our science – research directory $b

V
V

$a Our science – research directory $b
$a Our science – research directory $b

V

$a Research directory $b

V
W
W
W

$a Research directory $b
$a Abstracts database $b
$a Abstracts database $b
$a Abstracts Database $b

W

$a Library abstracts $b

W

$a Library abstracts $b

W

246$a
Our Science – Center for Cancer Research
Our science – Center for Cancer Research
Our science – Center for Cancer Research
Our science research directory
Our science;Center for Cancer Research’s
online research directory
Research directory
Research directory;CCR annual research
directory;Center for Cancer Research annual
research directory

Our science – research directory;Center for
Cancer Research annual research center
directory;Center for Cancer Research annual
research directory;Annual research
directory;ARD
NCJRS Abstracts database
NCJRS Abstracts Database;Library/abstracts
NCJRS Abstracts Database;NCJRS Abstracts
Database Search
Library abstracts;National Criminal Justice
Reference Service library abstracts;NCJRS
abstracts
Abstracts database – National Criminal
Justice Reference Service;NCJRS abstracts
database
NCJRS;Abstracts database

$a National Criminal Justice Reference
Service $b
W
$a National Criminal Justice Reference
NCJRS abstracts database;Abstracts
Service abstracts database $b
database
W
$a NCJRS $b library abstracts
National criminal justice reference service
W
$a NCJRS $b National Criminal Justice
National Criminal Justice Reference
Reference Service
Service;Abstracts database
*Ending punctuation removed since it is undetermined if the system entered the ending punctuation

Number
of times
Text was
used
2
2
1
1
1
1
1
3
1
1
3
1

1
2
1
1
1
1
1
1
1
1

(table continues)

122

Table 4.13 (continued).

Item

245$a and $b

W

$a NCJRS $b

W

$a NCJRS abstracts database $b

W

$a NCJRS abstracts database $b

W

$a NCJRS abstracts database $b

W

$a NCJRS, National Criminal Justice
Reference Service $b
$a Search the NCJRS abstracts database $b

W
X

$a al-Umam al-muttahÌ£idah = $b Huan
ying lai dao lian he guo = United Nations =
Nations Unies = Organizatï¸ sï¸¡iiï¸ aï¸¡
ObÊºedinennykh Natï¸ sï¸¡iiÌ† (OON) =
Bienvenidos a las Naciones Unidas

X

X

$a United Nations $b we the peoples... a
stronger UN for a better world
$a United Nations $b we the peoples... a
stronger UN for a better world = Nous,
peuples des Nations Unis... une ONU plus
forte pour un monde meilleur = Naciones
Unidas : nosotros los pueblos... unidos por
un mundo mejor
$a United Nations $b

X

$a United Nations $b

X
X
X

$a United Nations $b
$a United Nations – your world $b
$a Welcome to the United Nations $b it’s
your world

X

246$a
Abstracts Database – National Criminal
Justice Reference Services
Abstracts database;National Criminal Justice
Reference Service abstracts database
National Criminal Justice Reference Service
abstracts database
National Criminal Justice Reference Service
abstracts database;Library
abstracts;Abstracts database – National
Criminal Justice Reference Service
National Criminal Justice Reference
Service;NCJRS abstracts database
NCJRS abstracts database;Abstracts
database :

Number
of times
Text was
used
1
1
3
1

1
1

Huan ying lai dao lian he guo;United
Nations;Nations Unies;Organizatï¸ sï¸¡iiï¸ aï¸¡
ObÊºedinennykh
Natï¸ sï¸¡iiÌ†(OON);Bienvenidos a las
Naciones Unidas;Welcome to the United
Nations. It’s your world!;United Nations, We
the peoples... a stronger UN for a better
world.;U.N. website
Welcome to the United Nations

1

Nous, peuples des Nations Unis... une ONU
plus forte pour un monde meilleur;Naciones
Unidas;Welcome to the United Nations

1

Welcome to the United Nations

2

Welcome to the United Nations :;United
Nations :;UN

1

Bienvenue aux Nations Unies ;Bienvenidos a
las Naciones Unidas

X

$a Welcome to the United Nations $b it’s
United Nations
your world
*Ending punctuation removed since it is undetermined if the system entered the ending punctuation

2

2
1
2
1

(table continues)
123

Table 4.13 (continued).

Item

245$a and $b

246$a

X

$a Welcome to the United Nations $b it’s
your world

X

$a Welcome to the United Nations $b it’s
your world

United Nations :;Nations Unies
:;Organizatï¸ sï¸¡iiï¸ aï¸¡ ObÊ¹edinennykh
Natï¸ tsï¸¡iiÌ†;Naciones Unidas son su mundo
United Nations, we the peoples, a stronger
UN for a better world

X

$a Welcome to the United Nations $b it’s
your world

United Nations–it’s your world!;United
Nations

X

$a Welcome to the United Nations $b it’s
your world
X
$a Welcome to the United Nations $b it’s
United Nations;[Arabic title];[Chinese
your world! = [Arabic title] : [Arabic other
title];Bienvenue aux Nations Unies;Dobro
title information] = [Chinese title] :
pozhalovatÊ¹ v OON;Bienvenidos a las
[Chinese other title information] =
Naciones Unidas
Bienvenue aux Nations Unies : c’est votre
monde = Dobro pozhalovatÊ¹ v OON :
eÌ‡to vash mir = Bienvenidos a las
Naciones Unidas : son su mundo
X
$a Welcome to the United Nations. It’s
United Nations. It’s your world
your world $b
Y
$a CSA $b
Cambridge Scientific Abstract
Y
$a CSA $b
CSA Illumina
Y
$a CSA $b
ProQuest
Y
$a CSA $b
Y
$a CSA Illumina $b
Cambridge Scientific Abstracts Illumina
Y
$a CSA illumina $b
CSA
Y
$a CSA Illumina $b
CSA;Illumina
Y
$a CSA illumina $b
Illumina
Y
$a CSA Illumina $b
Y
$a ProQuest $b
CSA
Y
$a ProQuest $b
*Ending punctuation removed since it is undetermined if the system entered the ending punctuation

Number
of times
Text was
used
1
1
1
1
1

1
1
1
1
2
1
1
1
1
4
4
2

It is important to look at the separate parts to the whole. Because of how the 245 and
246 fields are indexed users may not find all of the information. The next set of tables (4.14,
4.15, 4.16, and 4.17) provides the breakdown of each subfield to identify further agreement.
Table 4.14 provides the text that each participant entered for 245 $a and $b
simultaneously. Item V had five variations, Item W had ten, Item X had eight, and Item Y had
124

three. There were a total of 26 variations for the 75 records. The RDA rules that provide
guidance for 245 $a and $b are 2.3.2 (title proper) and 2.3.4 (other title information).
Table 4.14
Text Entered into MARC 245 $a and $b for Items V, W, X, and Y
Item

Text Entered in 245 $a and $b*

V
V
V
V
V
W
W
W
W
W
W
W
W
W
W
X

Number of
Times Text
was Used
4
1
1
11
2
4
2
1
1
1
1
1
5
1
1
1

$a Our science $b research directory
$a Our science $b research directory / Center for Cancer Research
$a Our Science $b
$a Our science – research directory $b
$a Research directory $b
$a Abstracts database $b
$a Library abstracts $b
$a National Criminal Justice Reference Service $b
$a National Criminal Justice Reference Service abstracts database $b
$a NCJRS $b library abstracts
$a NCJRS $b National Criminal Justice Reference Service
$a NCJRS $b
$a NCJRS abstracts database $b
$a NCJRS, National Criminal Justice Reference Service $b
$a Search the NCJRS abstracts database $b
$a al-Umam al-muttahÌ£idah = $b Huan ying lai dao lian he guo = United Nations =
Nations Unies = Organizatï¸ sï¸¡iiï¸ aï¸¡ ObÊºedinennykh Natï¸ sï¸¡iiÌ† (OON) =
Bienvenidos a las Naciones Unidas
X
$a United Nations $b we the peoples... a stronger UN for a better world
X
$a United Nations $b we the peoples... a stronger UN for a better world = Nous,
peuples des Nations Unis... une ONU plus forte pour un monde meilleur = Naciones
Unidas : nosotros los pueblos... unidos por un mundo mejor
X
$a United Nations $b
X
$a United Nations – your world $b
X
$a Welcome to the United Nations $b it’s your world
X
$a Welcome to the United Nations $b it’s your world! = [Arabic title] : [Arabic other
title information] = [Chinese title] : [Chinese other title information] = Bienvenue aux
Nations Unies : c’est votre monde = Dobro pozhalovatÊ¹ v OON : eÌ‡to vash mir =
Bienvenidos a las Naciones Unidas : son su mundo
X
$a Welcome to the United Nations. It’s your world $b
Y
$a CSA $b
Y
$a CSA Illumina $b
Y
$a ProQuest $b
*Ending punctuation removed since it is undetermined if the system entered the ending punctuation

125

2
1
5
1
7
1

1
5
8
6

Table 4.15 provides the text that each participant entered for 245 $a. Items V and Y had
three variations, Item W had eight, and Item X had five. There were 19 variations for the total
of 75 records.
Table 4.15
Text Entered into MARC 245 $a for Items V, W, X, and Y
Item

Text Entered in 245 $a*

Number of Times Text was Used

V

Our science

6

V

Our science – research directory

11

V

Research directory

2

W

Abstracts database

4

W

Library abstracts

2

W

National Criminal Justice Reference Service

1

W

National Criminal Justice Reference Service abstracts database

1

W

NCJRS

3

W

NCJRS abstracts database

5

W
W
X

NCJRS, National Criminal Justice Reference Service
Search the NCJRS abstracts database
al-Umam al-muttahÌ£idah =

1
1
1

X

United Nations

8

X

United Nations – your world

1

X

Welcome to the United Nations

8

X

Welcome to the United Nations. It’s your world

1

Y

CSA

5

Y

CSA Illumina

8

Y

ProQuest

6

*Ending punctuation removed since it is undetermined if the system entered the ending punctuation

Table 4.16 provides the text that each participant entered for 245 $b. Item V and W had
three variations, Item X had six, and Item Y had no variations. The total of variations for the
seventy-five records is twelve.

126

Table 4.16
Text Entered into MARC 245 $b for Items V, W, X, and Y
Item
V
V
V
W
W
W
X
X
X

X
X
X

Y

Text Entered in 245 $b*
research directory
research directory / Center for Cancer Research
No Text Entered
No Text Entered
library abstracts
National Criminal Justice Reference Service
Huan ying lai dao lian he guo = United Nations = Nations Unies =
Organizatï¸ sï¸¡iiï¸ aï¸¡ ObÊºedinennykh Natï¸ sï¸¡iiÌ† (OON) =
Bienvenidos a las Naciones Unidas
we the peoples... a stronger UN for a better world
we the peoples... a stronger UN for a better world = Nous, peuples
des Nations Unis... une ONU plus forte pour un monde meilleur =
Naciones Unidas : nosotros los pueblos... unidos por un mundo
mejor
No Text Entered
it’s your world
it’s your world! = [Arabic title] : [Arabic other title information] =
[Chinese title] : [Chinese other title information] = Bienvenue aux
Nations Unies : c’est votre monde = Dobro pozhalovatÊ¹ v OON :
eÌ‡to vash mir = Bienvenidos a las Naciones Unidas : son su mundo
No Text entered

Number of Times Text was Used
4
1
14
16
1
1
1
2
1

7
7
1

19

*Ending punctuation removed since it is undetermined if the system entered the ending
punctuation
Table 4.17 provides the text that each participant entered for 246. Item V had seven
variations, Item W had fifteen, Item X had twelve, and Item Y had nine. The total of variations
for the 75 records was 43.

127

Table 4.17
Text Entered into MARC 246 for Items V, W, X, and Y
Item

Number of
times Text
was used
4
8
1
1
3

246$a

V
V
V
V
V

Our Science – Center for Cancer Research
No Text Entered
Our science research directory
Our science;Center for Cancer Research’s online research directory
Research directory

V

Research directory;CCR annual research directory;Center for Cancer Research annual
research directory
Our science – research directory;Center for Cancer Research annual research center
directory;Center for Cancer Research annual research directory;Annual research
directory;ARD
NCJRS Abstracts database

1

1
1
1

W
W
W

NCJRS Abstracts Database;Library/abstracts
NCJRS Abstracts Database;NCJRS Abstracts Database Search
Library abstracts;National Criminal Justice Reference Service library abstracts;NCJRS
abstracts
Abstracts database – National Criminal Justice Reference Service;NCJRS abstracts
database
NCJRS;Abstracts database
NCJRS abstracts database;Abstracts database
National criminal justice reference service

W
W
W

National Criminal Justice Reference Service;Abstracts database
Abstracts Database – National Criminal Justice Reference Services
Abstracts database;National Criminal Justice Reference Service abstracts database

1
1
1

V
W
W
W
W
W

W
W

National Criminal Justice Reference Service abstracts database
National Criminal Justice Reference Service abstracts database;Library
abstracts;Abstracts database – National Criminal Justice Reference Service
W
National Criminal Justice Reference Service;NCJRS abstracts database
W
NCJRS abstracts database;Abstracts database :
*Ending punctuation removed since it is undetermined if the system entered the ending punctuation

1
2

1
1
1
1

3
1
1
1

(table continues)

128

Table 4.17 (continued).
Item

246$a

X

Huan ying lai dao lian he guo;United Nations;Nations Unies;Organizatï¸ sï¸¡iiï¸ aï¸¡
ObÊºedinennykh Natï¸ sï¸¡iiÌ†(OON);Bienvenidos a las Naciones Unidas;Welcome to the
United Nations. It’s your world!;United Nations, We the peoples... a stronger UN for a
better world.;U.N. website
Welcome to the United Nations
Nous, peuples des Nations Unis... une ONU plus forte pour un monde
meilleur;Naciones Unidas;Welcome to the United Nations
Welcome to the United Nations :;United Nations :;UN
No Text Entered
Bienvenue aux Nations Unies ;Bienvenidos a las Naciones Unidas
United Nations

X
X
X
X
X
X
X

Number of
times Text
was used
1

4
1
1
4
2
1
1

X
Y

United Nations :;Nations Unies :;Organizatï¸ sï¸¡iiï¸ aï¸¡ ObÊ¹edinennykh
Natï¸ tsï¸¡iiÌ†;Naciones Unidas son su mundo
United Nations, we the peoples, a stronger UN for a better world
United Nations–it’s your world!;United Nations
United Nations;[Arabic title];[Chinese title];Bienvenue aux Nations Unies;Dobro
pozhalovatÊ¹ v OON;Bienvenidos a las Naciones Unidas
United Nations. It’s your world
Cambridge Scientific Abstract

Y

CSA Illumina

1

Y

ProQuest

1

Y

Cambridge Scientific Abstracts Illumina

1

Y

CSA

5

Y

CSA;Illumina

1

Y

Illumina

1

Y

No Text Entered

8

X
X
X

1
1
1
1
1

*Ending punctuation removed since it is undetermined if the system entered the ending punctuation

Table 4.18 provides a summary of the Tables 4.14, 4.15, 4.16, and 4.17. There appears
to be greater agreement between the catalogers in the 245 field opposed to the 246 field. Part
of this may be attributed to two different reasons: (1) the 246 field is repeatable which means
that the participant could enter multiple titles, and (2) catalogers were determined to enter
foreign language titles for Item X for which there were options to enter the website using
translated versions (RDA 2.3.5).

129

Table 4.18
Summary of Variations within the 245 and 246 Fields and Subfields
Item

N

245 & 246

245

245 $a

245 $b

246

V
X
W
Y
Totals

19
18
19
19
75

13
15
15
11
54

5
10
8
3
26

3
3
8
5
19

3
3
6
0
12

7
15
12
9
43

Associations were examined between categorical variables and the text that was
recorded in the MARC records. The text was categorized into two groups; the presence of text
and no text present. There was a statistical association for 245 $b and minutes spent consulting
others, p = 0.050 and a Cramer’s Ѵ = .116, a weak effect size. There was also a statistical
association for 246 $a and years of experience cataloging ER, p = 0.007 and a Cramer’s Ѵ = .142,
a weak effect size; and minutes consulting others, p = .028 and a Cramer’s Ѵ = .214, a moderate
effect size. There was no statistical association between the other variables. Tables 4.19 and
4.20 provide a summary of the results of these tests.

130

Table 4.19
Chi-Square p-value/Fisher’s Exact Results for Titles
Chi-square p-value/Fisher’s Exact
Years of
Experience
Cataloging
.062
N/A
.492*
.223*
.933
1.000
1.000

Years of
Experience
Cataloging ER
.694
N/A
.770*
.007*
.334
.291
.251

Minutes
Spent
Creating
Record
.180
N/A
.502*
.395*
.639
.137
.350

Minutes
Consulting
Others
.386
N/A
.050*
.028*
.444
.620
.506

Minutes
Spent
Creating
Record
.170
N/A
.124
.137
.116
.155
.147

Minutes
Consulting
Others
.091
N/A
.166
.182
.108
.085
.091

Data Field
Job Title
Field_130$a
.483
Field_245$a
N/A
Field_245$b
.836
Field_246$a
.178
Field_246$b
.519
Field_246$f
1.000
Field_740$a
.478
The shaded cells indicate significance.
*Variable met Chi-square assumption; therefore, the Chi-square is reported. The p-values are based on all cases
with valid data.

Table 4.20
Cramer’s Ѵ Results for Titles
Cramer’s Ѵ

Data Field
Field_130$a
Field_245$a
Field_245$b
Field_246$a
Field_246$b
Field_246$f
Field_740$a

Job Title
.093
N/A
.077
.148
.076
.046
.109

Years of
Experience
Cataloging
.178
N/A
.105
.142
.047
.087
.035

Cramer’s V – Effect Size Categories
.000 - .099 (Negligible Association)
.100 - .199 (Weak Association)
.200 - .399 (Moderate Association)

131

Years of
Experience
Cataloging ER
.060
N/A
.049
.214
.100
.105
.096

Recording the Statement of Responsibility
The statement of responsibility is part of the title statement in MARC 245 and is
recorded in subfield $c. As defined in the RDA glossary at the time of the test, the statement of
responsibility “is a statement relating to the identification and/or function of any persons,
families, or corporate bodies responsible for the creation of, or contributing to the realization
of, the intellectual or artistic content of a resource.” Overall, the participants had general
agreement on 245 $c. However, two of the items, Q and W demonstrated the greatest amount
of difference in interpretation. Table 4.21 provides the item and number of text variations
found in the records.
Table 4.21
Variations in the Text Entered into the MARC 245 $c Subfield
Item
H
I
J
M
N
O
Q
V
W
X
Y
Totals

Number of Records
20
22
22
17
18
21
22
19
18
19
19
217

Number of Text Variations
2
3
3
4
2
2
7
5
6
2
2
39

In the analysis of Items Q and W (Table 4.22), there was agreement that Ruixin Zhang
for Item Q and lesser agreement that the National criminal Justice Reference Service for Item W
should be placed into 245 $c. Overall, much of the disagreement centered on how and to what
extent the person, corporate body and any additional information should be represented.

132

Table 4.22
Items Q and W Variations in Text Entered in MARC 245 $c
Item

Text Entered in 245$c

Q

Dr. Ruixin Zhang

Number of
times Text
was used
5

Q

Dr. Ruixin Zhang, Center for Integrative Medicine, University of Maryland School
of Medicine

10

Q

Dr. Ruixin Zhang, Center for Integrative Medicine, University of Maryland School
of Medicine ; National Cancer Institute

1

Q

1

Q
Q

Dr. Ruixin Zhang, Center for Integrative Medicine, University of Maryland School
of Medicine ; produced by National Cancer Institute
Dr. Ruixin Zhang; NCI, OCCAM.
Ruixin Zhang

Q

No Text Entered

1

W

administered by the Office of Justice Programs, U.S. Department of Justice

3

W

National Criminal Justice Reference Service

5

W

National Criminal Justice Reference Service. Administered by the Office of
Justice Programs, U.S. Department of Justice

1

W

NCJRS

1

W

NCJRS, National Criminal Justice Reference Service ; Administered by the Office
of Justice Programs, U.S. Department of Justice
No Text Entered

1

W

1
3

7

*Ending punctuation removed since it is undetermined if the system entered the ending punctuation

In Item Q, there were two types of differences noted. The first difference was whether
or not Ruixin Zhang should be preceded with the title “Dr.” The second was to what extent the
Center for Integrative Medicine, the University of Maryland, and the National Cancer Institute
should be listed, and if so, should it be spelled out or abbreviated. Only one person did not
input any text for 245 $c.
In Item W, there were three overall disagreements. The first was whether or not there
should be any information entered at all. Seven of the 19 participants determined this was not
necessary. The next disagreement was whether or not to abbreviate or spell out the text for the
National Criminal Justice Reference Service (NCJRS). Finally, there were discrepancies as to
133

whether or not the Office of Justice programs and U.S. Department of Justice should be a part
of the statement of responsibility.
There was no statistical association between the categorical variables and the text that
was recorded in the MARC record to report the presence of text and no text. Tables 4.23 and
4.24 provide a summary of the results of these tests. However, based on the Cramer’s Ѵ, there
were weak association effect levels for some of the data.
Table 4.23
Chi-Square p-value/Fisher’s Exact Results for Statement of Responsibility
Chi-square p-value/Fisher’s Exact
Years of
Experience
Cataloging
.080*
1.000
1.000

Years of
Experience
Cataloging ER
.692*
1.000
1.000

Minutes
Spent
Creating
Record
.510*
.267
.433

Minutes
Consulting
Others
.454*
.438
1.000

Minutes
Spent
Creating
Record
.123
.164
.153

Minutes
Consulting
Others
.084
.096
.060

Data Field
Job Title
Field_245$c
.363*
Field_245$f
1.000
Field_245$n
1.000
The shaded cells indicate significance.
*Variable met Chi-square assumption; therefore, the Chi-square is reported. The p-values are based on all cases
with valid data.

Table 4.24

Cramer’s Ѵ Results for Statement of Responsibility
Cramer’s Ѵ

Data Field
Field_245$c
Field_245$f
Field_245$n

Job Title
.121
.032
.032

Years of
Experience
Cataloging
.176
.061
.061

Cramer’s V – Effect Size Categories
.000 - .099 (Negligible Association)
.100 - .199 (Weak Association)
.200 - .399 (Moderate Association)

134

Years of
Experience
Cataloging ER
.058
.047
.047

Recording Publishing Information
The publishing information in the MARC record includes the place, name and dates
associated with the resource described.
The place of publication was a core element during the RDA National Test. At the time
of the test, the MARC 264 field for Production, Publication, Distribution, Manufacture, and
Copyright Notice was not in use, so the only field studied in this test relating to publication
information is the 260 field, Publication Distribution, etc. Of the 217 records, 216 participants
entered a 260 $a subfield for the place of publication. The major RDA rules that assist
catalogers in determining the text to be included for publication information include 2.82 (place
of publication), 2.8.4 (publisher’s name), 2.8.6 (date of publication), and 2.11 (copyright date);
additionally, basic instructions are also used to construct the text.
Generally, there was agreement for the place of publication for most items. For
instance, all 20 records create for Item H, an E-Monograph, had “Washington” as the place of
publication; however, four individuals added “D.C.” Rule 2.8.2.3 along with 2.8.1 and 2.8.1.4
(basic instructions), does allow for an option to, “Include the full address as part of the local
place name if it is considered to be important for identification or access.” Although this option
exists, one could interpret it to mean that full address would include more than just adding
“D.C.”
Item I, the eBook that was accessed through Feedbooks, had the greatest amount of
variance as it relates to place of publication. For the 22 records created for this item, there
were 15 different ways in which the participants entered the place of publication. Table 4.25
provides the text and the number of participants that entered the text.
135

Table 4.25
Place of Publication Entries for Item I
260 $a Text

Number of Participants Entering Text

[France?] :
[France] :
[Garches, France] :
[New York] :
[Paris, France?] :
[Paris, France] :
[Paris?] :
[Place of publication not given] :
[Place of publication not identified] :
[Salt Lake City, UT] :
[United States?] :
France :
Garches, France :
Paris, France :
Paris:

1
3
3
2
1
3
1
1
1
1
1
1
1
1
1

Based on the entered data, there was no agreement in how the place of publication
should be entered. However, 18 of 22 did use square brackets (the cataloging standard for
indicating that information was added by the cataloger based on inference) to demonstrate
that the information was not provided in the item, and that they entered the data with other
information they were able to infer. RDA 2.8.2.2 does allow the cataloger to take the place of
publication from several sources such as “another source within the resource itself” or from the
list found in 2.2.4 which includes accompanying files, descriptions published about the resource
or any other source.
The publisher’s name was also a core element for test participants. Of the 217 records,
214 of them included some form of text for the name of the publisher. Based on the records,
there was a lower level of agreement among test participants.
136

For Item J, an E-Monograph, a majority of individuals agreed that the Lois Bolk Institute
was the appropriate name for the publisher; however, there was disagreement as to which
spelling of the institute should be used. Nineteen of the 22 entries listed some version of Lois
Bolk Institute. Fourteen spelled institute as “institute” and five used the spelling “instituut.”
When consulting the resource, “Louis Bolk Instituut” appeared on the first page of the PDF
document (Figure 4.6), and on the verso of the title page, it appeared as “Lois Bolk Institute”
(Figure 4.7).

Figure 4.6: Publisher Information Listed on Cover of Item J

Figure 4.7: Publisher Information as Listed on the Verso of the Title Page

137

For Item M, there was less agreement in the name of the publisher. A total of 17 records
for this item were considered for study. With some variations of each, seven of the participants
listed the online journal name as the publisher, six listed the editor as the publisher or probable
publisher, and four could not identify a publisher name with one of them not entering any text
(Table 4.26). For seventeen records, there was a total of eight different variations of the 260 $b
including one of them that used a misspelling of the editor’s name.
Table 4.26
Publisher Names Entered for Item M
Text Entered in 260 $b
The Criterion
Criterion,
Dr. Vishwanath Bite?
Vishwanath Bite,
[Dr. Nishwanath Bite?]
Dr. Vishwanath Bite,
[publisher not identified],
No Text Entered

Number of Participants Using Text
6
1
2
2
1
1
3
1

The date of publication is an element included in the publisher statement. RDA defines
the date of publication as the publication, release, or issuing of a resource (RDA 2.8.6) and is a
core element if it is available. RDA provides guidance for the cataloger should there be no
publication date on the item. The two options are to give an approximate date or record the
phrase “date of publication not identified” instead (RDA 2.8.6.).
In the records analyzed for this study, more than a quarter of them did not include any
date in the 260 $c. Each of the records should have included a date or a notation that there was
not a date available. For the records that did include text in the subfield, there were 23
variations in how the information was represented. The descriptions in Table 4.27 are
138

representative of the types of data included in 260 $c, and all dates that were entered by
participants would fall into one of the descriptive categories. The examples are a sample of the
data that was entered by test participants to illustrate the descriptive category.
Table 4.27
Types of Information Included in the Date of Publication (MARC 260 $c)
Description of Date

Sample of Actual Text

Printed year
Unknown but probable century of publication known
Unprinted, known starting year
Unknown, but probable beginning year of resource
Unknown, but probable decade of work (with extra dash)
Unprinted, known beginning year of resource and includes
copyright date

1922
[19??]
[1955][1997?][1998?[200-?]
[2001], ©2001-

Unknown, but probable year of publication
Unknown, but probable year of publication plus copyright date
Unknown, but probable beginning year of resource
Unprinted, known beginning year of resource and includes
unprinted year of update

[1997?]
[2002?], ©2002.
[2007?-]
[2007]- [updated 2011]

Unprinted, known starting year and copyright

[2010]-, ©2010[2010-], c2010[2010], ©20101955[after 2002][not before 2002]
[before November 30, 2010][between 2002 and 2010?]
[between 2006 and 2010]
[date of publication not identified], 2010-.

Unprinted, known year and copyright date
Printed year of first issuance
Unknown, but probable date published after date
Unknown, but probable date published before date
Unknown, but probable between two dates
Unknown specific date, but probable between two dates
Unknown and no date to determine and includes copyright date
Unknown and no date to determine and includes probable
copyright date

[no publication date, ©2000]-

Unknown and no date to determine
No date of publication, but include unprinted update
No date of publication, but includes copyright statement

[Date of publication not identified].
[updated 2010]
©2006-2010, 1922.
Copyright 2000-2001.
Copyright 2010.
© 2010
© 2010-

139

Associations were examined between categorical variables and the text that was
recorded in the MARC records. The text was categorized into two groups; the presence of text
and no text present. There was a statistical association for 260 $a and job title, = 0.043 and a
Cramer’s Ѵ = .268, a moderate effect size; and the number of years of experience cataloging ER,
p = .007 and a Cramer’s Ѵ = .321, a moderate effect size. There was no statistical association
between the other variables. Tables 4.28 and 4.29 provide a summary of the results of these
tests.
Table 4.28
Chi-Square p-value/Fisher’s Exact results for publication information
Chi-square p-value/Fisher’s Exact
Years of
Experience
Cataloging
.444
.007
.968*
1.000
1.000

Years of
Experience
Cataloging ER
.542
1.000
.378*
.291
.291

Minutes
Spent
Creating
Record
.515
.171
.875*
.237
.237

Minutes
Consulting
Others
.191
.230
.067*
1.000
1.000

Data Field
Job Title
Field_260$a
1.000
Field_260$b
.043
Field_260$c
.451
Field_260$e
.335
Field_260$f
.335
The shaded cells indicate significance.
*Variable met Chi-square assumption; therefore, the Chi-square is reported. The p-values are based on all cases
with valid data.

140

Table 4.29
Cramer’s Ѵ results for publication information
Cramer’s V

Data Field
Field_260$a
Field_260$b
Field_260$c
Field_260$e
Field_260$f

Job Title
.046
.268
.116
.158
.158

Years of
Experience
Cataloging
.089
.321
.034
.087
.087

Years of
Experience
Cataloging ER
.082
.056
.095
.105
.105

Minutes
Spent
Creating
Record
.130
.162
.075
.179
.179

Minutes
Consulting
Others
.135
.094
.158
.042
.042

Cramer’s V – Effect Size Categories
.000 - .099 (Negligible Association)
.100 - .199 (Weak Association)
.200 - .399 (Moderate Association)

Recording Extent
Physical Descriptions
The physical description of a resource refers to the various characteristics of a physical
container or online resource. In RDA 3.1.1, the directions guide the cataloger to base the
description on the resource’s carrier and if necessary, other sources.
Rule 3.1.5 states, “Record online resources as the carrier type for all online resources.”
Additionally, if the resource is complete, then the completeness should be included (e.g., 1
image file, 75 pages, etc.). For the case of the electronic resources in this study, and based on
the records studied, it was generally accepted that the notations for the physical description
would be represented in this way:

141

300 _ _$a 1 online resource (75 pages)
338 _ _ $a online resource
The extent of the item would be expressed in pages, if the format is text-like (e.g., paginated) or
if it could be described by the type of file (e.g., video, audio, data files). In this case it would be
represented as “1 online resource (1 video file).” For the one streaming video recording
resource included in the test, it would be appropriate to include the duration or video
characteristics as found in RDA 7.22 which is a sub-rule of Chapter 7 which deals with
“Describing Content” of Work and Expression.

Physical Description – Extent (MARC 300 $a)
In the study, test participants generally followed the RDA guidelines to construct the
text for the 300 fields; however, there were variations within the text. RDA 3.1.5 and 3.4.1.3
provide the greatest guidance when recording the extent for online resources. Together, they
state to record the extent as “1 online resource” followed by the extent (e.g., number of pages)
if the total is known. Participants had variations that included the number of pages, duration,
number of resources, etc. There were many variations found in the text entered for Item Q, a
streaming video. Participants used terms such as computer file, online resource, streaming
video file to describe the extent. The use of “streaming video” is mentioned in RDA 3.19.2.4,
which instructs the cataloger to record the details of the file type if he/she feels it necessary for
identification or selection; however, that rule is mapped to MARC field 516, Type of Computer
File or Data Note. Participants were not consistent with the duration of the video. Some

142

entered no duration while others listed it as 36 minutes; 36 min., 4 sec; 36:04; 37 min. etc.
There were 20 variations for the 22 records studied for Item Q (Table 4.30).
Table 4.30
Text Entered for Physical Description (MARC 300 $a) for Item Q
Physical Description

Number of Participant Entries

1 computer file (36 minutes)
1 online resource (1 streaming video file) (36 minutes 4 seconds) :
1 online resource (1 streaming video file, running time 36:04) :
1 online resource (1 video file (36:04)) :
1 online resource (1 video file) (36 min, 04 sec.) :
1 online resource (1 video file) (36 min.) :
1 online resource (1 video file) :
1 online resource (1 video file, 37 min.) :
1 online resource (1 video file, 69 bytes, 36 min., 4 sec.) :
1 online resource (1 video file: 46 min.)
1 online resource (36 min.) :
1 online resource (36 min., 4 sec) :
1 online resource (37 min.) :
1 online resource :
1 streaming video (36:04)
1 streaming video
1 streaming video file (36 min.)
1 streaming video file (36 min., 4 sec.)
streaming video (36:04 minutes) :

1
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
3
1

For Item Q, 14 of the 22 did use the term online resource to describe the item, seven
used streaming video, and one used the term computer file. No rules were found stating that
streaming video or computer file could be used to describe the extent of an online resource.
When referring to the MARC Bibliographic to RDA Mapping information in the RDA
Toolkit, it references a variety of RDA rules that could be used to enter various types of
additional information such as base material, layout, sound characteristics, encoding format,
illustrative content, etc. Based on the MARC to RDA mapping, it appears that there are three
different areas that could typically be considered for inclusion in the MARC for 300 $b, Other
143

Physical Details. They are encoding format (RDA 3.19.3), illustrative content (RDA 7.15) and
color content (RDA 7.17).
In analyzing the studied records, 58.1% of the participants did not enter any detail in the
300 $b subfield. Others used a variety of terms such as PDF, HTML, sound, color, and
illustrations; illustrations was the most common term used.
In Item H, and E-Monograph, exactly half (10), of the participants included text in the $b
of the physical description. The most popular detail provided was that the resource contained
illustrations. Some also included that the resource was a PDF and/or a text file (Table 4.31).
Table 4.31
Text Entered for Other Physical Details (MARC 300 $b) for Item H
Other Physical Details
illustrations
illustrations, PDF
illustrations, text file, PDF
PDF.
text file, PDF
(No Text Entered)

Number of Participant Entries
5
1
2
1
1
10

RDA 7.15.1.1 states, “Illustrative content is content designed to illustrate the primary
content of a resource.” It further states in 7.15.1.3, “If the resource contains illustrative
content, record illustration or illustrations, as appropriate. Tables containing only words and/or
numbers are not considered as illustrative content. Disregard illustrated title pages, etc., and
minor illustrations.” The term “minor illustrations” requires cataloger judgment. Since all of the
items in this study allowed the participants to examine the entire information resource, they
could use their judgments to determine the presence of illustrations. MARC records with no

144

mention of illustrations imply that catalogers judged the illustrative matter to be minor or
absent.
Associations were examined between categorical variables and the text that was
recorded in the MARC records. The text was categorized into two groups; the presence of text
and no text present. There was a statistical association for 300 $a, Extent, and minutes spent
creating the MARC record, p = 0.038 and a Cramer’s Ѵ = .216, a moderate effect size. There was
no statistical association between the other variables. Tables 4.32 and 4.33 provide a summary
of the results of these tests.
Table 4.32
Chi-Square p-value/Fisher’s Exact Results for Extent of Item
Chi-square p-value/Fisher’s Exact
Years of
Experience
Cataloging
.190*
.289*

Years of
Experience
Cataloging ER
.370*
.659*

Minutes
Spent
Creating
Record
.038*
.546*

Minutes
Consulting
Others
.143*
.660*

Data Field
Job Title
Field_300$a
.188
Field_300$b
.606*
The shaded cells indicate significance.
*Variable met Chi-square assumption; therefore, the Chi-square is reported. The p-values are based on all cases
with valid data.

145

Table 4.33
Cramer’s Ѵ Results for Extent of Item
Cramer’s Ѵ

Data Field
Field_300$a
Field_300$b

Job Title
.148
.092

Years of
Experience
Cataloging
.148
.132

Years of
Experience
Cataloging ER
.096
.062

Minutes
Spent
Creating
Record
.216
.119

Minutes
Consulting
Others
.134
.062

Cramer’s V – Effect Size Categories
.000 - .099 (Negligible Association)
.100 - .199 (Weak Association)
.200 - .399 (Moderate Association)

Content, Media and Carrier Types and Characteristics (MARC 33x)
In AACR2, the General Material Designation (GMD) was recorded as a part of the Title
and Statement of Responsibility; however, in RDA, this practice was abandoned and the old
GMD is redefined to meet the growing demands of the digital environment. The GMD was
more of a description of an item’s format, such as a video recording. In today’s digital
environment, a video recording could be a DVD, an MP4 file, streaming video, etc. For this
reason a new structure was created to provide additional detail on content (6.9), media (3.2),
and carrier types (3.3). A controlled vocabulary has been created for these types and is included
in RDA rules 6.9.1.3, 3.2.1.3, and 3.3.1.3.

Content, Media and Carrier Types (336, 337, 338)
RDA defines content type as, “... categorization reflecting the fundamental form of
communication in which the content is expressed and the human sense through which it is
intended to be perceived... (RDA 6.9.1.1)” For the electronic resource items in this study, most
146

of the resources would be considered “text” as one content type since they were textual in
nature in order to understand the content. However, Item Q was a streaming video, so it would
be considered a “two-dimensional moving image.” In the analysis of the records in the study,
other types were included such as computer dataset, computer program, and text along with
other content types (i.e., Text and still image) (Table 4.34).
Table 4.34
Text Entered for Content Type (MARC 336 $a) for All Items
Content Type Description

Number of Participant Entries

computer dataset
computer program
other
text
text, still image(s)
text;other
text;still image;spoken word;two-dimensional moving image
text;still image;two-dimensional moving image
text;still image;two-dimensional moving image;spoken
word;cartographic image

1
2
1
180
5
1
1
1
1

text;two-dimensional moving image;sounds;spoken word;still image

1

text;two-dimensional moving image;spoken word
text;video;audio
two dimensional moving image
two-dimensional moving image
two-dimensional moving image;spoken word
two-dimensional moving image;still image

1
1
2
16
2
1

RDA defines media type as the, “... categorization reflecting the general type of
intermediation device required to view, play, run, etc., the content of a resource” (3.2.1.1). For
the electronic resource items in this study, the resource needed would be a computer since all
of these resources were created to require some type of computer device to view them.

147

However, a few listed something other than the computer. The one item that had the most
disagreement was Item Q, the streaming video item, (Table 4.35).
Table 4.35
Text Entered for Media Type (MARC 337 $a) for All Items
Media Type Description

Number of Participant Entries

computer
video
computer ; video
other
text
projected
unmediated

199
6
4
3
3
1
1

RDA defines content type as the, “categorization reflecting the format of the storage
medium and housing of a carrier in combination with the type of intermediation device
required to view, play, run, etc., the content of a resource” (3.3.1.1). For the electronic resource
items in this study, most of the resources would be considered “online resources” as the format
appeared as a resource that was posted on the Web. The participants overwhelmingly agreed
with this description, 96.8% of them chose “online resource” as the carrier type for these
resources. However, Item Q was a streaming video, so it could be considered a “twodimensional moving image.” In the analysis of the records in the study, other types such as
computer dataset, computer program, and text along with other content types (i.e., text and
still image) was included (Table 4.36).

148

Table 4.36
Text Entered for Content Type (MARC 338 $a) for All Items
Carrier Type Description
online resource
computer
computer carriers
online resource ; other video carrier
volume

Number of Participant Entries
210
3
1
1
1

Associations were examined between categorical variables and the text that was
recorded in the MARC records. The text was categorized into two groups; the presence of text
and no text present. There was a statistical association for 336 $b, Content Type Code, and job
title, p = 0.010 and a Cramer’s Ѵ = .219, a moderate effect size; and the number of minutes
spent creating the MARC record, p = .010 and a Cramer’s Ѵ = .256, a moderate effect size. There
was also a statistical association for 337 $b, Media Type Code, and job title, p = 0.010 and a
Cramer’s Ѵ = .219, a moderate effect size; and the number of minutes spent creating the MARC
record, p = .010 and a Cramer’s Ѵ = .256, a moderate effect size. There was a statistical
association for 338 $b, Carrier Type Code, and job title, p = 0.010 and a Cramer’s Ѵ = .219, a
moderate effect size; and the number of minutes spent creating the MARC record, p = .010 and
a Cramer’s Ѵ = .256, a moderate effect size. There was no statistical association between the
other variables. Tables 4.37 and 4.38 provide a summary of the results of these tests.

149

Table 4.37
Chi-Square p-value/Fisher’s Exact results for Content, Media, and Carrier Types
Chi-square p-value/Fisher’s Exact
Years of
Experience
Cataloging
N/A
.256
N/A
N/A
.256
N/A
.447
.256
.447

Years of
Experience
Cataloging ER
N/A
.242
N/A
N/A
.242
N/A
1.000
.242
1.000

Minutes
Spent
Creating
Record
N/A
.010
N/A
N/A
.010
N/A
.659
.010
.659

Minutes
Consulting
Others
N/A
.093*
N/A
N/A
.093*
N/A
1.000
.093*
1.000

Minutes
Spent
Creating
Record
N/A
.256
N/A
N/A
.256
N/A
.126
.256
.126

Minutes
Consulting
Others
N/A
.148
N/A
N/A
.148
N/A
.060
.148
.060

Data Field
Job Title
Field_336$a
N/A
Field_336$b
.010
Field_336$2
N/A
Field_337$a
N/A
Field_337$b
.010
Field_337$2
N/A
Field_338$a
1.000
Field_338$b
.010
Field_338$2
1.000
The shaded cells indicate significance.
*Variable met Chi-square assumption; therefore, the Chi-square is reported. The p-values are based on all cases
with valid data.

Table 4.38
Cramer’s Ѵ Results for Content, Media, and Carrier types
Cramer’s Ѵ

Data Field
Field_336$a
Field_336$b
Field_336$2
Field_337$a
Field_337$b
Field_337$2
Field_338$a
Field_338$b
Field_338$2

Job Title
N/A
.219
N/A
N/A
.219
N/A
.032
.219
.032

Years of
Experience
Cataloging
N/A
.120
N/A
N/A
.120
N/A
.126
.120
.126

Cramer’s V – Effect Size Categories
.000 - .099 (Negligible Association)
.100 - .199 (Weak Association)
.200 - .399 (Moderate Association)

150

Years of
Experience
Cataloging ER
N/A
.114
N/A
N/A
.114
N/A
.047
.114
.047

Notes
General Notes (MARC 500)
By the nature of the MARC 500 field, these can be general notes or notes specific to
previously entered information such as title, publication, layout, extent, changes in content
characteristics, etc. For this reason, the general note field was the most complex field to
analyze. This is not only due to the broad nature of the types of information but it was also due
to the variety of free text that participants entered.
It was necessary to first determine which type of note was present for each resource
and the rule associated with it, and then attempt to classify based on the meaning of the note,
which required revisiting the various items to gather meaning.
It was observed that 18 types of notes were recorded by participants in the MARC 500
field. There were also 68 records in which no notes were included. Table 4.39 provides the
note types and the frequency in which they occurred.

151

Table 4.39
Note Types and the Frequency of Types Utilized in MARC 500 $a
Rule Number and Note Rule Name

Frequency

2.20.2 – Note on Title
2.20.3 – Note on Statement of Responsibility
2.20.7 - Note on Publication Statement
2.20.10 – Note on Copyright Date
2.20.13.3
2.20.13.4 – Note on Iteration Used as the Basis for the Identification of an Integrating Resource
2.20.13.5 – Note on Date of Viewing of an Online Resource
3.19 – Note on Encoding Format
3.20 – Note on Equipment or System Requirements
7.2 - Notes (General)
7.14 – Note on Accessibility Content
7.15 – Note on Illustrative Content
7.16 – Note on Supplementary Content

81
9
34
8
3
7
83
11
2
22
8
1
5

7.19 – Note on Aspect Ratio
7.22 – Note on Duration
25.1 – Note on Related Work
27.1 – Note on Related Manifestation
Miscellaneous Note (No Rule Found)
Provided No Notes

1
1
1
10
21
68

Associations were examined between categorical variables and the text that was
recorded in the MARC records. The text was categorized into two groups; the presence of text
and no text present. There was a statistical association for 500 $a, General Note, and total years
of experience cataloging, p < 0.001 and a Cramer’s Ѵ = .277, a moderate effect size. There was
also a statistical association for 500 $a utilizing RDA rule 2.20.2 and total years of experience
cataloging, p = .025 and a Cramer’s Ѵ = .208, a moderate effect size; and the number of years
cataloging ER, p = .010 and a Cramer’s Ѵ = .207, a moderate effect size. There was also a
statistical association for 500 $a utilizing RDA rule 2.20.13.5 and total years of experience
cataloging, p = .008 and a Cramer’s Ѵ = .234, a moderate effect size; and the number of minutes
152

spent creating the MARC record, p = .042 and a Cramer’s Ѵ = .214, a moderate effect size. There
was also a statistical association for 500 $a utilizing RDA rule 3.19 and total years of experience
cataloging, p < .001 and a Cramer’s Ѵ = .230, a moderate effect size; and the number of minutes
spent consulting others, p = .017 and a Cramer’s Ѵ = .210, a moderate effect size. There was
also a statistical association for 500 $a utilizing RDA rule 3.20 and job title, p = .033 and a
Cramer’s Ѵ = .254, a moderate effect size, and 500 $a utilizing RDA rule 25.1 and total years of
experience cataloging, p = .041 and a Cramer’s Ѵ = .327, a moderate effect size. There was no
statistical association between the other variables. Tables 4.40 and 4.41 provide a summary of
the results of these tests.

153

Table 4.40
Chi-Square p-value/Fisher’s Exact Results for the General Notes Field
Chi-square p-value/Fisher’s Exact

Data Field
Field_500$a
Field_500_9999
Field_500$a_2.20.2
Field_500$a_2.20.3
Field_500$a_2.20.7
Field_500$a_2.20.10

Job Title
.576
1.000
.215
1.000
.690
.452

Years of
Experience
Cataloging
.001*
1.000
.025*
.631
.947*
.208

Years of
Experience
Cataloging ER
.624*
.778
.010*
.877
.688*
.057

Minutes
Spent
Creating
Record
.401*
.797
.302*
.679
.268*
.294

Minutes
Consulting
Others
.065*
.637*
.169*
.341
.237*
.203

Field_500$a_2.20.13.3

1.000

.655

.071

.609

.099

Field_500$a_2.20.13.4

.765

.581

.307

1.000

.608

Field_500$a_2.20.13.5
.380
.008*
.451*
.042*
.079*
Field_500$a_3.19
.061
.001
.639
.212
.017
Field_500$a_3.20
.033
.113
1.000
.137
1.000
Field_500$a_7.2
.154
.517
.358
.114
.847*
Field_500$a_7.14
.258
.786
1.000
.205
.244
Field_500$a_7.15
1.000
.221
.138
.433
1.000
Field_500$a_7.16
1.000
.583
.126
.928
1.000
Field_500$a_7.19
.184
1.000
1.000
.659
.438
Field_500$a_7.22
1.000
.447
1.000
.433
.438
Field_500$a_25.1
1.000
.041
1.000
.433
.101
Field_500$a_27.1
.325
.139
.342
.986
.088
The shaded cells indicate significance.
*Variable met Chi-square assumption; therefore, the Chi-square is reported. The p-values are based on all cases
with valid data.

154

Table 4.41
Cramer’s Ѵ Results for the General Notes Field
Cramer’s Ѵ

Data Field
Field_500$a
Field_500_9999
Field_500$a_2.20.2
Field_500$a_2.20.3
Field_500$a_2.20.7
Field_500$a_2.20.10

Job Title
.100
.038
.148
.099
.081
.084

Years of
Experience
Cataloging
.277
.029
.208
.100
.041
.131

Years of
Experience
Cataloging ER
.066
.056
.207
.024
.059
.147

Minutes
Spent
Creating
Record
.136
.086
.150
.101
.155
.132

Minutes
Consulting
Others
.159
.064
.128
.089
.115
.104

Field_500$a_2.20.13.3

.056

.077

.183

.122

.166

Field_500$a_2.20.13.4

.077

.118

.081

.045

.050

Field_500$a_2.20.13.5
Field_500$a_3.19
Field_500$a_3.20
Field_500$a_7.2
Field_500$a_7.14
Field_500$a_7.15
Field_500$a_7.16
Field_500$a_7.19
Field_500$a_7.22
Field_500$a_25.1
Field_500$a_27.1

.123
.230
.254
.157
.143
.032
.073
.241
.032
.032
.069

.234
.380
.226
.086
.069
.145
.095
.061
.126
.327
.177

.086
.057
.067
.088
.033
.170
.120
.047
.047
.047
.107

.214
.146
.155
.194
.163
.153
.099
.126
.153
.153
.062

.149
.210
.042
.039
.125
.060
.052
.096
.096
.203
.148

Cramer’s V – Effect Size Categories
.000 - .099 (Negligible Association)
.100 - .199 (Weak Association)
.200 - .399 (Moderate Association)

Notes in Other Fields (MARC 538 and 588)
The 538 MARC field is used to include any system or equipment requirements needed in
order to technically access or use a resource. This was not a core element of the test, yet 40.1%,
or 87 of the respondents, entered some type of system note. Most of the time, the text of a
155

538 note will begin with “System requirements” or “Requires.” In the test, 19 notes started the
note statement with “System requirements” or “Requires,” 57 began the note using “Mode of
access,” and 11 started with something different (e.g., HTM, PDF, Streaming video, etc.).
While analyzing the results of this field, RDA was consulted to gain a better
understanding about the acceptable forms of entry (Figure 4.8). It was noted that “Mode of
access” is not listed in RDA as an acceptable entry for the 538 MARC field.

Figure 4.8: Rule for Equipment or System Requirements
156

MARC 588, the source of description note, is used to describe what specific resource
was used to complete the cataloging record. This field is most useful when describing serials
and integrating resources since there are multiple iterations or issues of the work possible.
Typically the note will begin with “Identification of the resource based on...” or “Description
based on...” The statement will most often include the date in which the resource was viewed.
This is especially important for online resources since the resource is more easily editable.
In the RDA National test, 109 (50.2%) of the respondents entered a 588 field to the
record. Of these, 88 of them started with “Identification of the resource based on...” or
“Description based on...”; 16 began with “Title from...”; 7, “Last issue consulted...”; 4, “Viewed
on...”; and 1 started with “Earlier title proper...”
Associations were examined between categorical variables and the text that was
recorded in the MARC records. The text was categorized into two groups; the presence of text
and no text present. There was a statistical association for 588 $a utilizing RDA rule 2.20.13.5
and job title, p = 0.034 and a Cramer’s Ѵ = .200, a moderate effect size; and the total years of
experience cataloging, p = .029 and a Cramer’s Ѵ = .203, a moderate effect size. There was no
statistical association between the other variables. Tables 4.42 and 4.43 provide a summary of
the results of these tests.

157

Table 4.42
Chi-Square p-value/Fisher’s Exact Results for Other Notes Fields
Chi-square p-value/Fisher’s Exact

Data Field
Field_538$a
Field_588$a
Field_588$a_2.20.2.3

Job Title
.595
.561*
.920

Years of
Experience
Cataloging
.423*
.188
.455*

Years of
Experience
Cataloging ER
.914*
.153*
.933*

Minutes
Spent
Creating
Record
.138*
.876*
.497*

Minutes
Consulting
Others
.121*
.979*
.139*

Field_588$a_2.20.13.3.1

.882

.217*

.799*

.701*

.987*

Field_588$a_2.20.13.4

.147

.240*

.084*

.085*

.583*

Field_588$a_2.20.13.5
.034*
.029*
.650*
.186*
.855*
The shaded cells indicate significance.
*Variable met Chi-square assumption; therefore, the Chi-square is reported. The p-values are based on all cases
with valid data.

Table 4.43
Cramer’s Ѵ Results for Other Notes Fields
Cramer’s Ѵ

Data Field
Field_538$a
Field_588$a
Field_588$a_2.20.2.3

Job Title
.097
.097
.062

Years of
Experience
Cataloging
.114
.148
.110

Years of
Experience
Cataloging ER
.029
.132
.025

Minutes
Spent
Creating
Record
.179
.075
.125

Minutes
Consulting
Others
.139
.014
.135

Field_588$a_2.20.13.3.1

.039

.143

.045

.100

.011

Field_588$a_2.20.13.4

.164

.139

.151

.194

.071

Field_588$a_2.20.13.5

.200

.203

.063

.169

.038

Cramer’s V – Effect Size Categories
.000 - .099 (Negligible Association)
.100 - .199 (Weak Association)
.200 - .399 (Moderate Association)

158

Electronic Location and Access (MARC 856 $u)
The MARC 856 field is used to provide the Uniform Resource Locator (URL) for the
resource being described. The URL is the web address where the online resources are able to be
accessed. A large majority, 89.4% of the respondents, included a URL in the 856 field. Twentythree participants did not include one at all.
Of those that did include a URL, it was strange that there were several variations of the
URL represented. Item V, an integrating E-Resource, had the greatest number of variations
(Table 4.44). Fourteen respondents entered one URL, two respondents entered two, and one
did not include any URL at all. In the test surrogate (Appendix B) several of the electronic
resources had two different URLs, and Item V was one of them. The two individuals that listed
two URLs for Item V included both addresses. It is interesting that one of the variations below
begins with http://http://. It is unknown if the participant entered this by mistake or if the
cataloging entry system automatically adds http:// to any URL entered.
Table 4.44
Frequency of URLs Entered in MARC 856 $U for Item V
URLs Entered in 856 $u for Item V

Frequency

http://ccr.nci.nih.gov/research/research_directory.asp
http://ccr.nci.nih.gov/research/research_directory.asp;http://web.archive.org/web/200211082
24555/http://ccr.nci.nih.gov/research/annual_research_dir.asp

11
2

http://ccr.ncifcrf.gov/research/research_directory.asp
http://http://ccr.nci.nih.gov/research/research_directory.asp
http://web.archive.org/web/20021108224555/http://ccr.nci.nih.gov/research/annual_research
_dir.asp

1
1
1

No URL Listed

1

159

Associations were examined between categorical variables and the text that was
recorded in the MARC records. The text was categorized into two groups; the presence of text
and no text present. There was a statistical association for 856 $u, Uniform Resource Identifier,
and total years of experience cataloging, p = 0.025 and a Cramer’s Ѵ = .235, a moderate effect
size; years of experience cataloging ER, p = .024 and a Cramer’s Ѵ = .164, a weak effect size; and
minutes spent creating MARC record, p = 0.014 and a Cramer’s Ѵ = .231, a moderate effect size.
There was no statistical association between the other variables. Tables 4.45 and 4.46 provide a
summary of the results of these tests.
Table 4.45
Chi-Square p-value/Fisher’s Exact Results for Electronic Location and Access
Chi-square p-value/Fisher’s Exact
Years of
Experience
Cataloging
.025

Years of
Experience
Cataloging ER
.024

Minutes
Spent
Creating
Record
.014

Minutes
Consulting
Others
.428*

Minutes
Spent
Creating
Record
.231

Minutes
Consulting
Others
.088

Data Field
Job Title
Field_856$u
.076
The shaded cells indicate significance.
*Variable met Chi-square assumption; therefore, the Chi-square is reported. The p-values are based on all cases
with valid data.

Table 4.46
Cramer’s Ѵ Results for Electronic Location and Access
Cramer’s Ѵ

Data Field
Field_856$u

Job Title
.181

Years of
Experience
Cataloging
.235

Cramer’s V – Effect Size Categories
.000 - .099 (Negligible Association)
.100 - .199 (Weak Association)
.200 - .399 (Moderate Association)

160

Years of
Experience
Cataloging ER
.164

Summary of p-value with Significance
There were a total of 34 subfields in which the Chi-square/ Fisher’s Exact p < .05. For
these 34 subfields, it was determined that the differences among the groups were great
enough to be considered significant. Tables 4.47, 4.48, 4.49, 4.50, and 4.51 provide the
breakdowns of the categorical groups and a summary of the residual value of the expected
results for those subfields that demonstrated a significant value (p < .05). The residual is the
difference between the expected number of occurrences that text would be present and the
actual. For the purpose of this analysis, if the residual value is between -0.9 and 0.9, then the
value will be considered that it met the expectation. If the value is greater than or equal to 1.0
then the result is considered as greater than expected, and if the residual value is less than or
equal to -1.0 the result is considered as less than expected.
In the categorical groups represented for job title, the librarian category had two of the
six residual values that exceeded more than expected and four of the six less than expected. For
the job category of “other” three of the six subfields had a greater number of individuals that
included text, one of the six had less than expected, and the remaining two as expected. The
paraprofessionals had four out of the six subfields that had expected results greater than
expected, zero that were less than expected, and two as expected. Finally, the student group
had none greater than expected, two that were less than expected, and four that were as
expected. Table 4.47 provides a summary of these results.

161

Table 4.47
Frequency of Expected Results for Job Title
Categorical Group

Total Number of
Subfields p < .05

Job Title
Librarian
Other
Paraprofessional
Student

6

Number of Expected Results with p-value Determined Significant
Greater than Expected
2
3
4
0

Less than Expected
4
1
0
2

Met Expectation
0
2
2
4

For the categorical groups represented for total years of experience cataloging, the 0 – 3
years of experience category had three of the ten residual values that were exceeded than
expected and five of the ten lower than expected, and two that were as expected. For those
with 3 – 6 years of experience, three of the ten subfields had a greater number of individuals
that included text, two of the ten less than expected, and the remaining five as expected. For
those with 6 – 22 years of experience, seven out of the ten subfields that had greater than
expected results, two that had less than expected and one as expected. Finally, those with
greater than 22 years of cataloging experience, there was one of the ten subfields in which they
exceeded the number expected, five of the ten that were lower than expected, and four in
which was as expected. Table 4.48 provides a summary of these results.

162

Table 4.48
Frequency of Expected Results for Total Years of Cataloging Experience
Categorical Group
Total Years of
Experience
Cataloging
0 – 3 Years
3 – 6 Years
6 – 22 Years
22+ Years

Total Number of
Subfields p < .05
10

Number of Expected Results with p-value Determined Significant
Greater than Expected

Less than Expected

Met Expectation

3
3
7
1

5
2
2
5

2
5
1
4

For the categorical groups represented for total years of experience cataloging
electronic resources, the 0 – 3 years of experience category had two of the six residual values
that were exceeded, and four of the six that were lower than expected. For those with 3 – 6
years of experience, two of the six subfields had a greater number of individuals that included
text, three of the six less than expected, and the remaining one as expected. For those with 6 or
more years of experience, two out of the six subfields had greater than expected results, three
that had less and one as expected. Table 4.49 provides a summary of these results.
Table 4.49
Frequency of Expected Results for Total Years of Cataloging Experience ER
Categorical Group
Years of Experience
Cataloging ER
0 – 3 Years
3 – 6 Years
6+ Years

Total Number of
Subfields p < .05
6

Number of Expected Results with p-value Determined Significant
Greater than Expected

Less than Expected

2
2
2

4
3
3

Met
Expectation
0
1
1

For the categorical groups represented for number of minutes spent cataloging the
record, those that spent between 0 and 30 minutes had three of the seven residual values that
163

were higher than expected, and four of the seven lower than expected. For those that spent 31
– 60 minutes creating the record, one of the seven subfields had a greater number of
individuals that included text and six of the seven less than expected. For those that spent 61 –
91 minutes creating the record, four out of the seven subfields that had greater than expected
result and three that were less than expected. For those that spent 91 – 120 minutes creating
the record, six of the seven had a greater value than expected, and one was as expected.
Finally, those who spent 121 – 300 minutes creating the record, there were two of the seven
subfields in which they exceeded the number expected, two of the seven that they were lower
than expected, and three in which was as expected. Table 4.50 provides a summary of these
results.
Table 4.50
Frequency of Expected Results for Total Years Cataloging Experience
Categorical Group

Total Number of
Subfields p < .05

Minutes Spent
Creating Record
0 – 30 Minutes
31 – 60 Minutes
61 – 90 Minutes
91 – 120 Minutes
121 – 300 Minutes

7

Number of Expected Results with p-value Determined Significant
Greater than Expected

Less than Expected

Met Expectation

3
1
4
6
2

4
6
3
0
2

0
0
0
1
3

For the amount of time spent consulting others, the subgroup that did not consult
anyone at all, had residual values that were less than expected. For those that consulted others
between 1 and 30 minutes, four of the five subfields had a greater number of individuals that
included text and one of the five was as expected. For those that consulted with other 31

164

minutes or more, two of the five were greater than expected, three of the five were lower, and
none were as expected. Table 4.51 provides a summary of these results.
Table 4.51
Frequency of Expected Results for Total Years Cataloging Experience ER
Categorical Group

Total Number of
Subfields p < .05

Minutes Spent
Consulting Others
Did Not Consult Others
1 – 30 Minutes
31 + Minutes

5

Number of Expected Results with p-value Determined Significant
Greater than Expected

Less than Expected

Met Expectation

0
4
2

5
0
3

0
1
0

Conclusion
Based on the observations of the data, there are many variations to the data and each
record is unique. The question that is left unanswered is whether or not there are significant
differences between groups. Chapter 5 will discuss how this data analysis was used to address
the research questions.

165

CHAPTER 5
FINDINGS
Introduction
This study used existing data from the National Libraries test on RDA, a new content
standard for cataloging. The data included 11 of the items cataloged from the common original
set of resources that each of the 26 institutions were instructed to catalog. Through the analysis
of the data, important results were revealed about cataloger’s judgment and the text that was
entered within the MARC records. These findings assisted in answering the research questions,
although not necessarily to the level I had hoped. This chapter restates the problem statement;
the major findings including the research questions posed, further discussion about how the
data analysis answers the questions, future research projects to be considered, and
implications and recommendations for the cataloging community.
Restatement of the Problem
As stated in Chapter 1, the problems of this study address cataloger’s judgment—a topic
rarely studied, but often referred to when describing how a cataloger enters text into a MARC
record. According to Cutter, the library catalog should serve the convenience of the user, and
therefore, catalogers should exercise some judgment as to the information included in the
library catalog. Bounded rationality explains judgment making as a series of decisions based on
the constraints of cognitive ability and time of the decision maker.
In this study, I sought to analyze the decisions catalogers make within the constraints of
time and cognitive ability. The data analyzed for this study comes from the common set of
MARC records created by the formal test group of the RDA national test. As previously stated in
166

this study, the testing procedures for the formal test of RDA required each cataloging
organization to construct original cataloging records for the same set of items; this created a
great opportunity to study the similarities and differences in how cataloging staff interpret
cataloging rules.
Major Findings
This study is based on two research questions. The first research question was very
broad and for this reason two sub-questions were asked to limit the scope in order to analyze
specific groups and the constraints they had during the RDA national test. Since the subquestions are interrelated, they are grouped with the main question for this discussion of the
major findings.
RQ1 How did catalogers participating in the Resource Description and Access (RDA) National
Test exercise cataloger’s judgment as they created RDA-based MARC records for
electronic resources?
1a: What are the similarities and differences of the records?
1b: To what extent can the differences in text entered in the records be explained by
differences in characteristics of the catalogers (e.g., level of position, experience,
prior course work and/or training, etc.)?
RQ2 How can cataloger’s judgment be explained through the lens of Bounded Rationality?
2a: How can cataloger’s judgment be predicted using the constructs of bounded
rationality?

167

Research Question 1
To answer question 1, how catalogers participating in the Resource Description and
Access (RDA) National Test exercise cataloger’s judgment as they created RDA-based MARC
records for electronic resources, the MARC records and the survey instruments collected by the
RDA National Test were analyzed for significant differences. Based on the preliminary informal
look at the data for one of the non-electronic resource items, it was determined that there
were differences in the text or lack of text entered into the MARC fields. These preliminary
findings also raised questions if these differences were based on categorical differences
between catalogers (experience levels, job titles, and time spent creating the MARC record and
consulting others).
In the formal study, there were only five instances of unanimous agreement out of the
73 subfields studied where text should be present in a specific subfield (Title Statement, 245 $a;
Content Type, 336 $a and $2; and Media Type, 337 $a and $2).
Although there was agreement found in these five subfields, it is important to note that
there was less agreement on the actual text that was entered into the subfield for Title
Statement, 245 $a, Title. For instance, there were 19 variations of the 245 $a for 75 records
submitted for items V, W, X, and Y. Item W alone has eight variances.
When stepping back and looking at the elements relating to titles in subfields 245 $a,
Title, 245 $b, Remainder of Title and Varying Form of Title, 246 $a, there was even greater
disagreement. For items V, W, X and Y, 75 records were analyzed and there were a total of 54
variations across all three of these subfields. Greater agreement could be found within a single
subfield, such as 245 $b, Remainder of Title, that had 12 variations among the 75 records. This
168

was not a unique phenomenon limited to just this example, but across all items and cataloging
elements.
Question 1a, examined the similarities and differences of the records and Question 1 b,
extended the question to ask to what extent the differences in text entered in the records could
be explained by differences in characteristics of the catalogers (e.g., level of position,
experience, prior course work and/or training, etc.)? This was answered using Chisquare/Fisher's Exact tests to determine associations between groups. Due to the small sample
size per record, and average of 19.72 records per item, the Chi-square test was not applicable
for a majority of the tests due to not meeting the assumption of having at least 5 records for
each cell in 80% of the crosstabs. Only 104 of the 365 tests performed (28.49%) were able to be
analyzed using Chi-square. The remaining 261 records were analyzed using the Fisher's Exact
test. Thirty-four of the 365 tests (9.31%) resulted in significant associations.
In addition to the Chi-square and Fisher's Exact tests, the Cramer's Ѵ, a post-test, was
performed to determine the effect size of the associations. The post-test determined that 29 of
the 34 had a moderate effect size and five of the significant associations had a weak effect size.
The Cramer's Ѵ is independent of the Chi-Square p-value; however, the Cramer's Ѵ does tend to
correlate with the Chi-square p-value. For this reason it is important to look at the Cramer's Ѵ
results for non-significant associations as well. Four of non-significant associations show a
moderate effect size and 185 of the non-significant associations demonstrated a weak effect
size. Even though these did not yield a significant association based on the Chi-square/Fisher's
Exact test, it is important to note that sample size for Chi-square has a great influence on the
result for significance. Therefore, although a majority of tests were not found to have
169

significant associations, the post-test does demonstrate that with a larger sample, the result
could be far different. Although question 1a, what are the similarities and differences of the
records?, cannot be answered to full certainty, the data does support that there are differences
in the presence of text among the groups.

Research Question 2
To answer the second research question, how can cataloger’s judgment be explained
through the lens of Bounded Rationality? and its sub question, how can cataloger's judgment
be predicted using the constructs of bounded rationality?, a predictive test such as logistical
regression would need to performed. Unfortunately, using quantitative statistics to answer this
question was not possible due to the size of the data set.
Since logistic regression could not be completed due to the sample size, the Chisquare/Fisher's Exact results are used to describe some trends that were emerging from the
test data. In chapter four the expected results of the crosstabs were compared with the
residual values. This analysis provides a window into whether or not the theory of bounded
rationality supports the decision making among catalogers as they make their determination of
whether to include text or not.
According to bounded rationality, decisions are made within the constructs of cognitive
ability and time. The assumption would be that the greater cognitive ability and more time
spent on a decision would result in better judgment. It was not the intention of this study to
determine if the best judgments were made, but rather to study how different groups entered
text or not. It could also be assumed that those with greater cognitive ability would require less
170

time to make decisions since they would have greater knowledge of the rules. However, with a
new standard, prior knowledge could be a barrier as these catalogers may hold onto previous
truths.
Based on the analysis in chapter 4, this assumption does not hold true all of the time
since it does not hold true that the librarian, those with 22 or more years of experience
cataloging, those that have more than six years of cataloging electronic resources, those that
spent the most time cataloging records, and those that spend more time consulting others,
results in a greater amount of text present. Based on the analysis, the groups that exhibited the
greatest positive amount of residual value were the paraprofessional, those with 6 - 22 years of
experience, those that spent 91 - 120 minutes creating the catalog record, and those that spent
1 - 31 minutes consulting others. For the category of experience cataloging electronic
resources, there was not one single group that stands out as exceeding the expected amount.
Based on these results, it appears that the theory of bounded rationality does not completely
support the assumptions proposed in this study.
Results indicate that bounded rationality does not support the phenomena of
cataloger's judgment, but it is important to note that RDA is a new standard, and this study did
not cover the levels, types and quality of training each group received or the quality of the
records that were created. It is too early to determine if in fact bounded rationality can or
cannot explain cataloger's judgment.
Extending beyond the research questions posed for this study, the most significant
finding that was not anticipated, or I did not know to question at the start of this study, was
how participants would determine the preferred source of information. Through the analysis of
171

the data, I repeatedly found that the differences in cataloger's judgment had to do with the
preferred source(s) of information referred to in order to enter text. In most cases, I was able to
identify why catalogers determined the text they entered based on the source of information
they were using. This process was done by going back to the actual item and comparing the text
the cataloger entered and then searching to see if I could identify the source of information the
user may have used to assign the text they entered. One of the examples provided in Chapter 4
was for the E-Monograph Item J. The cover and title page of the document listed Ton Baars as
the author; however, on the title page verso, under the CIP-Data from Koninklijke Bibliotheek,
the author was listed as Baars, T.
RDA has moved away from the chief source of information in favor of a preferred source
with several options. It is my educated guess that test participants perceive these options to be
weighted the same without regard to an order of preference. The long-term effects of this
could be dire for cataloging as it will create a system of irregularities that later becomes a
disservice to our users as they search for various resources. An example of this would be if
there are variations of creator names, titles, or other types of controlled vocabularies; the
inconsistencies would require the user to use various terms in order to find all of the items that
should have been collocated together during a user’s search. This would also make it difficult
for systems to use linked data, as proposed in BIBFRAME.

Further Discussion
How titles were described was the focus of the major findings discussed above;
however, there were other elements that also provided the ability to make inferences as to the
172

outcome of the research questions. This section will discuss how the other elements have
contributed to the findings in this study.

Names
There was significant disagreement among the participants in how the element should
be expressed and what MARC field was to be used. It can be inferred, but cannot be proven,
that the amount of variance is based on the fact that because recording family names does not
occur as often as individual names, the test participants had less experience, and thus less
certainty of how to represent the access point in describing a family name.

Statement of Responsibility
There was not general agreement for the statement of responsibility for Item Q, the
streaming video resource. Participants included a variety of information such as the name of
the presenter, the name of the presenter and their affiliation, or in some case no information at
all. From what I observed and the rules that I consulted, it appears that there was a lack of
understanding of what was the preferred source of information to describe this element.

Publication Information
The area of the most apparent disagreement in the publication information is the place
of publication for Item I, the eBook. Some included brackets for place of publication, others did
not; some determined the place of publication to be in France, New York, or Salt Lake City;
some respondents entered text to let the user know that the location of publication was
173

unknown, while others did not include any information. When studying the actual resource, I
could tell why some users determined what they entered, and as in the statement of
responsibility, there was a lack of consensus on what the preferred source of information
should be.

Extent
Generally, participants agreed that "1 online resource" should be recorded in Physical
Description field 300$a; however, the amount of information that should be included for Item
Q, the streaming video, varied greatly. Some used "1 online resource" while others used a
variation of the term streaming video. There was also disagreement in how the time should be
recorded. There was a lack of agreement on whether to abbreviate minutes, include seconds,
or to include the term video or streaming video after online resource. Anecdotal evidence
suggests that experienced catalogers often times "memorize" based on previous practice. This
may be the best way to explain the variances in the recording of the extent.

Notes
While analyzing the results of this field, RDA guidelines were consulted to gain a better
understanding about the acceptable forms of entry. It was noted that "Mode of access" is not
listed in RDA as an acceptable entry for the System Details Note 538 MARC field; however,
upon further investigation, it appears that in the previous practice (under AACR2) this was an
acceptable entry. It can be inferred that these participants were accustomed to this practice in
AACR2 and have carried it over to the RDA-based records regardless of its absence in RDA.
174

Therefore, prior experience or possibly a lack of familiarization of the new set of rules or the
Toolkit itself could have played a negative role in the creation of these records during the
national test.

Electronic Location and Access
The lack of including a URL in Electronic Location and Access MARC field 856 $u was the
greatest surprise since the nature of the catalog is to provide access to materials. Since all of
the resources studied were electronic resources, I expected that every participant would have
included the URL to the resource in 856 $u. Of the 217 submitted records, 23 (10.6%) of them
did not include a URL. In this case, I believe that the cataloger has violated Cutter's Rules for a
Dictionary Catalog. By including a URL, it does make it easier, if not possible, for the user to
access the material. This demand on the cataloger is very low since it requires very little
encoding and most likely would require a simple cut or copy and paste. It is unknown why
catalogers omitted the URL, but it does make me wonder why such an important component to
the ability to access the information resource was not included.

Future Research
In the course of this study, I reflected on where future research on cataloger's judgment
should be directed. Three main ideas come to the forefront. First, I suggest conducting the
same type of study as this one. Second, a study on cataloger's judgment and as it relates to
organizational policy, and thirdly, a study on preferred source(s) of information and how they
are used in cataloging e-resources.
175

Replicate Current Study
This study was limited by the sample size and the survey instruments. In a future study,
the structure would remain similar to this one; however the design must be altered to yield
additional data that could be analyzed with greater rigor.
The first change would be to increase the number of catalogers that would catalog each
of the items. In the current study, the same individuals did not create records for each item.
Each of the participants would catalog the same 10 electronic resources. The profile survey that
participants would complete would be based on the profile survey used by the RDA National
Test; however, it would be more detailed in the number of years of experience, level of
education, percentage of time spent cataloging, and the type of library they work in. It would
also include questions about their familiarity with the rules, the amount of time they have had
using RDA prior to the study, and the types of training they have received (formal and informal).
Finally, the follow up survey, similar to the COS survey in this study would be completed by
each participant to provide greater insight into the decisions they made.
In addition to the surveys and the record data, participants would record the preferred
source of information and rule sequences used as well as interviews with participants would be
conducted. The rule sequences would assist in determining the decision catalogers make and
how they utilize the Toolkit. The interviews would be used for participants to explain the
processes they used, problems they encountered, and recommendations for rules they would
like to see added, deleted, or edited to make them less ambiguous.

176

Receiving more detailed information about the individual and a greater number of
participants would allow for the study to use other statistical procedures that then could
provide a deeper understanding about catalogers and their judgments.

Organizational Policy Based on Cataloger's Judgment
As pointed out in previous research, library administrators often do not understand the
role of cataloging or the needs of the cataloging department (Snow, 2011). Further research in
cataloger's judgment may assist library administrators in determining the appropriate in-house
policies that will facilitate an environment that honors the needs of cataloger's judgment to
provide quality cataloging. For instance, if the theory of bounded rationality can explain
cataloger's judgment, then policy recommendations could be made to facilitate a higher quality
of cataloging. According to this theory, people make decisions based on the constructs of
cognitive ability and time. The assumption is that there is an intersection between cognitive
levels (e.g., training and experience) and time spent on task that will allow for a more efficient
model of cataloging practice. In the end, this could reduce costs, provide differentiated and
targeted professional development, and increase cataloging quality and consistency that aids
the user.
A study to assist in determining policy could include two control groups that would
complete a series of cataloging tasks; however, the two groups would receive differing levels of
professional training to determine if providing additional cognitive experiences will lead to
more efficient judgments that have a positive impact on cataloging quality.

177

Study on Preferred Sources of Information
Based on the inferences made from this study, there is a great discrepancy in how to
determine where to identify the data for electronic resources to include in the cataloging
record. A future study on preferred sources should provide greater guidance in how cataloging
rules are constructed and then used to report various elements of a resource. This study could
help explain why individuals determined their "preferred" source over another. It would also
test the inferences that catalogers often rely on their memory of rules as opposed to consulting
the rules to determine if their current practice is actually in agreement with the rules.

Implications and Recommendations for the Cataloging Community
The amount of data for this study was extremely rich, and the results of the study have
implications for the cataloging community. In analyzing the results and through observations of
the data, this study has led to some recommendations for the cataloging community such as
suggestions for training, workflows, indexing, and further edits to RDA.

Training
One of the most obvious implications is to ensure the cataloging community has the
necessary resources, tools, and training to support catalogers. Based on the findings, it appears
that novice catalogers are not the only group that need continuing training on cataloging
standards, but veteran catalogers, those with the most experience, need continued support in
the form of professional development as well. It appears that experience levels of those
catalogers who are in mid-career (6 - 22 years of experience) are better prepared for RDA than
178

any other group. No matter the demographic, all groups can benefit from additional
professional learning opportunities. Because of the enormity of electronic resources that users
search for, it is extremely important that formal education and training experiences include the
cataloging of electronic resources. Without proper training, records will be created that do not
meet the needs of the users, thereby impeding retrieval by users.
Since the group of catalogers that consulted others for 1 - 31 minutes had higher
residual amounts, it supports the notion that a collaborative workflow, whether at the same
location or not, will yield more results than an isolated one. Snow (2011) described these as
communities of practice.

Workflows
Workflows are the local procedures or systems which catalogers should follow to
catalog materials. It is recommended that these workflows are reviewed to make sure they
align with RDA. Based on the review of what was entered into the cataloging records, the
previous practice of AACR2 is still evident in areas that have changed. By reviewing and aligning,
and then providing additional training on these changes, catalogers will have greater guidance
in how to create records that meet the expectations of RDA and ultimately benefit the user.

Indexing
Specifically based on the results of the observations within the Title Statement 245 and
Varying Form of Title 246 MARC fields, it is extremely important that these fields are indexed
appropriately so that users are able to find, identify, select and acquire the resources they
179

need. In the study records, there were many variations to the 245 and 246 fields. However, if
one looks closely at the actual content of these fields, there are similarities within the text.
Some participants placed everything in 245 and nothing into 246 while others represented the
titles in other ways. The inconsistent entries were often due to the variety of judgments
catalogers made in determining the preferred source information. If the 246 is not indexed in
the automation system, then the information in 246 would not be discovered. For this reason, it
is extremely important that the librarian or person responsible for the automation system
understands that appropriate indexing is necessary for users to find the information they are
looking for.

Further Edits to RDA
The final implication to be discussed is that the profession needs to voice when the
rules are ambiguous or difficult to follow and seek clarity or advocate for change when
necessary. Clarity can come from the JSC, LC, professional organizations, a local cataloging
agency or the broader cataloging community. At the same time, the JSC and Committee on
Cataloging: Description and Access should spend time interviewing catalogers so that those
who use the rules have the opportunity to voice how they apply the rules in practice. A dialog
among the community will allow others to think more deeply about their work and challenge
assumptions. The one area from this study that is recommended for increased clarity is that of
the preferred source of information. There seems to be confusion about which sources to use
for electronic resources. It is recommended that the JSC consider revisions to provide greater
guidance.
180

Conclusion
The RDA National Test data provided a unique opportunity to study cataloger's
judgment, a topic that has for the most part, been largely ignored as an area of study.
Cataloger's judgment has been referred or alluded to throughout the literature since the time
Cutter allowed such permission in the Rules for a Dictionary Catalog (1876) as long as it benefits
users.
Cronin (2011), Intner (2006) and others have expressed that RDA will require an
increase in cataloger's judgment during the process of creating bibliographic records. Prior to
even reviewing the records in a formal study, it was noted the number of variances that
occurred between one record and the next. It was noted that for the records that were
studied, no two were the same. Each was unique and required cataloger’s judgment to not only
determine the actual text, but to also decide if the presence of any text was necessary.
The significance of this study lies in its attempt to better understand cataloger's
judgment. More specifically, it sought to determine if cognitive ability and time influence the
text that catalogers enter into bibliographic records. This exploratory study set out to
determine whether or not there were differences among various categorical groups in how
they make judgments for entering text into bibliographic records, and if this could be explained
using Simon's theory of bounded rationality.
There are indications that both support and refute the assertion that catalogers make
decisions based on the constructs of time and cognitive ability. However, this study does
provide a baseline of data that warrants further research. Continued research will provide

181

greater insight into not only the text included in a MARC record, but how to improve RDA to
make the rules less ambiguous.
The most important outcomes of this study are the implications for the field. Training
and communities of practice will provide the knowledge needed to lead to better cataloging
decisions. Ensuring the proper indexing of MARC fields will lead to greater discovery. Finally,
the broader cataloging community needs to advocate for clarity of the rules that they find
confusing. All for the benefit of the user, as Cutter intended.

182

APPENDICES

183

Appendix A
List of Common Original Set (COS) Items

184

Table A.1
List of Common Original Set (COS) Items
ID
A
B
C
D
E
F
G

Resource Type
Print Mono 1
Print Mono 2
Print Mono 3
Print Mono 4
Print Mono 5
Print Mono 6
Print MultiPart Mono 1

Short title
Macroeconomics
Winnie
Twain
Barbie
Mysterius
Gospel
Aunt Lute

H*
I*
J*

E- Mono 1
E -Mono 2
E -Mono 3

Americans with Disabilities
Benjamin Button
Reconciling Scientific Approaches for Organic Farming Research [thesis in two
parts, published in Netherlands]
Modern Drug
PACIIA
Criterion
Utley
San Diego
March
Acupuncture
Rattletrap
CFA
5Billion
Multichannel
Our Science – Research Directory
NCJRS

K
L
M*
N*
O*
P
Q*
R
S
T
U
V*
W*

Print Serial 1
Print Serial 2
E serial 1
E-Serial 2
E-Serial 3
AV 1 film DVD
AV 2 streaming video
AV 3 sound recording on CD
AV 4 audiobook
AV 5 poster
Integrating Resource 1 –print loose-leaf
Integrating Resource 2 e-resource
Integrating
Resource 3 -e-resource
X*
Integrating Resource 4 –e-resource
UN
Y*
Integrating Resource 5 –e-resource
ProQuest
*Electronic resources to be studied
Table modified from http://www.loc.gov/catdir/cpso/RDAtest/commonsets.pdf

185

Appendix B
Survey Questions

186

Table B.1
Record Creator Profile (RCP)
RCP -Q01*
RCP -Q02*

Assigned RDA Tester ID: Please provide your unique RDA Test tester ID, as assigned by your institution, based on your institution’s general RDA
Test ID.
Please supply your overall opinions about RDA, if you wish.

RCP -Q03*
RCP -Q04*
RCP -Q05*
RCP -Q06*

Do you think that the US community should implement RDA?
What is your position at your institution?
How many years of cataloging experience did you have as of October 1, 2010?
What formats of material do you have significant (in your own opinion) experience in cataloging? Check as many as apply:

RCP -Q07*
RCP -Q08*

Please specify any formats, as listed in Question no. 6 above, for which you feel that RDA did not offer adequate guidance. If RDA offered
adequate guidance for all formats you described in the Test, please record "N/A."
What cataloging instructions do you use most frequently in your current work?

RCP -Q09*
RCP -Q10*

What type of cataloging documentation do you normally consult?
Did your training in RDA consist of (check all that apply):

If you took distance learning sessions or classroom training other than those listed in Question no. 10 above, please specify the source. Enter
RCP -Q11*
"N/A" if you did not take distance learning sessions or classroom training.
*Questions to be considered

187

Table B.2
Common Original Set (COS)

COS-Q01*

Tester ID: Please provide your unique RDA Test tester ID. (Your unique tester ID is assigned to you by your institution, based on your
institution’s general test ID.)

COS-Q04*
COS-Q05*
COS-Q06*
COS-Q07*
COS-Q08*

What is the sequential number of this record in your personal bibliographic record production? Of all the bibliographic records you've
produced since the start of the formal RDA Test record submission period, was this your first (no. 1), second (no. 2), f
Please supply the alphabetical identifier of the resource, A-Y. Please see Instructions for Testers.
Please provide any comments you wish to make concerning your experience in creating this bibliographic record and/or any associated
authority records.
How much experience do you have in cataloging this type of resource? (Your experience does not need to have been full-time.)
What descriptive cataloging instructions did you apply to complete this record?
For RDA records only: Did you use workflows in the RDA Toolkit as you created/updated this record?
What is the communication format/coding/tagging scheme for the bibliographic record you have just completed?

COS-Q09*
COS-Q10*

How many minutes did it take you to complete this bibliographic record? Exclude any outside interruptions or consultation time (which is
recorded below). Exclude time spent on authority records (see questions no. 12-16 below). Express your answer as a who
In creating this record, which of the following did you encounter difficulties with? Please check all that apply:

COS-Q11*

How many minutes did you spend in consulting others as you completed this bibliographic record? Exclude time spent in consultation
regarding authority records (see questions no. 12-16 below). Record only your own time, not the time of others whom you cons

COS-Q12*

How many minutes did it take you to create authority records associated with this item in the Common Original Set? Exclude any outside
interruptions or consultation time (which is recorded below). Express your answer as a whole number, e.g., not "1.6 hour

COS-Q13*
COS-Q14

How many new authority records did you create in describing this item? Express your answer as a whole number. If you did not create any
authority records, record a zero.
What type of new authority records did you create in describing this item? Please check all that apply:

COS-Q15

In creating authority records for this item, which of the following did you encounter difficulties with? Please check all that apply:

COS-Q02*
COS-Q03*

As you created authority records for this item, how many minutes did you spend in consultation with others? Record only your own time, not
COS-Q16
the time of others whom you consulted. Express your answer as a whole number, e.g., not "1.6 hours" or "96 minutes,"
* Questions to be considered

188

Table B.3
Institutional Questionnaire (IQ)
IQ-00
IQ-Q01
IQ-Q02*
IQ-Q03
IQ-Q04

US RDA Test Partners Institutional Questionnaire (IQ)
Please give the name of your institution:
Please provide any general comments you wish concerning the test, the RDA Toolkit, or the content of the RDA instructions:
Do you think that the US community should implement RDA?
If the US national libraries do NOT implement RDA, will your institution decide to implement RDA anyway?

IQ-Q05
IQ-Q06*
IQ-Q07*
IQ-Q08

If the US national libraries implement RDA, will your institution decide NOT to implement RDA anyway?
What approach to RDA options did your institution apply in creating/updating original RDA records? Check all that apply.
If you have further comments about the RDA options, please provide them here. If you have no comments about the options, please record "N/A."
What approach did your institution apply in creating/updating records using copy? Check all that apply:

IQ-Q09

Please describe briefly any macros your institution created for use in creating/updating RDA records for the RDA Test. If you did not use macros,
please record "N/A."

IQ-Q10*

Please describe the additional RDA workflows that your institution created using the wizard. If you did not create any additional workflows, please
record "N/A."

IQ-Q11
IQ-Q12
IQ-Q13*
IQ-Q14*
IQ-Q15*

Please add any general comments on the RDA Toolkit workflows and the wizard.
Can your institution's ILS accept records with the new MARC 21 changes related to RDA?
What training did your institution’s testers receive before they began producing records for the US RDA Test? Please check as many as apply:
Please describe any local documentation that your institution created or revised for use with RDA. If there was none, please record "N/A."
Please describe any consortial documentation that your institution created or revised for use with RDA. If there was none, please record "N/A."

IQ-Q17*

Please describe any national-level documentation that your institution identified as needing to be created or revised for use with RDA. If there was
none, please record "N/A."
Were your staff able to move back and forth from local documentation to the cataloging instructions in the RDA Toolkit as they needed, via hot
links?

IQ-Q18*

Were your staff able to move back and forth from consortial documentation to the cataloging instructions in the RDA Toolkit as they needed, via
hot links? "Consortium" means a group of institutions that share a cataloging enterprise and policies, e.g., CC

IQ-Q19*
IQ-Q20*

Were your staff able to move back and forth from national-level documentation (e.g., PCC documentation) to the cataloging instructions in the
RDA Toolkit as they needed, via hot links? (For consistency's sake, please consider OCLC documentation as national
Did any of your staff make personal annotations in the RDA Toolkit?

IQ-Q16*

189

IQ-Q21*
IQ-Q22*

Did your institution/consortium make annotations in the RDA Toolkit?
If RDA is implemented, what will be the impact on your existing documentation?

IQ-Q23*

Will the impact on your existing documentation be a barrier to or a benefit in implementing RDA?

IQ-Q25

Is your institution considering using the RDA Toolkit to replace any currently existing documentation? (For consistency's sake, please consider
OCLC documentation as national-level.)
Is your institution considering ceasing subscriptions to any other cataloging instructions or tools if RDA is implemented? Please check all that
apply:

IQ-Q26*
IQ-Q27*
IQ-Q28*

How much impact on local operations do you anticipate if your institution implements RDA?
What do you believe the negative impacts will be if your institution implements RDA?
What do you believe the positive impacts will be if your institution implements RDA?

IQ-Q29

Were you able to create acceptable RDA records in MARC from non-MARC, non- RDA metadata, e.g., from an ONIX feed? If you do not actually
import non-MARC data to create MARC records, please record "N/A."

IQ-Q30

Were you able to create acceptable RDA records in DC or other non-MARC formats (if they are your institution's usual formats) from non-MARC,
non-RDA metadata? If you do not usually produce records in non-MARC formats, please record "N/A."

IQ-Q31

After the US RDA Test is completed, the RDA Toolkit will no longer be available to your institution free of charge. Will the expense of subscribing to
the RDA Toolkit for use by your staff be greater than your current cost of providing cataloging tools?

IQ-Q24

IQ-Q32
What will be the impact on local operations of any increased costs in subscribing to the RDA Toolkit?
IQ-Q33
Does your institution anticipate cost adjustments to any cataloging contracts/vended work as a result of RDA if it is implemented?
* Questions to be considered

190

REFERENCES

Acedera, A. (2014). Are Philippine librarians ready for Resource Description and Access (RDA)?
The Mindanao experience, Cataloging & Classification Quarterly, 52(6/7), 600-607. DOI:
10.1080/01639374.2014.891164
Adamich, T. (2007). FRBR cataloging’s future is closer than you think! Knowledge Quest, 36(1),
64-69.
Agosto, D. E. (2002). Bounded rationality and satisficing in young people's web-based decision
making. Journal of the American Society for Information Science & Technology, 53(1), 1627. doi:10.1002/asi.10024
Anglo-American Cataloging Rules 2nd edition, 2002 revised (AACR2r). (2005). Chicago: American
Library Association.
Bade, D. (2002). The creation and persistence of misinformation in shared library catalogs:
language and subject knowledge in a technological era: Vol. 211. GSLIS Occasional
Papers. Champaign, IL: GSLIS, University of Illinois
Bair, S. (2005). Toward a code of ethics for cataloging. Technical Services Quarterly, 23(1), 1326. doi:10.1300/J124v23n01_02
Beall, J. (2005, June 12). Metadata and data quality problems in the digital library. Journal of
Digital Information, 6. Retrieved from
http://jodi.tamu.edu/Articles/v06/i03/Beall/Beall.pdf

Beall, J. & Kafadar, K. (2004). The effectiveness of copy cataloging at eliminating typographical
errors in shared bibliographic records. Library Resources & Technical Services, 48(2), 92101.
Behrens, R., Frodl, C., & Polak-Bennemann, R. (2014). The adoption of RDA in the GermanSpeaking Countries, Cataloging & Classification Quarterly, 52(6/7), 688-703. doi:
10.1080/01639374.2014.882872
Benerjee, K. (1998). Describing remote electronic documents in the online catalog: Current
issues. Cataloging & Classification Quarterly, 25 (1), 5-20. doi: 10.1300/J104v25n01_02
Bianchini, C. & Guerrini, M. (2009). From bibliographic models to cataloging rules: Remarks on
FRBR, ICP, ISBD, and RDA and the relationship between them. Cataloging and
Classification Quarterly, 47(2), 105-124. doi:10.1080/01639370802561674
Biella, J. C. & Lerner, H. G. (2011). The RDA test and Hebraica cataloging: Applying RDA in one
cataloging community. Cataloging & Classification Quarterly, 49(7-8), 676-695. doi:
10.1080/01639374.2011.616450
Bloss, M. E. (2011). Testing RDA at Dominican University's Graduate School of Library and
Information Science: The students' perspectives. Cataloging & Classification Quarterly,
49(7-8), 582-599. doi: 10.1080/01639374.2011.616264
Boeuf, P. (2005). FRBR: Hype or cure all? Introduction. Cataloging and Classification Quarterly,
39(3), 1-13. doi:10.1300/J104v39n03_01
Bothmann, R. (2004). Cataloging electronic books. Library Resources & Technical Services, 48(1),
12-19.
Boughton, G. (1951). Catalog rules and the teaching of cataloging. PNLA Quarterly, 15, 32-35.
192

Boydston, J. & Leysen, J. (2014). ARL cataloger librarian roles and responsibilities now and in the
future. Cataloging & Classification Quarterly, 52(2), 229-250. DOI:
10.1080/01639374.2013.859199
Brown, C. C. & Meagher, E. S. (2008). Cataloging free e-resources: Is it worth it? Interlending &
Document Supply, 36(3), 135-141. doi:10.1108/0264160810897845
Buczynski, J. (2005). Satisficing digital library users. Internet Reference Services Quarterly, 10(1),
99-102. doi:10.1300/J136v10n01_08
Buizza, P. (2004). Bibliographic control and authority control from Paris Principles to the
present. Cataloging and Classification Quarterly, 38(3), 117-133.
doi:10.1300/J104v38n03_11
Carr, P. L. (2007). The shape of things to come: Resource Description and Access (RDA). The
Serials Librarian, 52(3/4), 281-289. doi:10.1200/J123v53n03_06
Caswell, J. V., Gulden, F. H., Parsons, K. A., Wendell, D. C., & Wiese, W. H. (1995). Importance
and use of holding links between citation databases and online catalogs. The Journal of
Academic Leadership, 21(2), 92-96.
Chen, X., Colgan, L., Greene, C., Lowe, E., & Winke, C. (2004). E-Resource cataloging practices: A
survey of academic libraries and consortia. The Serials Librarian, 47(1/2), p. 153-179.
doi:10.1300/J123v47n01_11
Choi, K., Yusof, H., & Ibrahim, F. (2014). RDA: National Library Board Singapore's learning
journey, Cataloging & Classification Quarterly, 52(6/7), 608-620. doi:
10.1080/01639374.2014.891165

193

Chu, F. (1994). Reference service and bounded rationality: Helping students with research.
College and Research Libraries, 55, 457-461.
Clare, F. (1950). The new library school programs as they affect cataloging. Journal of
Cataloging & Classification, 6, 63-67.
Connaway, L. S., Dickey, T. J., & Radford, M. L. (2011). "If it is too convenient I'm not going after
it:" Convenience as a critical factor in information-seeking behaviors. Library &
Information Science Research, 33, 179-190. doi: 10.1016/j.lisr.2010.12.002
Cox, E.J. & Myers, K. D., (2010). What is a professional cataloger? Perception differences
between professionals and paraprofessionals. Library Resources & Technical Services,
54(4), 212-226.
Coyle, K. & Hillmann, D. (2007). Resource Description and Access (RDA): Cataloging rules for the
20th century. D-Lib Magazine, 13(1/2).
Creider, L. S. (2009) A comparison of the Paris Principles and the International Cataloguing
Principles. Cataloging and Classification Quarterly, 47(6), 583-599.
doi:10.1080/01639370902946270
Cronin, C. (2011). From testing to implementation: Managing full-scale RDA adoption at the
University of Chicago. Cataloging & Classification Quarterly, 49, 626-646. doi:
10.1080/01639374.2011.616263
Cross, E., Andrews, S., Grover, T., Oliver, C., & Riva, P. (2014) In the company of my peers:
Implementation of RDA in Canada, Cataloging & Classification Quarterly, 52(6/7), 747774. doi: 10.1080/01639374.2014.899535

194

Cutter, C. A. (1904). Rules for a printed dictionary catalogue (4th ed.). Washington, D.C.:
Government Printing Office. Retrieved from
http://digital.library.unt.edu/ark:/67531/metadc1048/m1/6/e
Delsey, T. (2005). Modeling subject access: Extending the FRBR and FRANAR conceptual
models. Cataloging and Classification Quarterly, 39(3), 49-61.
doi:10.1300/J104v39no03_04
Dickey, T. J. (2008). FRBRization of a library catalog: Better collocation of records, leading to
enhanced search, retrieval, and display. Information Technology and Libraries, 27(1), 2332.
Dinkelman, A. & Stacy-Bates, K. (2007). Accessing e-books through academic library web sites.
College & Research Libraries, 68(1), 45-58.
Dinur, A. R. (2011). Common and un-common sense in managerial decision making under task
uncertainty. Management Decision, 49(5), 694-709. doi: 10.1108/00251741111130797
Dunkin, P. S. (1969). Cataloging U.S.A. Chicago: American Library Association.
Eklund, A., Miksa, S., Moen, W., Snyder, G., & Polyakov, S. (2009). Comparison of MARC content
designation utilization in OCLC WorldCat records with national, core, and minimal level
record standards, Journal of Library Metadata, 9(1/2), 36-64. doi:
10.1080/19386380903095073
Elrod, J. M. (2008). The case for cataloguing education. The Serials Librarian, 55(1-2), 1-10. doi:
10.1080/03615260801970709
Fain, M., Brown, M., & Faix, A. (2004). Cross-training reference librarians to catalog. Technical
Services Quarterly, 22(1), 41-53. doi: 10.1300/J124v22n01_05
195

Ferris, A. M. (2008). The ethics and integrity of cataloging. Journal of Library Administration,
47(3), 173-190.
Fidel, R. (2008). Are we there yet?: Mixed methods research in library and information science.
Library & Information Science Research, 30, 265-272. doi:10.1016/j.lisr.2008.04.001
Gigerenzer, G. & Selten, R. (2001). Rethinking rationality. In G. Gigerenzer & R. Selten (Eds.),
Bounded rationality: The adaptive toolbox (pp. 1-12). Cambridge, MA: MIT Press.
Goldberga, A., Kreislere, M., Sauka, J., Stürmane, A. & Virbule, I. (2014) RDA: From strategy to
experiments and implementation in Latvia (Including an overview of the situation in the
Baltic States), Journal of Library Metadata, 14(3/4), 205-221. doi:
10.1080/19386389.2014.992710
Goldsmith M. & Adler, E. (2014) RDA in Israel. Cataloging & Classification Quarterly, 52 (6/7),
677-687. doi: 10.1080/01639374.2014.925023
Gorman, M. (2007). RDA: Imminent debacle. American Libraries, 38(11), 64-65.
Guerrini, M. (2009). In the praise of the un-finished: The IFLA statement of cataloguing
principles. Cataloging and Classification Quarterly, 47(8), 722-740.
doi:10.1080/01639370903206906
Hall-Ellis, S. & Ellett, R. O., Jr. (2011). Introduction. Cataloging & Classification Quarterly, 49(78), 567-571. doi:10.1080/01639374.2011.618377
Hanson, H. & Schalow, J. (1999). Two aspects of quality in technical services: Automating for
quick availability, and identifying problems, effecting solutions. Library Collections,
Acquisitions, & Technical Services, 23(4), 433-441.

196

Harmon, J. C. (1996). The death of quality cataloging: Does it make a difference for library
users? The Journal of Academic Librarianship. 22(4), 306-307.
Hickey, T. B. & O’Neill, E. T. (2005). FRBRizing OCLC’s WorlCat. Cataloging and Classification
Quarterly, 39(3), 239-251. doi:10.1300/J104v39n03_15
Higgins, M. (1999). Meta-information, and time: Factors in human decision making. Journal of
the American Society for Information Science, 50, 132-139.
Hillman, L. I. (2006). RDA for who? Technicalities, 26(3), 8-10.
Hines, S. S. (2009). Librarians at the bounds of rationality: How bounded rationality can help us
help others. Behavioral & Social Sciences Librarian, 28(3), 80-86.
doi:10.1080/01639260903088927
Hinton, M. J. (2002). On cataloging internet resources: Voices from the field. Journal Of Internet
Cataloging, 5(1), 53.
Holt, G. (2010). Saving time: Ranganathan and the librarian as teacher. Public Library Quarterly,
29(1), 64-77. doi:10.1080/01616840903563024
Horn, A. H. (1955). Introduction. Library Trends, 4(2), 119-122.
Humeston, E. J. (1951). Teaching cataloging today: A survey. Journal of Cataloging &
Classification, 7, 37-41.
IFLA Study Group on the Functional Requirements for Bibliographic Records (IFLA) (1998).
Functional Requirements for Bibliographic Records: Final Report. Retrieved from
http://archive.ifla.org/VII/s13/frbr/frbr1.htm#3
International Conference on Cataloguing Principles (ICCP) (1961). Statement of Principles.
Retrieved from http://www.d-nb.de/standardisierung/pdf/paris_principles_1961.pdf
197

International Federation of Library Associations (IFLA) (2009). Statement of International
Cataloguing Principles. Retrieved from
http://www.ifla.org/files/cataloguing/icp/icp_2009-en.pdf
Intner, S.S. (2008). RDA: Progress or problem? Technicalities, 28(4), 1, 14-15.
Intner, S. S. (2006). RDA: Will it be cataloger’s judgment or cataloger’s judgment day?
Technicalities, 26(2), 1, 10-12.
Intner, S. S. (1998). Stream of consciousness: Lessons in cataloger's judgment. Technicalities,
18(10), 2-3.
Intner, S. S. (1985). Problems and solutions in descriptive cataloging of microcomputer
software. Cataloging & Classification Quarterly, 5(3), 49-56.
Johnson, B. L., & Kruse, S. D. (2009). Decision making for educational leaders: Underexamined
dimensions and issues. Albany, NY: State University Of New York Press.
Joint Steering Committee for the Development of RDA (2009). RDA: Resource Description and
Access: Prospectus. Retrieved from http://www.rda-jsc.org/rdaprospectus.html
Kemp, R. (2008). Catalog/cataloging changes and Web 2.0 functionality. The Serials Librarian,
53(4), 91-112. doi:10.1200/J123v53n04_07
Koehler, D. J., & Harvey, N. (2004). Preface. Blackwell handbook of judgment and decision
making (pp. xiv-xvi). Malden, MA: Blackwell Publishing
Kotrlik, J., Williams, H., & Jabor, K. (2011). Reporting and interpreting effect size in quantitative
agricultural education research. Journal of Agricultural Education, 52(1), 132-142.
Kraus, D. (2007). Controversies in cataloging: The debate over AACR2’s successor. American
Libraries, 38(9), 66-67.
198

Krippendorff, K. (2004). Content analysis: An introduction to its methodology (2nd ed.).
Thousand Oaks, CA: Sage.
Kroeger, A. (2013). The road to BIBFRAME: The evolution of the idea of bibliographic transition
into a post-MARC future. Cataloging & Classification Quarterly, 51(8), 873-890.
DOI: 10.1080/01639374.2013.823584
Lavoie, B. F., Connaway, L., & O'Neill. (2007). Mapping WorldCat's digital landscape. Library
Resources & Technical Services, 51(2), 106-115.
Library of Congress (2011), Report and recommendations of the U.S. Test Coordinating
Committee. Retrieved from http://www.loc.gov/bibliographic-future/rda/rdatestingfinalreport-20june2011.pdf
Library of Congress. (2009a). Instructions for Common Original Set and Common Set Test
Surrogates. Retrieved from http://www.loc.gov/bibliographicfuture/rda/source/commonsetsinstructions.pdf
Library of Congress. (2009b). Proposed methodology for U.S. National Libraries RDA Test .
Retrieved from http://www.loc.gov/bibliographic-future/rda/testing.html
Library of Congress. (2008). Joint statement of the Library Congress, the National Library of
Medicine, and the National Agricultural Library on Resource Description and Access.
Retrieved from http://www.loc.gov/bibliographic-future/news/RDA_Letter_050108.pdf
Library of Congress. (n.d.). About the U.S. National Libraries RDA test plan. Retrieved from
http://www.loc.gov/bibliographic-future/rda/
Luo, C., Zhao, D., & Qi, D. (2014). China's Road to RDA, Cataloging & Classification Quarterly,
52(6/7), 585-599. doi: 10.1080/01639374.2014.917446
199

Madison, O. M. A. (2005). The origins of the IFLA study on Functional Requirements for
Bibliographic Records. Cataloging and Classification Quarterly, 39(3), 15-37.
Doi:10.1300/J104v39n03_02
Mann, T. (1991). Cataloging quality, LC priorities, and models of the library’s future. Washington
D.C.: Library of Congress.
Mansourian, Y. & Ford, N. (2007). Search persistence and failure on the web: A "bounded
rationality" and "satisficing" analysis. Journal of Documentation, 63(5), 680-701.
doi:10.1108/0022041071827754
Mansourian, Y., Ford, N., Webber, S., & Madden, A. (2008). An integrative model of
"information visibility" and "information seeking" on the Web. Program: Electronic
Library & Information Systems, 42(4), 402-417. doi:10.1108/00330330810912089
Marchionini, G. (1995). Information seeking in electronic environments. Cambridge: Cambridge
University.
Martin, K., Dzierba, J., Fields, L., & Roe, S. (2011). Consortial cataloging guidelines for electronic
resources: I-Share survey and recommendations. Cataloging & Classification Quarterly,
49(5), 361-386. DOI: 10.1080/01639374.2011.588996
McCracken, E. (2007). Description and of access to electronic resources (ER): Transitioning into
the digital age. Collection Management, 32(3/4), 259-275. doi:10.1300/J105v32n03_02
McCutcheon, S. (2011). RDA testing in triplicate: Kent State University's experience with RDA
testing. Cataloging & Classification Quarterly, 49(7-8), 607-625. doi:
10.1080/01639374.2011.616262

200

Miksa, S. D. (2008). Educators: What are the cataloging issues students get excited about?-professional and intellectual appeals of cataloging and students' misconceptions of
cataloging. Cataloging & Classification Quarterly, 45(3), 17-24. doi:
10.1300/J104v45n03_03
Moore, J. R. (2006). RDA: New cataloging rules, coming soon to a library near you! Library High
Tech News, 23(9), 12-16. doi:10.1108/07419050610725021
Neuendorf, K. (2002). The content analysis handbook. Thousand Oaks, CA: Sage Publications.
National Education Association, Department of Audiovisual Instruction (DAVI) (1968). Standards
for cataloging, coding and scheduling educational media. Washington D.C.: National
Education Association.
Naun, C. C. & Braxton, S. M. (2005). Developing recommendations for consortial cataloging of
electronic resources: Lessons learned. Library Collections, Acquisitions, & Technical
Services, 29(3), 307-325. doi:10.1016/j.lcats.2005.08.005
Norgard, B. A., Berger, M. G., Buckland, M. K., & Plaunt, C. (1993). The online catalog: From
technical services to access service. Advances in librarianship, 17, 111-148.
Paiste, M. S. (2003). Defining and achieving quality in cataloging in academic libraries: A
literature review. Library Collections, Acquisitions, & Technical Services, 27, 327-338. doi:
10.1016/S1464-9055(03)00069-1
Parks, B. & Wang, J. (2005). Are they too dynamic to describe? The Serials Librarian, 48(3/4),
237-241. doi:10.1300/J123v48n03_01
Patton, G. E. (2005). Extending FRBR to authorities. Cataloging and Classification Quarterly,
39(3), 39-48. Doi:10.1300/J104v39n03_03
201

Pazooki, F., Zeinolabedini, M., & Arastoopoors, S. (2014). RDA implementation issues in the
Iranian National Bibliography: An analysis of bibliographic records. Cataloging &
Classification Quarterly, 52(6/7), 621-639. doi: 10.1080/01639374.2014.945022
Porter, G. M. & Bayard, L. (1999). Including web sites in the online catalog: Implications for
cataloging, collection development and access. The Journal of Academic Librarianship,
25(5), 390-394.
Prabha, C., Connaway, S., Olszewski, L., & Jenkins, L. R. (2007). What is enough? Satisficing
information needs. Journal of Documentation, 63(1), 74-89.
doi:10.1108/00220410710723894
Preston, C. A. (2011). Cooperative e-book cataloging in the OhioLINK Library Consortium.
Cataloging & Classification Quarterly, 49(4), 257-276.
doi:10.1080/01639374.2011.571147
Oliver, C. (2007). Changing to RDA. Feliciter, 53(5), 250-253.
Rea, L. M., & Parker, R. A. (1992). Designing and conducting survey research. San Francisco, CA:
Jossey–Bass.
RDA Toolkit. (9 February, 2010). Chicago: ALA Publishing. Retrieved from http://rdaonline.org
RDA Toolkit (28 February, 2015). Chicago: ALA Publishing. Retrieved from http://rdatoolkit.org
Rider, M. M. (1996). Developing new roles for paraprofessionals in cataloging. The Journal of
Academic Librarianship. 22(1), 26-32.
Rose, L. D., & Duncan W. E. (1976). LC’s national standard for cataloging children’s materials.
School Library Journal, 22(5), 20 – 23.

202

Ruschoff, C. (1995). Cataloging’s prospects: Responding to austerity with innovation. Journal of
Academic Librarianship, 21(1), 51-57.
Salaba, A. & Zhang, Y. (2007). From a conceptual model to application and system
development. Bulletin of the American Society for Information Science and Technology,
33(6), 17-23.
Sanchez, E., Fatout, L., Howser, A., & Vance, C. (2006). Cleanup of NetLibrary cataloging records:
A methodical front-end process. Technical Services Quarterly, 23(4), 51-71.
doi:10.1300/j124v23n04_04
Santamauro, B. & Adams, K. C. (2006). Are we trained monkeys or philosopher-kings? The
meaning of catalogers' judgment. Technicalities, 26(5), 11-16.
Sapon-White, R. (2014). E-Book cataloging workflows at Oregon State University. Library
Resources and Technical Services, 58(2), 127-136.
Schultz-Jones, B., Snow, K., Miksa, S., & Hasenyager, R. (2012). Historical and current
implications of cataloguing quality for next-generation catalogs. Library Trends, 61(1),
49-82. doi: 10.1353/lib.2012.0028
Schwartz, C. A. (1989). Book selection, collection development, and bounded rationality.
College & Research Libraries, 50, 328-343.
Selten, R. (2001). What is bounded rationality? In G. Gigerenzer & R. Selten (Eds.), Bounded
rationality: The adaptive toolbox (pp. 14-36). Cambridge, MA: MIT Press.
Shedenhelm, L. D. & Burk, B. A. (2001). Book vendor records in the OCLC database. Library
Resources & Technical Services, 45(1), 10-19.

203

Shieh, J. (2011). Participation in the U.S. RDA Test program helped transform work habits at
George Washington University Libraries. Cataloging & Classification Quarterly, 49(7-8),
647-654. doi: 10.1080/01639374.2011.620224
Shorten, J. (2006). What do libraries really do with electronic resources? The practice in
2003. Acquisitions Librarian, 18(35/36), 55-73.
Simon, H. A. (1972). Theories of bounded rationality. In C. B. McGuire & R. Radner (Eds.),
Decision and Organization (pp. 161-176). New York: North-Holland Publishing Company.
Simon, H. A. (1957). Models of Man: Social and rational; mathematical essays on rational
human behavior in a social setting. New York: Wiley.
Simon, H. A. (1956). Rational choice and the structure of the environment. Psychological
Review, 63, 129-138.
Simon, H. A. (1955). A behavior model of rational choice. Quarterly Journal of Economics 69(1),
99-118.
Simpson, B., Lundgren, J., & Barr, T. (2007). Linking print and electronic books: One approach.
Library Resources & Technical Services, 51(2), 146-152.
Snow, K. (2011). A study of the perception of cataloging quality among catalogers in academic
libraries (Unpublished doctoral dissertation). University of North Texas, Denton, TX.
Spanhoff, E. R. (2002). Principle issues: Catalog paradigms, old and new. Cataloging and
Classification Quarterly, 35(1), 37-59. doi:10.1300/J104v35n01_04
Tashakkori, A. & Creswell, J. W. (2007). Editorial: The new era of mixed methods. Journal of
Mixed Methods Research, 1, 3-7.
Tauber, M. F. (1953). Training of catalogers and classifiers. Library Trends, 2, 330-341.
204

Taylor, A. G. (2004). Introduction to cataloging and classification (10th ed.). Westport, CT:
Libraries Unlimited.
Taylor, A. G. (1999). Where does AACR2 fall short for Internet resources? Journal of Internet
Cataloging, 2(2), 43-50. doi:10.1300/J141v02n02_05
Tillett, B. B. (2005). FRBR and cataloging for the future. Cataloging and Classification Quarterly,
39(3), 197-205. doi:10.1300/J104v39n03_12
Tillett, B. (2004). What is FRBR? A conceptual model for the bibliographic universe. Washington
D.C.: Library of Congress Cataloging Distribution Service. Retrieved from
http://www.loc.gov/cds/FRBR.html
Tillett, B. (1998). Report on the International Conference on the Principles and Future
Development of AACR, held October 23-25, 1997 in Toronto, Canada. Cataloging and
Classification Quarterly, 26(2), 31-55. doi:10.1300/J104v26n02_05
U.S. RDA Test Coordinating Committee. (2011). Report and recommendations of the U.S. RDA
Test Coordinating Committee. Retrieved from http://www.loc.gov/bibliographicfuture/rda/source/rdatesting-finalreport-20june2011.pdf
Van Ballegooie M. & Borie J. (2014). From record-bound to boundless: FRBR, linked data, and
new possibilities for serials cataloging, The Serials Librarian: From the Printed Page to
the Digital Age, 66(1/4), 76-87. doi: 10.1080/0361526X.2014.879527
Veve, M. (2009). Applying the FRAD conceptual model to an authority file for manuscripts:
Analysis of a local implementation. Cataloging and Classification Quarterly, 47(2), 125144. Doi:10.1080/0169370802575765

205

Wacker, M., Han, M., & Dartt, J. (2011). Testing Resource Description and Access (RDA) with
non-MARC metadata standards. Cataloging & Classification Quarterly, 49(7-8), 655-675.
doi: 10.1080/01639374.2011.616451
Wakimoto, J. C. (2009). Scope of the library catalog in times of transition. Cataloging and
Classification Quarterly, 47(5), 409-426. doi:10.1080/01639370902865371
Warwick, C. & Rimmer, J. (2009). Cognitive economy and satisficing in information seeking: A
longitudinal study of undergraduate information behavior. Journal of the American
Society for Information Science and Technology, 60(12), 2402-2415. doi:
10.1002/asi.21179
Watt, I. (2010). Members use of information and changing visions of the parliamentary library.
Library Trends, 58(4), 434-458.
Weihs, J. (2011). A somewhat personal history of nonbook cataloguing. Cataloging and
Classification Quarterly, 31(3-4), 159-188. doi:10.1300/J104v31n03_03
Weihs, J. R. (1972). The standardization of cataloging rules for nonbook materials: A process
report - April 1972. Library Resources & Technical Services, 16(3), 305-314.
Weiss, A. K. (2003). Proliferating guidelines: A history and analysis of the cataloging of
electronic resources. Library Resources & Technical Services, 47(4), 171-187.
White, M. D. & Marsh, E. E. (2006). Content analysis: A flexible methodology. Library Trends,
55(1), 22-45.
Wolf, M. J., & Grodzinsky, F. S. (2006, April). Good/fast/cheap: Contexts, relationships and
professional responsibility during software development. In Proceedings of the 2006
ACM symposium on Applied computing (pp. 261-266). ACM.
206

Wu, A. & Mitchell, A. M. (2010). Mass management of e-book catalog records: Approaches,
challenges, and solutions. Library Resources & Technical Services, 54(3), 164-174.
Young, J. B. & Bross, V. (2011). Results of the CRCC informal RDA Testing Task Force. Cataloging
& Classification Quarterly, 49(7-8), 600-606. doi: 10.1080/01639374.2011.620223
Zach, L. (2005). When is "enough" enough? Modeling the information-seeking and stopping
behavior of senior arts administrators. Journal of the American Society for Information
Science and Technology, 56(1), 23-35. doi: 10.1002/asi.20092
Zhao, L. (2006). How librarians used e-resources: An analysis of citations in CCQ. Cataloging &
Classification Quarterly, 42(1), 117-131. doi:10.1300/J104v42n01_08

207

