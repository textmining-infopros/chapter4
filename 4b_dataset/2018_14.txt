THE CORRELATION BETWEEN INFORMATION LITERACY
AND CRITICAL THINKING OF COLLEGE STUDENTS:
AN EXPLORATORY STUDY
Shelly Lynne McMullin, M.S.

The Correlation between Information Literacy and
Critical Thinking of College Students: An Exploratory Study. Doctor of Philosophy
(Information Science), May 2018, 138 pp., 29 tables, 1 figure, references, 79 titles.
This exploratory, mixed-methods study investigated the relationship between
information literacy and critical thinking. The research question guiding the first portion
of the study was: How do information literacy and critical thinking relate in
undergraduate students conducting academic research? Using two standardized
assessments, the study assessed the information literacy and critical thinking skills of a
small population of college students from a private, university in Texas. The
correlational analysis of the scores from the two assessments showed a statistically
significant, positive, moderate correlation. The study also explored the likelihood of
gender differences in cognitive processing using information literacy and critical think
skills assessments. The independent samples t-tests for both assessments
demonstrated no statistically significant differences between female and male
participants. Finally, a qualitative component comprised of a questionnaire provided
context to the assessment scores through items requesting information on participant
source selection priorities via the three middle stages of Kuhlthau’s information search
process model as well as their criteria for selecting sources of information. Though only
a small number of the participants completed the questionnaire, the responses
highlighted areas of interest for future research.

CHAPTER 1
INTRODUCTION
Introduction
Mitchell Kapor, founder of the Lotus Development Corporation, is attributed with
stating that, “Getting information off the Internet is like taking a drink from a fire hydrant”
(Hardoon & Shmueli, 2013, p.6). This statement seems apt as the Internet and Web are
teeming with information, which is easily and readily available. Unfortunately, not all
information sources are of the same quality and, although the Internet and the Web
have legitimate and reputable sources of information, they are also rife with
propaganda, misinformation and disinformation.
Yet, for students seeking information for course work, the Internet and the Web
alone are not the problem. The problem is much more complex and involves students’
abilities to recognize reliable information sources from less reliable information sources.
A solution will require the combined efforts of librarians, faculty and students.
Perhaps, part of the solution is a better understanding of the relationship
between the higher order cognitive skills, in this case information literacy and critical
thinking, that many academic professionals, business executives and politicians say
students should possess upon graduation from college.

Background of the Problem
Some professional literature suggests that there is a correlation or relationship
between information literacy and critical thinking (Alfino, Pajer, Pierce & Jenks, 2008;
Deitering & Jameson, 2008; Johnson, Lindsay & Walter, 2008). For instance, Alfino,

1

Pajer, Pierce & Jenks (2008) reported on a case study they conducted with freshmen
enrolled in a block of courses called “thought and expression” at Gonzaga State
University. These courses focused on teaching students how to think critically and
communicate clearly.
Using three collaborative assignments, the faculty introduced information literacy
skills in an effort to help students weed through the multitude of sources they
encountered during their assignments. Students learned about source authority and
point of view and developed their own opinions on complicated topics. At the end of the
semester, faculty reported promising results. “While still operating within a Freshman
grading rubric, the papers demonstrated more of the qualities of information literate,
college-level [sic] thinkers and writers. . .” (p.97).
A slightly earlier study by Deitering & Jameson (2008) explored similar area as
Alfino, Pajer, Pierce, & Jenks (2008). Working off a collaboration between librarians and
writing faculty established in 2001, Oregon State University changed its first-year
composition curriculum in 2004 to include six information literacy assignments. Faculty
and librarians designed these assignments to help students explore topics thoroughly–
accounting for multiple points of view–and to develop critical thinking skills as they
conducted research. Overall, the venture proved successful, with students opening up
to new ways of thinking about controversial issues and being able to clearly
communicate the divergent points of view in their research (Deitering & Jameson,
2008).
Johnson, Lindsay & Walter (2008) reported on an information literacy and critical
thinking initiative at Washington State University. Based on research indicating that

2

critical thinking could enhance the information literacy component of their freshman
seminar and their teacher-training course, they incorporated critical thinking instruction
more deeply into those programs. Instructors and librarians at WSU also worked
together to develop critical thinking and information literacy instruction materials and
assessments for instructors to use in their courses (Johnson, Lindsay, & Walter, 2008).
All three of these professional articles use information literacy and critical thinking
interchangeably. However, although the idea of a correlation between information
literacy and critical thinking is plausible, there appear to be no empirical studies in
library and information science literature to support the assumption of a relationship.
Information literacy as a formal concept is relatively new having been coined in
1974 by Zurkowski, but not really widely studied or taught until the mid-to-late 1990s.
Critical thinking is much older. Evidence exists of its origins going back to at least the
time of Socrates (Florence, 2014; Paul, 1995).
Even with such different beginnings, information literacy and critical thinking
share qualities that complement each other. For instance, both require a person to
evaluate the truthfulness and accuracy of the source of his/her information and both are
used within different disciplines with some caveats. However, information literacy and
critical thinking are different in that information literacy deals with the efficient and
effective location, evaluation and use of information, while critical thinking encourages
the use of logic to solve complex problems.
Furthermore, while students can transfer information literacy skills fairly easily
within different disciplines, critical thinking skills are not so easily transferred. Once
learned, students can use information literacy skills for English, science, etc., whereas

3

critical thinking is much more difficult to use in this manner. The caveat mentioned
previously has to do with the ability to teach general critical thinking skills for use across
disciplines. Some research indicates that critical thinking is discipline-specific meaning
that learning to use it in history is a different experience than learning to use it in political
science (Glaser, 1984; Gray & Orasanu, 1987). According to this research, there is no
“generalization” of critical thinking skills; it must be taught within specific contexts.
Other researchers such as Halpern (1998) look at the situation differently (e.g.,
Billing, 2007; Perkins & Grotzer, 1997; Reece, 2005). Critical thinking may be difficult to
teach and learn across domains or disciplines, but it is not impossible. However, it will
take concentrated effort by the teacher and student to overcome the barriers to
cognitive transfer.
Information literacy and critical thinking abilities are typically measured through
specific objective assessments. Therefore, the correlation or relatedness of information
literacy and critical thinking would be based primarily on objective testing of the two
subjects and then a statistical analysis of the scores. However, within the scope of the
literature review for this study, I found no empirical studies that measure the correlation
of information literacy and critical thinking in that way. Several professional articles
assume the two concepts are related, but none of those studies tested for a correlation
(e.g., Alfino, Pajer, Pierce & Jenks, 2008; Deitering & Jameson, 2008; Johnson, Lindsay
& Walter, 2008).
Also, research suggests that males and females learn differently and that males
are better at certain skills than females and vice versa (Halari, Sharma, Hines, Andrew,
Simmons & Kumari, 2006; Halpern, 2004). Within the world of cognitive science, which

4

is where concepts like information literacy and critical thinking abilities reside,
researchers use specific measures to assess differences between genders in cognitive
functioning. These measures include objective tests, but they also include technologies
like brain imaging.

Statement of the Problem
Information literacy skills and critical thinking skills are both important concepts
for college students to master in an information-rich environment. For the academic
professional, which includes librarians as well as other faculty members and
researchers, helping students discover sources of academic information that are
reputable is a major concern.
Professionals indicate that information literacy skills and critical thinking skills are
inherently related (e.g., Alfino, Pajer, Pierce & Jenks, 2008; Deitering & Jameson, 2008;
Johnson, Lindsay & Walter, 2008), but what is known about how the two skills relate at
a practical level? It is true that information literacy and critical thinking share the
objective of helping people assess information. However, information literacy and critical
thinking appear to also have differences that make the question of their correlation
compelling. What can researchers and professionals in fields like Library & Information
Sciences and Education learn from the similarities and/or differences between
information literacy and critical thinking? As skills, do they complement each other or
are they simply identical constructs with two different names? I was not able to answer
these questions definitively, but I wanted to begin the conversation.

5

Information literacy has been widely heralded among information professionals
since the late 1990s as an essential skill for students seeking higher education. In the
early 2000s the American Library Association (ALA) formally defined information literacy
as an individual’s ability to “recognize when information is needed and have the ability
to locate, evaluate, and use effectively the needed information” (ALA, Presidential
Committee on Information Literacy: Final Report, 2006). This study used this definition.
During this time, the Association of College and Research Libraries (ACRL), a
division of the ALA, also developed competencies and outcomes as an instructional aid
for librarians and faculty members called the “Information Literacy Competency
Standards for Higher Education.” More recently, in response to a dynamic information
environment, the ACRL developed a Framework for Information Literacy for Higher
Education (http://www.ala.org). Although the competencies and outcomes were
intended to give specific goals for college students to master, the Framework is
intended to give guidance for some of the broader questions in a rapidly changing
information environment.
As a result of providing the Framework, the ACRL rescinded the Competency
Standards in June 2016. However, this study used the Competency Standards as the
basis for assessment.
Like information literacy, some researchers have suggested that critical thinking
may be instrumental to helping students conduct academic research and find reliable
sources of information (Atton, 1994, Bodi, 1988). However, there are several definitions
of critical thinking, which makes operationalizing critical thinking as a concept difficult.
This study uses the definition from Facione (1990b), which is the “purposeful, self-

6

regulatory judgment which results in interpretation, analysis, evaluation, and inference,
as well as explanation of the evidential, conceptual, methodological, criteriological, or
contextual considerations upon which that judgment is based” (p.3). This definition
opens the door to the possibility of a link between information literacy and critical
thinking. More evidence is needed, though, to determine whether the ability to think
critically is correlated with information literacy.
Additionally, recent research is divided on the question of cognitive differences
between males and females and the way they process information. Because these
findings may have a bearing on information literacy and critical thinking skills, this study
tested for gender differences in both of these areas.

Purpose Statement
The purpose of this mixed-methods, exploratory study is to investigate the
correlation between information literacy and critical thinking abilities in a select group of
undergraduate college students. Using the California Critical Thinking Skills Test
(CCTST) from Insight Assessment and the Standardized Assessment of Information
Literacy Skills (SAILS) from Project SAILS, this study seeks to determine whether there
is a correlation between information literacy and critical thinking. Further, this study
attempts to establish whether there is a difference between male and female cognitive
functioning with regard to information literacy and critical thinking. Finally, through
questionnaires, this study seeks to provide context to the information gained through the
assessment process.

7

Research Questions
This study researches the following questions:
1. How do information literacy and critical thinking relate in undergraduate
students conducting academic research?
2. What differences are there in information literacy skills between males and
females conducting academic research?
3. What differences are there in critical thinking skills between males and
females conducting academic research?

Conceptual Framework
As mentioned previously, information literacy and critical thinking are concepts
that share some commonalities of purpose. For instance, both concepts concern the
accurate use of information. Additionally, several professional articles treat the two
concepts as if they are related. However, what is the overarching construct that
underlies the assumption of a relationship? This study proposes that higher order
thinking is the relational factor that underlies both information literacy and critical
thinking.
Lewis and Smith (1993) indicate that higher order thinking “occurs when a person
takes new information and information stored in memory and interrelates and/or
rearranges and extends this information to achieve a purpose or find possible answers
in perplexing situations” (p. 136). Geertsen (2003) states that higher-level thinking is “a
disciplined, systematic way of using the mind to confirm existing information or to
search for new information using various degrees of abstraction” (p.4). Both of these
definitions deal with the manipulation of information to extract new information.
However, the Geertsen definition is a little more overarching. Additionally, the Geertsen
8

definition mentions the concept of levels of abstraction, which was most notably outlined
by Bloom.
Bloom’s taxonomy originally developed in1956 is often used to define the scope
of higher order thinking. Bloom’s taxonomy is a framework for levels of learning. The
original taxonomy had six levels with evaluation as the highest and most abstract level
and knowledge as the lowest and most concrete level. The levels are defined as:
•

Evaluation: Judgments about the value of material and methods for given
purposes.

•

Synthesis: The putting together of elements and parts so as to form a whole.

•

Analysis: The breakdown of a communication into its constituent elements or
parts such as the relative hierarchy of ideas is made clear and/or the relations
between ideas expressed are made explicit.

•

Application: The use of abstractions in particular and concrete situations.

•

Comprehension: It refers to a type of understanding or apprehension such
that the individual knows what is being communicated and can make use of
the material or idea being communicated without necessarily relating it to
other material or seeing its fullest implications.

•

Knowledge: Knowledge, as defined here, involves the recall of specifics and
universals, the recall of methods and processes, or the recall of a pattern
structure or setting. (Bloom, 1956, p.201-207)

In later years, a group of cognitive professionals and curriculum specialists revised the
taxonomy. The professionals reorganized and relabeled the categories to better
describe the cognitive process and moved the creation (Create) category to the top.
These categories are defined as:
•

Create: Putting elements together to form a novel, coherent whole or make an
original product.

•

Evaluate: Making judgments based on criteria and standards.

•

Analyze: Breaking material into its constituent parts and detecting how the
parts relate to one another and to an overall structure or purpose.
9

•

Apply: Carrying out or using a procedure in a given situation.

•

Understand: Determining the meaning of instructional messages, including
oral, written and graphic communication.

•

Remember: Retrieving relevant knowledge from long-term memory.
(Krathwohl, 2002, p. 215)

Specifically, the revised taxonomy’s top three categories–create, evaluate and analyze–
are typically considered the higher order thinking categories. They are also the
categories that have the greatest abstraction, which means they require greater focus.
The analyze and evaluate categories are definitely present in the definitions of
information literacy and critical thinking chosen for this study and even more so in the
elaboration of those definitions.
Also, both information literacy and critical thinking have ties to the Lewis and
Smith (1993) definition and the Geertsen (2003) definition of higher order thinking.
Information literacy uses higher order thinking skills as a part of the information-seeking
process, while critical thinking uses higher order thinking as part of the inferential
process. Further, like information literacy and critical thinking, higher order thinking is
purposeful and deliberate.
However, higher order thinking is not the same as information literacy or critical
thinking. Rather, higher order thinking is a construct that transcends both information
literacy and critical thinking concepts. Under the umbrella of higher order thinking,
information literacy and critical thinking have their own defining characteristics, which
give shape to the individual concepts.

Methodology
The methodology proposed in this exploratory study is a mixed-methods
10

approach. In the field of research, the mixed-methods design has only been widely used
relatively recently as compared to quantitative or qualitative designs (Creswell, 2009). A
mixed-methods design combines the strengths of both the quantitative and the
qualitative designs and provides the opportunity for a triangulation of data.
The quantitative portion of the study looks at the correlation between information
literacy and critical thinking skills while the qualitative portion provides context for
interpreting the results of the two assessments.

Significance of the Study
The results of this study contribute to the knowledge base of information literacy
and critical thinking. Separately, information literacy and critical thinking each have their
own histories and defining characteristics. Information literacy, as defined by the ALA
and ACRL, is a practical construct focused on measurable information-seeking skills.
Critical thinking is a philosophical construct concerned with the correct formulation of
thinking in truth-seeking exercises. Both constructs are important in a changing
information landscape where the need to find reliable information is tantamount.
Additionally, observing the correlation between information literacy skills and
critical thinking skills may shed light on situations students face while seeking academic
information. Situations such as library anxiety and satisficing are among the areas that
may be informed by studies like this.
Although many professionals assume information literacy and critical thinking are
related, there is no concrete evidence in the literature to justify the assumption.
Information literacy is an applied construct meant to address the problem of finding

11

reliable information in any information arena. Information literacy, as conceptualized by
the ALA and ACRL, is meant to give structure to a process that can be perplexing.
As a philosophical construct, critical thinking is different. It has a long history and
defining it to the point of systematization is much more difficult. It is challenging to teach
and even more challenging to learn if not taught properly. Colleges have invoked the
critical thinking mantra for decades with few objective results to show (Arum and Roska,
2011). Ascertaining whether there is a correlation between information literacy and
critical thinking may help future researchers identify the points in those constructs where
they complement and strengthen the other. Revelations like these could lead to new
instructional methods and a better understanding of how to teach critical thinking.
Further, studying the correlation between information literacy and critical thinking
may lead to new discoveries in how students locate and use information. As technology
continues to develop and change, students need skills that transcend the variation of
technology. Higher order thinking skills such as information literacy and critical thinking
are aptly suited to enable students to seek information.

Assumptions and Limitations
One of the assumptions of the study was that students were cognitively prepared
to engage in high-level critical thinking activities. As will be discussed later, researchers
such as Perry (1999) indicated that students are still developing their cognitive and
intellectual abilities in their college years. Arnett (2000) stipulates that the ages between
18 and 25 are pivotal years in the development of worldviews, which is categorized as

12

intellectual development. However, students develop at varying levels so it is possible
that students will be sufficiently capable of participating in critical thinking at a high level.
Also, by recruiting students from second-semester, freshmen courses, I hoped to
have a small population of students who had been in college for more than one
semester, which increased the chances that they were intellectually ready to engage in
higher levels of critical thinking.
Another assumption was that the sample of students meaningfully represented
the larger university population demographically. As demonstrated, students at the
study university were nearly representative of the U.S. college population
demographically and, although generalizability to U.S. university students as a whole
was not achievable in this study, demographic similarity helps lend credibility to the
study.
One of the limitations of this study was the small sample size, which prevented
the statistical results from being generalizable to a larger population. The reason for this
limitation was two-fold. First, the university asked that the disturbance to the courses be
minimized so I recruited students from select second-semester, freshmen courses.
Second, I had a limited budget with which to purchase the information literacy and
critical thinking exams. To mitigate the effect of the small sample size, I added a
qualitative component to the study for a richer data set.

Definitions
•

Academic information: Data resources students use to fulfill school-related

assignments.

13

•

Critical thinking: The “purposeful, self-regulatory judgment which results in

interpretation, analysis, evaluation, and inference, as well as explanation of the
evidential, conceptual, methodological, criteriological, or contextual considerations upon
which that judgment is based” (Facione, 1990b, p. 3).
•

Information literacy: An individual’s ability to “recognize when information is

needed and have the ability to locate, evaluate, and use effectively the needed
information” (ALA, Presidential Committee on Information Literacy: Final Report, 2006).
•

Information-seeking behavior: “the purposive seeking for information as a

consequence of a need to satisfy some goal” (Wilson, 2000, p. 49).
•

Higher order thinking: “a disciplined, systematic way of using the mind to

confirm existing information or to search for new information using various degrees of
abstraction” (Geertsen, 2003, p.4).
•

Misinformation/Disinformation: Misinformation is defined as erroneous

information and disinformation is defined as deliberate deception (Fallis, 2015).
•

Propaganda: “Propaganda is the expression of opinions or actions carried out

deliberately by individuals or groups with a view to influencing the opinions or actions of
other individuals or groups for predetermined ends and through psychological
manipulations” (Ellul, 1965, p. xi-xii).

Summary
Although there is an assumption of a correlation or relationship between
information literacy and critical thinking, I have found no studies that validate that
assumption. However, that does not discount the similarities between the two constructs

14

such as the ability to evaluate information. Perhaps, there is an assumption of a
relationship because the constructs are united by an overarching concept like higher
order thinking. Certainly, information literacy and critical thinking both employ skills that
are associated with higher order thinking. This association does not prove a
relationship, but it gives insight to the processes of each construct.

15

CHAPTER 2
LITERATURE REVIEW
Introduction
Undergraduate students face a myriad of challenges when it comes to seeking
academic information in today’s media-rich environment. Some of these challenges
include finding and narrowing a topic, knowing what information databases and
repositories to consult, finding reputable sources among propaganda, misinformation
and disinformation and knowing how to use the sources they find. Higher education
faculty and academic librarians alike have sought out the most effective practices to
help students face these challenges. One of the more recent efforts to help students
seek academic information is teaching the use of information literacy and critical
thinking skills.
Although critical thinking is not a new concept, teaching it as an explicit part of
library instruction is a concept that is only a few decades old (Lubans & McCormick,
1983). In that time, researchers and professionals have assumed that critical thinking
abilities have a positive correlation with information literacy. However, I found no studies
that explore the correlation between information literacy and critical thinking.
This literature review investigates information literacy and critical thinking in order
to discover what research others have conducted on information literacy and critical
thinking and to demonstrate the need for further research. It begins by providing a
review of Kuhlthau’s model and her contributions to information literacy research. It
moves to information literacy and critical thinking as separate constructs and, then, to a

16

discussion of the relationship between information literacy and critical thinking. Finally,
the review will turn to gender differences in cognitive abilities.

The Information Search Process Model
Kuhlthau’s model of information-seeking behavior may most readily explain the
process that college students experience during the academic information-seeking
process. The relevance to this study is a better understanding of the cognitive
progressions students undergo as they search for information and determining whether
their choice of sources in specific stages aligns with any information literacy or critical
thinking scores.
Researchers in information science have studied information-seeking behavior
for several decades. During that time, they have developed various definitions of
information-seeking behavior. For instance, Krikelas (1983) defined information-seeking
behavior as “any activity of an individual that is undertaken to identify a message that
satisfies a perceived need” (p. 6). Wilson’s (2000) definition runs in a similar vein. He
defined information-seeking behavior as “the purposive seeking for information as a
consequence of a need to satisfy some goal” (Wilson, 2000, p. 49). Case’s (2012)
definition also picked up on these threads by defining information-seeking behavior as a
“conscious effort to acquire information in response to a need or gap in your knowledge”
(p.5).
All of these definitions are predicated on the information need of the user and,
therefore, presumes that the user is actively involved in the information-seeking

17

process. Anyone of these definitions, then, could be used to describe the informationseeking behavior of students searching for academic information.
Kuhlthau (1983) published her dissertation based on research she conducted
with high-school students. This study informed her information search process (ISP),
which posited six stages of information seeking: initiation, selection, exploration,
formulation, collection and presentation. Figure 1 illustrates the ISP model in its final
form.

Figure 1. Kuklthau’s ISP model (2004).

During each stage, students undergo certain affective and cognitive experiences
that intimate what actions they will take during the search process. For instance, during
the Initiation phase, students experience uncertainty. Their thoughts are vague, and
their actions tend toward searching for background information (Kuhlthau, 1991, p.367).
The Presentation phase brings feelings of relief along with satisfaction or dissatisfaction
depending on how the search process ended (p. 368). Thoughts are clear and focused,
and their actions are more about summarization of their findings (p.368).

18

Her follow-up study tracked the same students immediately following their
college years. This study validated her previous findings indicating that college students
also go through the stages of the Information Search Process (Kuhlthau, 1991).
The cognitive aspects of the ISP model are pertinent to this study. Particularly, in
the stages of exploration, formulation and collection where information literacy and
critical thinking could have major input. Interestingly, according to Figure 1, the cognitive
portion of Kuhlthau’s model has the least content associated with it. However, this
observation does not indicate that the cognitive aspects of ISP are dormant or
unimportant. In fact, strong cognitive faculties may help students overcome the initial
anxiety and lingering doubt that comes with doing academic information seeking (Kwon,
2008).
The stages of exploration, formulation and collection represent the phases of ISP
where a majority of the cognitive transitions happen. Thoughts generally proceed from
vague to focused. The cognitive aspects of the three stages are further described
below.
•

Exploration: “Thoughts center on becoming oriented and sufficiently informed
about the topic to form a focus or personal point of view.” (Kuhlthau, 1991,
p.366)

•

Formulation: “Thoughts involve identifying and selecting ideas in the
information from which to form a focused perspective of the topic.” (p. 367368)

•

Collection: “Thoughts center on defining, extending, and supporting the
focus.” (p. 368)

Of the exploration stage, Kuhlthau (1991) says, “Information encountered rarely
fits smoothly with previously-held constructs and information from different sources
commonly seems inconsistent and incompatible” (p. 366-367). Kuhlthau’s description of
19

the exploration stage suggests an intense process of discovery and assimilation (or
rejection) of new information.
In this manner, exploration has similar qualities as Perry’s (1999) stages of
intellectual development in that, as students encounter new information, they move from
concrete assumptions in their worldview through an iterative process of integration (or
refutation) of the new information. This process can be very disconcerting for students,
which aligns with Kuhlthau’s affective traits during the exploration stage: confusion,
frustration and doubt.
However, students who persist through exploration, typically find that the
formulation stage brings with it some clarity and narrowing of focus. Kuhlthau (1991)
refers to it as “the turning point of the ISP” (p. 367). Within Formulation, students are
able to gain greater insight into the potential of their topic. They start to see patterns or
repetition in ideas, which they are then able to use to hone their perspectives. They may
still have reservations about some of the ideas they encounter, but it no longer causes
such virulent upheaval in their thinking processes.
Finally, as students become more comfortable with the information and their own
representation of the information, they enter the collection stage. In this stage, general
information is usually no longer helpful as students have moved past the need for
surface facts. This stage centers on seeking very specific pieces of information that will
contribute to the students’ well-developed topic.
The recognized need for information literacy and critical thinking skills interweave
throughout these three stages. In exploration, successful students use both information
literacy and critical thinking skills as they seek information and try to assimilate new

20

information into their worldview. Strong information literacy skills provide students with
the practical abilities they need to navigate their information landscape. Also,
information literacy skills, as defined by the ALA and ACRL, give students tools to
evaluate the reliability of the information they encounter.
In addition to the evaluation skills information literacy and critical thinking share,
critical thinking skills help students analyze the information for inconsistencies. In this
case, analysis is specifically defined as the need, “to identify the intended and actual
inferential relationships among statements, questions, concepts, descriptions, or other
forms of representation intended to express belief, judgment, experiences, reasons,
information or opinion” (Facione, 1990a, p.7). The analysis part of critical thinking allows
students to make informed judgments about ideas and information based on the
consistency of the argument.
In the formulation stage, students employ information literacy skills to uncover
new sources of information, while their critical thinking skills assist them in making
sense out of the information they confront. While not as affectively intense as the
exploration stage, formulation requires concentrated effort in cognitive activities.
Students use critical thinking skills to help them sort through the increasingly
complicated ideas and information they find during research.
During collection, strong information literacy skills should make seeking specific
pieces of information for their topic a more efficient process. Students employ much of
their critical thinking resources toward making relevance decisions with regard to the
new sources of information.

21

Kuhlthau’s ISP model is a good representation of the academic informationseeking process students experience affectively, cognitively and behaviorally.
Accordingly, it is fitting that this literature review bring to light the importance of
information literacy and critical thinking skills to the academic information-seeking
process through a review of the ISP model.

Information Literacy
As previously defined, information literacy is an individual’s ability to “recognize
when information is needed and have the ability to locate, evaluate, and use effectively
the needed information” (ALA, Presidential Committee on Information Literacy: Final
Report, 2006). However, this was not the first or only description of information literacy.
Zurkowski, the president of the Information Industry Association, coined the term
“information literacy” in 1974 when he wrote about the current state of library and
information sciences’ relationship with industry and called for a national program to
teach information literacy (Zurkowski, 1974). While Zurkowski (1974) did not explicitly
define information literacy, he made general references to skills that the informationliterate person has and that the information-illiterate person does not have.
People trained in the application of information resources to their work can be
called information literate. They have learned techniques and skills for utilizing
the wide range of information tools as well as primary sources in molding
information solutions to their problems. The individuals in the remaining portion of
the population, while literate in the sense that they can read and write, do not
have a measure for the value of information, do not have an ability to mold
information to their needs, and realistically must be considered to be information
illiterates. (p. 6)
Zurkowski’s (1974) paper was insightful in that he raised the alarm about the
need for information literacy before the World Wide Web and the Internet helped usher
22

in the “Information Age.” Zurkowski’s statement philosophically aligns closely with part
of the American Library Association’s (ALA) definition in that it speaks about the
individual’s ability to use information to solve problems. However, it does not go far
enough in defining information literacy, which is why this literature review will use the
ALA’s definition.
Between the time of Zurkowski’s coining of the phrase and 1994, researchers
published only a handful of articles dealing with information literacy. However, since
1995, the topic of information literacy has steadily grown in library and information
science literature (Web of Knowledge, “Information Literacy”). In 2000, the Association
of College and Research Libraries (ACRL) approved the Information Literacy
Competency Standards for Higher Education (http://www.ala.org). The five standards
are as follows:
1. The information literate student determines the nature and extent of the
information needed.
2. The information literate student accesses needed information effectively and
efficiently.
3. The information literate student evaluates information and its sources critically
and incorporates selected information into his or her knowledge base and
value system.
4. The information literate student, individually or as a member of a group, uses
information effectively to accomplish a specific purpose.
5. The information literate student understands many of the economic, legal,
and social issues surrounding the use of information and accesses and uses
information ethically and legally. (Information Literacy Competency Standards
for Higher Education, 2000)
These standards encompass a wide range of skills that the information literate
person should be able to employ. To make them more tangible, the ACRL also
developed performance indicators for each one of the standards as well as learning
23

outcomes for each performance indicator. For instance, the second performance
indicator for Standard Three says, “The information literate student articulates and
applies initial criteria for evaluating both the information and its sources” (Information
Literacy Competency Standards for Higher Education, 2000, p. 11). While the outcome
for that same performance indicator reads, “Examines and compares information from
various sources in order to evaluate reliability, validity, accuracy, authority, timeliness,
and point of view or bias” (p.11). This specificity allows academic professionals to
assess how information literate the students truly are and know where to begin with the
instruction process.
In 2016, the ACRL rescinded the Information Literacy Competency Standards for
Higher Education in favor of the Framework for Information Literacy for Higher
Education. While the competencies provided a starting point and a foundation for the
assessment of information literacy skills, the framework moves beyond a limited
understanding of information sources to a deeper recognition of how information is
constructed and used (Framework for Information Literacy for Higher Education,
www.ala.org). This change was intended to assist students in gaining greater insights
into the ever-changing information landscape so that they may adjust with the changes.
Considering the progressively evident need for information literacy skills among
the general public, the fact that there are only a few information literacy assessments
available is not surprising. Assessing information literacy skills in today’s information
environment requires measuring both cognitive and practical skills.
The first information literacy assessment discussed here is the Tool for Realtime Assessment of Information Literacy Skills or TRAILS. Through a federally-funded

24

grant, Kent State University began development of the ninth-grade version of the
TRAILS in 2004. The intention was to assess the information literacy skills of high
school age students based upon the Common Core State Standards Initiative and the
American Association of School Librarians’ Standards for the 21st Century Learner
(www.trails-9.org). Eventually, Kent State expanded the assessment from the third
through the twelfth grades.
The TRAILS is a proctored, web-based assessment that measures five
information-seeking categories: a.) Topic Development b.) Identifying Appropriate
Sources c.) Applying Applicable Research Strategies d.) Evaluating Sources of
Information e.) Understanding the Ethical and Legal Implications of Using Information
(http://www.trails-9.org). These categories closely align with the ACRL’s Information
Literacy Competency Standards for Higher Education indicating a broad consensus
among academic professionals regarding the key proficiencies necessary for an
information literate society.
Interestingly, the individual student scores from the TRAILS are not meaningful in
themselves, but only as they relate to the scores of the other students who have taken
the assessment. This manner of scoring does not provide students with a real measure
of their abilities, but rather a comparison of their abilities with other students.
Another company that uses the ACRL’s Information Literacy Competency
Standards for Higher Education as the basis for their assessment is Madison
Assessment. During the early part of 2003, librarians in consultation with assessment
experts at James Madison University began work on an information literacy assessment
based on the ACRL’s Information Literacy Competency Standards I, II, III and V (Swain,

25

M., Sundre, D. & Clarke, K., 2014). The Madison Assessment is a 60-item web-based,
multiple-choice assessment. The assessment has met overall reliability and validity
measures. However, the assessment creators have indicated that, due to test design
and weakness of reliability within some of the subskills, the Madison Assessment is only
intended to measure cohorts of students in aggregate and not really suitable to assess
subskills (Swain, M., Sundre, D. & Clarke, K.; 2014).
Another undertaking of Kent State University was Project SAILS or the
Standardized Assessment of Information Literacy Skills. Beginning in 2001, Kent State
University in conjunction with the Institute of Museum and Library Studies and
Association of Research Libraries assembled a committee of specialists in librarianship,
assessment and data analysis to develop an assessment to measure information
literacy skills in university students based upon the ACRL’s Information Literacy
Competency Standards for Higher Education (https://www.projectsails.org). The
developers employed Item Response Theory (IRT) to assist in the generation of the test
items, which permitted the strategic development of questions at varying levels of
difficulty (https://www.projectsails.org). In 2012, Carrick Enterprises, Inc. took over the
responsibility for the continued development and maintenance of the SAILS.
The SAILS is a proctored, web-based, multiple-choice assessment and provides
both a cohort-based option and an individual score option. The cohort-based
assessment furnishes institutions with group scores for each skill set, while the
individual score option provides institutions with overall individual student scores. Both
options also give access to the national norms for SAILS.

26

In response to the ACRL’s anticipated amendment of the Information Literacy
Competency Standards to the Framework for Information Literacy for Higher Education,
Carrick Enterprises, Inc. amassed leading experts in the field in 2014 to begin
development of the Threshold Achievement Test for Information Literacy (TATIL)
(https://thresholdachievement.com). Using many of the strategies that SAILS
developers employed, TATIL developers created a web-based assessment with
questions of varying levels of difficulty. Additionally, the reporting features of the TATIL
have been enhanced to provide detailed information about students’ strengths and
weaknesses.
Information literacy is no less important now when people imbibe a steady flow of
electronic data and information daily than it was a few decades ago when information
came at a slower pace. However, studies suggest that college students, in particular,
are not as information literate as they need to be to navigate the glut of information
available to them (Freeman & Lynd-Balta, 2010; Porter, 2011; Taylor & Dalal, 2014).
Information literacy of college students is particularly important due to their
current academic pursuits, their future responsibilities as a part of the workforce and
their position as members of a society that values information. Some undergraduate
students are not yet in a position to make meaningful contributions to their disciplines,
while others are and the information skills they acquire could be an important part of
that process.
Further, companies rely on timely and accurate information from their employees
in order to do business so knowing where to locate the information and how to evaluate
it for use is critical for the success of the organization. Finally, a fully functioning

27

democratic form of government requires an informed citizenry that is able to negotiate
today’s information landscape. All aspects of modern American society are influenced
by the ever-pervasive information environment.

Critical Thinking
Some writers have indicated that the idea of critical thinking is at least as old as
Socrates (Florence, 2014; Paul, 1995). Socrates (469-399 B.C.E.) practiced deep
questioning whereby he could discover people’s real ability to think through a topic
(Paul, 1995). Faculty members at multiple universities still use this method of
questioning, often called the Socratic method, to help students explore subjects and
reveal their depth of knowledge regarding an issue. At this point, it is important to note
that thinking is a skill that humans possess, but critical thinking must be taught, and it is
a skill people need to consciously invoke and consistently build upon.
From Socrates’ time down through the centuries other intellectuals such as Plato
(429-327 B.C.E), Aristotle (384-322 B.C.E.), Thomas Aquinas (1225-1274 A.D.),
Francis Bacon (1561-1626 A.D.), and John Stuart Mill (1806-1873 A.D.) have all
contributed to the critical thinking movement in their own time (Florence, 2014; Paul,
1995; Standford Encyclopedia of Philosophy, www.plato.standford.edu).
However, recent concern about U.S. undergraduate college students’ academic
progress has renewed interest in this ancient, but enduring, skill. In particular,
institutions of higher education have expressed trepidation over increasing data that
suggests students are not excelling in core writing and information-seeking
competencies (Arum & Roksa, 2011; Head, Van Hoeck, Eschler, & Fullerton; 2013).

28

As a solution to this problem, researchers have suggested that teaching higher
order thinking skills such as critical thinking may help students more effectively and
efficiently seek and use academic information. However, researchers have registered
some skepticism about critical thinking’s usefulness outside certain structured contexts.
The controversy centers on the effectiveness of teaching critical thinking as a
transferable skill in real-world situations. Certain researchers contend that critical
thinking, along with other cognitive skills, cannot transfer across domains. They assert
that, just because a student learns the critical thinking skills to use in one course or one
assignment, it does not hold that he/she will be disposed to use it on another
assignment or course (Glaser, 1984; Gray & Orasanu, 1987). This issue is noteworthy
for proponents of critical thinking instruction. As Reece (2005) writes, “Without transfer,
the work of trying to foster critical thinking and information literacy is in vain” (p.485).
Yet, other research seems to indicate that instructors can teach critical thinking
skills for transfer to various situations (Billing, 2007; Halpern, 1998; Perkins & Grotzer,
1997; Reece, 2005). The process of teaching critical thinking for transfer is far from
easy, though. As cognitive skills like critical thinking do not automatically transfer to new
situations, instructors must be deliberate, strategic and consistent in how they teach
critical thinking (Perkins & Salomon, 1989; Reece, 2005).
At this point, the controversy is ongoing with proponents and skeptics both
presenting arguments for their point of view, but the arguments for transferability seem
to be winning out (Anderson & Reid, 2013; Geertsen, 2013).
Another area of consideration is critical thinking dispositions. Although it is not a
part of this study, the disposition to think critically is an important concept because there

29

are two sides to the critical thinking problem–ability and disposition. Disposition is
different than ability in that it references students’ likelihood of using their critical
thinking abilities when the situation arises. Critical thinking dispositions and abilities
complement each other. However, without the disposition to use critical thinking in
certain situations, the fact that a student has abilities may be a moot point.
For instance, Kwon (2008) found that undergraduate students with strong critical
thinking dispositions were able to overcome library anxiety and reinstate their critical
thinking abilities and students with weak critical thinking dispositions were overcome by
their library anxiety. These results seem to indicate that the disposition to think critically
is as important as the ability to think critically. This idea opens up a new layer of inquiry,
which is outside the scope of this study, but the connection between library anxiety, an
affective state, and critical thinking should not go unnoticed.
Critical thinking dispositions are typically measured differently than abilities.
Whereas abilities are assessed mainly through objective tests, dispositions are
measured through a series of philosophical-based questions on motivations and beliefs
(Facione, Facione, & Giancarlo, 2000).
Finally, when discussing a cognitive skill such as critical thinking, especially in
adolescents and young adults, it suggests the question, are they cognitively mature
enough to engage in high-level critical thinking?
Perry (1999) proposed that college students could go through a series of nine
intellectual or epistemological positions during their academic career. These positions
are (a) Basic Duality, (b) Multiplicity Pre-Legitimate, (c) Multiplicity Subordinate, (d)
Multiplicity Correlate/Relativism Subordinate, (e) Relativism Correlate, Competing, or

30

Diffuse, (f) Commitment Foreseen, (g) Initial Commitment, (h) Orientations in Implication
of Commitment, (i) Developing Commitment(s) (Perry, 1999, Chart of Development).
Put simply, Basic Duality is the beginning positions where students view the world in
terms of right or wrong and good or bad based upon the view(s) of an authority figure
whom they revere, while Developing Commitment(s) is the final position where students
view the world more as a series of decisions and take responsibility for their own
choices (Perry, 1999). The other positions describe the journey from Basic Duality to
Developing Commitment(s), which is more iterative than straight-forward.
Perry’s research calls into question the intellectual abilities of first-year college
students – abilities that could have a bearing on higher order thinking skills such as
critical thinking. However, students mature intellectually at different paces and;
therefore, it is probable that some college students are able to demonstrate a relatively
high degree of critical thinking abilities even in their first year.
The issue of how instructors and librarians use critical thinking as a tool to assist
in the information-seeking process may depend on the definition to which instructors
and librarians ascribe. Many academics and researchers have come up with definitions
of critical thinking. In fact, one reason that critical thinking is difficult to teach may be
that there is no agreed upon definition.
Perhaps the best-known definition comes from Paul and Elder (2009) at the
Foundation for Critical Thinking. The late Paul was the founder of the Foundation for
Critical Thinking (www.criticalthinking.org), which was started in 1991 to raise
awareness of the need for critical thinking education across the entire academic
spectrum. Elder is a Senior Fellow and President of the Foundation for Critical Thinking.

31

According to The Miniature Guide to Critical Thinking, critical thinking is defined as “the
art of analyzing and evaluating thinking with a view to improving it” (Paul & Elder, 2009,
p.2). However, the simplicity of that definition belies the complexity of their schemata for
improving critical thinking skills.
Paul and Elder (2009) laid out a plan for improving critical thinking skills through
adhering to and practicing three categories – Universal Intellectual Standards, The
Elements of Thought (Reasoning), and Essential Intellectual Traits. Universal
Intellectual Standards include standards such as “Clarity, Accuracy, Precision,
Relevance, Depth, Breadth, Logic, and Fairness” (p. 8-9). The Elements of Thought
include “Point of View, Purpose, Question at Issue, Information, Interpretation and
Inference, Concepts, Assumptions, Implication and Consequences” (p. 3). The
Essential Intellectual Traits are “Intellectual Humility, Intellectual Courage, Intellectual
Empathy, Intellectual Autonomy, Intellectual Integrity, Intellectual Perseverance,
Confidence in Reason, and Fairmindedness” (p. 14-15). The three categories are
arranged so that observance of the Universal Intellectual Standards leads to the use of
the Elements of Thought, which in turn leads people to acquire the Essential Intellectual
Traits.
Another well-known definition comes from Facione and the critical thinking
experts of the Delphi Consensus on Critical Thinking supported by the American
Philosophical Association. This definition posits that critical thinking is the “purposeful,
self-regulatory judgment which results in interpretation, analysis, evaluation, and
inference, as well as explanation of the evidential, conceptual, methodological,
criteriological, or contextual considerations upon which that judgment is based”

32

(Facione, 1990a, p. 3). He separated the definition into six concepts: Interpretation,
Analysis, Inference, Evaluation, Explanation and Self-Regulation.
Interpretation is “to comprehend and express the meaning or significance of a
wide variety of experiences, situations, data, events, judgments, conventions, beliefs,
rules, procedures, or criteria” (Facione, 1990a, p.6). Interpretation involves a person
recognizing and understanding what he/she is experiencing.
Analysis is “to identify the intended and actual inferential relationships among
statements, questions, concepts, descriptions, or other forms of representation intended
to express belief, judgment, experiences, reasons, information or opinion” (Facione,
1990a, p.7). When a person engages in analysis he/she is making connections between
representative ideas that either support or refute the underlying claim.
Inference is
to identify and secure elements needed to draw reasonable conclusions; to form
conjectures and hypotheses; to consider relevant information and to educe the
consequences flowing from data, statements, principles, evidence, judgments,
beliefs, opinion, concepts, descriptions, questions, or other forms of
representation. (Facione, 1990a, p.9)
Put succinctly, inference involves making suppositions of likely outcomes based upon
known evidence.
Evaluation is
to assess the credibility of statements or other representations which are
accounts or descriptions of a person’s perception, experience, situation,
judgment, belief, or opinion; and to assess the logical strength of the actual or
intended inferential relationships among statements, descriptions, questions or
other forms of representation. (Facione, 1990a, p.8)

Evaluation requires a person to gauge the reliability of another person’s account of a
situation.
33

Explanation is “to state and to justify that reasoning in terms of the evidential,
conceptual, methodological, criteriological, and contextual considerations upon which
one’s results were based; and to present one’s reasoning in the form of cogent
arguments” (Facione, 1990a, p.10). Possibly the highest form of learning, explanation
requires a person to parse out his/her thinking and present it in an understandable
manner to others.
Self-regulation is
self-consciously to monitor one’s cognitive activities, the elements used in those
activities, and the results educed, particularly by applying skills in analysis and
evaluation to one’s own inferential judgments with a view toward questioning,
confirming, validating, or correcting either one’s reasoning or one’s results.
(Facione, 1990a, p.10)
Much like the Paul and Elder (2009) definition of critical thinking, the Delphi study
definition hinges on the initiative and judgment of the user. As stated previously, while
thinking is an innate skill, the user must consciously invoke critical thinking and
consistently practice it.
Halpern (1999) is another well-known critical thinking advocate. Her definition of
critical thinking is “the use of cognitive skills or strategies that increase the probability of
a desirable outcome. Critical thinking is purposeful, reasoned, and goal-directed. It is
the kind of thinking involved in solving problems, formulating inferences, calculating
likelihoods, and making decisions” (p.70). Halpern’s definition of critical thinking is
different from other definitions in that it does not prescribe specific skills that students
must learn. It does mention broad-based thinking strategies, but it is not as prescriptive
as the previous two definitions (Freeman & Lynd-Balta, 2010).

34

Ennis is a long-time critical thinking promoter. In fact, his first article on critical
thinking, titled Critical Thinking: More on Its Motivation, was published in 1956 – three
years prior to his doctoral dissertation, which was also on critical thinking. Ennis’
definition of critical thinking is “reasonable reflective thinking that is focused on deciding
what to believe or do” (Ennis, 1991 p.6).
However, like Paul and Elder’s (2009) definition, Ennis’ simple definition hides
the complexity of his critical thinking model. Ennis (1991) breaks his model up into
twelve dispositions and sixteen abilities. The dispositions aim to describe who the
critical thinker is, while the abilities describe what a critical thinker does. Dispositions
such as “to be open-minded” and “to try to be reflectively aware of one’s own basic
beliefs” are consistent with other models of critical thinking (Ennis, 1991, p.8). The
abilities are arranged around five broad concepts of critical thinking: “clarification, basis
for the decision, inference, supposition and integration, and auxiliary critical thinking
abilities” (p.9). These five concepts are also consistent with other models of critical
thinking.
However, perhaps Atton (1994) had the most straightforward definition. Atton
(1994) defined critical thinking as “a readiness to question all assumptions, an ability to
recognize when it is necessary to question; and an ability to evaluate and analyze”
(p.310). This definition encapsulates the essential skills students need to know to
conduct research.
First, the ability to question assumptions and to evaluate and analyze is a vital
skill that helps students determine the reliability of sources they find during academic
research. Prior research suggests that students are subject to propaganda and mis-

35

information even while conducting research for assignments (Bodi, 1995; Graham &
Metaxas, 2003).
Bodi (1995) found that students consistently had a lower positive response of
their ability to “evaluate the significance and value of books and journal articles” (p.21).
These responses were collected after students were required to take bibliographic
instruction through their first-semester composition course.
Graham and Metaxas (2003) conducted a study during the 2000-2001 academic
year with 180 Wellesley College students attending a “Computers and the Internet”
class in which they surveyed the class about their research habits. The three research
questions for the study were: “How strongly do students rely on the Internet for
information?”; ”What claims are students more likely to believe?” and “Who is most
susceptible to misleading claims?” (p.72). Results indicated that students relied heavily
on the Internet for information, they were susceptible to misinformation and their
performance was not significantly impacted by how long they had been in school
(Graham & Metaxas, 2003).
However, more recent research indicates that students may be more aware of
the need to assess for credibility (Head & Eisenberg, 2010; Porter, 2011; Rieh &
Hilligoss, 2008). Head and Eisenberg (2010) surveyed over 8,000 students and found
that the majority of students were selective about the sources they used to conduct
academic research. Likewise, Porter (2011) concluded that, “Credibility of sources
emerged as an important issue for students during information searches” (p.279). Porter
reached this conclusion using qualitative methods such as case study. In another
qualitative study, Rieh & Hilligoss (2008) found that college students were aware of the

36

potential credibility problems with digital sources and attempted to mitigate those
problems by also consulting trusted sources such as professors.
These later studies may mean that two decades worth of information literacy
promotion are possibly having an effect. However, librarians and instructors still need to
be diligent about source credibility, as researchers have found that many students have
other priorities when it comes to conducting academic searches (Duke &Asher, 2012).
In fact, the same researchers that found that students were starting to check for
credibility also found that students often select sources based upon convenience rather
than reliability (Rieh & Hilligoss, 2008; Head, 2013). While Rieh and Hilligoss’s (2008)
study largely dispelled the assertion that students do not check for credibility, their study
also indicated that in some instances students still based their judgments on
convenience. This finding means that the ability to recognize when it is necessary to
question the reliability of a source is no less important than the ability to evaluate a
source.
Within the world of critical thinking, there are nearly as many assessment options
as there are definitions. The Foundation for Critical Thinking’s International Critical
Thinking Basic Concepts and Understandings Test, developed by Elder, Paul and
Cosgrove (2007), is based upon the concepts and definition of critical thinking
propagated by Paul and Elder (2009). According to the testing site, the test measures
critical thinking abilities based upon five dimensions, “1. the analysis of thought, 2. the
assessment of thought, 3. the dispositions of thought, 4. the skills and abilities of
thought and 5. the obstacles or barriers to critical thought” (Elder, Paul & Cosgrove,

37

2007). Interestingly, the developers of the Basic Concepts and Understandings Test
assumed that critical thinking abilities and dispositions could be measured concurrently.
The California Critical Thinking Skills Test (CCTST) from Insight Assessment
does not make the same assumption. The developers of the CCTST also created the
California Critical Thinking Dispositions Inventory (CCTDI) to assess the user’s
disposition to use critical thinking. Although the intent of the developers was to give the
tests in conjunction with one another to have a more complete understanding of a
person’s critical thinking, the tests were developed separately.
Both the CCTST and the CCTDI are multiple-choice tests based upon the work
of Peter Facione and the Delphi Consensus convened by the American Philosophical
Association. The Delphi study identified six core critical thinking skills that help define
what it means to be a critical thinker. Again, these six skills are: Interpretation, Analysis,
Inference, Evaluation, Explanation, and Self-Regulation. The developers composed the
CCTST items largely based on the six core skills described above with a couple of
exceptions. First, they did not include self-regulation (i.e., metacognition) as a subskill in
the CCTST (Facione, 1990b). However, many of the attributes of self-regulation were
included in the later CCTDI.
Second, the developers cultivated the CCTST in collaboration with the California
State University (CSU) system, which had updated their general education objectives to
include critical thinking skills just a few years earlier. In accordance with general critical
thinking theory, part of CSU’s objectives included the assessment of inductive and
deductive skills in students (Facione, 1990b). While the developers originally rejected
the use of Induction and Deduction subskills due to the equivocal definitions of those

38

terms, they ultimately included them with specific definitions to assist the CSU system
with their assessment (Facione, 1990b).
Another fairly well known critical thinking assessment is the Watson-Glaser
Critical Thinking Appraisal. Initially developed by Goodwin Watson and E. M. Glaser in
1925, the Watson-Glaser Critical Thinking Appraisal (WGCTA) is the oldest critical
thinking tests reviewed in this study (http://www.thinkwatson.com). The WGCTA is a
multiple-choice test currently owned and distributed by Pearson Education. According to
the website, Pearson bases its assessment of critical thinking on the RED Model
(http://www.thinkwatson.com). RED stands for Recognize Assumptions, Evaluate
Arguments and Draw Conclusions, which is similar to both the Foundation for Critical
Thinking’s model and the CCTST model.
Tennessee Technological University (TTU) developed the Critical Thinking
Assessment Test (CAT). Then, with the support of the National Science Foundation
funding, six other universities across the United States helped revise it (Stein, Haynes &
Redding, 2006). The CAT is unusual in that the faculty did not start with a narrow
definition or model for critical thinking. Instead, faculty started with a foundation for good
assessment practices and developed the CAT with four guiding principles:
1. Identify critical thinking skills across disciplines that faculty genuinely believe
underlie critical thinking.
2. Develop an instrument that involves faculty and students in activities that
reveal weaknesses and encourages quality improvement initiatives.
3. Develop a reliable instrument that students find intrinsically interesting.
4. Develop an instrument based upon contemporary theory in learning sciences.
(Stein, Haynes & Redding, 2006, p.291)
Each of these principles identifies with best practices in teaching and learning.

39

Additionally, the CAT relies on faculty to assess student understanding of critical
thinking. However, the unusualness of the CAT may also be problematic in that it does
not adhere to a definition or model for critical thinking. Rather, faculty collaborated and
developed the items based upon specific skills they thought good critical thinkers should
have (Stein, Haynes & Redding, 2006).
Yet, the CAT seems to correlate well with other measures of student
achievement. Specifically, the CAT had a statistically significant, positive moderate
correlation (r =.645) at p < .01 with the CCTST (Stein, Haynes & Redding, 2006, p.294).
This statistic suggests that the CAT is as adept at measuring critical thinking as the
CCTST, which is based upon a specific model of critical thinking.
The Cornell Critical Thinking Assessment was developed in 1985 by Robert
Ennis and Jason Millman. The test has several levels with Level Z usually reserved for
undergraduates, graduates and other adults (Norris, 1986). The test is in a multiplechoice format and covers “deductive reasoning, fallacy identification, acceptability of
premise, inductive reasoning, definition and premise identification, and implicit premise
identification” (Possin, 2008, p. 217).
Another critical thinking assessment developed, in part, by Ennis is the EnnisWeir Critical Thinking Essay Test (Ennis and Weir, 1985). Unlike many other critical
thinking assessments, the Ennis-Weir is not a multiple-choice assessment. Rather, it is
an essay-based assessment. The assessment covers the following areas of critical
thinking:
•

Getting the point

•

Seeing the reasons and assumptions

•

Stating one’s point
40

•

Offering good reasons

•

Seeing other possibilities

•

Responding appropriately to and/or avoiding –
o Equivocation
o Irrelevance
o Circularity
o Reversal of an if-then relationship
o The straw person
o Fallacy
o Overgeneralization
o Excessive skepticism
o Credibility problems
o Use of emotive language to persuade (Ennis & Weir, 1985, p.1)

The Collegiate Learning Assessment (CLA+) is a relatively new critical thinking
assessment. Developed in the early 2000s by the Council for Aid to Education (CAE),
the format is a mixture of essay and multiple-choice responses that not only assesses
critical thinking skills, but also written communication abilities (http://cae.org).
The CLA+ is split into the Performance Task, which is the written assessment
and the Selected-Response Questions, which is the objective assessment portion.
Participants are given 60 minutes to complete the Performance Task, which assesses
the following skills: analysis and problem solving, writing effectiveness and writing
mechanics. The Selected-Response Questions portion is composed of 25 items and
participants are allotted 30 minutes to complete it. It assesses scientific and quantitative
reasoning, critical reading and evaluation, and critique an argument.
Due to the method of assessment the CLA+ uses, test results are not
immediately available to participants or test administrators. In fact, results can take

41

several weeks due to scoring procedures. Therefore, test administrators should keep
this constraint in mind when considering this assessment.

Relationship between Information Literacy and Critical Thinking Skills
Among some professionals and researchers there seems to be an assumption of
a relationship between critical thinking skills and information literacy skills (Alfino, Pajer,
Pierce & Jenks, 2008; Deitering & Jameson, 2008; Johnson, Lindsay & Walter, 2008).
Alfino, Pajer, Pierce and Jenks (2008), Deitering and Jameson (2008) and Johnson,
Lindsay and Walter (2008) conducted studies at their respective universities where the
concepts of information literacy and critical thinking were used synonymously.
This assumption has merit as information literacy and critical thinking have a
similar goal of helping people to assess information. However, as constructs information
literacy and critical thinking have differences. Information literacy is defined as an ability
to “recognize when information is needed and have the ability to locate, evaluate, and
use effectively the needed information” (ALA, Presidential Committee on Information
Literacy: Final Report, 2006). Critical thinking is the “purposeful, self-regulatory
judgment which results in interpretation, analysis, evaluation, and inference, as well as
explanation of the evidential, conceptual, methodological, criteriological, or contextual
considerations upon which that judgment is based” (Facione, 1990b, p. 3).
Whereas, information literacy concerns helping people define and efficiently and
effectively capitalize on an information need, critical thinking is concerned with
analyzing information that a person already has obtained.

42

Further, Weiner (2011) conducted an Idea Analysis whereby he used computer
software to reveal the major subject threads in information literacy and critical thinking
articles. He summarized part of his findings as follows,
The use of critical thinking is specific to individual disciplines. Critical
thinking enhances the status of the individual by focusing on the results. The
process is a mixture of private acts, learned by trial and error, observation and
experience. The learning period is long. There is little opportunity for quality
control of the process as critical thinking is performed mentally, using procedures
known only by the individual.
Information literacy is a more public process involving techniques linking
computer with human actions. A commonly held belief is that the primary
emphasis in application of information literacy is identification and retrieval of
relevant literature. The findings of this study showed that information literacy is
applied in all of the cognitive functions. This finding is consistent with the
definitions of that idea. (Weiner, 2011, p. 85)
Although his findings about information literacy are somewhat one-dimensional in
that they deal only with the “identification and retrieval of relevant literature,” his
observations about the differences between information literacy and critical thinking
merit some thought. For instance, does limiting information literacy to the “identification
and retrieval of relevant literature” really encompass the entirety of that information
behavior? The ALA definition of information literacy suggests otherwise. According to
the ALA (2006), identification and retrieval of information are only parts of the
information literacy equation. Evaluation and proper use of that information are also a
part of the process.
As alluded to above, some would argue that information literacy and critical
thinking have foundational similarities that should not be overlooked. Authors of
professional articles such as Alfino, Pajer, Pierce and Jenks (2008) and Johnson,
Lindsay and Walter (2008) wrote with the underlying assumption that information

43

literacy and critical thinking are related. However, defining that relationship may prove to
be challenging since both information literacy and critical thinking are rather nebulous
concepts about activities located in the human brain.
Short of empirical studies, researchers and professionals have to rely on traits
that the two constructs share. One of those traits–deliberate engagement–entails the
subject’s willingness and ability to use information literacy and critical thinking skills.
Other traits that both information literacy and critical thinking constructs share are the
analysis and evaluation aspects. The information literacy and critical thinking definitions
employed in this study each mention analysis and evaluation as an integral part of their
processes (Facione, 1990b; ALA, Presidential Committee on Information Literacy: Final
Report, 2006).
Perhaps the reason that researchers and professionals assume there is a
relationship is that information literacy and critical thinking both are constructs of higher
order thinking. Higher order thinking is defined as “a disciplined, systematic way of
using the mind to confirm existing information or to search for new information using
various degrees of abstraction” (Geertsen, 2003, p.4). Information literacy and critical
thinking are separate constructs that are part of the larger concept of higher order
thinking.

Gender Differences in Information Literacy and Critical Thinking Abilities
Abilities such as information literacy and critical thinking are cognitive skills that
originate in the frontal lobe of the brain. There is conflicting research about gender
differences in cognitive functioning. Some research seems to indicate that males and

44

females are different in their cognitive processes while other research suggests that
there are more similarities than differences. An example of a review that suggests that
gender differences exist in cognitive functioning related to learning and memory is
Andreano and Cahill (2009). Andreano and Cahill (2009) reviewed several articles and
found substantial evidence to conclude that there are differences between genders
when it comes to cognitive functioning.
However, Hyde (2005) conducted 46 meta-analyses of articles related to female
and male cognitive function and concluded that genders are more similar than different
across most psychological variables. Ten years later, Zell, Krizan, and Teeter (2015)
used a meta-synthesis approach and largely substantiated the claims of Hyde (2005).
All of the above conclusions were based on reviews of original research, but they
cannot be classified as empirical studies. Because cognitive functions are internal, there
are only so many ways to test for differences. One of those ways is functional magnetic
resonance imaging (fMRI). The fMRI is a non-invasive procedure whereby researchers
can detect changes in the brain based on blood flow to certain regions of the brain.
Researchers have found some differences between female and male cognitive
functioning during studies using fMRI, but the differences are rather small.
Halari, Sharma, Hines, Andrew, Simmons and Kumari (2006) conducted a small
study in England that looked at mental rotation and verbal fluency in men and women.
The study found behavioral differences between men and women when tested on the
mental rotation task and verbal fluency with men being better at mental rotation and
women performing better on verbal fluency (Halari, et al., 2006). Although women
activated more voxels, a 3D image representing millions of brain cells, than men in

45

certain aspects of the study, there was little statistically significant difference between
men and women in areas of brain activation or neural activity (Halari, et al., 2006).
However, it was hypothesized that if men and women had performed equally on mental
rotation and verbal fluency tasks, researchers may have observed significant
differences between the two groups (Halari, et al., 2006).
Yang, Eaves, Ng, Carpenter, Mai, Schroeder, Condon, Colom and Haier (2010)
directed a study where they found very small differences between men and women in
brain activation. However, the researchers cautioned that to validate the findings future
studies should have larger participant sample sizes (Yang, et al., 2010).
Another way to explore differences in female and male cognitive functioning is to
employ objective tests. Researchers use tests such as intelligence tests, memory tests,
and tests of spatial awareness to investigate the differences between men and women.
In the previously mentioned Halari, et al. (2006) study, researchers employed mental
rotation and verbal fluency tests as measures of male and female ability.
Other researchers use subject-based tests. For example, Halpern (2004) found
that in the United States 15-year-old girls scored statistically significantly higher in
reading literacy than 15-year-old boys. Also, while not statistically significant, the same
study indicated that eighth-grade boys tended to perform better on math and science
tests than eighth-grade girls (Halpern, 2004).
Further, Hohlfeld, Ritzhaupt and Barron (2013) conducted a study in which the
findings revealed that eighth-grade girls outperformed eighth-grade boys on skills
related to Information and Communication Technology (ICT) literacy. However, the
researchers found that, when using multilevel modeling statistical methods, gender was

46

no longer a significant factor (Hohlfeld, Ritzhaupt & Barron, 2013). These findings may
indicate that gender difference results may depend on the type of statistical model
researchers use.
Some researchers and studies have concluded that there are relatively few
differences between females and males when it comes to cognitive functioning.
Particularly relevant to this literature review is a study Williams and Evans (2008)
conducted that looked at the factors that influence information literacy in college
students. The study found that on the pretest of information literacy skills female and
male students scored relatively the same (Williams & Evans, 2008).
With regard to other studies that specifically look at gender differences within
information literacy or critical thinking, I was able to locate only two such studies. The
first study is a dissertation (Leach, 2011) looking at critical thinking abilities of university
students and how those students scored based upon academic discipline and gender.
The study posed five research questions with three null hypotheses each. The
second null hypothesis of each research question was a variation on the difference
between male and female students’ average scores within certain aspects of the test
(Leach, 2011). The study used the California Critical Thinking Skills Test (CCTST),
which assessed five key components of critical thinking – analysis, induction, deduction,
evaluation and inference. Leach (2011) ran an ANOVA and found the following results:
1. Analysis: There were no statistically significant differences between males
and females.
2. Induction: Males had a statistically higher mean than females.
3. Deduction: Males had a statistically higher mean than females.

47

4. Evaluation: Males had a statistically higher mean than females.
5. Inference: Males had a statistically higher mean than females.
These results seem to indicate a statistically significant difference between male
and female performance in most areas of a critical thinking assessment, which would
lend credence to the researchers who stipulate there is a difference between males and
females in cognitive function.
Another study conducted by Walsh and Hardy (1999) measured critical thinking
dispositions based upon gender and academic major. Walsh and Hardy (1999) used the
California Critical Thinking Dispositions Inventory (CCTDI), part of the California Critical
Thinking Assessment suite, to assess the differences between male and female critical
thinking dispositions. The CCTDI measures dispositions through seven subscales:
Truth-seeking, Open-Mindedness, Analyticity, Systematicity, Confidence,
Inquisitiveness and Maturity. The results indicated that females scored higher overall
(Walsh & Hardy, 1999).

Summary
The purpose of this literature review is to survey the field of research regarding
information literacy and critical thinking to ascertain whether researchers had already
conducted a correlational study. Additionally, this literature review explored Kuhlthau’s
model associated with information-seeking behavior to give shape to the experience
that students undergo during the information-seeking process. The review revealed
these findings:
1. Information literacy and critical thinking are complex constructs that require
careful definition and instruction.
48

2. I identified no correlational studies of information literacy and critical thinking.
3. I found conflicting studies about gender differences in cognitive functioning,
which in this case includes information literacy and critical thinking abilities.

49

CHAPTER 3
METHODOLOGY
Introduction
This exploratory study used a mixed-methods approach to determine whether
there was a correlation between information literacy and critical thinking. Specifically,
this study researched the following questions:
1. How do information literacy and critical thinking relate in undergraduate
students conducting academic research?
2. What differences are there in information literacy skills between males and
females conducting academic research?
3. What differences are there in critical thinking skills between males and
females conducting academic research?
I employed quantitative methods to study the correlation between information
literacy and critical thinking skills and qualitative methods to further contextualize the
quantitative results.

Sample
The location of this study was a small, private, faith-based university in Texas.
The university was in the middle of a critical thinking initiative as a part of its
reaffirmation of accreditation. The focus of the initiative was to increase critical thinking
skills among the students. This initiative included both learning and assessment
components. The assessment component, in particular, was advantageous to this study
in that it allowed exposure to some of the critical thinking assessments prior to starting
the study.

50

The sample for this study consisted of undergraduates from selected freshmanlevel courses. I chose certain freshman-level courses over others based upon the
requirement of an assignment that would involve information-seeking behaviors.
Additionally, some freshman-level courses were more likely to be taken in the second
semester of the student’s freshmen year, giving the student some college experience,
which possibly worked toward one of the assumptions of the study: that the participants
would have the requisite intellectual development.

Data Collection Instruments
This study used two standardized assessments and one questionnaire
developed specifically for this study. Table 1 provides an overview of the characteristics
of the two assessments in this study.
Table 1
Standardized Assessment Instruments
Assessment
Instrument

Individual Assessment of
Information Literacy by Project
SAILS (SAILS)

California Critical Thinking Skills
Test (CCTST)

Construct
Assessed

Information Literacy

Critical Thinking

Discipline
Origins

Education and Library Science

Education and Philosophy

Basis of
Assessment

ACRL Information Literacy
Standards for Higher Education

American Philosophical Association’s
Delphi Expert Consensus on Critical
Thinking

Assessment
Design

Proctored, Web-based, Multiple
Choice

Proctored, Web-based, Multiple Choice

Completion
Time

35-50 minutes (approximate)

45 minutes (timed)

51

The two assessments were the Individual Assessment of Information Literacy by
Project SAILS (Standardized Assessment of Information Literacy Skills)
(https://www.projectsails.org) and the California Critical Thinking Skills Test from Insight
Assessment (www.insightassessment.com). The Project SAILS assessment is based
on the Association of College and Research Libraries’ (ACRL) Information Literacy
Competency Standards for Higher Education, which is one of the reasons I chose this
assessment. Additionally, the SAILS provided each participant with a score for the
correlation analysis as well as subskill scores that provided detailed information about
specific strengths and challenge areas for the participants. For instance, the SAILS
measured the participants’ abilities to use certain library searching tools as well as to
evaluate sources of information.
In addition to the information literacy assessment, I also administered the
California Critical Thinking Skills Test (CCTST) from Insight Assessment
(http://www.insightassessment.com). The CCTST was based on the work of the
American Philosophical Association’s (APA) Delphi study on critical thinking. I chose the
CCTST for different pragmatic and philosophical reasons.
First, the work of the Delphi study, on which the CCTST was grounded, brought
together many of the foremost researchers and educators in critical thinking to define
and operationalize the concept of critical thinking. This type of study with authority
figures in the field afforded credibility to the instrument construct.
Second, the CCTST is a validated, multiple-choice assessment that can be
completed in less than an hour via a proctored online format. Due to time restrictions,
this type of flexibility was important.

52

Next, Insight Assessment provided individualized scores for each participant as
well as the ability to ask a limited number of pre-assessment questions to gain some
more information about the participants. As a correlational study, the individualized
scores were absolutely necessary, and the option of the additional questions permitted
me to gain more information about the participants.
Finally, the CCTST was one of the assessments that was financially feasible.
Because I wanted a larger study population and I had limited funds with which to
purchase both assessments, I had to choose an assessment that would be within my
budget. Insight Assessment also offered a discount to doctoral candidates completing
their dissertation.
The final data collection instrument in the study was a questionnaire (Appendix
A) with some of the participants who completed the SAILS and the CCTST. The
questionnaire was meant to provide context about the participants’ mindset with regard
to information literacy and critical thinking. For example, the first item in the
questionnaire requested information about the participants’ source selection priorities
within the three middle stages of Kuhlthau’s (1991) Information Search Process (ISP)
model: Exploration, Formulation and Collection. The three middle stages were chosen
due to the heavy cognitive engagement required throughout those stages, which would
be a factor in information literacy and critical thinking assessment. The pre-determined
source options provided to the respondents were: Books About Topic, General
Reference Books, Textbook, Online Databases at the Library, Internet Search,
Professor or Other Authority on the Topic, Friends, and Other. These options were
chosen based on prior research on information literacy suggesting that these sources

53

were the most often used by students in academic research (D’Couto &Rosenhan,
2015; Head, 2007; Head, 2013). However, an “Other” option was included in case a
source was overlooked due to a changing information landscape.
The next three items requested information about the participants’ decisionmaking criteria for choosing and trusting sources of information. The responses were
designed to be open-ended to permit the participants the opportunity to reveal
something about their cognitive processes as well as their criteria. Ultimately, the
intended purpose of these three items was to provide greater context for the
standardized assessment results.
The final item was also open-ended but asked how the participants’ decided
when to stop seeking information. This item speaks to the participants’ perseverance
and topic development abilities, which are pertinent to this study.

Procedures
Original Design
The original design of this study called for mostly freshmen and sophomore
students to take a Web-based information literacy assessment and a Web-based critical
thinking assessment during two, separate one-hour, evening periods in a proctored,
computer classroom. Students were recruited in person from second-semester
freshman composition courses in brief ten-minute presentations at the beginning of
class.
The presentations introduced the students to the study and provided a concise
explanation of the goals of the study along with the intended incentives: gift cards. Only

54

students who were age 18 and older were eligible to participate in this study. Students
were given a flyer with the dates, times and location of each assessment along with a
Web address to an online signup site. The following schedule is a basic outline of how I
envisioned the testing administration to proceed. The actual description of a typical
testing administration is included below under Modified Design.

Test Administration
Day 1
•

Students arrive and take a seat at an open computer. I remind them of the
purpose of the study and provide them the option to opt out of the study.

•

Students are given two copies of the disclosures and consent form. One copy
is for them to keep while the other is for them to read, sign and return. No
parental consents are required because the students are 18 or older. While
the study requires the use of individual student names, students are informed
of the procedures to maintain the confidentiality of their individual results.

•

As soon as the disclosures and consent forms are returned, students are
instructed on how to take the test and asked to log on to the Project SAILS
online test. The test takes approximately 45-50 minutes to complete. Students
leave after completing the test.

•

Students receive a brief reiteration of the goals of the study and the measures
provided to maintain the confidentiality of their individual results.

•

Students are given instruction on how to take the CCTST and then asked to
login and begin. The assessment takes 45 minutes to complete, after which
the students leave.

Day 2

Interviews
Within two weeks of completing the assessments, the initial goal was to select
ten to fifteen students based upon the results of their Project SAILS and CCTST
55

assessments to participate in interviews lasting no more than one hour. Selection of the
participants would have been as follows: 1.) One to three student(s) scoring high on the
Project SAILS assessment 2.) One to three student(s) scoring low on the Project SAILS
assessment 3.) One to three student(s) scoring high on the CCTST and 4.) One to three
student(s) scoring low on the CCTST. Participants were not to be informed as to why
they were chosen.
Interviews were to be conducted face-to-face in my office and recorded for
transcription at a later time. Informed consent and confidentiality notification procedures
would have been followed before beginning the interviews.

Modified Design
For several reasons related to low institutional enrollment and lack of student
participation, much of my methodology had to be rethought while still preserving the
integrity of the study.
The recruitment began as intended by contacting the faculty members teaching
the second-semester composition courses via email to request a brief audience with
their students. I sent the initial email on February 20, 2017 and completed five class
presentations by the first testing date, which was February 27, 2017. I had distributed
the flyers with a signup site Web address during the presentation so that I could track
potentially interested students.
However, by mid-week, all indicators suggested that student participation was
going to be low. On the first and second day of testing, there were no student

56

participants. After consulting with my committee, I requested a modified design from the
IRB in the following manner:
•

I sought and was granted permission from the study institution’s
administration to administer the assessments during the chapel hour and offer
students who participated a chapel release as an added incentive.

•

I expanded the recruitment pool to include other select freshman-level
courses.

•

I increased the chances of winning one of the gift cards.

These modifications helped raise the participation rate considerably. However,
some unintended consequences of the modifications included the fact that I was not
able to recruit in person due to the broader range of classes and class times and I did
not have complete control over who showed up for testing, which could have potentially
impacted my data.

Test Administration: SAILS and CCTST Administrations
•

Students began arriving one or two at a time from about 10 minutes prior to
the exam start to approximately 20 minutes into the exam period.

•

As students arrived, I made sure they read and signed the informed consent
form and completed the SAILS ID form needed to access their results.

•

Once the informed consent procedures were completed, I helped them login
to the appropriate exam site and made sure they understood how it
functioned.

•

I repeated this process for each new participant until they came too late to be
able to complete the exams in the time allotted.

•

I remained in the classroom until the last student finished or timed out and
left.

The initially low participation rates and subsequent modifications had
ramifications going forward as well. For instance, the time required to request and

57

receive the IRB modification approval put a halt on all further progress until approval
was received. Additionally, testing dates were not under my control and occurred over a
two-week period in late-March and early-April. Finally, the lateness of the testing
periods necessitated a change in the qualitative design from face-to-face recorded
interviews to questionnaires sent via school email.
With regard to confidentiality procedures, test results from both the SAILS and
the CCTST contained semi-sensitive information about individual students. Therefore,
during the analysis, I stored electronic results on a password-protected spreadsheet on
my personal laptop, which was also password protected. Results of the analyses run on
the test scores and the questionnaire were also stored on the laptop. Only I know the
passwords to the spreadsheet and laptop. Upon completion of the study, all data was
transferred to a password protected flash drive for long term storage.

Data Analysis
At the most basic level, this was a correlational study. With this in mind, I ran a
Pearson’s r on the individual participant results of the SAILS and the CCTST to
determine whether a basic correlation existed. Pearson’s r is a standard correlation
statistic employed with variables that are at interval level or above (Krathwohl, 2009).
Additionally, I used a two-tailed test to determine whether the SAILS and CCTST
correlation was statistically significant. Two-tailed tests are the appropriate test to use
when the researcher does not know what result to expect (Krathwohl, 2009, p.444).

58

In addition to the analysis of the overall SAILS and CCTST results, I also
conducted Pearson’s r analyses between the SAILS and CCTST subskill scores along
with effect size statistics.
In order to test the differences between females and males on the SAILS and the
CCTST, an independent samples t-test was the appropriate statistical analysis to use
for these types of scores and anticipated results. Independent samples t-test is a
statistical test that compares the means of two autonomous groups to determine
whether there is a statistically significant difference.
With regard to the questionnaire, the first item was multiple-choice and the final
four items were open-ended, which required slightly different methods of analysis. As a
multiple-choice item, the first response was a little more amenable to quantitative
analysis while providing additional context to the quantitative results. I ran summative
statistics on the responses and means analyses on the respective SAILS and CCTST
scores.
Finally, using Qualitative Methods in Social Research (Esterberg, 2002) for
guidance on the analysis of qualitative data, I reviewed the responses from the openended items at several different times and observed patterns in responses within
individual items and across the field of the final four items. I employed inductive content
analysis to create categories based on the broad consensus of the participants’
responses and, then, further refined the categories in later review sessions. Using the
newly created categories, I was able to provide some context to the SAILS and CCTST
scores of the six participants (Appendix B). Due to the small number of responses,
using more than basic descriptive statistics and, then, only with qualification, was not

59

going to provide reliable, meaningful data. However, running means analysis on the
SAILS and CCTST scores in relation to the categorized responses provided a few
cautious insights for further exploration.

Methodological Considerations
Assumptions
An assumption of the study was that participants were intellectually prepared to
engage in higher order thinking activities at an advanced level. Perry (1999) suggested
that students are still developing their intellectual abilities in their college years and that
they develop at different paces. Because the instruments in this study were
administered in courses designated for students in their second semester of college,
participants were more likely to have reached comparable levels of higher order
thinking.
Another assumption was that the population of students at the university was
fairly representative of the U.S. college population. Student demographics were
obtained for the on-campus population during the Spring 2017 semester. The national
demographics were obtained from the Fall 2015 Integrated Postsecondary Education
Data System (IPEDS) Trends Generator, which were the latest national statistics
available. Table 2 shows the comparisons between the university and national
demographics.
As of Fall 2016, the average American College Test (ACT) Composite score for
first-time, full-time, degree-seeking students at the university was 20.3, which was close

60

to the national average of 21 for students who took the ACT from 2013-2015
(http://www.act.org).
Table 2
University and National Demographic Comparison
University a

National b

50/50

56/44

26

N/A

Black

13%

13%

Hispanic/Latino

24%

16%

Other Ethnicities

7%

20%

White

56%

52%

Demographic Category
Gender (Female/Male)
Age

Race/Ethnicity

a

Spring 2017 On Campus population b (http://nces.ed.gov).

Limitations
A limitation of this study was the small sample size. Based on the study
population, the sample size needed to achieve generalizability would have exceeded
100 participants. However, it was clear from the early stages that generalizability was
not achievable. The two reasons for this were: a) The university asked that the
disruption to the courses be minimized. b) I had a small budget with which to purchase
both assessments.
The study required participants to take two 45-minute assessments at different
times in a proctored environment. Additionally, the announcement of this study was
conducted in person during the first part of class. The institution’s administration, while
supportive of the study, was concerned about the amount of class time being consumed
by the administrative portions of the study.
61

Another limitation of this study was the possible introduction of bias throughout
the research process. Two types of bias, in particular, were the most likely to occur:
proficiency bias and selection bias.
Proficiency bias has to do with the varying levels of prior instruction the
participants have received in information literacy and/or critical thinking instruction.
Though it is likely that a majority of the participants would have had some type of
information literacy instruction at the secondary level, students who started their college
studies at the institution directly after high school would have been enrolled in an
orientation course where an introduction to the university library and searching tools
were part of the curriculum. However, for students who transferred to the institution as a
first semester sophomore, their background in information literacy instruction at any
level is difficult to quantify.
With regard to critical thinking instruction, there are a couple of different
concerns. First, the popularity of critical thinking as a learning concept in secondary and
post-secondary education makes it probable that the greater part of the on-campus
population has had some exposure to critical thinking instruction. Second, the institution
was engaged in a critical thinking study for its accreditor where students were being
instructed in critical thinking concepts. However, the recruitment pool of students who
are new to the institution limited the exposure they received from the university to only
one or two sessions of critical thinking instruction.
Selection bias revolves around how the participants for the study were recruited
and selected. Ideally, the study population should represent the target population in
characteristics that pertain to the study research questions or hypotheses. Within this

62

study, selection bias was most probable through the recruitment process where
students were introduced to the components of the study.
Because the study required participants to sit for two standardized assessments
lasting approximately 45 minutes each and, then, complete a qualitative component, it
was plausible that motivated and academically high-achieving students would make up
the majority of the participants and, thus, skew the results.
However, due to early issues with low participation and the resulting
modifications to the study, including a strategic broadening of the recruiting pool and
increased incentives, the pool of possible participants grew as well as the probability
that participants would be more intellectually and academically diverse than I had
intended.

Reliability and Validity
At the most basic level in scientific research, reliability is a measure of the
consistency of a result. The question being, “If I repeated the experiment under the
same conditions, would the results be the same?” With regard to reliability of the SAILS,
developers tested the assessment using Cronbach’s Alpha, which returned α=.80. Like
correlation statistics, Cronbach’s Alpha is measured on a scale of zero (0) to one (1).
Therefore, most researchers consider a score of over .70 to be sufficiently reliable.
When measuring the reliability of the CCTST, Insight Assessment used the KR20 coefficient. The results of the KR-20 coefficient measure indicated that the CCTST
was between .77 and .83, which is high enough to consider the test sufficiently reliable
(http://www.insightassessment.com).

63

The other measure that is important to the assessment of a construct is validity.
Simply put, validity is a measure of whether the assessment is actually evaluating the
construct it is purporting to evaluate. There are three types of validity pertinent to this
study: Construct validity, Content validity and Internal validity.
Construct validity assesses the ability to make inferences from the constructs
that are being assessed. Put another way, “Construct validity is an assessment of how
well you translated your ideas or theories into actual programs or measures”
(www.socialresearchmethods.net).
The developers of the SAILS correlated the assessment with another validated
assessment of information literacy, the Information Literacy Test from James Madison
University. The disattenuated correlation was .72 (https://www.projectsails.org). The
strong positive correlation suggests that there is confidence that the SAILS is sufficiently
valid.
The developers of the CCTST also correlated their assessment with other
assessments that measure critical thinking. For instance, the CCTST has a strong
correlation with the Graduate Record Exam (GRE) with a Pearson’s r of .719 (August,
2017). Additionally, several researchers found that students’ scores on the CCTST
increased after learning about critical thinking concepts (Barlett & Cox, 2002; Spelic,
Parsons, Hercinger, Andrews, Parks & Norris, 2001).
Another form of validity is content validity where the construct being measured
needs to be properly and thoroughly defined. Content validity is a measure of how well
the test represents the construct it is assessing. Drost (2011) states that, “content
validity is a qualitative means of ensuring that indicators tap the meaning of a concept

64

as defined by the researcher” (p.118). Further, Krathwohl (2009) states, “Once we know
the content of the subject matter that the instrument is intended to sample, we can
analyze the instrument to show that it does, indeed, representatively sample it” (p.407).
The important concept to grasp in this definition is that the test developer has to
have a good understanding of the construct that he/she wants to assess. Without
content validity, construct validity is impossible to obtain because the construct to be
measured is ill-defined.
The SAILS was constructed around the ACRL’s Information Literacy
Competency Standards for Higher Education, which were developed by qualified
professionals and designed in such a way that the knowledge and skills were broken
down into three levels of increasing specificity. Due to this attention to detail, the SAILS
assessment is founded on a clear construct.
The CCTST is based upon the definition and concepts of critical thinking
developed by the APA’s Delphi study, which was comprised of experts in critical
thinking. Additionally, the definition and concepts were further operationalized by the
experts through skill and subskill definition (Facione, 1990a). Therefore, the construct is
well-defined, and the assessment is based on a solid foundation (August, 2017).
The final type of validity relevant to this study is internal validity. Internal validity
measures the characteristics of the relationship between two variables in a study. For
instance, researchers may have influences that are unaccounted for in their study that
bias the results in one direction or another. Among the threats to this type of validity are,
“history, maturation, testing, instrumentation, selection, mortality, diffusion of treatment
and compensatory equalisation [sic], rivalry and demoralisation [sic]” (Drost, 2011,

65

p.115). However, most of these threats can be mitigated by a well-planned and
executed study.
With regard to the questionnaire created for this study, reliability and validity were
not assessed prior to using the instrument. However, the questionnaire was originally
meant to provide a qualitative context to help mitigate the small sample size of the
quantitative results and not necessarily to serve as an additional quantitative instrument.

Summary
This chapter described the methodology used in this exploratory, mixed-methods
study investigating the correlation between information literacy and critical thinking. To
accomplish this task, this study employed two standardized assessments, the SAILS
and the CCTST, and a questionnaire created for this study. The SAILS is an information
literacy assessment based on Competencies I, II, III and V of the ACRL’s Information
Literacy Competency Standards for Higher Education. It is a proctored, web-based,
multiple-choice measure of students’ information literacy knowledge and abilities.
The critical thinking assessment used in this study was the CCTST by Insight
Assessment. The CCTST is also a proctored, web-based, multiple-choice assessment
and is constructed around the work of the American Philosophical Association’s (APA)
Delphi study.
The final instrument used for this study was a questionnaire created specifically
for this study. The intent of the questionnaire was to provide additional information
about the participants and context to their SAILS and CCTST scores.

66

CHAPTER 4
RESULTS
Introduction
The purpose of this exploratory study was to investigate the possibility of a
correlation between information literacy skills and critical thinking skills in undergraduate
college students. For the quantitative aspect of this study, I used IBM’s SPSS statistical
analysis software. I also consulted the Office of Research Consulting (ORC) in the
College of Education at the University of North Texas for guidance as to the appropriate
statistical tests to employ to answer the three research questions:
1. How do information literacy and critical thinking relate in undergraduate
students conducting academic research?
2. What differences are there in information literacy skills between males
and females conducting academic research?
3. What differences are there in critical thinking skills between males and
females conducting academic research?

Sample Characteristics
I chose to conduct this study at a small, private university in Texas. The choice of
this location was guided by access considerations and the critical thinking activities in
which the university was already engaged as a result of its reaffirmation of accreditation.
The university had both on-campus and online instructional methods with an overall
population of approximately 2,000 students ranging from recent high school graduates
to adult-learners. The on-campus student population, from which the sample for this
study was drawn, consisted mostly of recent high school graduates or students with

67

limited post-secondary experience beyond high school who made up roughly half of the
overall institution population.
Initial invitations were delivered in person to eight sections of the secondsemester composition and rhetoric course. Enrollment in the sections ranged from 7
students to 17 students. The broader dissemination of study invitations necessitated by
the lack of initial participation went to 11 additional sections of freshman-level courses in
other areas within General Education.
The second invitation was emailed directly to the professors, who then distributed
the information to their classes. The second recruitment invitation resulted in 35
participants who took one or both assessments, which finally yielded 28 viable study
participants.
The study sample was roughly similar to the demographic makeup of the
institution as a whole (Table 3). The original study design targeted freshmen and
sophomores, who comprised 86% of the final sample under the revised invitational pool.
Table 3
Sample and University Demographic Comparison
Sample

University a

54/46

50/50

20

26

Black

11%

13%

Hispanic/Latino

14%

24%

Other Ethnicities

7%

7%

White

68%

56%

Demographic Category
Gender (Female/Male)
Age

Race/Ethnicity

a

Spring 2017 On Campus population

68

In addition to basic demographic information, the CCTST permitted the collection
of self-report information about the participants’ enrollment status at the institution and
any prior critical thinking instruction. Tables 4 and 5 display the aggregate of those
responses.
A majority of the participants were within their first year at the institution (Table
4), which helps inform the data in Table 5 regarding the participants’ prior critical
thinking instruction.
Table 4
Initial Enrollment Status as Reported by Participants on CCTST
Initial Enrollment Status

First-time Freshman

Transfer Student

a

Sem a

n

4

3

2

17

1

2

3

1

2

4

1

1

Number of semesters at the study institution

Table 5 shows that the preponderance (n = 16) of the participants had no prior
critical thinking instruction. Of the participants indicating that they had received prior
instruction in critical thinking, most (n = 10) specified that their instruction was at the
high school or high school/college level. In light of the results in Table 5, it is likely that a
majority of the participants received their prior critical thinking instruction in high school.

69

Table 5
Prior Critical Thinking Instruction as Reported by Participants on CCTST
Participant Response

Instruction Venue

No Prior Instruction

N
16

Had Prior Instruction

High School

6

High School/College

4

College

2

Unfortunately, the SAILS did not request information on the participants’ prior
information literacy instruction.

Quantitative Results and Analysis
SAILS and CCTST Scores
The two assessments I used for this study were the Standardized Assessment of
Information Literacy Skills (SAILS) from Carrick Enterprises and the California Critical
Thinking Skills Test (CCTST) from Insight Assessment. In addition to producing an
overall score for each skill, the SAILS and CCTST produced categorical subskill scores
to provide information on the narrower skills related to information literacy and critical
thinking. The highest possible score for the SAILS is 100% and the highest possible
score for the CCTST is 100 points. The overall scores for the 28 participants who took
both assessments are listed in Table 6.
The scores referenced in Table 6 indicate that participants generally performed
better on the critical thinking assessment (CCTST) than the information literacy
assessment (SAILS). This observation is interesting because the CCTST appeared to
be the more difficult of the two assessments. The SAILS assessment had 55 items and
70

took students an average of 26 minutes to complete while the CCTST had 34 items with
students taking an average of 35 minutes to complete.
Table 6
Overall SAILS and CCTST Scores by Participant
Participant

SAILS

CCTST

1

56.4

66.0

2

54.5

66.0

3

61.8

66.0

4

47.3

73.0

5

52.7

71.0

6

58.2

78.0

7

58.2

69.0

8

60.0

71.0

9

50.9

73.0

10

52.7

75.0

11

52.7

72.0

12

38.2

69.0

13

49.1

66.0

14

58.2

65.0

15

58.2

72.0

16

40.0

68.0

17

69.1

72.0

18

49.1

76.0

19

50.9

75.0

20

70.9

82.0

21

49.1

73.0

22

27.3

69.0

23

47.3

71.0

24

83.6

94.0

25

40.0

62.0

26

47.3

66.0

27

63.6

75.0

28

74.5

86.0

Mean

54.4

72.2

71

Put into further context, participants answered a little over two items per minute
on the SAILS and approximately one item per minute on the CCTST. However, in spite
of their apparent difficulty with the CCTST, participants generally scored higher on it
than on the SAILS. The mean for the SAILS assessment was 54.4 with a high score of
83.6 and a low score of 27.3, while the mean for the CCTST assessment was 72.2 with
a high score of 94.0 and a low score of 62.0.

Correlational Analysis of SAILS and CCTST Scores
I conducted a Pearson’s Product-Moment Correlation Coefficient, (Pearson’s r)
analysis on the overall information literacy (SAILS) and critical thinking (CCTST)
assessment scores for each of the 28 participants as referenced in Table 6. In many
studies where a correlation analysis is the requisite statistical test, Pearson’s r is
typically the standard test researchers use.
One of the assumptions underlying Pearson’s r is that the variables are normally
distributed, meaning that the random variables under analysis fall evenly within a bellshaped curve when plotted on a chart. If this assumption is not met, then researchers
usually employ another bivariate analysis such as Spearman’s rho. The Shapiro-Wilk
Test of Normality is one of the tests that assesses the normal distribution of variables.
The significance statistic of the Shapiro-Wilk is the primary indicator of normality.
If p < .05, then the variable is not normally distributed. Unfortunately, as indicated by
Table 7, while the Shapiro-Wilk test indicated that the SAILS scores were normally
distributed (p =.653), it also indicated that the scores of the CCTST (p =.005) were not
normally distributed.

72

Table 7
Shapiro-Wilk Test of Normality of SAILS and CCTST Scores
Variable

Statistic

df

Sig

SAILS Scores

.973

28

.653

CCTST Scores

.884

28

.005

However, to verify the findings of the Shapiro-Wilk test, I also ran skewness and
kurtosis statistics. Skewness and kurtosis statistics are another way of determining
normal distribution. Skewness is a measure of the symmetry of a distribution around the
mean, while kurtosis is a measure of the outliers within the data (Kline, 2011).
Researchers typically state that the closer that the skewness statistic is to zero,
the more normally distributed the data, and that the kurtosis statistic for normal
distribution is between -3 and 3. As suggested by Table 8, the SAILS statistics are
within normal parameters, but the CCTST statistics indicate more skewness and
kurtosis than is generally accepted by many researchers.
Table 8
SAILS and CCTST Score Skewness and Kurtosis Statistics
Assessment

Skewness

Kurtosis

SAILS

.280

.996

CCTST

1.484

3.156

However, some researchers assert that given the right circumstances, like a
small study population, skewness and kurtosis statistics could be considered within
normal range if they are less than 3 and 10, respectively. Therefore, continuing with the
Pearson’s r analysis rather than Spearman’s rho was appropriate.

73

The Pearson’s r test results indicated that there was a statistically significant,
positive moderate correlation (r(26) = .659, p = .000) between the information literacy
and critical thinking scores, which gives credence to the suggestion that the two
concepts might be related in certain areas. Additionally, I calculated the coefficient of
determination for greater insight into the results. The coefficient of determination
expresses the size of the effect that one variable has on another. The coefficient of
determination for SAILS and CCTST scores was r2 = .43, indicating that 43% of the
variability in the CCTST scores can be explained by the SAILS scores.

Correlational Analysis of SAILS and CCTST Subskill Scores
A review of the average subskill scores for each assessment also provided some
perspective about the participants’ information literacy and critical thinking skills. The
SAILS defined eight subskills to assess the students’ abilities throughout the research
process from the development of a research question to understanding the issues
surrounding primary and secondary research.
While Project SAILS provided the overall information literacy score for each
participant, it did not provide the individual subskill scores. Therefore, I derived the
scores manually for each participant based on the information provided by Project
SAILS identifying the questions that relate to each subskill. The subskill scores were
converted to percentages by dividing the number of correct answers by the total number
of questions for that skill set, then the scores were averaged across the participants.
Because I chose to use this method to calculate the subskill scores, they cannot be

74

used to calculate or refer back to the whole. They must only be used within the specific
context of the subskill scores.
The SAILS subskill score means of the 28 participants who completed both the
SAILS and the CCTST assessments are shown in Table 9. Participants had an overall
average of 54.4% with the highest subskill score (Developing a Research Strategy) of
63.2% and the lowest subskill score (Searching) of 46.7%.
Table 9
SAILS Subskill Score Means
Subskill

Mean

Developing a Research Strategy

63.2

Selecting Finding Tools

53.1

Searching

46.7

Using Finding Tool Features

57.1

Retrieving Sources

63.1

Evaluating Sources

55.0

Documenting Sources

48.9

Understanding Economic, Legal & Social Issues

47.6

Mean

54.4

Generally, the participants appear to have scored low on most skill sets of the
SAILS assessment, which subsequently led to a low overall score. However, as a part
of the assessment package, Project SAILS also provided a benchmark spreadsheet that
gives the collective scores of over 16,300 students from 57 other institutions.
Although the participants’ overall score initially seemed low, it is slightly higher
than the aggregated mean score (53.4%) of the other institutions that used the SAILS
assessment. This provides some perspective about the abilities of the study participants
compared to students at other institutions. While the relative parity of scores indicates
75

that the participants at the study university are not behind other institutions, it also
demonstrates that U.S. students as a whole may have substantial difficulties with
information literacy skills.
According to the CCTST User’s Manual and Resource Guide (August, 2017),
Insight Assessment categorizes critical thinking abilities into five areas of performance:
Not Manifested, Weak, Moderate, Strong, and Superior. Table 10 indicates the score
ranges for each performance category. The mean CCTST score for the participants was
72.2 (Table 6), which puts them on the low end of the Moderate category.
Table 10
CCTST Performance Categories
Performance Category

Score Range

Not Manifested

50-62

Weak

63-69

Moderate

70-78

Strong

79-85

Superior

86-100

Table 11 indicates that the highest mean subskill score attained by participants
taking both the SAILS and CCTST assessments was in Interpretation at 78.5% and the
lowest mean subskill score was for Evaluation at 71.6%. The CCTST performance
categories from Table 10 hold true for the subskill areas as well. As such, the
Interpretation score is on the border between Moderate and Strong ability and the
Evaluation score demonstrates Moderate ability.
It is consequential to this study that the mean Evaluation subscore was the
lowest. One of the underpinnings of information literacy is the ability to effectively

76

evaluate information and the sources of information. Therefore, the fact that the
participants scored the lowest on that particular critical thinking subskill may partly
explain their difficulty with the SAILS assessment as a whole.
Table 11
CCTST Subskill Score Means
Subskill Area

Mean

Interpretation

78.5

Analysis

72.0

Inference

73.7

Evaluation

71.6

Explanation

73.3

Induction

75.9

Deduction

72.4

Mean

72.2

In addition to examining each assessment’s subskill scores, I also performed a
correlational analysis between the individual SAILS and CCTST subskill scores.
However, in light of the distribution issues encountered with the overall CCTST scores,
skewness and kurtosis analyses were run. Unsurprisingly, the skewness and kurtosis
statistics indicated slight distribution irregularities with some of the CCTST subskill
scores.
Table 12 shows that the skewness and kurtosis statistics for Evaluation and
Induction are marginally higher than some researchers would find acceptable. However,
as explained previously, under certain circumstances such as small sample sizes,
skewness statistics under three and kurtosis statistics under ten would still be

77

acceptable for normal distribution. Therefore, using Pearson’s r to analyze the subskill
scores was the best statistical test for this situation.
Table 12
Skewness and Kurtosis Statistics for CCTST Subskill Scores
Subskill

Skewness

S.E.

Kurtosis

S.E.

Analysis

.542

.441

-.280

.858

Interpretation

-.194

.441

-.726

.858

Inference

.716

.441

.829

.858

Evaluation

1.481

.441

3.592

.858

Explanation

.974

.441

1.636

.858

Induction

1.087

.441

2.935

.858

Deduction

.870

.441

.517

.858

Note. S.E.= Standard Error

As indicated by Table 13, of the 56 possible combinations, 14 subskill score
combinations were significantly, positively and moderately correlated. Of the 14 subskill
score combinations, the focus shifted to the four highest correlation combinations with
the greatest significance.
The four highest SAILS and CCTST subskill score pairs are the first four
combinations and designated by asterisks. In addition to the correlation coefficient and
significance statistic, Table 13 also includes the coefficient of determination or r2 statistic
for the four highest combinations.
The first of these combinations, Evaluating Sources & Analysis, represents a
high moderate correlation (r = .628) that was significant at p < .001. Additionally, the
coefficient of determination was r2 = .39, which indicates that 39% of the variability in the
CCTST subskill score is explained by the SAILS subskill score.

78

Evaluating Sources is the SAILS subskill relating to how students make credibility
decisions regarding information. It measures the student’s ability to assess sources of
information during research. Analysis is the CCTST subskill that evaluates a student’s
aptitude for identifying the assumptions and assertions undergirding an argument.
The second highest subskill score combination was Using Finding Tool Features
& Induction, with a moderate correlation (r =. 517) and a statistical significance of p <
.01. The coefficient of determination was r2 =.27 indicating that 27% of the variation in
the CCTST subskill score is explained by the Induction SAILS subskill score, Using
Finding Tool Features.
Using Finding Tool Features is a SAILS subskill that measures a student’s ability
to effectively use the various tools offered by information repositories to search for
information, while Induction is the CCTST subskill representing one of two methods of
inferential thinking. In particular, Induction, according to the CCTST developers, has to
do with drawing general conclusions from a set of data while understanding that the
conclusions could still be false even if the premises were true.
The third and fourth highest subskill score combinations, respectively, were
Evaluating Sources & Deduction (r =.490) and Developing a Research Strategy &
Evaluation (r =.489). Both combinations were statistically significant at p < .01.
With a .490 correlation, Evaluating Sources & Deduction is firmly within the
moderate strength territory. As Evaluating Sources has been defined previously, I will
turn to the CCTST subskill, Deduction. The CCTST developers described the Deduction
subskill as logical reasoning, which asserts that the premise of an argument and the

79

resulting conclusion are intricately linked in such a way that if one is true, the other must
be true as well.
Table 13
SAILS and CCTST Subskill Combinations with Four Highest Designated
CCTST
Subskill

SAILS Subskill

Corr.

Sig.

r2

Evaluating Sources

Analysis

.628*

.000

.39

Using Finding Tool Features

Induction

.517*

.005

.27

Evaluating Sources

Deduction

.490*

.008

.24

Developing a Research Strategy

Evaluation

.489*

.008

.24

Searching

Deduction

.467

.012

--

Searching

Evaluation

.458

.014

--

Evaluating Sources

Inferences

.456

.015

--

Developing a Research Strategy

Induction

.449

.017

--

Developing a Research Strategy

Analysis

.444

.018

--

Understanding Economic, Legal & Social
Issues

Analysis

.443

.018

--

Searching

Inference

.408

.031

--

Using Finding Tool Features

Evaluation

.406

.032

--

Developing a Research Strategy

Explanation

.403

.034

--

Evaluating Sources

Induction

.377

.048

--

* One of four highest correlations.

Gender Differences in SAILS and CCTST Scores
The second and third research questions for this study involved the investigation
of possible gender differences in the SAILS and CCTST scores. I ran an independent
samples t-test on the SAILS and CCTST scores disaggregated by gender. The
independent samples t-test is the appropriate analysis for testing the means of
continuous variables between two different randomly selected populations. The intent of

80

the t-test is to determine whether the observed differences between populations are a
result of anything other than chance. Table 14 displays the SAILS and CCTST results
for each participant by gender. Of the 28 participants who took both assessments, 15
were females and 13 were males. The mean scores of the SAILS and CCTST
assessments between females and males indicate that females (µ = 56.2) scored
slightly higher overall than males (µ = 52.2) on the SAILS, while males (µ = 73.2) scored
slightly higher than females (µ = 71.3) overall on the CCTST.
Table 14
Individual Participant SAILS and CCTST Scores by Gender
Females (n = 15)

Males (n = 13)

56.4

66.0

47.3

73.0

54.5

66.0

58.2

78.0

61.8

66.0

58.2

69.0

52.7

71.0

52.7

72.0

60.0

71.0

49.1

66.0

50.9

73.0

58.2

72.0

52.7

75.0

40.0

68.0

38.2

69.0

49.1

76.0

58.2

65.0

50.9

75.0

69.1

72.0

49.1

73.0

70.9

82.0

27.3

69.0

47.3

71.0

63.6

75.0

83.6

94.0

74.5

86.0

40.0

62.0

--

--

47.3

66.0

--

--

56.2

71.3

52.2

73.2

Mean

Although the results of the SAILS by gender showed that females scored better
than males overall, the t-test results (t(26) = -.92, p =.37) indicated no statistically
81

significant difference between males and females, suggesting that the mean results are
possibly the result of chance. Additionally, while the mean results of the CCTST
between females and males indicated that males performed better than females on the
assessment, the independent t-test results for the CCTST (t(26) = .76, p = .45) again
indicated that the results were not statistically significant.

Qualitative Results and Analysis
The intent of adding the qualitative portion of the study was to offer additional
insight into the participants’ cognitive processes with regard to their information literacy
and critical thinking abilities. I created a questionnaire (Appendix A) and distributed it to
all 28 participants through their school email addresses.
Six of the 28 participants returned completed questionnaires. The six
questionnaire respondents had a wide distribution of assessment scores for the SAILS
and CCTST, although their mean scores of 62.1 on the SAILS and 77.5 on the CCTST
(Table 15) were higher than the mean scores of 54.4 and 72.2, respectively, for all
participants.
Table 15
Questionnaire Respondents’ SAILS and CCTST Scores
Respondent

SAILS

CCTST

1

54.5

66.0

2

47.3

73.0

3

60.0

71.0

4

52.7

75.0

5

83.6

94.0

6

74.5

86.0

Mean

62.1

77.5

82

The questionnaire consisted of five items asking for information about the
students’ academic information-seeking practices as they relate to information literacy
skills. I was not seeking a tally of student practices as much as their decision-making
paradigm while using the practices. Item 1 was a three-part ranking of information
sources. Items 2 through 5 were open-ended inquiries about the students’ information
source selection criteria.

Source Preferences
The first item used Kuhlthau’s Information Search Process as a basis for inquiry
(Kuhlthau, 1983; Kuhlthau, 1991). Specifically, I was interested to know what
information sources respondents considered useful during the three middle stages of
Kuhlthau’s information-seeking theory: exploration, formulation and collection.
As a brief review, exploration involves the students’ initial, extended foray into
their topic. In this stage, students generally search for broad definitions or explanations
of their subject as a way of orientation. It is typically the most turbulent of the three for
students as they attempt to reconcile their preconceived ideas of a topic with the new
information they encounter. Students enter a more focused search for information as
their topic narrows during formulation. Students may still encounter new information, but
it is not wholly unanticipated, and they are usually able to either assimilate it or reject it
based on their inquiry and their information literacy abilities. Collection involves the
students’ more confident and narrow search of their particular topic. Collection moves
students away from broad explanations and definitions and into more complex
discussions of their specific areas of interest. As the stage just prior to the presentation

83

of their information, collection requires a refinement of the students’ thoughts as they
interpret their findings through research (Kuhlthau, 1991).
The respondents’ answers to the first item were intriguing, but also showed the
necessity of continued instruction in information literacy skills. The respondents were
asked to select their first three choices of information sources for each of the three
stages. They chose six of the eight source categories provided (Appendix A).
Table 16 provides their responses, across the stages, as well as the average
SAILS and CCTST scores for those responses. Respondents indicated that they used
online databases more often than any other information source, with textbooks and
topic-specific books following closely behind. Those who chose internet search had the
highest average SAILS scores and those who chose topic-specific books had the
highest average CCTST scores.
Table 16
Information Source Choice by ISP Stage with SAILS and CCTST Scores
Source

n

Poss

%

Expl.

Form.

Coll.

SAILS

CCTST

Textbook

11

54

20

5

3

3

50.4

76.5

Professor

7

54

13

3

2

2

54.7

69.6

Internet Search

10

54

19

3

4

3

66.5

80.5

Books about
Topic

11

54

20

3

4

4

64.3

81.5

General
Reference

2

54

4

1

1

0

61.5

65.5

Online
Databases

13

54

24

3

4

6

65.3

76.4

Note. Expl. = Exploration, Form. = Formulation, Coll. = Collection

Tables 17, 18 and 19 display the respondents’ responses by each stage with the
average SAILS and CCTST scores per response. Table 17 shows the responses
84

collected with regard to the Exploration stage of Kuhlthau’s Information Search Process
in addition to the average SAILS and CCTST scores for the respondents.
Table 17
Information Source Choice in ISP Exploration Stage with SAILS and CCTST Scores
na

SAILS

CCTST

Textbook

3

55.7

70.6

Internet Search

2

65.5

83.5

Online Databases

1

74.5

86.0

Textbook

2

79.1

90.0

Professor

2

57.3

68.5

Internet Search

1

52.7

75.0

Books about Topic

1

47.3

73.0

Books about Topic

2

67.3

78.5

Online Databases

2

65.5

83.5

Professor

1

52.7

75.0

General Reference Books

1

54.5

66.0

Source
First Choice of
Sources

Second Choice of
Sources

Third Choice of
Sources

a

No. of Responses

There were so few responses that outlier scores in either direction greatly affect
the mean. For instance, two respondents indicated that the Internet was their first
choice of information sources. The average SAILS and CCTST scores representing this
answer were 65.5 and 83.5, respectively. These scores would place this choice as the
second highest among the three. However, the individual scores reveal that there is a
wide disparity between the two sets of scores, with one respondent scoring 47.3 and
73.0 on the SAILS and CCTST and the other respondent scoring 83.6 and 94.0,
respectively. Therefore, the means have limited value for evaluating respondent
answers. This situation is true for all respondent results across the three ISP stages.

85

Despite these limitations, two sets of scores warrant some attention. Within the
first choice of sources, online databases received one response, but that respondent
had high SAILS (74.5) and CCTST (86.0) scores relative to the group. Additionally, in
the second choice of sources, textbook received two responses with both respondents
scoring the highest on the SAILS and the CCTST in the group leading to the highest
overall SAILS and CCTST scores for that stage.
Analysis of the responses for the formulation stage (Table 18) of the information
search process reveals that internet search appears consistently across the three
choices as do textbook sources and books about topic. Additionally, there was more
variation in the responses than in the exploration stage.
Table 18
Information Source Choice in ISP Formulation Stage with SAILS and CCTST Scores
na

SAILS

CCTST

Online Databases

2

79.1

90.0

Books about Topic

2

54.5

66.0

Internet Search

1

56.4

66.0

Textbook

1

47.3

73.0

Books about Topic

2

65.5

83.5

Internet Search

1

74.5

86.0

Textbook

1

52.7

75.0

General Reference Books

1

60.0

71.0

Professor

1

54.5

66.0

Online Databases

2

56.4

73.0

Internet Search

1

83.6

94.0

Books about Topic

1

74.5

86.0

Professor

1

47.3

73.0

Textbook

1

54.5

66.0

Source

First Choice of
Sources

Second Choice
of Sources

Third Choice of
Sources

a

No. of Responses

86

In light of the issues regarding the small response pool, acknowledgement of the
SAILS and CCTST scores is limited to a general recognition of the wide range of scores
between responses and some attention to specific sets of scores. Within each of the
three choices, there is over a 20-point difference between the lowest and highest
reported means of the SAILS and CCTST assessments. The difference is a result of the
wide variation in responses that revealed the individual respondents’ assessment
scores rather than averaged scores.
In addition, as with the exploration stage, the formulation stage has some source
choice scores worth recognizing. Online databases under the first choice of sources
category had two respondents who scored high on both the SAILS and CCTST
assessments, which provided average scores of 79.1 and 90.0, respectively. Internet
search under both the second choice of sources and third choice of sources also
received relatively high SAILS and CCTST scores, though in each instance the scores
were supplied by one respondent.
Table 19 demonstrates a narrowing of response variation in the collection stage
as opposed to the formulation stage as well as a slightly reduced range of score means
between responses. Interestingly, none of the sources are listed in all three choices.
However, all of the sources are mentioned twice.
Of interest with respect to the assessment scores is that, within the first choice of
sources category, online databases with three respondents had the highest SAILS and
CCTST scores, though, that same response with the same number of respondents in
the third choice of sources category had the lowest SAILS and CCTST scores. The
Internet search response had a similar, though, reverse occurrence with the lower

87

scores appearing in the first choice of sources and the higher scores showing in the
third choice of sources.
Table 19
Information Source Choice in ISP Collection Stage with SAILS and CCTST Scores

First Choice of
Sources

Second Choice of
Sources

Third Choice of
Sources
a

Source

na

SAILS

CCTST

Online Databases

3

72.7

83.7

Internet Search

1

52.7

75.0

Books about Topic

1

47.3

73.0

Textbook

1

54.5

66.0

Books about Topic

3

72.7

83.7

Textbook

2

50.0

74.0

Professor

1

54.5

66.0

Online Databases

3

51.5

71.3

Internet Search

2

79.1

90.0

Professor

1

60.0

71.0

No. of Responses

Tables 17 through 19 indicate some general areas of note in this study. First,
students were fairly consistent in their reliance on textbooks throughout the three
stages. However, with the exception of the first stage, exploration, student scores
representing textbook use were generally lower. As suggested by other research (Head,
2007), the use of textbooks as a source of information is generally expected at the
beginning of a search for information about a topic and, indeed, the data indicated that
the majority (5) of respondents wanted to use textbooks during the exploration stage.
However, half of the respondents in each of the last two stages suggested that they
continued to use textbooks as a source of information.

88

Second, as respondents progressed through the ISP stages, their responses
indicating that they relied on online databases located at the institution library increased.
Additionally, online database usage was associated with the second highest average
SAILS score for the respondents. Finally, respondents indicated that they used topicspecific books at approximately the same rate as the Internet, with Internet users (66.5)
scoring approximately two points higher on the SAILS assessment than topic-specific
book users (64.3).
Lastly, in light of the perceived similarities between Web-based searching and
searching via online research databases, I also conducted additional analysis of the
SAILS subskill scores for respondents choosing Internet search and online databases.
Their responses to specific questions on the SAILS assessment suggested that the
respondents have a basic understanding of the differences between Internet searches
and online research databases.
This analysis required the disaggregation of the subskill score calculations within
each information seeking stage by response (Internet search or online databases).
However, instead of counting each instance of the response, I only counted the
response if it was the primary choice of the two. For instance, within the three available
information source choices of the exploration stage, if the respondent chose Internet
search as the first choice and online databases as the third choice, then the subskill
scores for online databases would not be counted for that respondent.
Although there was only a 1.2-point difference in the SAILS average score
between respondents who chose Internet search (66.5) and those who chose online

89

databases (65.3) (Table 16), the subskill average scores in Tables 20 and 21 provide a
contrasting picture.
Of interest in Table 20 is that the Evaluating Sources subskill means trend
downward as respondents continue to rely on the Internet as a primary information
source in the later ISP stages.
Table 20
SAILS Subskill Scores for Respondents Choosing Internet Search First by ISP Stage
SAILS Subskill

Exploration

Formulation

Collection

Searching

81.5

38.0

38.0

Evaluating Sources

70.0

50.0

40.0

Retrieving Sources

66.5

83.0

83.0

Documenting Sources

69.0

56.5

50.0

Selecting Finding Tool

64.0

50.0

57.0

Developing a Research Strategy

60.0

70.0

70.0

Understanding Economic, Legal & Social
Issues

41.5

50.0

33.0

Using Finding Tool Features

70.0

50.0

40.0

Note. Scores represent mean of the subskill for the respective stage of ISP.

Table 21 displays the SAILS subskill scores for respondents choosing Online
Databases as a primary source. According to Tables 20 and 21, respondents who
chose Online Databases prior to the Internet as a source of academic information
scored higher in several subskills, including Evaluating Sources, one of the primary
subskill interests of this study. Although there was a slight decline in the Evaluating
Sources subskill score at the Collection stage for respondents choosing Online
Databases (Table 21), the trend was considerably higher than for respondents choosing
Internet Search (Table 20).

90

Table 21
SAILS Subskill Scores for Respondents Choosing Online Databases First by ISP Stage
SAILS Subskill

Exploration

Formulation

Collection

Searching

63.0

75.5

63.0

Evaluating Sources

80.0

80.0

73.3

Retrieving Sources

67.0

75.0

77.7

Documenting Sources

50.0

69.0

67.0

Selecting Finding Tool

57.0

64.0

57.0

Developing a Research Strategy

100.0

95.0

86.7

Understanding Economic, Legal & Social
Issues

100.0

91.5

83.3

Using Finding Tool Features

80.0

80.0

73.3

Note. Scores represent mean of the subskill for the respective stage of ISP.

The point difference between the scores represented on Table 16 and Tables 20
and 21 above largely revolve around the method of calculating scores. Table 16
represents the overall average SAILS and CCTST scores for each instance of the
response (Internet Search, Professor, General Reference, etc.), while Tables 20 and 21
represent the average subskill score for the first of two possible responses (Internet
Search or Online Databases) from each information-seeking stage for each respondent.

Source Selection Criteria
The remaining part of the questionnaire consisted of four open-ended items
asking respondents about their source selection criteria. Using inductive content
analysis, the responses were compiled and reviewed over various sessions to
determine categories broadly representative of the responses within each item
(Appendix B).

91

Item 2 asked the respondents to consider how they chose certain sources over
others. Five of the six respondents provided responses. For this item, the respondents’
primary or first mentioned criterion was the only response categorized. I noticed two
prominent themes within the responses. The first theme was trust/credibility of sources.
Trust/credibility of sources has to do with the respondent’s desire to choose sources
that he/she could rely on to be accurate. The second theme was convenience. As the
name suggests, convenience has to do with the respondent’s need to find sources to
fulfill his/her requirements as effortlessly and efficiently as possible. After reducing the
answers to these two themes, I conducted an analysis of the respondents’ SAILS and
CCTST scores.
Table 22 shows the results of the analysis, indicating that there were slightly
more respondents who chose trust/credibility of sources over convenience as a primary
consideration. However, one respondent who initially indicated that he/she looked for
accuracy in sources also stated that sometimes convenience was a factor. The average
CCTST scores between the two groups showed very little difference, while the SAILS
scores were more pronounced and in a direction that was unexpected. Respondents
whose primary concern was trust/credibility of sources scored nearly four points lower
than respondents who viewed convenience as a primary factor in choosing sources.
Table 22
Source Choice Considerations with SAILS and CCTST Scores
na

SAILS

CCTST

Trust/Credibility of Sources

3

63.6

78.3

Convenience

2

67.3

78.5

Prominent Theme

a

No. of Respondents

92

In light of these observations, additional analyses on the SAILS and CCTST
subskill scores were warranted. Table 23 shows the respondents’ average SAILS
subskill scores by response.
After reviewing the SAILS subskill score, Evaluating Sources, this anomaly was
even more prominent. Respondents under the trust/credibility premise had an average
Evaluating Sources subskill score of 60.0 while respondents under the convenience
premise had an average Evaluating Sources subskill score of 71.5.
Table 23
SAILS Subskill Scores for Respondents by Prominent Theme
SAILS Subskill

Trust/Credibility

Convenience

Searching

67.0

50.5

Evaluating Sources

60.0

71.5

Retrieving Sources

61.0

75.0

Documenting Sources

67.0

56.5

Selecting Finding Tool

57.0

50.0

Developing a Research Strategy

73.3

85.0

Understand Economic, Legal & Social Issues

55.3

83.5

Using Finding Tool Features

60.0

70.0

Note. Scores represent mean of subskill for respective theme.

Table 24 displays the average CCTST subskill scores for the respondents based
on their response to Item 2. In contrast to the original analysis and the SAILS subskills
analysis, the analysis of the CCTST Evaluation subskill showed the trust/credibility
respondents with an average of 78.0 and the Convenience respondents with an
average of 73.5, though four of subskills were still slightly higher for convenience
respondents.

93

Table 24
CCTST Subskill Scores for Respondents by Prominent Theme
CCTST Subskill

Trust/Credibility

Convenience

Analysis

76.7

80.0

Interpretation

83.0

84.0

Inference

79.7

80.5

Evaluation

78.0

73.5

Explanation

85.0

74.0

Induction

81.7

79.0

Deduction

78.3

80.5

Note. Scores represent mean of subskill for respective theme.

Item 3 was more direct in asking respondents how they decide to trust sources of
information. Table 25 shows a breakdown of their responses along with the average
SAILS and CCTST scores associated with those responses.
Table 25
Respondents’ Criteria for Trusting Information Sources with SAILS and CCTST Scores
na

SAILS

CCTST

Author’s Credentials/Background

5

62.5

78.8

Professorial Recommendation

2

56.4

73.0

Source Genre

2

79.1

90.0

Publish Date

2

68.2

84.5

Trust Criterion

a

No. of Responses

Most respondents relied on the author’s stated academic credentials or other
authoritative background related to the topic he/she was discussing. The other three
criteria were evenly split. Unfortunately, many of the respondents mentioned only one or

94

two measures of reliability, which still leaves them open to various types of propaganda
and misinformation.
With regard to the SAILS and CCTST scores, respondents signifying source
genre as a measure of reliability had the overall highest SAILS and CCTST scores
followed by publish date, author’s credentials/background and professorial
recommendation.
Tables 26 and 27 show the SAILS and CCTST subskill scores based on the
respondents’ responses to Item 3. Within the SAILS subskills (Table 26), respondents
who responded with professorial recommendation scored particularly low on several
subskills including: Searching, Evaluating, Selecting Finding Tool, Understanding
Economic, Legal & Social Issues and Using Finding Tool Features. In fact, the
intersection of the author background response and Understanding Economic, Legal &
Social Issues subskill is the next closest low score.
Table 26
SAILS Subskill Scores for Respondents by Trust Criterion
Author
Background

Professor
Recommend

Source
Genre

Publish
Date

Searching

67.8

38.0

75.5

63.0

Evaluating Sources

64.0

50.0

80.0

60.0

Retrieving Sources

60.0

83.0

75.0

83.0

Documenting Sources

60.2

56.5

69.0

69.0

Selecting Finding Tool

57.0

50.0

64.0

64.0

Developing a Research Strategy

70.0

70.0

95.0

80.0

Understanding Economic, Legal &
Social Issues

53.2

50.0

91.5

58.0

Using Finding Tool Features

64.0

50.0

80.0

60.0

SAILS Subskill

Note. Scores represent means of subskill for the respective criterion.

95

Additionally, the CCTST subskill scores (Table 27) also indicate a suppression of
scores on several subskills for respondents choosing Professorial Recommendation
over other criteria. In particular, the Evaluation subskill score (67.0) under Professorial
Recommendation was appreciably lower than any other subskill and Trust criterion.
Table 27
CCTST Subskill Scores for Respondents by Trust Criterion
Author
Background

Professor
Recommend

Source Genre

Publish Date

Analysis

78.0

72.5

90.0

82.5

Interpretation

82.2

87.0

87.5

90.5

Inference

81.6

73.5

90.5

83.5

Evaluation

77.8

67.0

90.0

83.5

Explanation

83.2

71.0

93.5

90.5

Induction

82.2

72.5

92.0

85.5

Deduction

78.6

76.5

89.5

85.5

CCTST Subskill

Note. Scores represent means of subskill for the respective criterion.

The fourth item posed to the respondents asked the inverse of Item 3 as way of
further clarification of their priorities for trusting information sources. Specifically,
respondents were asked to stipulate what would cause them to distrust a source of
information. Table 28 indicates their categorized response, the frequencies of
responses and the average assessment scores.
Two-thirds of the respondents indicated that the perception that the author was
unreliable was enough for them to distrust the source of information. These respondents
tied for the highest average CCTST score with respondents choosing Internet Source,
while respondents indicating Internet Source as a criterion of distrust had the highest
SAILS average. The other response gleaned from the respondents, Publish Date, had

96

few overall responses, but its mention as a point of distrust draws attention to the
information literacy instruction the participants may have been receiving.
Table 28
Respondents’ Criteria for Distrusting Information Sources with SAILS and CCTST
Scores
na

SAILS

CCTST

Author Perceived as Unreliable

4

64.5

82.0

Internet Source

3

70.9

82.0

Publish Date

2

56.4

73.0

Distrust Criterion

a

No. of Responses

The fifth and final item of the questionnaire asked the students to describe the
thoroughness of their academic information-seeking methods. The item, “At what point
do you decide you have enough information to fulfill the requirements of the
assignment?” was meant to gain a better understanding of the respondent’s decisionmaking process with regard to information-seeking secession.
All respondents answered Item 5 with some variation of “when the
assignments/questions are fulfilled.” However, as Table 29 suggests, two respondents
also added a caveat that included a time component to their searching process. Again,
due to the low number of responses, very little can be deduced from the assessment
scores.
Table 29
Respondent’s Decision-Making Criteria Regarding Information-Seeking Secession with
SAILS and CCTST Scores
na

SAILS

CCTST

Answered the Questions/Fulfilled Assignment

6

62.1

77.5

Time Constraint

2

71.8

82.5

Secession Criterion

a

No. of Responses

97

Lastly, in addition to analyzing the responses in aggregate, I also reviewed the
individual responses in light of the respondents’ SAILS and CCTST scores (Appendix
B). This type of analysis revealed a couple other items of note.
First, respondents who had SAILS and CCTST scores that were lower than or
close to the mean of the study population seemed to rely on professors and textbooks
at a much higher rate than the respondents who scored higher on those two
assessments. However, respondents’ reliance on their professors’ source
recommendations and even their professors’ direct knowledge of the topic is in line with
other research (Head & Eisenberg, 2010; Thomas, Tewell & Willson, 2017).
Second, lower scoring respondents also appeared to rely principally on the
author’s credibility as opposed to the higher scoring respondents who also considered
other criteria such as source genre.

Summary
Chapter 4 reports the findings of this study. The quantitative portion of the study
revealed a statistically significant, positive, moderate correlation between the SAILS
means and the CCTST means. Additionally, 14 SAILS/CCTST subskill combinations
were significantly, positively and moderately correlated, of which, this study focused on
the four highest correlations.
While the overall means were slightly elevated in favor of one sex or the other
based on assessment, the Independent Samples T-Test showed no statistically
significant difference for SAILS or CCTST means between males and females.

98

Owing to a small response and a limited amount of time, the qualitative portion of
the study was challenging to interpret and quantify. However, students received a
questionnaire with five items.
The first item used Kuhlthau’s middle three cognitive stages of exploration,
formulation and collection as a backdrop to ask students to prioritize their information
sources by stage. Items 2 through 4 were open-ended queries regarding the
respondents’ thoughts about the reliability of information sources while item 5 asked
them about their decision-making processes regarding information needs. I analyzed all
responses and provided average SAILS and CCTST scores for each category of
response, but with hesitation due to the low response rate and wide variation in scores.

99

CHAPTER 5
DISCUSSION AND CONCLUSIONS
Introduction
Both information literacy and critical thinking are cognitive skills that entail higher
order thinking abilities, but that does not necessarily imply that the two skills are
inherently related. In the late twentieth century, the American Library Association (ALA)
and the Association of College and Research Libraries (ACRL) originally framed
information literacy as an applied skill meant to guide individuals in the location,
evaluation and use of information, while proponents of critical thinking, ancient and
contemporary, envisioned it as a means to engage with the entire human experience
through a filter of logic. These differing origins and purposes clearly demarcate the
distinctions between information literacy and critical thinking. However, as suggested
previously, there are reasons to advocate that the skills have some type of relationship.
This exploratory study was conducted to determine whether such a relationship
may exist and whether more research is warranted in this area. There is an implicit
assumption among educators and other academic professionals that information literacy
and critical thinking are related. However, there appears to be no empirical evidence to
support such an assumption.
Additionally, this study explored the possibility of differences in information
literacy and critical thinking skills based on gender. Currently, there is wide debate in
the cognitive science community regarding the notion of cognitive incongruence
between females and males. Some scientists argue that there are differences between

100

females and males in specific cognitive functions, while others maintain that the
differences, if any, are inconsequential.
Finally, in order to provide a richer data set, this study attempted to provide
context for respondents’ average SAILS and CCTST scores based on their responses
to a questionnaire about their source choices in information-seeking.

Discussion of Quantitative Results
SAILS and CCTST Scores
The first research question of this study was: How do information literacy and
critical thinking relate in undergraduate students conducting academic research? Based
on the results of the correlational analysis, there is strong evidence to suggest that there
is a relationship between the two constructs. However, due to the small sample size, the
results are not conclusive.
Twenty-eight students took both the SAILS and CCTST assessments. The
average overall score for the SAILS assessment was 54.4% out of 100% while the
average overall score for the CCTST assessment was 72.2 points out of a possible 100.
The average SAILS score for the participants was quite low. However, in light of the
benchmark scores provided by the assessment company indicating that approximately
16,300 students from various U.S. institutions scored an average of 53.4% out of 100,
the study participants seem to be within the normal range of achievement for the U.S.
population. This realization raises other concerns about the information literacy
practices among U.S. students, but that is beyond the scope of this study.

101

With an average of 72.2 points out of 100 on the CCTST assessment, the
participants at the study university achieved a low moderate performance rating, which
is firmly in the center of the performance rating scale. Among U.S. students in four-year
colleges and universities who employed the CCTST, the study participants averaged
around the 34th percentile, meaning that 33% of all other students testing in that
particular cohort scored lower than the study participants. This figure is fairly low
considering their overall score of 72.2. However, to provide more perspective, the
percentiles ranged from 3 to 98 over a span of 28 students, so there were large gaps in
the percentiles over small spans in scores.
Additionally, according to the participants’ self-report, many had not had critical
thinking instruction outside of secondary school. Because students must continually and
deliberately use and develop their critical thinking skills until they become ingrained,
long periods of time away from instruction can cause deterioration in their critical
thinking abilities, which may have led to lower scores.
The Pearson’s result of (r(26) = .659, p =.000) between the SAILS and CCTST
scores indicates a statistically significant, positive, moderate correlation at p < .001.
Additionally, squaring the r statistic results in the coefficient of determination or effect
size (r2=.43), which suggests that 43% of the difference in the CCTST scores is
attributable to the SAILS scores. These two statistics provide a wide berth for
speculation that aspects of information literacy and critical thinking are related in some
manner. However, it is not immediately clear how the two concepts are correlated or
how the relationship informs the future of information literacy or critical thinking.

102

To further investigate the possible links between information literacy and critical
thinking, I correlated and analyzed the subskill scores. There were 56 SAILS and
CCTST subskill pair combinations. The analysis produced 14 statistically significant,
positive, moderate correlations, of which I focused on the four highest (Table 13). The
highest correlation was SAILS Evaluating Sources/CCTST Analysis.
This was not surprising, given the similar definitions/competencies associated
with these subskill scores. Both speak to the student’s ability to make judgments
regarding sources of information. Students with strong analysis skills are able to identify
underlying ideologies and explanations in information sources. They look for
inconsistencies and patterns that give shape to the argument. This skill is undoubtedly
helpful when evaluating unfamiliar information sources.
The next highest correlation was SAILS Using Finding Tool Features/CCTST
Induction. These two subskills were more difficult to synthesize because the SAILS
subskill refers to a strictly applied skill and the CCTST subskill refers to a type of logical
inference. The common link between the two appears to be finding familiarity within
uncertainty.
In critical thinking, inductive reasoning is predicated on uncertainty that is
counterbalanced by past experience or observation asserting that the outcome could be
different from the one typically expected. In information literacy, Using Finding Tool
Features deals with the uncertainty of conducting research through unfamiliar
information venues, while having some idea of how the various systems are structured
based on past experiences with other systems. Both of these subskills require students
to invoke prior experiences and situations and make decisions or perform actions based

103

on the full breadth of information available to them. The current experience is then
assimilated into their knowledge pool for later use.
The third highest correlation, SAILS Evaluating Sources/CCTST Deduction is
interesting in that it suggests that students may be using deductive reasoning when
evaluating sources of information. Deductive reasoning, according to Insight
Assessment, asserts that if all the premises are true, then the conclusion(s) must be
true as well.
Students using deductive logic to evaluate sources are likely seeking background
information in order to validate or invalidate source quality. The importance of this is not
necessarily that students are using deductive reasoning, although that is enlightening,
but rather that students might be vetting their sources, which is a hopeful sign that
students continue to grow in information literacy skills.
The fourth highest correlation, SAILS Developing a Research Strategy/CCTST
Evaluation, was also fairly challenging to reconcile. It may suggest that, while students
are not overtly evaluating information sources during the development stage at the
same level as during later stages of the research process, they are consistently and
broadly using evaluative reasoning skills throughout the development phase of their
research strategy.
These subskill score correlations, though not the focus of this study, shed more
light on the complexities of the participants’ abilities in information literacy and critical
thinking and may afford future researchers with additional information about specific
cognitive functions underlying information literacy and critical thinking.

104

Gender Differences in SAILS and CCTST Scores
The final two research questions for this study were: a.) What differences are
there in information literacy skills between males and females conducting academic
research? b.) What differences are there in critical thinking skills between males and
females conducting academic research?
The analyses of the SAILS and CCTST scores by gender produced no evidence
of statistically significant differences. Averaging the SAILS and CCTST scores by
female (n =15) and male (n =13) participants demonstrated a slightly elevated score for
females on the SAILS and a slightly elevated score for males on the CCTST. However,
the Independent Samples T-Tests indicated that the differences were not statistically
significant.
These findings align closely with others such as Williams and Evans (2008) and
Hohlfeld, Ritzhaupt and Barron (2013) that indicate that there may be few, if any,
cognitive differences between females and males. Additionally, they serve as a note of
caution when trying to identify cognitive differences based solely on biological markers
(i.e., gender, race/ethnicity, etc.) without also accounting for environmental influences.
Therefore, it is likely that information literacy and critical thinking curricula do not need
to necessarily account for gender differences as much as for individual student learning
differences.

Discussion of Qualitative Results
The questionnaire on source selection behavior that was intended to provide
context to the quantitative results comprised five items. I received six completed

105

questionnaires. My inductive content analysis of the responses yielded interesting,
though cautious, insights into the respondents’ priorities with regard to source selection
in light of their SAILS and CCTST scores. The results of the content analysis and the
statistical analysis of the SAILS and CCTST scores paint a multifaceted picture of the
respondents’ competing priorities during academic information-seeking.

Source Preferences
The first item asked respondents to select their choice of information sources in
order of preference based on the stage of information seeking as defined by Kuhlthau
(1991). Briefly, the choice of sources provided to the respondents were: Books about
Topic, General Reference Books (Encyclopedia, etc.), Textbook, Online databases at
the Library, Internet Search, Professor or other authority on topic, Friends and Other
and Kuhlthau’s Stages of Information-Seeking pertinent to this study were: Exploration,
Formulation and Collection.
Responses to Item 1 provided some enlightening perspectives on the ways in
which the respondents used information sources in light of their assessed information
literacy and critical thinking abilities.
Further disaggregation of respondents’ answers based on their first, second and
third choice by stage of information-seeking along with the average SAILS and CCTST
scores for each response are represented in Tables 17 through 19. The use of the
SAILS and CCTST scores in populations this small has its limitations. However, some
general observations are still useful.

106

For instance, respondents who chose Online Databases as a response for one of
the three possible choices during the Exploration stage generally had higher average
SAILS and CCTST scores suggesting that these respondents have a better grasp of
information literacy and critical thinking concepts. Although there is not much empirical
evidence to support their assumptions, it likely would not surprise many librarians or
other academic professionals that respondents who chose online databases over other
less reputable information sources such as the Internet would score higher on
information literacy or critical thinking skills. These results lend some credence to those
assumptions.
Yet, I was more curious to see how the sources of information generally seen as
being more reliable (Textbooks, General Reference Books, etc.) compared to each
other with regard to the respondents’ average scores. Unfortunately, the averages were
too varied between the three choices to garner much in the way of useful conclusions. It
is possible, though, that the priority of the respondents’ choices is as much of an
indicator of their abilities as their assessment scores.
Tables 17 and 19 seem to support, at least on the face of it, this assertion. Each
of the two stages following Exploration - Formulation and Collection - demonstrate that
the respondents’ average SAILS scores revolving around their information source
responses were based on the relative priority of their responses. Tables 18 and 19
clearly indicate that as Online Database moves from a first to third choice as an
information source, the average SAILS scores decline, while the opposite is true for
Internet Search as an information source. Within the Formulation stage, Textbook also

107

shows signs of assessment score variation based on the prioritization of the information
source.
Tables 20 and 21 show the results of an additional analysis conducted to
determine whether respondents scored differently on the SAILS subskills based on their
primary choice of the Internet or Online Database as an information source. Though the
numbers are too small to be conclusive, there were indications that respondents who
chose Online Databases consistently throughout the ISP stages scored better in most of
the SAILS subskills.
Additionally, the Evaluating Sources subskill score suggests that respondents
who use the Internet as an information source later in the research process are less
capable of recognizing misinformation and propaganda as well as lacking the necessary
skills to critically evaluate a point of view.
The importance of these observations rests in the suggestion that the students’
priorities in their choice of information sources throughout the information search
process may have as much consequence as the type of information sources that they
choose in general.
There are other considerations surrounding how students choose sources of
information that are not necessarily related to their information literacy or critical thinking
skills. For instance, the currency of the source in relation to the topic is a primary
concern for particular disciplines. Certain fields of study (e.g., information technology,
engineering, etc.) rely on online journals rather than books primarily because the
concepts and discoveries within those fields are constantly changing.

108

Books typically provide historical, introductory and comprehensive coverage of
the fundamental concepts and underpinnings of a field, but as observed by Head
(2007), students seeking current and specific information about a subset of a
developing field are more likely to find sources online. While most of the respondents for
this study were freshmen and sophomores taking general education coursework rather
than major-specific coursework, currency was referenced by four of the respondents as
a consideration in their search criteria in Items 3 and 4 of the questionnaire.
Respondents were fairly consistent in their reliance on textbooks throughout the
three middle Information Search Process stages. As suggested by other research
(Head, 2007), the use of textbooks as a source of information is generally expected at
the beginning of a search for information about a topic and, indeed, the data indicated
that the majority (5) of respondents wanted to use textbooks during the Exploration
stage.
However, half of the respondents in each of the next two stages suggested that
they continued to use textbooks as a source of information. This finding is interesting
because, while textbooks are a good primary source of high-level information, they do
not typically discuss narrow topics in detail. Therefore, respondent use of them during
the later stages of information-seeking is noteworthy.
Respondents also indicated that online databases were their most used source
of information for academic research and, as they progressed through the informationseeking stages, their responses indicating that they relied on online databases located
at the institution library increased. This observation is encouraging because it suggests
that students may be somewhat more particular about their sources as they narrow their

109

search. Students may still find sources that are not empirically-based or peer-reviewed
through online databases, but generally there are greater controls on the content in the
library databases than on the Internet. Additionally, unlike textbooks, the journal articles
in online databases provide a source of information at a more granular or detailed level
allowing students to go deeper into a topic.
Finally, respondents indicated that they used topic-specific books at
approximately the same rate as the Internet. This observation runs counter to some
earlier studies suggesting that students are primarily interested in using Internet
sources. Further, topic-specific book and Internet usage remained relatively consistent
throughout Kuhlthau’s ISP stages indicating that the respondents are seeking sources
for reasons other than just convenience or speed.

Source Selection Criteria
In addition to asking respondents about their information sources in light of
Kuhlthau’s Information Search Process stages, I also asked them about their decisionmaking process with regard to information sources. Tables 22, 25, 28 and 29 in Chapter
4 display the aggregate of the respondents’ replies along with their average SAILS and
CCTST scores.
Table 22 represents the first of the items, a general question about how the
respondents chose particular sources of information over others. There were two basic
categories of responses: trust/credibility of sources and convenience. The surprising
results from these responses had to do with their respective average assessment
scores. Many researchers and professionals would possibly assume that respondents

110

who were concerned about credibility of sources may have scored higher on the SAILS
assessment, in particular. However, that was not the case for this study.
Not only did the convenience respondents score almost 4-points higher than the
trust/credibility respondents on the overall average, but they also scored 11.5-points
higher on the Evaluating Sources subskill score average of the SAILS. With the
understanding that these observations are predicated on the accurate interpretation and
representation of the respondents’ answers, it is challenging to draw inferences from
these findings. Though the CCTST subskill scores may offer a line of thought.
While the overall CCTST average scores between the two groups were relatively
equal, the subskill scores, and, in particular, the Evaluation subskill scores, were not. In
fact, the Evaluation subskill score average for the convenience respondents was 73.5
and the subskill score average for the trust/credibility respondents was 78. Though only
4.5-points difference, these results are a departure from the SAILS results and may also
provide at least a partial answer as to why students concerned about source credibility
would score lower on an information literacy assessment than students who were more
concerned about convenience.
As a largely skills-based assessment, the SAILS is intended to measure how
students identify, locate and use information sources to meet a specific information
need. Therefore, it approaches assessment from an applied point of view. There are
aspects of the SAILS that require students to use evaluative strategies and analysis, but
that is not the core of the assessment.
The CCTST, on the other hand, was created in a much more philosophical
environment that relies less on applied skills. Instead, the CCTST measures the

111

respondents’ abilities to critically think through ideas, situations and events and to come
to firm conclusions based upon the information they know to be true or false.
I propose that the discrepancies between the SAILS and CCTST subskill scores
for this group of respondents is explained by the differences in their purpose.
Specifically, I suggest the respondents were having difficulty with the applied aspects of
the SAILS rather than the evaluative aspects. This thought is further reinforced by the
fact that the SAILS average scores for the entire cohort were lower than the CCTST
average scores even though the SAILS assessment appeared to be shorter and less
difficult.
Item 3, represented in Table 25, asked respondents to be slightly more specific
about the factors that helped them choose to trust a source of information. Author
Background and/or Credentials received five responses with the remaining responses –
Professor Recommendation, Source Genre and Publish Date – mentioned twice each.
As stated earlier, half of the respondents mentioned only one criterion for making their
determination about source credibility, which may still leave them vulnerable to different
types of misinformation and propaganda.
However, one of the encouraging signs gleaned from a further analysis of the
SAILS and CCTST subskill scores for each response demonstrates that the averages of
certain responses were several points higher than was expected based on the averages
of the other items to this point.
Responses such as Source Genre and Publish Date, though only mentioned
twice, show an upward trend in the SAILS and CCTST scores. Interestingly,
respondents who indicated that they based their decisions on professorial

112

recommendations had the lowest averages with the SAILS registering over 6-points
lower than the mean for the cohort and the CCTST coming in over 3-points lower.
These observations are reinforced by data in Tables 26 and 27. Obviously, causality
cannot be ascribed in this study, but it is worth considering possible explanations for the
low scores for that particular category of responses.
In addition to faculty–suggested sources, respondents also considered the
reliability of the author as part of their decision to trust or not trust a source of
information. Finally, respondents appeared to consider the type of source (Internet
webpage, book, journal, etc.) and, as mentioned previously, source currency in the
decision to trust or distrust a source of information.
Along with the observations about source reliability, many of the respondents
were candid about the role convenience played in their decision-making process.
However, the relative importance of convenience is less clear in their decision-making
paradigms (i.e., Will convenience override source reliability in certain situations?). The
respondents in this study had strict time constraints that dictated their ability to
thoroughly research a topic.
Besides the obvious constraint of the semester period, respondents also had to
consider the assignment due date, the requirements of their other courses, employment
responsibilities, extra-curricular responsibilities and areas that pertained to their overall
physical and mental health (e.g., sleep, meals, exercise, etc.). As mainly freshmen and
sophomores, academic discipline and time management are also possible
considerations. How well have respondents adjusted to the freedoms and

113

responsibilities of college life? Their ability to adequately adjust could have profound
impacts on their information-seeking practices.
This consideration leads to the last area of observation – satisficing. It was clear
that the respondents did participate in satisficing at some level whether from perceived
time limitations, as mentioned above, or from a simple lack of good information literacy
and/or critical thinking skills. Based on some of the respondents’ answers, it is my
conjecture, for this particular group of respondents, that although lack of information
literacy skills contributed to satisficing, time limitations played a more important role in
their satisficing activities than other factors.
Again, with such small numbers it is hard to speculate regarding the implications
of these findings. However, at some level, they seem to affirm that some students do
understand the criteria they should be applying when evaluating sources of information.
Additionally, students who put the time and effort into locating reputable sources of
information also appear to have the necessary critical thinking skills to assess what they
find.
Item 4 was designed to ascertain what specific factors would cause respondents
to distrust sources of information. The respondents replied with some familiar themes.
Once again, most respondents indicated that how they perceived the author had
a bearing on their decision-making abilities with regard to information sources.
However, their concern with the author in this case had the additional criterion of the
seeming bias of the author. Specifically, respondents indicated that the author’s formal
credentials were a deciding factor, as well as any bias they perceived in the author. It is
not clear what they meant by bias – whether it is an opinion that runs counter to their

114

current worldview (or the worldview of the authority figures in their life) or if they are
referring to an author who does not represent a topic in an even-handed manner.
This observation is not completely surprising in light of the Basic Duality position
of Perry’s (1999) development scheme. As the starting position of epistemological
development, Perry defined Basic Duality as the students’ awareness of right/truth and
wrong/falseness in terms of their authority figure’s worldview (Perry, 1999, p.66). In this
“us vs. them” model of decision-making, students perceive truth primarily through the
lenses of trusted authorities in their life and reject the views of the other with whom they
are unfamiliar.
Half of the respondents also indicated that certain types of Internet sources were
suspect. Specifically, they mentioned “crowd-sourced” and “homemade,” which could
refer to Wikipedia and blog sites, respectively. If this is the case, it would be welcome
news to many teachers and professors who have tried to help steer students away from
these types of sources to more reliable sources of information.
These results are not unexpected considering the amount of information literacy
education that public and private institutions have invested in for the past 20 years. It
appears, based on the respondents’ answers, that some students understand the
basics of investigating the author and proceeding carefully with Internet sources.
The final item asked respondents to explain how they decide to stop seeking
information for their assignments. Table 29 shows the responses and corresponding
SAILS and CCTST averages. All of the respondents indicated that their primary
benchmark for ceasing an information search is the fulfillment of the requirements of the
assignment. However, respondents 1, 2 and 4 did not elaborate on how thoroughly they

115

fulfilled the requirements for their assignments, which may suggest again that satisficing
was at play in their information-seeking activities. Further, two respondents added a
secondary condition of time limitations. These responses coincide with the earlier
discussion about the time constraints of students at the university.

Limitations
This study was primarily limited by small sample size. The initial response to the
invitation to participate was so anemic that the pool of potential participants had to be
expanded. This necessitated two modifications to the research design that required IRB
approval; first, to allow increased invitations and incentives, and second, to change from
face-to-face interviews to electronic questionnaires due to time constraints.
In the time available, I was not able to ask follow-up questions for clarification or
elaboration of questionnaire responses, which likely would have provided more
qualitative data to give context to the quantitative data.

Future Research
The purpose of this mixed-methods, exploratory study was to determine whether
there is a relationship between information literacy skills and critical thinking skills so as
to provide a bridge forward for future research. As presumed by academic professionals
and researchers, the results of this study provided evidence to suggest that, not only is
there a cognitively-based relationship, but that the relationship is multi-faceted as
relayed through the subskill scores.

116

Information literacy and critical thinking have been necessary skills to maintain
an educated and progressive society for centuries. However, as the rate at which
information reaches society increases and as the quantity of information that society
consumes in a day increases, the need for information literacy and critical thinking skills
also increases.
Digital technologies have democratized much in the way of information and
news, but that democratization has also led to the erosion of standards that once largely
helped protect society from misinformation and propaganda (e.g., journalism vs.
tabloid). The cavernous space left by those eroded standards should be filled by
citizens who are able to locate, evaluate and use information effectively in their daily
lives. However, researchers should explore the extent of information literacy and critical
thinking relationships, particularly in the intersection of cognitive science and learning
theory, in future studies if society is to make progress against propaganda and
misinformation.
The foremost direction for future research would be a replication of this study
with a sample large enough to achieve generalizable results and determine whether the
statistical results achieved here are truly significant. This concern is particularly valid for
the subskill score correlations and the qualitative portion of the study where the
respondents were in the single digits, limiting analysis to descriptive statistics. A larger
study would permit not only more significant quantitative results, but also a greater and
possibly more diverse sample for the qualitative part of the study.
Additionally, research needs to be conducted on the relationship between
information literacy and critical thinking dispositions to determine what role the

117

disposition to think critically plays when students conduct academic research. This type
of research could also draw from Kwon’s (2008) research on library anxiety.
Institutions seeking to establish good information literacy and critical thinking
practices among their students also have a vested interest in the epistemological
development of their students. Therefore, more research should be done to explore how
information literacy skills and critical thinking skills affect epistemological development
in college students.
With regard to this study, the question of epistemological development was
slightly more complex as it involved the annexing of their authority sources to include
university faculty and staff as well as family members and friends. This is a particularly
interesting question given the addition of an overarching, faith-based component of the
students’ education at the institution and the deliberate inculcation of a specific
worldview throughout the curriculum. The scope of this study does not permit more time
on this subject. However, the idea of worldview development within various types of
higher education institutions should be studied further as another facet of students’
information-seeking process.
Finally, in light of some of the subskill scores pertaining to sources of information
during certain stages of information-seeking, I believe researchers need to take a closer
look at not only what sources college students are using, but when they are using those
sources and how that is affecting their research. Item 1 of the Questionnaire introduced
some evidence to suggest that when students choose to use a particular information
source may be as important as what information source they choose. This finding has
implications for information literacy instruction.

118

Summary
This study, though small, provided evidence of a correlation between information
literacy and critical thinking skills in undergraduate students, but found no evidence to
suggest significant differences in skills between genders. Further, the qualitative portion
of the study garnered additional insights and contextual information about the
participants’ information literacy and critical thinking scores.
One of the potential ramifications of these findings is a readjustment of how
academic professionals teach information literacy and critical thinking skills. If
information literacy and critical thinking are cognitively-linked constructs, then it is
possible that the two constructs need to be taught closely together so that students
receive the full benefit of both constructs.
An information-saturated society needs citizens who are information-literate
critical thinkers in order to maintain a stable and progressive civilization. Therefore, the
imperative for students to learn good information literacy and critical thinking skills is
paramount in today’s information environment. It is hoped that the findings of this study
will be a springboard for further study and better practice.