Walden University
COLLEGE OF EDUCATION

This is to certify that the doctoral study by

Phyllis Usina

has been found to be complete and satisfactory in all respects,
and that any and all revisions required by
the review committee have been made.

Review Committee
Dr. Carol Spaulding, Committee Chairperson, Education Faculty
Dr. Janet Thomas, Committee Member, Education Faculty
Dr. Kathleen Montgomery, University Reviewer, Education Faculty

Chief Academic Officer
Eric Riedel, Ph.D.

Walden University
2015

Abstract
Impact of a California Community College’s
General Education Information Literacy Requirement
by
Phyllis Usina

MLS, University of Illinois at Urbana-Champaign, 1999
BA, Sonoma State University, 1996

Doctoral Study Submitted in Partial Fulfillment
of the Requirements for the Degree of
Doctor of Education

Walden University
November 2015

Abstract
Budget cuts at a California community college prompted stakeholders to consider
dropping the college’s general education information literacy (IL) requirement. Broad
institutional outcomes data showed learning gains, but no targeted assessment existed
regarding the IL requirement’s impact on those gains. This quantitative study used Astin
and Antonio’s Inputs-Environment-Outcomes (I-E-O) assessment model to address
relationships among student characteristics of demographic and prior preparation
(Inputs), the IL requirement (Environment), and student reports of information critical
analysis behavior and confidence (Outcomes). Study participants were 525 students aged
18 years and older who had completed the IL course with a grade of 2.0 or better and
volunteered to complete an anonymous survey. The majority of participants reported the
IL requirement had a positive impact upon subsequent coursework, with 87% stating that
taking it in the first or second term would be most helpful. Less preparedness for
information critical analysis prior to the IL course was significantly correlated (r = -.35, r
= -.38, p < .001) with higher reported frequency of 2 measures of information evaluation
changes following completion of the course. The 3 hierarchical multiple regression
analyses revealed that the predictors of student demographic characteristics, prior student
preparation, and IL course format contributed significantly to reported information
critical analysis and confidence. The study’s outcome was a white paper with
recommendations to support completion of the IL course requirement early, continue the
IL requirement, and repeat the study’s survey in the future. Effective IL education
promotes information evaluation behaviors essential to informed members of society.

Impact of a California Community College’s
General Education Information Literacy Requirement
by
Phyllis Usina

MLS, University of Illinois at Urbana-Champaign, 1999
BA, Sonoma State University, 1996

Doctoral Study Submitted in Partial Fulfillment
of the Requirements for the Degree of
Doctor of Education

Walden University
November 2015

ProQuest Number: 3735102

All rights reserved
INFORMATION TO ALL USERS
The quality of this reproduction is dependent upon the quality of the copy submitted.
In the unlikely event that the author did not send a complete manuscript
and there are missing pages, these will be noted. Also, if material had to be removed,
a note will indicate the deletion.

ProQuest 3735102
Published by ProQuest LLC (2015). Copyright of the Dissertation is held by the Author.
All rights reserved.
This work is protected against unauthorized copying under Title 17, United States Code
Microform Edition © ProQuest LLC.
ProQuest LLC.
789 East Eisenhower Parkway
P.O. Box 1346
Ann Arbor, MI 48106 - 1346

Acknowledgments
I would like to extend my gratitude to all of the family members, friends, and
colleagues who provided support and guidance without which I would not have been able
to complete this work. My enormous thanks go to Indigo Crone, Ginnie Kirsch, Barb
Lester, Sidney Lester, Chris Archambault, Toni Eaton, Chico Sumpf, Bianca Sumpf,
K.C. Greaney, Betty Masters, Elease Usina, Barbara Christman, John Greggs, David
Usina, Lois Usina, Vanessa Dixon, Jen Adair, Cris Gondak, Trisha Brown, Liz Lawson,
Sue Amalia, Julie Thomason, Gretchen Harig, Anna Woods, Betsy Penn, Maureen
Kirkpatrick, Kathy Simmons, Theresa Lester, Whitney Schultz, Sami Lange, Molly
Matheson, Alicia Virtue, Karen Petersen, David Rau, Yadira Camargo, Erin Daniels,
Cherry Li-Bugg, Julia Macdonald, Lori Kuwabara, Josh Adams, Kris Abrahamson, Mary
Kay Rudolph, Frank Chong, Jane Saldana-Talley, Pat Gage, Kathleen Miller, Lana
Common, and the countless others in my extended community who offered
encouragement or assistance to me on this journey. My unending gratitude goes to Dr.
Carol Spaulding for stepping in to lead my team and for sharing her amazing ability to
motivate, engage and inspire students to their best. You are truly a superwoman. I would
also like to give my appreciation to Dr. Janet Thomas for her methods guidance regarding
using care about making conclusions when interpreting self-reported data. I am grateful
to Dr. Kathleen Montgomery for her speedy reviews and sharp eye for detail. Thanks also
go to Dr. Delmus Williams for his limitless patience, thorough the reading of my early
work and ever thoughtful suggestions. A super special thanks goes to David Crone for
providing me with such a cozy home that nurtured and soothed me on so many levels.

Table of Contents
List of Tables .........................................................................................................................viii
List of Figures.......................................................................................................................... ix
List of Acronyms ...................................................................................................................... x
Section 1: The Problem............................................................................................................ 1
Introduction ........................................................................................................................ 1
Definition of the Local Problem ....................................................................................... 2
Definition of Terms ..................................................................................................... 3
Larger State and National Educational Context ........................................................ 6
Historical Evolution of the Information Literacy Need ............................................ 7
Underprepared Students Information Literacy Need .............................................. 10
State and National Response to Information Literacy Need ................................... 11
Local Educational Context ........................................................................................ 13
College Community, Mission, Values, and Programs ............................................ 13
Students’ Information Evaluation Preparedness ...................................................... 14
Information Literacy Education Delivery Method .................................................. 15
Information Literacy Requirement Assessment ...................................................... 16
Rationale ........................................................................................................................... 18
Evidence of the Problem in the Larger State and National Context ....................... 19
Evidence of the Problem in the Local Context ........................................................ 20
Significance of the Study................................................................................................. 22
Research Questions and Hypotheses .............................................................................. 23
i

Overarching Question ................................................................................................ 24
Research Questions and Hypotheses ........................................................................ 24
Research Question Alignment with Problem, Purpose, and Literature .................. 25
Review of the Literature .................................................................................................. 26
Theoretical Base and Conceptual Framework ......................................................... 26
Inputs-Environment-Outcomes Model Description ................................................ 28
Justification of Selection of the Inputs-Environment-Outcomes Model ................ 29
Conceptual Framework’s Contribution to Understanding of the Problem ............ 31
Larger Educational Context in the Literature .......................................................... 32
Student Information Evaluation Preparedness ......................................................... 33
Information Literacy Education Delivery Methods................................................. 35
Information Literacy General Education Requirement Assessment ...................... 37
Local Educational Context in the Literature ............................................................ 38
Underprepared Students Increasing in Number ....................................................... 39
Information Literacy Requirement Delivery and Assessment ................................ 39
Saturation Reached in Literature Review................................................................. 40
Relevance and Relationship of Literature to Proposed Research Study ................ 41
Implications ...................................................................................................................... 42
Summary........................................................................................................................... 43
Section 2: The Methodology ................................................................................................. 44
Introduction ...................................................................................................................... 44
Research Design and Approach ...................................................................................... 45
ii

Research Design ........................................................................................................ 45
Research Design and Approach Justification........................................................... 46
How Research Design Derives From the Problem .................................................. 47
Setting and Sample .......................................................................................................... 48
Population................................................................................................................... 48
Sampling Strategy ...................................................................................................... 49
Sample Size ................................................................................................................ 49
Participant Eligibility Criteria ................................................................................... 50
Recruitment of Participants ....................................................................................... 51
Characteristics of Selected Sample........................................................................... 52
Instrumentation and Materials......................................................................................... 52
Development of the Instrument ................................................................................ 53
Concepts Measured by Instrument ........................................................................... 55
Calculation of Scores and Their Meaning ................................................................ 58
Assessment of Reliability and Validity .................................................................... 60
Processes Needed to Complete Instrument by Participants .................................... 63
Where Raw Data Will be Available ......................................................................... 64
Data Collection and Analysis .......................................................................................... 64
Data Collection Required to Address Research Questions ..................................... 64
Data Collection Processes ......................................................................................... 65
Nature of the Scale for Each Variable ...................................................................... 66
Data Analysis Plan ..................................................................................................... 68
iii

Assumptions, Limitations, Scope, and Delimitations .................................................... 70
Assumptions ............................................................................................................... 70
Potential Limitations.................................................................................................. 72
Variables (Scope) and Boundaries of the Study (Delimitations) ............................ 76
Protection of Participants’ Rights ................................................................................... 78
Confidentiality ........................................................................................................... 79
Informed Consent ...................................................................................................... 80
Protection from Harm ................................................................................................ 80
Data Analysis Results ...................................................................................................... 81
Data Clean-up ............................................................................................................ 81
Response Rate ............................................................................................................ 84
Inputs-Environment-Outcomes Assessment Framework ........................................ 85
Questions and Hypotheses ........................................................................................ 87
Descriptive Analysis .................................................................................................. 89
Descriptive Analysis Interpretation ........................................................................101
Representativeness Analysis ...................................................................................102
Representativeness Analysis Interpretation ...........................................................106
Cross-Tabulation Analysis ......................................................................................107
Cross-Tabulation Analysis Interpretation ..............................................................108
Correlational Analysis .............................................................................................108
Correlation Analysis Interpretation ........................................................................111
Multiple Regression Analysis .................................................................................112
iv

Research Question 1 Answered ..............................................................................113
Research Question 2 Answered ..............................................................................119
Multiple Regression Analysis Interpretation .........................................................122
Results Support a White Paper for Policy Recommendations ..............................125
Summary......................................................................................................................... 125
Section 3: The Project .......................................................................................................... 128
Introduction .................................................................................................................... 128
Description and Goals.................................................................................................... 128
Project Description ..................................................................................................128
Project Goals ............................................................................................................129
Rationale ......................................................................................................................... 130
White Paper Connects Research to Recommendations .........................................130
White Paper Connects Findings to Recommendations .........................................131
White Paper Addresses the College’s Assessment Gap Problem .........................131
Review of the Literature ................................................................................................ 133
Historical Origins of the White Paper ....................................................................133
Structure of the White Paper ...................................................................................134
Benefits of White Paper as an Information Sharing Format .................................135
Research, Theory, and Results Support Content of Project ..................................136
Student Preparation..................................................................................................137
IL Requirement Education Delivery.......................................................................140
General Education Requirement Assessment ........................................................144
v

Saturation Reached in Literature Review...............................................................146
Project Implementation Description ............................................................................. 147
Potential Resources and Existing Supports ............................................................148
Potential Barriers and Solutions .............................................................................149
Implementation Proposal and Timetable................................................................150
Roles and Responsibilities of Student and Others .................................................153
Project Evaluation Plan ................................................................................................. 154
Project Implications ....................................................................................................... 155
Local Implications ...................................................................................................156
State and National Implications ..............................................................................157
Social Change Implications.....................................................................................158
Summary......................................................................................................................... 159
Section 4: Reflections and Conclusions.............................................................................. 161
Introduction .................................................................................................................... 161
Project Strengths & Limitations .................................................................................... 161
Strengths ...................................................................................................................161
Limitations ...............................................................................................................164
Recommendations for Alternative Approaches ........................................................... 166
Scholarship, Project Development, and Leadership and Change ............................... 167
Analysis of Self as Scholar .....................................................................................167
Analysis of Self as Practitioner ...............................................................................169
Analysis of Self as Project Developer ....................................................................171
vi

Reflection on the Importance of the Work ................................................................... 172
Implications, Applications, and Directions for Future Research ................................ 173
Social Change at the Individual Level....................................................................174
Social Change at the Organizational Level ............................................................174
Social Change at the Societal Level .......................................................................176
Methodological and Theoretical Implications .......................................................178
Recommendations for Practice ...............................................................................179
Recommendations for Future Research .................................................................180
Conclusion ...................................................................................................................... 181
References............................................................................................................................. 185
Appendix A: The Project ..................................................................................................... 207
Appendix B: Survey Instrument .......................................................................................... 230
Appendix C: Peer Expert Review ....................................................................................... 234
Appendix D: Pilot Test Invitation Email ............................................................................ 235
Appendix E: Prenotification of Invitation to Participate in Upcoming Study.................. 236
Appendix F: Invitation to Participate in Study ................................................................... 237
Appendix G: Reminder to Participate in Study .................................................................. 238
Appendix H: Protecting Human Research Participants Training Certificate ................... 239
Appendix I: ACRL Information Literacy Standard Three ................................................ 240
Appendix J: Institutional Learning Outcomes.................................................................... 242
Appendix K: Information Literacy Requirement Course Outline ..................................... 243

vii

List of Tables
Table 1. Student Enrollment Count and Success Count ......................................................51
Table 2. Information Literacy Requirement Impact Survey Codebook ............................. 59
Table 3. Information Literacy Requirement Impact Survey Item Descriptive Analysis ... 90
Table 4. Comparison of Student Demographic Characteristics ........................................104
Table 5. Comparison of LIR 10 Formats and Lengths ......................................................106
Table 6. Means, Standard Deviations, and Zero-Order Correlations for all Variables ...110
Table 7. Regression Coefficients and Standard Errors for SRJC Critical Analysis.........115
Table 8. Regression Coefficients and Standard Errors for ACRL Critical Analysis .......117
Table 9. Results of Hypotheses Tests for Research Question 1 ........................................118
Table 10. Regression Coefficients and Standard Errors for Confidence ..........................120
Table 11. Results of Hypotheses Tests for Research Question 2 ......................................121

viii

List of Figures
Figure 1. A diagram of Astin’s Inputs-Environment-Outcomes model ............................. 28
Figure 2. Inputs-Environment-Outcomes model with study’s points ................................. 86

ix

List of Acronyms
ACCJC: Accrediting Commission for Community and Junior Colleges
ACRL: Association of College & Research Libraries
ASCCC: Academic Senate for California Community Colleges
CCCCO: California Community Colleges Chancellor's Office
CCSSE: Community College Survey of Student Engagement
CIRP: Cooperative Institutional Research Program
ESL: English as a Second Language
GPA: grade point average
HERI: Higher Education Research Institute at University of California, Los Angeles
I-E-O: Inputs-Environment-Outcomes
IL: information literacy
LIR: library & information resources
OIR: Office of Institutional Research
NSSE: National Survey of Student Engagement
PIL: Project Information Literacy
SRJC: Santa Rosa Junior College
SPSS: Statistical Package for the Social Sciences
WASC: Western Association of Schools and Colleges

x

1
Section 1: The Problem
Introduction
The evolution of the Internet created a paradigm shift in information access at
community colleges. As students increasingly accessed information directly from the
Internet, academic libraries, traditionally the primary source for mediated access to
information necessary to support the learning process, had to compete. Throughout the
1990s, colleges raised concerns about students’ abilities to find information on the
Internet and critically evaluate it to produce successful outcomes (Association of College
& Research Libraries [ACRL], 1989, 1998; Kuhlthau, 1991; Reeves, 1996). These
concerns, subsequently identified as information literacy (IL) concerns, continue to affect
higher education today (Chen, Pedersen, & Murphy, 2012; Gross & Latham, 2012;
Gross, Latham, & Armstrong, 2012; Head, 2013; Ritzhaupt, Feng, Dawson, & Barron,
2013; Taylor, 2012; York, 2013).
To address students’ information use concerns, the ACRL (2000) developed IL
Competency Standards for Higher Education. These standards systematized expected IL
competencies in education using performance indicators and learning outcomes. regional
and discipline-specific accrediting bodies adopted IL as a core competency to ensure
institutions were teaching and assessing students’ IL abilities (Accrediting Commission
for Community and Junior Colleges [ACCJC], 2014; Western Association of Schools and
Colleges [WASC] Senior College and University Commission, 2013).
In 2002, the Santa Rosa Junior College (SRJC) initiated its general education IL
requirement to address this IL educational need. Since then, the college has not

2
conducted targeted assessments to determine to what degree this requirement has
achieved the desired outcome of instilling students with information evaluation
knowledge and skills. The ACRL (2000) defined critical information evaluation skills as
the ability to locate, analyze, synthesize, and evaluate information needed to conduct
academic research. The college has needed to assess its assumption that a relationship
existed between students’ IL requirement participation and development of information
evaluation behaviors. This quantitative study addressed this need.
In this project study, I assessed students’ perceptions of the impact that a general
education IL requirement had on their information evaluation abilities as a means to
gather data to develop recommendations for program assessment. In Section 1, I
introduce background information necessary to this study. This information includes the
definition and significance of the problem, related research questions, a comprehensive
literature review, and implications for the study results.
Definition of the Local Problem
The local problem prompting this study was the college’s lack of targeted
assessment of the impact of its general education information literacy (IL) requirement
for student development. More than a decade has passed since SRJC (2013c) instituted its
1-unit general education IL requirement for those pursuing a local associate degree. In
that time, the college had not tested the IL requirement to determine whether the course
produced the desired outcome of equipping students with the information evaluation
knowledge and skill set needed for successful involvement in other courses. Although
local shared governance committees composed of administrators, faculty, staff, and

3
students discussed discontinuing the IL requirement, no specific assessment process was
implemented. The college needed to assess its assumption that a relationship existed
between participation in the IL requirement and developing appropriate student IL
behaviors.
To provide an educational context for the study, I defined this problem in terms of
issues in the larger education context of the United States and the state of California, as
well as the local setting of a community college. Astin’s (1985) theory of student
involvement provided context for the study. The current study’s purpose was to examine
relationships among student characteristics, aspects of the general education IL
requirement, and subsequent frequency of student use of IL critical information
evaluation behaviors and levels of confidence in relation to writing papers and
participating in discussions in other courses.
Definition of Terms
Key terms associated with the assessment of the IL requirement problem are
defined here to clarify their usage in this study.
Critical analysis: SRJC (2013b) Institutional Learning Outcomes define critical
analysis as the ability to “locate, analyze, evaluate, and synthesize relevant information”
(para. 4). In this study, I used the phrase critical evaluation interchangeably with the
phrases critical analysis and critical thinking, except when specifically discussing critical
analysis from the SRJC Institutional Learning Outcomes.
Critical information evaluation behaviors: The ACRL (2000) defined critical
information evaluation behaviors using performance outcomes including confidence in

4
using criteria to evaluate information and its sources, comparing new with prior
knowledge to determine contradictions; understanding information through discourse
with others, and determining if a search query should be revised to improve results.
Critical evaluation: ACRL’s (2000) Information Literacy Standard Three (see
Appendix I) stated “the information literate student evaluates information and its sources
critically and incorporates selected information into his or her knowledge base and value
system” (p. 11). In this study, I used the phrase critical evaluation interchangeably with
the phrases critical analysis and critical thinking.
Critical thinking: Detmering and Johnson (2011) defined the word critical
thinking in the context of information literacy as the practice of analyzing and evaluating
information to direct belief and action. In this study, I used the phrase critical evaluation
interchangeably with the phrases critical analysis and critical thinking.
Effectiveness: Astin and Antonio (2012) defined effectiveness in terms of a
practice or program producing the desired outcomes. In this study, I used the word
effectiveness interchangeably with the word impact.
Engagement: Kuh (2008) defined the word engagement in relation to high-impact
activities that lead to student involvement in active-learning practices. In this study, I
used the word engagement interchangeably with the word involvement.
I-E-O model: Astin and Antonio (2012) defined the Inputs-EnvironmentOutcomes (I-E-O) model as a method to assess the student development connections
among the Inputs of student characteristics, the Environment of the educational program

5
being measured, and the Outcomes as the cognitive or affective measures that students
are expected to gain in the program.
Information literacy (IL): The ACRL (2000) defined information literacy as a set
of skills that require “individuals to recognize when information is needed and have the
ability to locate, evaluate, and use effectively the needed information” (p. 2).
IL requirement: SRJC (2014–15) identified a general education IL requirement,
delivered via a 1-unit course, as the best method to ensure students gained core
competencies as a component of the local associate degree.
Impact: Astin and Antonio (2012) defined impact in terms of measuring how
educational practices produced a change in student skill development. In this study, I
used the word impact interchangeably with the word effectiveness.
Involvement: Astin (1999) identified involvement as “what the individual does,
how he or she behaves” (p. 519). In this study, I defined the word involvement as a
student’s behavior of critically evaluating information needed for writing papers or
participating in academic class discussions.
LIR 10: SRJC (2014–15, 2013c) identifies LIR 10 as a 1-unit course that meets
the institution’s general education IL requirement.
Outcomes: Astin and Antonio (2012) defined a taxonomy to classify student
outcome measures using type of Outcomes (cognitive or affective), type of data
(behavioral or psychological), and time (short term or long term). They identified
affective Outcomes as a measure of a student’s beliefs, self-concepts, attitudes, etc., such

6
as a self-report of the amount of time spent doing a task. In this study, I defined the word
Outcomes as affective behavioral and psychological measures.
Research: The ACRL (2000) defined research as a set of skills used to identify
information needs, implement search strategies to locate information, critically evaluate
information for quality and relevance, and synthesize the information into their existing
knowledge for problem solving.
Student development: Astin and Antonio (2012) defined student development as
“changes in the student’s abilities, competence, knowledge, values, aspiration, and selfconcept that occur over time” (p. 23).
Student success: The California Community Colleges Chancellor's Office
(CCCCO; 2011) defined student success in its Student Success Initiative as educational
achievement and completion of a degree, certificate, or transfer.
Theory of student involvement: Astin (1985) described the student involvement
theory as “the amount of physical and psychological energy that the student devotes to
the academic experience” (p. 135).
Larger State and National Educational Context
Three relevant issues surrounded the college’s IL requirement assessment gap
problem. These issues included increasing numbers of underprepared students who may
not possess IL skills and abilities, the IL education delivery methods options available,
and the need to regularly assess general education requirements.

7
Historical Evolution of the Information Literacy Need
Beginning in the late 1980s, the Internet began to offer students alternative data
resources that competed with the mediated services offered by the academic libraries
(Horner & Thirlwall, 1988). Libraries continued to offer mediated access, but they could
no longer claim to be the sole gatekeepers to information resources. The library was no
longer the only available resource to support student research. As students increasingly
accessed information directly from the Internet, instructors, and others in educational
institutions raised concerns about the scope and quality of students’ research capabilities
and the quality of the information used to support their work (Bodi, 1988; Horner &
Thirlwall, 1988; Kemp & Nofsinger, 1988; Lewis, 1987). Lewis’ study of searches in
emerging online library catalogs showed students were confused and often failed to find
the needed information. Kemp and Nofsinger’s study established that a significant
number of beginning students did not possess the minimal research skills needed for
college-level work. Bodi’s study noted the growing need for college students to analyze
critical information and its sources to achieve successful outcomes in research-intensive
courses. As a result, the scope of IL education developed and expanded. The ACRL
(1989) began a conversation about the implications of the emerging Internet in a
Presidential Committee on Information Literacy report that emphasized the need for
improved IL education on a national level.
Continuing into the 1990s, the growing need for IL education paralleled the rise
of the Internet as a powerful decentralized information access point to a massive amount
of information from an increasing number of available sources resulting in information

8
overload. In addition, the fact that anyone could contribute information freely to the
Internet required that students be able to critically evaluate information sources to
determine the value of the information found (Kuhlthau, 1991). Reeves (1996) discussed
the need for students to filter ever-increasing amounts of information and use it
selectively. The ACRL (1998) published an updated report from the Presidential
Committee on Information Literacy that became a requirement within student learning.
This report extolled national progress on IL, including the work of regional accrediting
bodies. The report also warned of the continued challenges of incorporating teaching IL
skills into the educational system.
In the 2000s, the ACRL (2000) published its Information Literacy Competency
Standards for Higher Education. ACRL’s Information Literacy Standard Three (see
Appendix I) included critical thinking components related to analyzing information that
enabled students to use information to increase their knowledge. Multiple authors
continued to acknowledge the growing need for IL support in education. This research
was based on studies that showed a reduction in the quality of information sources
students used for research projects in academic settings (Grimes & Boening, 2001; Head
& Eisenberg, 2009; Thompson, 2003; Wieler, 2004). These researchers highlighted the
importance of the IL skills and abilities required to write research papers covering a
broad range of general education subjects in the community college associate degree
program. The need for IL education has persisted into the 2010s. Researchers are still
raising concerns that students’ IL information evaluation performance has not been

9
meeting expectations (Chen et al., 2012; Gross & Latham, 2012; Gross et al., 2012; Head,
2013; Ritzhaupt et al., 2013; Taylor, 2012; York, 2013).
The national response to the IL education needs resulted in studies showing
students were underprepared for the complexities of navigating an information-dependent
world (Grimes & Boening, 2001; Kuhlthau, 1991). Later studies showed similar findings
(Head, 2013; Taylor, 2012; Thompson, 2003; Wieler, 2004). Community college
students’ reduced level of academic preparation included deficiencies in the IL critical
evaluation component of information use and offered a rationale for the IL requirement
(Conteh-Morgan, 2002; Gross & Latham, 2012; Finley & Waymire, 2012; Head, 2013;
Showman, Cat, Cook, Holloway, & Wittman, 2013). This research, in turn, led to this
study, designed to determine whether the college’s IL requirement is an effective strategy
to appropriately develop students’ IL information evaluation abilities.
At the statewide level, the California Community Colleges Chancellor’s Office
(CCCCO, 2011) convened a task force to make recommendations for how the system
could meet the challenge of educating increasing numbers of academically underprepared
students. The task force recommendations centered on the need to equip students early in
the education process with college success skills, thereby increasing their chances of
graduating. In response to the task force recommendations, the California legislators
passed the Student Success Initiative bill mandating that community colleges implement
orientation practices to address statewide student preparedness concerns (CCCCO, 2013).
Researchers responded by documenting this IL need at the state and national
levels. The educational community responded nationally, and, more specifically, in

10
California to meet that educational need. The college’s IL requirement evolved from
events occurring in this larger educational setting that, beginning in the late 1980s,
highlighted how underprepared students struggled with academic IL critical information
evaluation expectations.
Underprepared Students Information Literacy Need
The California community college system’s open access mission provides
opportunities for students who may not otherwise be able to obtain degrees. These
students often face a variety of barriers to degree completion. Many arrive with a low
level of academic preparation. This barrier requires them to stay longer to attain their
degrees and increases the likelihood that they may not meet their educational goals
(Astin, 1999; Cabrera, 2014; Heaney & Fisher, 2011; Kim and Bragg, 2008). Kuh (2008)
originated the National Survey of Student Engagement (NSSE), which examined highimpact practices that addressed student development needs and increased success. Kuh
considered IL education to be a high-impact practice with the potential to provide
underprepared students with academic research support to increase overall success in
writing intensive courses. Head (2013) included SRJC in a study of how freshmen
conduct research. Head’s work highlighted how underprepared students were not meeting
desired academic research outcomes. Students reported that they struggled to complete or
did poorly on research-based assignments.
As numerous researchers have noted, one of the most important components of IL
education is a need for critical evaluation skills (Detmering & Johnson, 2011; Grimes &
Boening, 2001; Hogan and Varnhagen, 2012; Kuhlthau, 1991; Radom & Gammons,

11
2014; Taylor, 2012; Thompson, 2003; Wieler, 2004). These higher-level IL critical
thinking skills require students to analyze a research need, implement search strategies to
locate relevant information, evaluate it for quality and appropriateness, and then
synthesize it into their existing knowledge base for problem solving. Detmering and
Johnson studied the effects of including critical thinking research concepts in an IL
education module. They found that students valued critical thinking competencies such as
asking a question and using information to determine an answer. Researchers have thus
construed the capacity to perform critical evaluation of information as an active indicator
of learner behavior.
Students themselves expressed awareness of their need for IL education. The
University of Washington’s iSchool’s (2014) national-scale Project Information Literacy
(PIL) studies showed that students consider IL knowledge and abilities to be college
success skills. Head’s (2013) PIL study of the research habits of freshmen included a
sample of SRJC students. In this study’s findings, students reported they were
“unprepared to deal with the enormous amount of information they were expected to find
and process for college research assignments” (Head, 2013, p. 2).
State and National Response to Information Literacy Need
The identified IL education need and ACRL’s (2000) development of nationally
recognized standards initiated changes in higher education accrediting practices. The IL
competencies were defined using performance indicators and learning outcomes (ACRL,
2000). Regional and discipline-specific higher education accrediting bodies integrated

12
these competencies into their assessment expectations to ensure institutions were teaching
and assessing students’ IL abilities (WASC, 2013; ACCJC, 2014).
To meet accreditation standards, community colleges institutionalized and
implemented IL education using different delivery methods (Artman, FrisicaroPawlowski, & Monge, 2010; Bowles-Terry, 2012; Detlor, Julien, Willson, Serenko, &
Lavallee, 2011; Dunn, 2002; Fitzpatrick & Meulemans, 2011; Hellenius, 2007; McBride,
2011; Moore, Brewster, Dorroh, & Moreau, 2001; Orme, 2004; Radom & Gammons,
2014; Sherman, Martin, & An, 2012; SRJC, 2014–15; Zachery, 2010). The Academic
Senate for California Community Colleges (ASCCC; 1998) resolved that all California
community colleges should implement education programs to ensure that graduating
students meet IL competencies. Four IL education delivery methods emerged from the
system of 113 California community colleges, including using a stand-alone credit
course, infusing IL into a core research course, integrating library instruction sessions
into courses, and self-paced tutorials (CCCCO, 2014b; Hellenius, 2007).
The literature provided assessment results from studies showing how various IL
delivery methods have positively impacted student success (Bowles-Terry, 2012; Detlor
et al., 2011; Dunn, 2002; Kuh, 2008; Sherman et al., 2012). Bowles-Terry’s correlation
study looked at the effect of IL education on students’ grade point average (GPA). The
study results showed a relationship between participation in an IL course and higher GPA
at graduation. Moore et al. (2001) reported on a longitudinal study conducted by the
institutional research office at Glendale Community College. This study examined two IL
education delivery methods, one a workshop and the other a stand-alone course. The

13
statistically significant results indicated “up to a 35 percent higher pass rate in English
and English as a Second Language (ESL) composition classes for students who took the
workshops” (Moore et al., 2001, p. 302). These two studies provide examples of how
various IL education delivery methods impacted student development.
Local Educational Context
The local context for this study, the SRJC (2003) IL requirement program,
included details of the college’s community as well as information about students and
their level of preparedness for the evaluation of information. It also included information
about how the college institutionalized IL education delivery as a general education
requirement and the assessment that was conducted to test institutional outcomes to date.
College Community, Mission, Values, and Programs
SRJC is a large California community college founded in 1918. It is accredited by
the Accrediting Commission for Community and Junior Colleges (ACCJC), which
functions under the Western Association of Schools and Colleges (WASC). SRJC’s
district covers approximately 1,600 square miles, and it is in the heart of northern
California (SRJC, Office of Institutional Research [OIR], 2013).
SRJC (2013e) identifies its educational purpose in its vision, mission, and values
statements. It emphasizes employing innovative educational methods to develop students’
skills and knowledge. The CCCCO (2013) Student Success Initiative positively
influenced the college’s mission toward cultivating learning through student
development. The CCCCO’s influence extended the college’s mission to include defining

14
the skills and knowledge the college valued and intended to see addressed within its
general education requirements.
SRJC (2014) offers a variety of instructional and student service programs to
achieve its student development mission within the context of the open-access mission of
California community colleges. Institutional values can be an important component of
efforts to evoke developmental change (Branson, 2008). The college identifies the value
of providing innovation in its instructional and student support services to create a
learning- and learner-centered environment. The college’s student development mission
aligns with this value. The IL requirement is an example of an innovative instructional
program designed to support student’s critical evaluation development in a general
education delivery method that is uncommon within the community college system.
SRJC has offered a strong general education program that graduates more than
1,700 students annually with 2-year degrees, with more than 1,500 student transferring to
4-year colleges and universities each year (SRJC, OIR, 2013). SRJC’s (2013b)
Institutional Learning Outcomes include critical analysis, which indicates students are
expected to conduct academic level research across the curriculum in courses including
English 1A, Psychology 1A, Microbiology, Business, and Nursing, to name a few.
Students’ Information Evaluation Preparedness
SRJC enrolls more than 25,000 students annually in more than 2,500 classes at
two campuses, two centers, a farm, and online (SRJC, OIR, 2013). The ethnic
demographic is predominately white with a Latino population that has expanded from
“15% to 29%” (SRJC, OIR, 2013, pp. SD 6–7) within the last decade and continues to

15
grow at an accelerating rate. SRJC (2013a) has also experienced an increasing number of
students enrolling who are underprepared for college-level work. Students at the college
who began at the course levels of remedial English and English as a Second Language
(ESL) courses were less successful in completing English 1A within 6 years than other
groups (SRJC, 2013a). These students lack many of the skills necessary for academic
success. This growing demographic of underprepared students has required remediation
in foundational literacies such as reading and writing prior to enrolling in college-level
courses. This need has resulted in an internal demand for the IL requirement as increasing
numbers of underprepared students, including ESL students, need to learn how to conduct
the kind of IL information evaluation required for college-level research.
Information Literacy Education Delivery Method
SRJC (2003) addressed the ASCCC (1998) IL education recommendations in Fall
2002 by adding a general education requirement in the form of a 1-unit course. The
college’s faculty and administrators made the assumption that a general education
requirement would be the IL educational delivery method best suited to developing
student research competencies in other courses. In so doing, they signaled their
commitment to the development of IL critical evaluation skills and knowledge in
students. SRJC was one of only a few California community colleges that implemented a
general education IL requirement for graduation (Hellenius, 2007).
SRJC’s (2013c) Library & Information Resources (LIR) Department facilitated IL
education through its 1-unit course, LIR 10, Introduction to Information Literacy. This
course met the general education “Area I: Information Literacy Requirement” (SRJC,

16
2014–15, p. 1) for the local associate degree. A required general education course has
been an uncommon method of delivering IL education within the California community
college system. SRJC (2013b) Institutional Learning Outcomes reinforced this
commitment by including IL knowledge and skills under critical analysis as a component
of a graduating student’s expected overall level of proficiency.
SRJC’s (2003) IL requirement evolved from events occurring in the larger
educational setting. These events included students who were assumed to be
underprepared to meet academic critical evaluation expectations, the state and national
response to the IL education need, and how one community college institutionalized a
general education IL requirement as an education delivery method. The college continued
to support this program, with thousands of students completing the IL requirement yearly.
Information Literacy Requirement Assessment
SRJC’s (2003) general education IL requirement has been in place more than two
decades. The college has continued to make a commitment to supporting the IL
requirement based on the assumption that participation in the LIR 10 course increased
students’ IL information evaluation behaviors in academic settings. The college has not
gathered targeted data to test this assumption and determine if the IL requirement has, in
fact, developed students’ abilities to critically evaluate information necessary to conduct
research effectively in academic settings.
A broad measurement of institutional learning outcomes generates the assessment
data available at the college using a Student Survey (2013d) instrument of self-reported
affective outcome gains. SRJC’s (2013b) Institutional Learning Outcomes enumerates

17
the abilities students are expected to develop as part of their educational program. The
critical analysis outcome is defined as students possessing the ability to analyze and
evaluate information to solve problems. It is closely aligned with the ACRL’s (2000)
Information Literacy Standard Three (see Appendix I), that states “the information
literate student is able to evaluate information and its sources critically and incorporate
selected information into his or her knowledge base and value system” (p. 11).
The alignment of SRJC (2013b) Institutional Learning Outcomes critical analysis
outcome (see Appendix J) with LIR 10’s (SRJC, 2013c; see Appendix K) use of the
ACRL (2000) IL standards (see Appendix I) as learning outcomes led to the assumption
that completing the IL requirement relates to reported critical analysis gains (SRJC, 2010;
2013d). However, the Student Survey (2013d) instrument containing measures of
institutional learning outcomes did not address whether participation in the IL
requirement through LIR 10 is responsible for those gains. It is quite possible that
students may have obtained these gains through other means such as participation in
English 1A.
SRJC’s (2013b) Institutional Learning Outcomes measurement of the critical
analysis outcome allowed for a broad, standardized assessment of student development.
However, it did not assess the impact of the college’s choice of the general education
requirement for IL education delivery. The lack of literature published regarding similar
IL requirements imposed on students at the community college level compounded the
college’s lack of information to support decisions about IL education delivery. Despite
exhaustive research, I found no articles and only a single dissertation specific to IL

18
assessment linked to an IL general education requirement program. In that qualitative
study, Zachery (2010), related student learning and student success in three California
community colleges to IL requirements. The delivery methods studied were stand-alone
and linked library courses. Zachery reported anecdotal findings that IL education
positively influenced student performance on research papers and stressed the need for
the development of a quantifiable assessment instrument to determine the extent to which
IL affects student development.
Rationale
Community college administrators monitor general education requirements to
ensure they achieve institutional goals. SRJC’s (2014–15) IL requirement has been a
local general education requirement for more than a decade but the school has not
conducted a targeted assessment of the program’s effectiveness. Research shows that
students continue to struggle with the evaluation of information needed for writing papers
or participating in class discussions in academic courses. The college needed a targeted
assessment process to determine if the goals of its IL education delivery method were
being met and if they conformed to the changing needs of the institution and its students.
SRJC, like other colleges, struggles to respond to demographic shifts, budget
restrictions, accreditation standards, and changing government regulations. These
competing pressures make it essential for the college to regularly examine the relevance
of its required general education courses to students’ programs of study (Sorey &
DeMarte, 2013). This study supplemented the college’s institutional level assessment,
currently in use, with a targeted survey component designed to determine if the

19
requirement’s skill development goals for students were being met. The choice to study
the college’s lack of program targeted assessment was based on institutional documents,
California governing bodies’ resolutions and initiatives, and regional accreditation
standards as well as research studies in the professional literature.
Evidence of the Problem in the Larger State and National Context
The United States relies on recognized accreditors vested by the Department of
Education operating within regions to assess community college educational programs.
The ACCJC (2014), the regional accrediting body for California, required institutions to
provide evidence of IL education and proof of its assessment in its Standard IIC. The
ASCCC (1998) resolved that community colleges provide IL education in some form,
and 16 years later, this expectation persists.
However, state legislators did not earmark any funding for community college IL
education to implement this resolution. The Student Success Initiative legislation,
designed to address the state’s increasing numbers of academically underprepared
students, significantly influenced how the state funds its community colleges (CCCCO,
2013). The CCCCO reported that the system has experienced 5 years of severe budget
cuts beginning in 2007. These cuts resulted in a statewide 25% reduction in the number
of course sections individual community colleges could offer (CCCCO, 2014b). The
result of this reduced number of course offerings has been a delay for students needing to
complete degree requirements.

20
Evidence of the Problem in the Local Context
As a result of the statewide budget constraints, SRJC’s administration decreased
the number of course sections offered across the board. This reduction created an internal
pressure due to the college’s need to fund sufficient sections to support every student
coming through, particularly in hard economic times. Large numbers of students were
unable to enroll in the IL requirement courses needed to graduate, thereby delaying
program completion. The graduation delay caught the attention of the college’s
administrators as they faced state pressures to move more students to degree completion.
The graduation delay caused by high enrollment demand for the IL requirement
LIR 10 course was discussed in several of the college’s stakeholder and shared
governance committees (SRJC, Academic Senate, 2013; SRJC, Education Planning and
Coordination Council [EPCC], 2013). The Counseling Department complained about the
insufficient number of available LIR 10 sections with open seats to meet student demand.
SRJC’s EPCC raised concerns about the efficacy of the general education delivery
method for the IL requirement and noted the need to determine if this method should
continue in the future. SRJC’s Academic Senate acknowledged the EPCC enrollment
pressure concerns. Popular feeling was that the institution needed to make a decision
about keeping the general education IL requirement. If the college decided to keep the
requirement, it must provide support by increasing the number of sections available for
students. As funding sources perceive student completion as an indicator of a community
college’s success, lack of access to the IL degree requirement posed an institutional
dilemma. Stakeholders discussed the enrollment pressures, but still they did not consider

21
whether the IL requirement helped students acquire valuable skills leading to student
success throughout their program. They simply stated concerns about the high number of
students needing the requirement.
In 2002, SRJC (2014–15) added a general education IL requirement as a
component of its local associate degree. Since that time, SRJC’s LIR Department
received a number of testimonials from students regarding the impact of the IL
requirement. Other than these testimonials, no targeted assessment data existed to show
whether or not the IL requirement had been a relevant program component that fostered
student success in a liberal arts general education. IL critical analysis gains in student
learning were self-reported in the 2010 and 2013 versions of an SRJC (2010, 2013d)
longitudinal Student Survey. The 2013 survey assessed 10% of the credit students and
100% of online students in randomly selected courses resulting in “2,780 responses”
(SRJC, 2013d, p.1). The survey analysis used student characteristics such as age, gender,
ethnicity, and academic preparation as independent variables to ensure the sample
represented the student population as a whole (SRJC, 2013d). In SRJC’s Student Survey
students were asked to provide self-reports of how much their education had increased
their “knowledge, skills, and abilities” (SRJC, 2013d, p. 31) on 20 different institutional
learning outcomes. SRJC’s (2013a) Institutional Effectiveness Assessment Report
indicated students’ highest skills and abilities gains were in writing (81.0%) followed
closely by critical analysis “locating, analyzing, evaluating and synthesizing relevant
information” (80.8%; p. 7). Whether the IL requirement had an impact on gains made or

22
the maintenance of higher levels achieved was not directly assessed. Results indicated
that the objectives of the requirement were being met somewhere within the curriculum.
Student behaviors are indicators of their skills and abilities that, in turn, are
indicators of development. Astin (1999) believed that the concept of involvement related
more to how a student behaved rather than what the student thought or felt. A study of the
impact of the IL requirement on students’ information evaluation behavior could provide
insight into the continued need for the program. Research has shown the value of IL
education but its delivery as a core component of the general education program requires
further study for insight into the impact of such a program on student learning. In making
future decisions relating to the program, the college would benefit from a program
focused assessment of the impact of its current method of delivering IL education on
students’ critical information evaluation behaviors. The gap in assessment of the general
education IL requirement created the potential for the college to make decisions without
evidence. Assessment was needed to ensure that resources were being used to best effect
in meeting institutional objectives. This study examined relationships among student
characteristics, aspects of the general education IL requirement, and subsequent
frequency of student use of IL critical information evaluation behaviors and levels of
confidence in relation to writing papers and participating in discussions in other courses.
Significance of the Study
The local, statewide, and national evidence showed a gap in the practice of
assessment of the college’s general education IL requirement. The lack of assessment of
this requirement was relevant in the local context because external funding pressures

23
made it critical for the college to determine if it should continue to support the IL
requirement program. This study addressed the gap in assessment by quantifying
students’ information evaluation behavior changes in academic settings as a result of
participation in the LIR 10 course meeting the IL requirement.
This study provided a targeted way to assess students’ IL critical evaluation skills
and knowledge development in academic settings. The college can use these data as the
basis for examining the efficacy of the general education requirement as an IL education
delivery method. Study of SRJC’s IL requirement also produced data about the
relationship between successful IL requirement completion and student behavior changes
in other classes. Findings about these student behavior changes can inform institutional
decisions about the impact of the program and can be used as a basis and support for
future decision-making. The lack of targeted assessment of the IL requirement was
significant in the larger educational context because all California community colleges
must deliver IL education. The study added to the professional literature regarding the
impact of a general education delivery method for IL education and its assessment.
Research Questions and Hypotheses
This study had one overarching question and two researchable questions (RQ)
that were aligned with the problem, purpose, and literature. I developed null and
alternative hypotheses for each of the research questions that informed the research
design and approach. I used a survey to collect data for the 10 independent and three
dependent variables in the study. The student identified characteristics referred to broadly
in the research questions specifically included age, gender, ethnicity, primary language,

24
terms attended, English course level, research preparedness, and number of papers
comprised eight of the independent variables. The remaining two independent variables,
referred to specifically in the research questions, included the IL requirement course
characteristics of format and length. The dependent variables consisted of IL information
evaluation behavior changes and levels of confidence. Descriptive and correlation
statistical tests will be used to determine if relationships between independent and
dependent variables exist.
Overarching Question
Is a 1-unit general education requirement an effective IL education delivery
method for students at a community college?
Research Questions and Hypotheses
RQ1: What is the relationship between completion of the general education IL
requirement course with different formats and lengths and frequency of information
evaluation behavior changes among students with identified characteristics?
H01: There is no statistically significant relationship between identified
characteristics of students who completed the general education IL requirement course
with different formats and lengths in terms of information evaluation behavior changes.
Ha1: There is a statistically significant relationship between identified
characteristics of students who completed the general education IL requirement course
with different formats and lengths in terms of information evaluation behavior changes.
RQ2: What is the relationship between completion of the general education IL
requirement course with different formats and lengths and how skills learned contributed

25
to information evaluation confidence in other courses among students with identified
characteristics?
H02: There is no statistically significant relationship between identified
characteristics of students who completed the general education IL requirement course
with different formats and lengths in terms of how skills learned contributed to
information evaluation confidence in other courses.
Ha2: There is a statistically significant relationship between identified
characteristics of students who completed the general education IL requirement course
with different formats and lengths in terms of how skills learned contributed to
information evaluation confidence in other courses.
Research Question Alignment with Problem, Purpose, and Literature
The overarching question directly aligned with the identified problem of the
college’s IL requirement program assessment gap. It also aligned with my purpose to
examine relationships between student characteristics, aspects of the general education IL
requirement, and subsequent frequency of student use of IL critical information
evaluation behaviors and levels of confidence in writing papers and participating in
discussions in other courses. The study results will provide assessment data the college
can use to determine the impact of a general education requirement as a delivery method
for IL education at a community college. The institutional, regulatory, and, professional
literature provided evidence of the increasing number of underprepared students in
colleges and universities and the role of IL critical information evaluation education in
student success. The literature also addressed the importance of assessment of general

26
education requirements and the value of Astin’s (1985) Inputs-Environment-Outcomes
(I-E-O) model of assessment. SRJC’s limited documentation of IL skill development
underlined the IL requirement program assessment gap as a problem. The use of the I-EO assessment model to provide insight into the impact of SRJC’s IL requirement program
gave rise to one overarching and two research questions. I used relationship types of
quantitative research questions. These types of questions related one or more identified
student characteristics variables such as age or level of English course completed and one
or more program characteristics variables such as course format and length to how
frequently students use IL critical analysis behaviors or noted confidence level changes to
discover relationships among the variables.
Review of the Literature
Abundant studies have been published on IL skills and abilities and a large body
of literature on various aspects of IL education exists. However, a lack of studies
assessing the efficacy of a required general education course as the IL education delivery
method existed. The review focused specifically on institutional, regulatory, and
professional literature directly related to the student development impact of IL education.
I organized the review around the topics of student involvement, students’ IL critical
evaluation behavior development, and IL education delivery methods within local
educational contexts and in the higher education community.
Theoretical Base and Conceptual Framework
Astin (1985) described the theory of involvement as “the amount of physical and
psychological energy that the student devotes to the academic experience” (p. 135).

27
Based on longitudinal studies of student development, Astin concluded that involved
students spend more time and effort in educational activities, resulting in increased
student performance (Astin, 1999). Based on those findings, Astin concluded that the
more frequently students were involved in developmental opportunities, the more likely
they were to be engaged and achieve educational goals.
Astin and Antonio (2012) defined student development as “changes in the
student’s abilities, competence, knowledge, values, aspiration, and self-concept that occur
over time” (p. 23). Astin’s (1985) theory evolved from the idea that students’ knowledge
and skill development relate to their level of involvement. Knowledge and skill
development, two of Astin’s central concepts, also make up the core of SRJC’s (2013e)
mission. This alignment was fundamental to the association between Astin’s model and
the IL information evaluation development gap at the college.
IL education develops critical information evaluation competencies in students,
allowing them to make connections needed for successful involvement in courses that
require research for writing. Detmering and Johnson (2011) noted the fundamental value
of critical thinking in academic courses. Kuh (2008) listed IL as one of the high impact
practices affecting student engagement in intensive writing courses. As a result of this
research, educators have assumed that the general education IL requirement could
potentially have an impact on students’ success and completion rates through the
development of the information evaluation skills needed for research.

28
Inputs-Environment-Outcomes Model Description
Astin’s (1985) I-E-O model of assessment offered a conceptual framework for
examining the student development impact of a general education IL requirement. Astin
was the first director of HERI at the University of California, Los Angeles. The
longitudinal study of college students conducted nationally since 1973 through CIRP
brought attention to the student involvement issue (HERI, 2014). CIRP is the largest
empirical study of higher education including “1,900 institutions, over 15 million
students, and more than 300,000 faculty” (HERI, 2014, para. 2).
Astin (1985) developed the I-E-O model, shown in Figure 1, to facilitate the
assessment of student development by using the connection among Inputs, Environment,
and Outcomes. The Inputs are the individual characteristics and level of development that
students bring to the learning environment. The Environment is the educational program
being measured. The Outcomes are cognitive or affective measures of what students are
expected to gain in the program (Astin & Antonio, 2012).

Figure 1. A diagram of Astin’s Inputs-Environment-Outcomes model.

29
Justification of Selection of the Inputs-Environment-Outcomes Model
I chose Astin’s (1985) I-E-O model as a diagnostic tool and as a conceptual model
for measuring the student development impact of the IL requirement. Astin believed that
involved students learn more and are, therefore, more successful. I considered several
theoretical frameworks for this study, including the social constructivist theory of
education, IL education theory, critical thinking theory, and literacy acquisition theory.
Ultimately, I selected Astin’s theory of student involvement and I-E-O model as the
study’s theoretical basis and the conceptual framework that most closely matched the
needs of a quantitative study of behavior changes with a learner-centered focus.
A number of educational studies have used Astin's student involvement theory
and I-E-O model to evaluate the impact of various educational programs on students’
experiences or development. Thurmond and Popkess-Vawter (2003) studied the
intersection of the theoretical and empirical uses of the I-O-E model for evaluating the
effectiveness of online courses. These researchers concluded that Environment
characteristics were the most predictive of student satisfaction in a Web-based course and
that the model’s inclusion of Inputs and Environment characteristics provided more
insight into Outcomes than unitary assessments.
I found support for Astin's (1985) theory of involvement in Elkins, Forrester, and
Noel-Elkins’ (2011) survey of the influences of out-of-class activities on students’
perceptions of the campus community. Cluster and multivariate analysis of particular
activities produced significant results indicating that involved students showed a higher

30
sense of community (Elkins et al., 2011). Their study showed the power of Astin’s theory
and framework to provide assessment of an educational program.
A variety of studies covered Astin’s notion of student involvement/engagement.
Elkins et al. (2011) noted that the terms involvement and engagement are frequently used
interchangeably in the literature. Wolf-Wendel, Ward, and Kinzie (2009) supported that
claim in their extensive review of student development literature for use of the terms
involvement (Astin, 1985) and engagement (Kuh, 2008). In addition, personal interviews
of Astin and Kuh showed they assigned similar meanings to the two terms (Wolf-Wendel
et al., 2009). These authors identified involvement as being focused on student behaviors
and engagement as being focused on institutional behaviors specifically related to
providing an environment with development opportunities (Wolf-Wendel, et al., 2009).
Cabrera (2014) used Astin’s (1985) I-E-O survey model to complete a qualitative
analysis of college preparation characteristics affecting first-generation minority students
at a state university. Kim and Bragg’s (2008) investigation of community college student
relationships between the Inputs characteristics of gender and educational background
and Environment characteristics of career and technical education were significant to the
output of preparedness. These findings contributed to their conclusion that the I-E-O
model produced a valuable assessment of an educational program (Kim & Bragg, 2008).
Heaney and Fisher (2011) concurred that using Astin’s I-E-O model as their study’s
framework provided useful data to study education environments. Their survey resulted
in data showing relationships between Inputs and Environment factors affecting the
outcome of persistence in first-year university students (Heaney & Fisher, 2011).

31
Utilizing Astin’s (1985) I-E-O model of assessment provided a framework around
which to collect data and analyze reports of past behavioral changes (Astin & Antonio,
2012). An additional value of Astin’s I-E-O model was that it allowed adjust for
differences in student Inputs characteristics using multivariate analysis techniques. Astin
and Antonio (2012), posited that using these techniques produced a “less biased estimate
of the comparative effects of different environments on outputs” (p. 20). Lastly, the I-E-O
model provided a reputable and established measurement tool to gather students’ selfreported information evaluation behavior changes as quantifiable affective Outcomes of
the development impact of the IL requirement.
Conceptual Framework’s Contribution to Understanding of the Problem
Astin’s (1985) I-E-O model of assessment for data collection and analysis
methods provided the appropriate conceptual framework for assessing the impact of the
IL requirement program. This framework provided the background of the problem
including the preparedness issues surrounding student characteristic Inputs, the IL
education delivery methods characteristic of the Environment, and the students’ critical
analysis behavior and confidence Outcomes. These model factors have the potential to
fill the college’s gap in general education assessment practices. This framework outlined
how I could use students’ changes in their academic behaviors to measure the impact of
participation in the IL requirement for student development (Astin & Antonio, 2012),
reinforcing Astin’s (1999) belief that “it is not so much what the individual thinks or
feels, but what the individual does, how he or she behaves, that defines and identifies
involvement” (p. 519). Additional appeals of this model were its simplicity and its

32
potential to provide valuable insight into program impact. The model accommodated for
varying beginning student characteristic Inputs, selected measurable educational
environment aspects, and quantified the affective Outcomes of student behavioral
changes or psychological levels of confidence.
Larger Educational Context in the Literature
Key associations and organizations responded to the national IL skills problem
verifying the educational need (ASCCC, 1998; ACRL, 1989, 1998, 2000; CCCCO, 2011,
2013; Hellenius, 2007; SRJC, 2013c, 2014–15). The ACRL reports from the Presidential
Committee on Information Literacy documented the strong student developmental need
for IL education that resulted in the creation of the nationally recognized ACRL (2000)
IL standards. In response, the ASCCC recommended that all community colleges in the
state provide IL education in some form. The ASCCC supported further study of delivery
methods developed out of their recommendations (Hellenius, 2007). The CCCCO created
the Student Success Initiative from task force recommendations indicating the importance
of providing underprepared students with development opportunities. In the early 2000s
SRJC responded to the state and national call for IL education by institutionalizing a
general education IL requirement.
The academic needs for IL critical evaluation skills and training are well
documented in the professional literature. Numerous studies have been conducted that
examined the information skills competency levels of students entering and attending
colleges and universities. Gross et al. (2012) derived data from a series of empirical
studies that indicated the IL need of first-year college students with below proficient

33
scores on a standardized IL test. Gross and Latham (2013) published further results from
multiyear studies that indicated a clear need for the development of an IL educational
intervention in the form of instructional modules.
Ritzhaupt et al., (2013) conducted a study using a standardized IL assessment
with a focus on student demographic Inputs. Their results showed statistical differences
among groups when the population was divided by student socioeconomic status (SES),
ethnicity, and gender (Ritzhaupt et al., 2013). Chen et al. (2012), Head (2013), and
York’s (2013) findings all showed that substantial numbers of students struggle with
information overload that can affect participation and cognitive processes. Based on these
studies and reports, IL is necessary across disciplines so that students can be challenged
to develop the ability to find accurate information and evaluate resources in various areas
of endeavor. Taylor’s (2012) longitudinal study produced statistically significant results
indicating that millennial generation students’ information searching behaviors were
erratic and that these students did not routinely evaluate the quality of information
provided by the sources they found.
Student Information Evaluation Preparedness
A number of studies highlight the value of information evaluation behaviors in academic
research. Astin (1985) noted that student behavior is an important measure of student
development that can be useful for assessing program impact. When reviewing the
literature, I found few studies specific to the ACRL (2000) Information Literacy
Standard Three (see Appendix I) critical evaluation performance indicators that I used for
the study. These critical evaluation behaviors included confidence in applying criteria to

34
evaluate information and its sources, comparing new knowledge with prior knowledge to
determine contradictions, understanding information through discourse with others, and
determining if a search query should be revised to improve results.
Hogan and Varnhagen (2012) studied undergraduates who had been exposed to
minimal IL education intervention. Their study showed that these undergraduate students
had not developed information evaluation skills and were prone to using biased, dated,
and otherwise inappropriate websites when asked to do research. The research of Hogan
et al., was one of the only studies I found that addressed behaviors specific to the study.
Many librarian instructors used the currency, relevance, authority, accuracy,
purpose (CRAAP) test to teach students information evaluation criteria (Meriam Library,
California State University at Chico, 2010). Radom and Gammons (2014) conducted an
assessment study of teaching the five Ws (who, what, when, where, why) method of
inquiry as evaluation criteria. They concluded that both students and discipline faculty
found value in the method.
Finley and Waymire (2012) examined bibliographies. Their study highlighted the
importance of infusing IL education into business courses to meet underprepared
students’ need to analyze discipline specific information critically. Showman et al. (2013)
emphasized the role creativity and judgment play in the undergraduate research process.
Their findings also noted that students were not comfortable seeking help from others and
were reluctant to ask for assistance when solving problems.
Underprepared students pose an increasingly important factor in community
college education. Underprepared students require education in basic skills including IL

35
education to facilitate a successful pathway to completion (Gross & Latham, 2012).
Several studies and reports addressed the role of IL education in developing
underprepared students to a level where they are competitive with their peers in the
classroom (Community College Survey of Student Engagement [CCSSE], 2013; ContehMorgan, 2002; ETS, 2014; Gross & Latham, 2012; Head, 2013; SRJC, 2013a). However,
little information on IL education for Latino students existed in the literature. ContehMorgan discussed IL barriers that Latino English as a Second Language students
encounter and the importance of IL education to teach students to make connections
between new and known information.
A large percentage of the studies I found in the literature concerned the impact of
teaching IL at 4-year colleges and universities. Fewer studies covered community
colleges. Astin (1999) noted a finding from the CIRP longitudinal student development
study that, even when controlling for differences in student characteristics, the chance of
community college students dropping out is higher than if they attended 4-year colleges
and universities. As community colleges continue to incorporate new methods to meet
the needs of their underprepared students, studying the effects of the IL requirement can
offer data regarding the effects of the practice on student development.
Information Literacy Education Delivery Methods
For decades, educational literature has actively discussed information literacy.
The IL education movement of the late 1990s produced an abundance of scholarly
references to various aspects of IL educational delivery methods and students’ academic
IL critical evaluation research competencies. Numerous research studies provided

36
national evidence of problems in the quality of students’ research capabilities (ACRL,
1989; Grimes & Boening, 2001; Head & Eisenberg, 2009; Head, 2013; Kuhlthau, 1991;
Taylor, 2012; Thompson, 2003; Wieler, 2004).
Numerous studies in the literature assessed the success of various IL education
delivery methods (Bowles-Terry, 2012; Detlor et al., 2011; Dunn, 2002; Moore et al.,
2001; Sherman et al., 2012). However, I found few studies that addressed the role of IL
education in developing information evaluation skills (Detmering, & Johnson, 2011;
Gainer, 2012). Few studies examined the range of IL education delivery methods
employed by the 113 California community colleges. The exception was one study that
surveyed the IL education delivery methods used in the California community colleges
noting that the 1-hour instructional session was historically the predominant teaching
method (Hellenius, 2007). Zachery’s (2010) dissertation was the one study I found that
attempted to examine student development in relation to required IL courses in the
California community college system. Zachery noted the challenge of collecting evidence
in this qualitative study of student development resulting from participation in required
IL courses and recommended development of quantitative assessment instruments.
Artman et al. (2010) addressed the issue of research writing skills being taught in
English classes and the importance of "one-shot" library instruction sessions in
complementing the process. In an older study, Orme (2004) examined the IL education
development of first-year college students using the web-based Texas Information
Literacy Tutorial (TILT). The study’s findings showed the effectiveness of online
tutorials, and the concluded they were comparable to on-ground (face-to-face) instruction

37
(Orme, 2004). McBride’s (2011) study focused on the integration of 21st-century
literacies into IL courses. Fitzpatrick and Meulemans (2011) conducted a quasiexperiment examining two IL education delivery methods offered in conjunction with a
developmental psychology course. The two delivery methods tested were a stand-alone
self-paced IL assignment or the same assignment with the addition of a librarian-led IL
workshop. Results showed significantly better scores when testers coupled the
assignment with the workshop.
Information Literacy General Education Requirement Assessment
SRJC’s general education IL requirement assessment gap was compounded by the
lack of published literature regarding similar requirements at the community college
level. Despite exhaustive research, I found no articles that evaluated affective behavioral
changes linked to an IL degree requirement program.
The college committed to the general education IL requirement as its delivery
method but had not specifically assessed the student development impact of this method.
Assessment of general education requirements is essential to ensure students are learning
what the institution has placed as its highest priorities (Andrews, 2012; Robertson, 2013;
Siefert, 2011; Sorey & DeMarte, 2013). Sorey and DeMarte’s study illustrated the
importance of evaluating general education requirements for student development. Siefert
introduced the Valid Assessment of Learning in Undergraduate Education (VALUE) as
an assessment model of general education learning outcomes including written
communication, inquiry, critical thinking, and information literacy. Robertson noted the
importance of using evaluation criteria to keep the community college general education

38
program relevant, thereby increasing student engagement. This study also recommended
removing general education courses that no longer met the criteria. Andrews discussed
the important role that libraries play in supporting the general education program and
advocated using technology to ensure relevance.
Most of the studies I found assessed teaching IL at 4-year colleges. Few assessed
community colleges. I did not identify any scholarly studies in the professional literature
for the last 10 years that directly addressed the assessment of the impact of a general
education IL requirement course on student development of information evaluation
behaviors. In addition, I could find no studies in the professional literature published
within the last10 years that assessed the efficacy of various IL education delivery
methods. However, the college remains committed to teaching IL to its students,
suggesting the need for a reasonable model of targeted assessment.
Local Educational Context in the Literature
SRJC’s OIR, (2013) reports that increasing numbers of underprepared students
are enrolling aligned with state and national trends. These trends influenced SRJC’s
(2013e) focus on student development in its vision, mission, and values statement.
Increased pressures for assessment of programs from accrediting bodies (ACCJC, 2014;
WASC, 2013) encouraged the college’s interest in the assessment of the effectiveness of
its institutional programs. The college identified the general education IL requirement as
one of those programs under review. Several local institutional documents illustrated
SRJCs commitment to IL education and the gap in assessment to support its value to the
institution (SRJC, 2013a, 2013b, 2013c, 2013d, 2013e, 2014–15; SRJC, OIR, 2013).

39
Underprepared Students Increasing in Number
California community colleges, in general, and SRJC in particular are
experiencing increasing numbers of students testing into developmental English (SRJC,
OIR, 2013). I was unable to access data on the IL skill levels of incoming students
because the institution does not administer an IL placement test (ETS, 2014). It is
important to measure a student’s level of development because it is relevant to SRJC’s
(2013e) mission and supports statewide student success efforts (CCCCO, 2013). SRJC’s
Scorecard data showed that ESL students are not persisting to graduation as often as
other ethnic groups (CCCCO, 2014a). I included a measure of ethnicity because of the
high percentage (29.4%) of Latino students at SRJC (CCCCO, 2014a).
SRJC’s (2013a) student demographics was similar to national statistics
concerning the increases in enrollment of students underprepared in the information
evaluation learning behaviors needed for college-level academic research (CCCCO,
2014c). This growth required the college to focus its mission more tightly on the
development of students (SRJC, 2013e). Students at the college who began in remedial
English and ESL courses showed lower completion outcomes for finishing English 1A
within 6 years than did other groups (SRJC, 2013a). These students were found to lack
the academic preparation necessary to be successful in courses that require research.
Information Literacy Requirement Delivery and Assessment
SRJC (2003, 2014–15) approved adding an IL requirement to its general
education program. SRJC was one of the few California community colleges that
implemented a stand-alone course, general education requirement as the delivery method

40
for IL education. The requirement addressed the IL education recommendations of the
statewide Academic Senate (1998). SRJC (2013b) Institutional Learning Outcomes
requiring students to critically analyze information and sources needed to succeed in
academic settings was also addressed. To ensure the level of quality of IL education and
to justify the commitment, SRJC (2013c) aligned LIR 10’s official student learning
outcomes with the ACRL (2000) IL standards. These standards also aligned with SRJC
Institutional Learning Outcomes critical analysis outcomes and are forming the basis of
SRJC’s (2014) general education specific learning outcomes.
The college’s assessment efforts included a broad assessment of the institutional
learning outcomes performed longitudinally via SRJC’s (2010, 2013d) Student Surveys.
These Student Survey results showed students’ self-reported gains were high for the
critical analysis affective outcome. SRJC’s (2013a) Institutional Effectiveness Assessment
Report cited these gains as a benchmark measure of educational effectiveness. Despite
these IL related gains, the college’s shared governance committees responded in 2013 to
several years of statewide budget cuts with discussions of eliminating the IL requirement
without any talk of the impact of the IL requirement on student development (SRJC,
Academic Senate, 2013; SRJC, EPCC, 2013). These discussions showed the importance
of conducting targeted assessments to connect the broader institution level IL critical
analysis gains to student participation in the IL requirement.
Saturation Reached in Literature Review
I conducted an extensive review of the educational literature using EBSCO, Sage,
ERIC subscription databases, and a free database, Google Scholar. The searches focused

41
on the primary topic of information literacy and competency education. The secondary
searches included the following terms and phrases: IL need, students, research,
information overload, Internet, online, IL instruction delivery methods, IL state and
national history, assessment, IL behaviors and abilities, general education program
assessment, student success and development, evaluation, IL critical analysis, evaluation
and thinking, learning outcomes, student involvement and engagement, IL education
impact, community college, accreditation, survey methodology, student self-assessment,
student self-reporting, and data analysis.
Relevance and Relationship of Literature to Proposed Research Study
The literature retrieved was well aligned with the problem identified and the
rationale, purpose, and methods used in the study. The literature included an abundance
of scholarly documents extolling the positive benefits of IL education on student success.
The Sherman et al. (2012) qualitative study of a bachelor’s degree graduation
requirement found that an IL education positively impacted student performance.
Researchers measured the success of different IL educational delivery methods in the
literature. Some studies of the relationship between IL and student success were
conducted by examining research paper citations and grades in a research-intensive
course such as Psychology 1A. Others established correlations between IL education and
student retention or program completion (Bowles-Terry, 2012; Detlor et al., 2011; Dunn,
2002). However, a gap existed in the literature relating to the assessment of a general
education IL requirement program. This gap supported the need for a study of SRJC’s
program (2014–15). The literature review showed how student preparedness and IL

42
education delivery method assessment can have local implications for program
assessment and act as a contributor to larger social change.
Implications
This study’s research questions and hypotheses provided implications for a
project that evolved from the research literature and findings. The primary requirement
for the project is to communicate efficiently the study’s findings and recommendations to
busy faculty and administrative stakeholders who may not have time to read the entire
report. A white paper can provide an appropriate communication model for the study
results. The white paper’s design can allow for policy recommendations based on the
study’s research and results from the survey of learner-centered self-reported information
evaluation behavior changes to be communicated. Using a white paper as the product of
the study provides an informative context of the background of the problem using
evidence from the state, national, and local literature. The background can frame the
issues of student preparedness, IL education delivery methods, and the college’s IL
requirement assessment gap as evidence of the problem. A white paper can also provide a
description of the study’s methodology, use of Astin and Antonio’s (2012) I-E-O model
as the theoretical framework, a summary of the study’s data analysis results, and policy
recommendations regarding the college’s future assessment practices. These
recommendations can form the basis for institutional discussion and inquiry into a new
mechanism for assessing the effectiveness of the general education IL requirement.

43
Summary
The problem prompting this study was the lack of targeted assessment of the
impact on student development at the college of a general education course requirement
designated as an IL education delivery method. An IL requirement delivered as a 1-unit
course had been part of the local general education program at the college for more than a
decade, yet no targeted assessment had been conducted to assess its impact. The college
can use data from the study of program impact as a basis and support for future decisionmaking. The lack of specific assessment of the IL requirement was important locally
because external funding pressures made it critical for the college to determine if it
should continue to support the IL requirement at the college. Examining the success of
the general education requirement as a delivery method by relating it to aspects of student
development can have implications for policy recommendations regarding developing an
IL requirement assessment mechanism. Section 2 describes the study’s methodology,
including the research design and the approach used, particulars of the setting and sample
of participants, specifics of the instrument and materials, and details of data collection
and analysis. The section includes comments on the protection of participants and
potential limitations of the study and concludes with the data analysis results and
interpretation. In Section 3, I describe the project that developed from the study, the
assessment plan, and implications. In Section 4, I reflect on the project’s strength,
importance and implications for further research.

44
Section 2: The Methodology
Introduction
The purpose of this quantitative study was to examine relationships among
student characteristics, aspects of the general education IL requirement, and subsequent
frequency of student use of IL critical information evaluation behaviors and levels of
confidence in relation to writing papers and participating in discussions in other courses.
Astin’s (1985) involvement theory framed the analysis of relationships between the IL
requirement completion and student levels of confidence derived from its effect on their
information evaluation activities related to writing papers and participating in discussions
in other courses. I used survey methodology to gather data from students who completed
the general education IL requirement and descriptive, cross-tabulation, correlational, and
multiple regression analysis to determine relationships.
Astin’s (1985) I-E-O model identified a framework for using survey research
methods. The closed-ended items on the study’s survey collected data on students’
changes in information evaluation behavior experienced as a result of participation in the
IL requirement program. I analyzed the quantitative data and used the results to
determine whether data supported the institutional assumption of a relationship between
the IL requirement and students’ critical information evaluation development. Tables,
figures, and appendices support this study. Findings from this study may assist with
future institutional decision-making. Goals of this research were to gather data on student
critical information evaluation behaviors and to create a basis for the college’s future
scheduled assessment practices.

45
I divided Section 2 into seven areas. The first area includes a description of the
survey research design and approach, as well as the justification for and relationship of
the quantitative research design to the problem of the study. The second area includes a
description of the designated target population specifying size and makeup, criteria for
participation, and participant eligibility and selection. I also cover sampling method, size,
and characteristics. The third area describes in detail the survey instrument, the concepts
measured, and the score’s calculations and meanings. In addition, this section discusses
how the instrument’s validity and reliability were established using peer expert review
and a pilot study. The fourth area details data collection and analysis methods including a
description of procedures used and a discussion of the nature of the scale for each study
variable. I cover assumptions, potential limitations, scope, and delimitations of the study
in the fifth area. In the sixth area, I review the measures I took to gain informed consent,
protect participants from harm, and ensure confidentiality. The final area outlines the data
analysis results.
Research Design and Approach
Considering the intersection of the college’s IL requirement assessment problem
and its learner-centered values, I decided that survey research was a suitable approach for
exploring the relationships between the variables identified in the research questions.
Research Design
I conducted a quantitative correlational study that used institutional data and the
survey method for collecting data to explore relationships among selected student
characteristics, participation in the IL requirement, and critical evaluation behavior and

46
confidence related affective Outcomes. The study relied on the postpositivist philosophy
of scientific inquiry because I sought to gather quantitative data objectively using closedended items to test deductively Astin’s (1985) theory of student involvement. However, I
am a scientific realist who recognizes the challenge of making definite deductions in the
study of human behaviors. Therefore, even though I used scientific techniques, I
understood that I must consider and account for different self-concepts (Lodico,
Spaulding, & Voegtle, 2010). The data collection methods involved gathering
institutional data and designing a quantitative survey instrument using selected items
adapted from two sources in addition to self-developed items. Survey items used
multiple-choice and Likert-type scale response options. The data analysis methods used
descriptive and correlational statistical procedures to test for relationships among the
identified variables.
Research Design and Approach Justification
Using survey methodology had many advantages. Designing a survey provided a
precise quantitative instrument for collecting data about the variables under study,
answering the research questions, and testing the hypotheses. Astin and Antonio (2012)
stated that a survey can be an ideal tool for the purpose of linking students’ learning
development changes to the evaluation of an educational program. Students were able to
indicate how participation in the IL requirement affected their information evaluation
behaviors in subsequent courses. The Information Literacy Requirement Impact Survey
(see Appendix B) was administered 12 to 24 months after program participation. This
timeframe was long enough to allow students to gain experiences in other courses but

47
was not so long after completion of the IL requirement course, LIR 10, that students
forgot learning outcomes. Survey methodology supported the exploratory research goal
of using a cross-sectional design to collect data at one point in time and provided a
learner-centered basis for examining the impact of LIR 10 on student development. The
survey was an efficient and cost-effective method to reach the designated target
population, measure multiple variables, and test more than one hypothesis. An additional
advantage was that the survey could be administered online for anonymous collection of
a large volume of data.
Several constraints in the local setting prevented the use of an experimental study.
Because all students were required to complete the IL requirement, it was not possible to
form experimental and control groups. Pretests and posttests would have been of little
value given that the point of the study was to determine how well the course prepared
students for work in later courses. Also, grades could not be obtained because of
institutional privacy concerns relating to the confidentiality of student records.
How Research Design Derives From the Problem
A lack of targeted assessment of the general education IL requirement’s impact on
student development constituted the problem this quantitative descriptive and
correlational study was designed to assess. Astin and Antonio (2012) advanced the idea
that self-reported data can be a useful psychological, affective Outcomes measure for
initiating institutional discussion and self-evaluation of educational programs. Therefore,
a survey using Astin’s (1985) I-E-O model was an appropriate method to use learners’

48
critical evaluation development to assess the impact of a general education IL
requirement delivery method on subsequent work.
The quantitative survey design method can provide sufficient data for showing
relationships between program participation and self-reported information evaluation
behavior changes. This relationship data could be used by the college to assess the
general education IL requirement as a delivery method that may or may not have affected
development changes in student learning and success. Ultimately, administrators will be
able to use the results of the data analysis for making decisions regarding the program’s
educational impact, contribution, and potential continuation.
Setting and Sample
The college’s local setting was the site of the problem of lack of assessment of the
IL requirement as an education intervention. This northern California public 2-year
community college offers 152 certificates and 111 majors in the associate degree
program, with an unduplicated student headcount of 11,209 in the Summer 2013 term,
25,812 in the Fall 2013 term, and 26,735 in the Spring 2014 term.
Population
The students at the college who successfully completed the general education IL
requirement course, LIR 10, with a grade of 2.0 or better, during Summer 2013, Fall
2013, or Spring 2014 semesters comprised the target population for this study. I used
institutional data to identify a count of N = 2012 students who successfully completed
LIR 10 in the study’s designated period (CCCCO, 2014c). The enrollment, by semester,
included 456 students in Summer 2013, 765 in Fall 2013, and 791 in Spring 2014.

49
Sampling Strategy
I used the total population purposive sampling technique to invite students to
participate in the study. A purposive sample gives a nonrepresentative subset of students
from a larger population (Creswell, 2012). It is a nonprobability sampling strategy that
provides the potential to examine the items of interest for an entire target population of
students who completed the IL requirement within the designated period. This target
population was of a size that made it possible to invite all of the N = 2012 students
having the particular characteristics required by the eligibility criteria.
An advantage of purposive sampling was that it provided the opportunity to focus
on the specific population characteristics identified in the research questions. Total
population purposive sampling was the appropriate method to achieve the goal of
obtaining the largest sample of students from the population of interest and provided the
most potential for the sample to be as representative as possible. By inviting the total
target population to participate, the probability that each member could equally
participate in the anonymous survey was increased (Creswell, 2012). To increase the
likelihood of acquiring a representative response sample of the target population invited,
I paid careful attention to the survey design with brevity as a goal and actively
encouraged participation by sending prenotification and reminder emails.
Sample Size
To ensure the sample would be of sufficient size to allow conclusions regarding
the results I determine the minimum sample size required for conducting the study. To do
this, I calculated the sample number required for the study using an online a-priori

50
sample size calculator specifically designed for multiple regression (Soper, 2015). The
calculator was designed based on Cohen’s (1992) principles and statistical power tables.
Cohen listed an effect size (f2 ) of .02 as a small effect, .15 a medium effect, and .35 a
large effect. I used the anticipated effect size (f2) of 0.15 for a medium effect size and the
convention of an alpha of 0.80 for the statistical power level to achieve, the number of
predictor variables that are the independent variables of the study (n = 8), and the
standard probability level of 0.05 of statistical significance. The calculation showed the
study required a minimum sample size of N = 108 for statistical power.
Using the total population purposive sampling strategy, I invited all 2012 students
in the target population to engage the most participants possible, aiming at achieving a
20% response rate. The study sample of N = 592 well exceeded the minimum sample size
identified by the multiple regression power analysis calculator.
Participant Eligibility Criteria
The eligibility criteria I used to determine which community college students to
include as study participants were program participation, age, and a passing grade for the
class. Inclusion in the study’s target population was based on the specific eligibility
criteria of a minimum of 18 years of age and successful completion of the college’s IL
requirement course, LIR 10, with a grade of 2.00 or better during the designated semester
time period. The inclusion criteria of the study excluded minors and students who had not
passed the course. Table 1 indicates the number and percentage of students completing
the information literacy requirement course in Summer 2013, Fall 2013, and Spring 2014

51
and the count and percentage of students who passed the course. This number of students
comprised the target population of the study.
Table 1
Student Enrollment Count and Success Count

Spring 2014
Fall 2013
Summer 2013
Total

Enrollment
(N)
1105
1067
609
2781

Success
(N)
791
765
456
2012

% who passed
71.6
71.7
74.9
72.4

Note. Institutional data from the California Community College Chancellor’s Office Data
Mart (CCCCO, 2014c).
I administered the survey 12 to 24 months after student participation in the IL
requirement. This period was short enough to ensure students could respond with recent
memories that connected their learning development to the IL requirement. It was also
long enough to allow ample time for students to have had information evaluation
experiences using IL skills and abilities in subsequent courses.
Recruitment of Participants
The OIR identified individuals meeting the full study’s inclusion criteria from the
college’s student information system. Identification was done using the study’s eligibility
criteria of students who were 18 years of age or older and who successfully completed
the college’s IL requirement course, LIR 10, with a grade of 2.00 or better during the
designated semesters (Summer 2013, Fall 2013, or Spring 2014). I then worked with the
OIR using email to invite all participants in the designated target population.

52
Characteristics of Selected Sample
In the survey, students were asked to provide data about their individual
demographic characteristics including age category, gender, ethnicity, and primary
language. They were also asked about their academic preparation or developmental
characteristics as indicated by the number of college terms attended, English courses
completed, prior information evaluation preparedness self-concept, and number of
college-level research papers they had written. This demographic and academic
preparation data formed the Inputs portion of Astin’s (1985) I-E-O model. Inputs
assessment took into account the differences that students brought to the study.
The survey also asked students to provide data related to the characteristics of the
IL requirement course, LIR 10. The characteristics used in the study were course format,
and the course length. The formats included on-ground or online. The course lengths
included courses of less than 8 weeks in length, which was the 6 weeks length option, and
courses of more than 8 weeks, which included the 9 or 12 weeks length options. This
program characteristics data formed the Environment portion of Astin’s (1985) I-E-O
model. Environment assessment took into account the different aspects of the LIR 10
course that students participated in that could have a potential impact on student success.
Instrumentation and Materials
Due to the distinctive nature of the general education IL requirement as an
education delivery method, I designed the survey instrument used. I performed an
extensive review of the higher education literature that revealed an abundance of
quantitative instruments focused on measuring a student’s IL ability to search and cite

53
sources. Of the more than 30 instruments I reviewed, none was designed to capture the
measurements I sought to assess in the variables. Therefore, I decided to use selected
items from other instruments and scales, as well as self-developed items, to design the
Information Literacy Requirement Impact Survey (see Appendix B). This survey
provided an easy to use method for gathering learner-centered data from the study’s
population regarding the dependent variables identified in the research questions.
Development of the Instrument
Developing a survey instrument consists of numerous steps (Creswell, 2012). I
identified the purpose for gathering data from these students as a first step. To do this, I
selected several issues in the larger and local educational literature. These issues had been
the basis for the selection of the conceptual framework, formulation of the research
questions, and ultimately, for the development of the survey instrument. The foremost
issue was community college student preparedness. The second issue was the variety of
IL educational delivery methods being used by California community colleges. The third
issue was the lack of professional or local assessment literature specifically addressing
the student development impact of a general education IL requirement.
I gathered potential items from other instruments, and from the variables
identified by the literature. I reviewed this list with an informal target group of one ESL
instructor, two instructional librarians, one English instructor, and one director of
institutional research. I used the group’s comments to refine the the construction of the
survey instrument. The objective was to focus on student Inputs and Environment
characteristics, and the Outcomes of critical information evaluation behavior changes,

54
and levels of confidence. The instrument used a cross-sectional design and was online
self-administered using the survey software, Survey Monkey. A closed-ended item design
allowed for gathering of quantitative data from the designated target population at one
point in time. I chose this design in order to achieve a numeric value more quickly for
statistical analysis. Most survey items had only one question. Two had subitems. I kept
the items short and avoided biased terms. All items used consistent response methods,
taking into account students’ likely abilities and willingness to answer the items.
I compiled the selected closed-ended items from other instruments and scales and
wrote the self-developed items as the next step in constructing the instrument. I selected
seven items from the SRJC (2013d) Student Survey instrument to reflect student
characteristics and learning gains. SRJC’s OIR gave permission to use items from the
instrument. The Student Survey items had established reliability and validity based on use
in a longitudinal institutional study since 2001. In addition, that instrument’s
demographic items aligned with the exact wording of the measures used by the CCCCO
(2014c) statewide. For example, the Latino ethnicity was measured using the word
Hispanic. I adapted the English course level item to add two answer choices. I adapted
the learning gains item to make it specific to LIR 10. I wrote three self-developed items
to gather college experiences since taking LIR 10 and to assess student perceptions of
academic preparation. I compiled the behavior change item and responses from two
sources. The item’s wording was from the Astin and Antonio (2012) discussion of how to
gather self-reported behavior changes based on items from Astin’s CIRP longitudinal
study (HERI, 2014). The response choices used the exact wording from ACRL’s (2000)

55
Information Literacy Standard Three (see Appendix I) critical evaluation scale from
selected performance indicator outcomes. This wording alignment added validity to the
survey item because of the reliability of using nationally accepted outcomes. I selfdeveloped two other items. I wrote the first using the exact wording structure suggested
by Astin et al. for assessing students’ level of confidence in an educational program. I
wrote the second based on a specific need in the local setting to determine what students
perceive would be the most satisfactory timing to take the IL requirement.
Concepts Measured by Instrument
Astin and Antonio’s (2012) I-E-O assessment model conceptually framed the
study and defined what concepts needed to be measured by the instrument. The concepts
were student and Environment characteristics, the affective Outcomes of critical
information evaluation behavior changes, and level of information evaluation confidence.
Inputs measures provided demographic and preparedness experience data about
participants who completed the IL requirement. For purposes of this study, the student
characteristic Inputs variables included the following measures: age, gender, ethnicity,
primary language, terms attended, English course level, research preparedness, and
number of papers written since completing LIR10. The Inputs variables were significant
because they identified qualities that students brought with them and could show whether
the IL requirement might be more effective for specific audiences and whether the level
of success in the course was of greater impact than just being exposed to the information
(Astin, 1991). Student characteristics data could be used to provide insight into how the
IL requirement impacted affective behavior and psychological Outcomes for specialized

56
segments of the student population. The literature review identified the importance of IL
education for underprepared and Latino students that influenced the choice to include
Inputs survey items on these student characteristics to ascertain information like whether
a student took any developmental English courses.
Environment measures provided specific institutional program data about students
who successfully completed the general education IL requirement (SRJC, 2013c). For
purposes of this study, the IL requirement specific Environment variables consisted of the
program characteristics, including format/ mode of instruction (on-ground, hybrid, or
online), and length (1, 6, 9, or 12 weeks). Astin (1999) described Environment variables
as those that influence aspects of students’ abilities to involve themselves in the
educational experience. The Environment can be examined to identify any potential
relationships with both Inputs and Outputs variables (Astin & Antonio, 2012). The I-E-O
model showed how different Environment program characteristics could impact the
affective behavior change Outcomes in a study. For example, the course length aspect of
the Environment could have an effect on the affective information evaluation behavior
change Outcomes for students who may have had particular educational needs.
Outcomes measures provided self-reports of behavior change and levels of
confidence of participants who completed the IL requirement. For purposes of this study,
I aligned the affective Outcomes data regarding student behavior change directly with
measures from the college and national library standards. The SRJC (2013b) Institutional
Learning Outcomes critical analysis outcome (see Appendix J) used exact wording
directly from SRJC’s (2013d) Student Survey. This longitudinal survey collected general

57
data indicating that students self-reported high gains for the critical analysis institutional
learning outcome, but did not identify whether those gains specifically related to the IL
requirement. The specific measures that defined IL critical information evaluation
behaviors derived from selected performance indicators and Outcomes from ACRL’s
(2000) Information Literacy Standard Three (see Appendix I). The survey items that
addressed the dependent variables used the exact wording from the performance indicator
outcomes numbered 2 a, 4 a, b, and g, 6 a, and 7 a. These outcomes directly related to
behaviors that community college students needed to critically evaluate information and
its sources in academic settings. The affective Outcomes psychological data regarding
student level of confidence in relation to the IL requirement aligned with Astin’s (1985)
student involvement theory. The study examined students’ self-reported levels of
confidence after completing the IL requirement in terms of writing papers and
participating in discussions in other and subsequent courses. Astin considered these
activities involvement and indicators of a student’s level of confidence and therefore,
affective psychological Outcome indicators of student development success.
Astin and Antonio (2012) designed a taxonomy to classify student Outcomes
measures using type of Outcomes (cognitive or affective), type of data (behavioral or
psychological), and time (short term or long term). They identified affective Outcomes as
a measure of a student’s beliefs, self-concepts, attitudes, etc. such as a self-report of the
amount of time spent doing a task or participating in activities. In this study, I defined the
word Outcomes as affective behavioral and affective-psychological long-term measures.
The survey items related to affective behavioral Outcomes measures collected data about

58
the impact the IL requirement had on students’ subsequent critical evaluation of the
information needed in other courses. The survey item related to affective-psychological
Outcomes measures collected data regarding the impact the IL requirement had on
students’ subsequent level of confidence regarding the ability to critically evaluate
information needed in other courses.
The last item on the survey was students’ recommendations of when it would be
most helpful to take the IL requirement. The college had no set general education
requirement pathway. As a result, many students completed the IL requirement at the end
of their time at the college. California’s Student Success Initiative stressed the
importance of orienting students early to ensure all students have the foundational skills
essential for achievement of a degree, certificate, or transfer (CCCCO, 2011). The
concept of timing had implications for the IL education needs of underprepared students.
Calculation of Scores and Their Meaning
I separated the items of the survey instrument into Inputs of student demographic
and preparation characteristics, Environment of LIR 10 characteristics, and Outcomes of
behavior changes and levels of confidence. I assigned a numerical score to each response
category for each item on the survey instrument using a codebook. For example, Item 9
measured the number of research papers written and used these scales: 1 (0 papers), 2 (1–
2 papers), 3 (3–4 papers), 4 (5–6 papers), 5 (7–8 papers), 6 (9–12 papers), and 7 (13+
papers). Table 2 illustrated the study’s codebook, listing the survey items and indicating
the score (code) that corresponded to the meaning for each response category option as
listed in the survey.

59
Table 2
Information Literacy Requirement Impact Survey Codebook
#
1.

Survey items
Where have you taken classes since you
completed LIR 10?

Numerical score for response category
1 SRJC or another 2-year college, 2 4-year college/university, 3 private
college/university, 4 no college/university

Inputs = student demographic & academic preparation characteristics (independent variables)
2. How old were you when you took LIR 10?
1 19 or younger, 2 20–24, 3 25–29,4 30–34, 5 35–39, 6 40–49, 7 50 or
older
3. What is your gender identification?
1 Female, 2 Male, 3 Other
4. What is your racial/ethnic background?
1 American Indian, 2 Asian, 3 Black, 4 Filipino, 5 Hispanic, 6 Pacific
Islander, 7 White, 8 Other
5. Is English your primary language?
1 Yes, 2 No
6. How many terms had you attended college before
1 0 Terms (just started college), 2 1–2 Terms (1st year of college study),
you took LIR 10?
3 3–4 Terms (2nd year of college study), 4 5–6 Terms (3rd year of
college study), 5 7–8 Terms (4th year of college study), 6 9–12 Terms
(5th year of college study), 7 13+ Terms (6th+ year of college study)
7. Up to and including the semester you took LIR
1 Any College Skills English courses, 2 Any English as a Second
10, had you EVER taken any of the following
Language (ESL) courses, 3 English 302 or 305, 4 English 100, 5
courses?
English 1A, 6 English 5, 7 No English course
8. Before taking LIR 10 how prepared were you to
5 Super prepared, 4 Somewhat prepared, 3 Don’t know, 2 Somewhat
evaluate the information required to write papers
unprepared, 1 Completely unprepared
or participate in discussions in other courses?
9. How many college research papers that required
1 0 papers, 2 1–2 papers, 3 3–4 papers, 4 5–6 papers, 5 7–8 papers, 6 9–
you to evaluate information had you written
12 papers, 7 13+ papers
before you took LIR 10?
Environment = LIR 10 characteristics (independent variables)
10. In what format was your LIR 10 class?
1 On-ground (Face-to-face), 2 Online, 3 Hybrid
11. What length was your LIR 10 class?
1 1 week (Credit by Exam), 2 6 weeks, 3 9 weeks, 4 12 weeks
Outcomes = behavior change & confidence level (dependent variables)
12. To what extent do you think taking LIR 10 contributed to your knowledge, skills, and abilities in the following areas:
a. Locating, analyzing, evaluating, and synthesizing
5 A lot, 4 Some, 3 A little, 2None, 1 Don’t know/ Can’t answer
relevant information
b. Drawing reasonable conclusions in order to make
5 A lot, 4 Some, 3 A little, 2None, 1 Don’t know/ Can’t answer
decisions and solve problems
13. How frequently do you do these actions now compared to how often you did them before you took LIR 10?
a
I now determine whether the information satisfies
5 A lot more frequently, 4 Somewhat more frequently, 3 No Change, 2
my research need.
Somewhat Less frequently, 1 A lot less frequently
b
I now review my search strategy and incorporate
5 A lot more frequently, 4 Somewhat more frequently, 3 No Change, 2
additional concepts as necessary.
Somewhat Less frequently, 1 A lot less frequently
c
I now determine whether the information
5 A lot more frequently, 4 Somewhat more frequently, 3 No Change, 2
contradicts or verifies information used from
Somewhat Less frequently, 1 A lot less frequently
other sources.
d
I now compare information from various sources
5 A lot more frequently, 4 Somewhat more frequently, 3 No Change, 2
in order to evaluate reliability, validity, accuracy,
Somewhat Less frequently, 1 A lot less frequently
authority, timeliness, and point of view or bias.
e
I now select information that provides evidence
5 A lot more frequently, 4 Somewhat more frequently, 3 No Change, 2
for the topic.
Somewhat Less frequently, 1 A lot less frequently
f
I now participate in classroom and other
5 A lot more frequently, 4 Somewhat more frequently, 3 No Change, 2
discussions.
Somewhat Less frequently, 1 A lot less frequently
14. After taking LIR 10 what is your level of
5 Super confident, 4 Somewhat confident, 3 Neutral, 2 Somewhat
confidence in writing papers or participating in
unconfident, 1 Completely unconfident
discussions in other courses based on the
information evaluation skills you learned?
Students’ recommendation of timing
15. Which terms do you recommend as the most
helpful to take LIR 10?

1 1–2 Terms (1st year of college study), 2 3–4 Terms (2nd year of
college study), 3 5–6 Terms (3rd year of college study), 4 7–8 Terms
(4th year of college study), 5 9–12 Terms (5th year of college study), 6
13+ Terms (6th+ year of college study), 7 Term taken does not matter 2

60
Assessment of Reliability and Validity
I gathered formal feedback of the survey instrument by testing for internal
consistency, reliability, and validity as the final step of the development process. I did
this by conducting a peer expert review and a pilot test.
Peer expert review. I began by using a panel of peer experts to establish the
instrument’s content validity. I invited four librarians from SRJC to do peer expert
reviews of the survey instrument for usability and to determine if I needed to make any
modifications. To qualify as peer experts, reviewers had to have a master’s degree in the
field of Library and Information Science and at least 3-years’ experience teaching LIR 10
at the college. I structured the review process using a form (see Appendix C) to garner
reviews that were consistent across the group.
Reviewers first reported how long it took to complete the survey. This measure
ensured that the estimate of the time needed for survey completion was accurate. The
times reported were 6, 7, 9, and 10 minutes, all within the time investment range for
completing the survey I had listed. Next, the reviewers examined the survey’s
instructions and informed consent. All gave positive feedback. The last step in the peer
review process included their evaluation of each of the survey items.
The peer review form had places for reviewer’s notes to indicate if the items were
appropriate, easy to understand, complete, and if I should use them in the survey. I
established the criterion that 75% of the panel had to indicate that I should use an item for
it to remain in the survey. Only one reviewer noted that an item should be deleted
because it repeated what another item measured. In addition, the peer review form asked

61
about alignment of the affective behavioral Outcomes items on the survey with selected
ACRL’s (2000) Information Literacy Standard Three performance indicator outcomes
(see Appendix I), as well as SRJC’s (2013b) Critical Analysis Institutional Learning
Outcomes (see Appendix J). All reviewers reported that the language in the items
matched the exact wording of selected elements in the outcomes being measured. These
reviews indicated that the content validity of those items was high and that they should
measure what they were intended to measure. The reviewers made assorted comments
recommending adjustments in wording, questioning structure, or noting questions as to
why I included an item in regards to the length of the survey. Based on the comments, I
deleted seven items that resulted in a more concise survey of only 15 items. I also
rephrased two items for clarity and to increase ease of understanding. The instrument was
then ready for pilot testing.
Pilot test. After I obtained approval from the Walden Institutional Review Board
(IRB), I administered the pilot survey. The Walden approval number was 04-23-150319952. For the pilot study, I randomly selected one LIR 10 course in an attempt to
capture a cross section of participants similar to the target population invited to
participate in the formal study. I did this by putting each of the 43 section numbers
offered in Spring 2015, excluding 2 sections I taught, into a hat and drawing one out. The
instructor of the randomly selected LIR 10 section identified the individuals meeting the
pilot study’s eligibility criteria of students who were 18 years of age or older, and who
had successfully completed the college’s IL requirement course, LIR 10, with a grade of
2.00 or better during the designated semester (Spring 2015). I provided the LIR 10

62
instructor with the pilot study email text inviting students to complete the pilot version of
the Information Literacy Requirement Impact Survey. The instructor then sent the pilot
invitation email to the 24 eligible students. Participants were expected to complete the
anonymous survey at their convenience within a 48-hour period.
To help identify potential problems with administration and implementation, I
used the same online survey tool, Survey Monkey, for the pilot test as for the full study.
Students took the survey and at the same time answered pilot specific items (see
Appendix D) about the full survey’s informed consent notice and items as a way to assess
the instrument’s accuracy in measuring what it intended to measure (Lodico et al., 2010).
The first pilot specific item asked for a Yes/No response to determine if the survey item
was easy to understand. The second asked for: if you answered no, please explain. The
third asked for a Yes/No response to determine if the item should be used in the survey.
Given the voluntary nature of the pilot survey, only six of the 24 students invited
chose to participate, representing a 25% response rate. This low response rate could have
been due to the short timeframe for students to return the pilot survey. Student response
rate may have benefited from a longer deadline and a reminder email. I saved the pilot
survey responses into a secure digital file in Statistical Package for the Social Sciences
(SPSS) and Microsoft Excel formats. The number of responses to the pilot test was too
low to allow for analysis comparing the pilot test response sample group to the full study
target population to examine for representativeness. The group included a mix of age
categories ranging from 19 to 49 with no two from the same category. The group

63
included two females and four males. The group included four white, one American
Indian, and one Hispanic ethnicity.
Analysis of the results showed that all of the six respondents answered each full
survey item as well as all of the additional pilot specific items. Two students had attended
college for 1–2 terms, three for 3–4 terms and one for 13+ terms. Prior to taking LIR 10,
one student had written no papers, another 1–2 papers, three wrote 3–4 papers, and one
wrote 5–6 papers. Responses on the behavior change and level of confidence Outcomes
items varied, but most indicated positive responses. Responses to the pilot specific survey
items showed all six students answering Yes, that the survey items were easy to
understand and Yes, that they recommended use in the full survey. Based on pilot test
results and the positive feedback from the respondents about their experiences, I decided
that the full survey was ready to administer.
Processes Needed to Complete Instrument by Participants
To administer the survey, the college’s OIR sent the invitation email (see
Appendix F) inviting the eligible students in the study’s entire target population to
participate in the online survey via the Survey Monkey software. Students clicked the
link and self-administered the short survey (see Appendix B) by following simple
instructions. These instructions contained the informed consent notice, including the
purpose, risks, benefits, voluntary and confidential nature of the survey, participant’s
ability to decline to participate without penalty, contact information for the researcher
and Walden University representative, and a statement about the student’s ability to print

64
or save a copy. If students agreed to continue, the survey asked them to answer 15 closedended items taking 3–10 minutes to complete.
Where Raw Data Will be Available
The raw data for this study included the survey response data and aggregated
institutional program characteristic data. I stored all of the raw data in secure electronic
files accessible only to me. The survey response raw data I collected using Survey
Monkey was stored in the SPSS file format and in a Microsoft Excel spreadsheet. The
aggregated institutional program characteristic data provided by the college OIR was
stored in a Microsoft Excel spreadsheet.
Data Collection and Analysis
I conducted the survey and compiled the response data. Using institutional data, I
compiled the student demographic characteristics of the invited target population
(CCCCO, 2014c). In addition, the college’s OIR department provided aggregated
institutional program data. The research questions and hypotheses influenced the nature
of the scale I used for each variable. The types of variables, in turn, determined the data
analysis methods and statistics methods I planned for the study.
Data Collection Required to Address Research Questions
The data collection process required to address the research questions involved
conducting the survey in cooperation with the college as a community partner. A high
response rate was critical for confidence in survey study results, so I focused all steps on
that goal (Creswell, 2012). The college’s OIR provided a letter of cooperation that
outlined the OIR’s authorized activities and responsibilities. Prior to data collection, I

65
provided the OIR with eligibility criteria designating the target population and the text for
the prenotification of the survey (see Appendix E) email they would send. The OIR
obtained the list of 2012 student email addresses for the eligible target population
(CCCCO, 2014c). The list was kept confidential and stored in a secure file. Using that
list, the OIR sent prenotification emails to the target population 1 week prior to the
study’s start date to inform them about the upcoming study.
Data Collection Processes
The data collection steps included the researcher administering the survey and the
college gathering and disseminating aggregated institutional data activities. Using the
online tool, Survey Monkey, I entered the text of the informed consent and survey items
and generated a survey link. I provided the OIR with the text for the invitation to
participate in the survey email (see Appendix F). The email explained the survey’s
purpose, procedures, and protections provided and provided a link to the online survey
instrument allowing participants to complete it at their convenience within a designated
2-week period. The OIR sent an invitation email to each student from the designated
target population list on the start date. I administered the survey including monitoring the
anonymous online responses using the secure Survey Monkey program. The OIR sent a
follow-up reminder email 2-days prior to the survey deadline to encourage a higher level
of response using text I provided (see Appendix G). Because the response rate was below
the 20% goal, the reminder email included an extension of 1 week to the survey deadline.
I saved all the anonymous survey response results data in files in the Microsoft Excel and
the SPSS formats. In addition, the OIR collected the Limited Data Set (LDS) containing

66
institutional program data for the number, format, and length of LIR 10 courses offered
during the designated study period, as agreed. All of these data were required in various
combinations to address the research questions.
Nature of the Scale for Each Variable
The survey instrument organized items into three groupings, including eight
student Inputs characteristics (independent variables), two Environment characteristics
(independent variables), and three Outcomes (dependent variables) consisting of two
behavior changes, and one level of confidence. I assigned numerical scores for the
responses to these items as outlined in Table 2. I analyzed these numerical scores to
determine the types of measurement scales that were needed.
The survey items all had categorical type responses that required nominal and
ordinal measurement scales. Nominal scale items included the participants’ gender (Item
3), ethnicity (Item 4), primary language (Item 5), English course level (Item 7), and LIR
10 format or mode of instruction (Item10). Ordinal scale items included age category
(Item 2), number of terms of college attended (Item 6), prior preparation self-concept
(Item 8), number of papers written (Item 9), and LIR 10 length (Item 11). The survey
items measuring Outcomes used 5-point Likert-type categorical responses that were
ordinal measurement scales measuring two different gains in student information
evaluation learning (Item 12), changes in the frequency for six information evaluation
behaviors (Item 13), and levels of confidence with the learning experiences (Item 14). In
addition, the survey had two items not specific to the research questions. A nominal scale
item asked where participants had taken classes since completing LIR 10 (Item 1) to

67
identify any respondents who had not attended college so I could remove them in data
clean-up. An ordinal scale item asked respondents to recommend the most helpful term
for taking LIR 10 (Item 15) to gather student’s opinion on optimal timing.
For the individual response type items, the numerical score for each response
category increased as the value for the category increased. For example, the numerical
score was higher for students in the older age category, those that had attended more
terms, and those who had achieved higher levels of English courses. For the Likert scale
type items, the numerical score for each response category increased as the value for the
category become more positive. For example, the numerical score was higher for the
category A lot more frequently than it was for the category A lot less frequently.
The survey contained two items with subitems that were measured using the
Likert-type ordinal scale. Item 12 contained two subitems, and Item 13 contained six
subitems. In the data analysis phase, I computed the sum of the numerical score of the
responses for each item’s subitem to form a composite. The scale of the composites
formed in the computation became a continuous scale. The composite scale for Item 12,
measuring gains in student information evaluation learning, ranged from 2 to 10. The
composite scale for Item 13, measuring changes in the frequency of 6 information
evaluation behaviors, ranged from 6 to 30. As the numerical score on the composite scale
increased, the value for the response category became more positive indicating higher
frequencies of information evaluation learning gains and actions.
The literature indicated some controversy about how to assign a measurement
scale for Likert-type scale response data. Jamieson (2004) believed that Likert-type scale

68
data responses were ordinal categories and the intervals between them were not equal.
Therefore, only nonparametric statistical tests should be used. Other researchers provided
strong arguments for using parametric statistical tests, including factor analysis and
correlation for numerically scored, Likert-type scale responses (Brown, 2011; Carifio &
Perla, 2007). They noted the importance of using a 5-point scale or above with response
categories that were conceptually continuous, intervals between responses approximately
equal, and approximating a normal distribution. Given that I could locate no definitive
answer, I considered the implications of these conflicting ideas for the study and decided
that the Likert-type response categories in the ordinal survey items were equal distances
apart. This decision allowed analysis of these data points using parametric correlation and
regression tests. As a precaution, I could run equivalent nonparametric tests prior to
making any data analysis conclusions, to determine if the tests had similar results or if I
needed to run tests using a more conservative alpha level.
Data Analysis Plan
I planned to analyze the raw survey response data and aggregated institutional
data using the most appropriate methods for addressing the research questions and
rejecting or failing to reject the null hypotheses. Analysis of the data provides a method
to examine the relationships among affective IL information evaluation behavior changes
and confidence levels (Outcomes), aspects of the IL requirement (Environment), and
differing student characteristics (Inputs) as variables (Astin & Antonio, 2012). These data
interrelationships were the foundation for the assessment of the student development

69
impact of the college’s IL requirement program. Therefore, they addressed the study’s
overarching question.
The categorical variables that result from the survey items can have nominal or
ordinal measurement scales depending on what the items measured. I can compute
composite variables from ordinal items that contain subitems resulting in variables with
interval measurement scales. The different variables can contain a mix of measurement
scales dictating the use of parametric and nonparametric types of statistical tests to
examine relationships among the Inputs-Environment-Outcomes datasets.
The data analysis plan included response rate analysis, descriptive analysis,
representative analysis, cross-tabulation analysis, correlation analysis, and multiple
regression analysis. Response rate analysis can show the response level received from the
designated target population that had been invited. Descriptive analysis can identify the
survey participants and program characteristics. Univariate descriptive analysis
techniques describe the response sample using frequencies, percentages, measures of
central tendency, range, and standard deviation. Representativeness analysis can use the
chi-square goodness of fit test to show if the response sample had a similar distribution to
the target population. Bivariate analysis techniques, including cross-tabulation, chisquare for association, and correlation can examine relationships between pairs of
categorical nominal and ordinal scale variables. Cross-tabulation analysis can show
minor relationships. Correlation analysis, such as the Pearson product-moment, can show
how strongly pairs of variables are related and in what direction. Correlations analysis
will not differentiate between the independent and dependent variables and does not

70
indicate that a change in one variable causes a change in another (Creswell, 2012).
Multiple regression analysis can help explain the degree and character of the relationship
between a dependent variable (criterion) and a set of independent variables (predictor).
Regression analysis can help control for potential bias due to differences in student Inputs
(Astin & Antonio, 2012). It can provide a way to study “naturally occurring variations in
Environmental conditions and to approximate the methodological benefits of true
experiments by means of complex multivariate statistical analyses” (Astin & Antonio,
2012, p. 29). Multiple regression allows for examination of the relationships among a
single dependent variable (Outcomes) and two or more independent variables (Inputs and
Environment).
Assumptions, Limitations, Scope, and Delimitations
I made several assumptions in this study about the college’s general education IL
requirement and survey research methodology. The limitations identified mostly related
to the survey methodology. The scope of the study was narrow in range and posed some
delimitations stemming from Walden University’s project study requirements.
Assumptions
Assumptions about survey research methodology described the facts assumed to
be true but not yet verified. I identified several assumptions about the college’s general
education IL requirement and survey methodology that were relevant to this study.
The college’s general education IL requirement assumptions.
1. The college’s general education IL requirement could be considered a highimpact practice. It increased students’ research performance and oriented them

71
with critical information evaluation confidence. This practice created involved
students more likely to engage in academic courses and able to successfully
achieve their educational goals (Astin, 1999; CCCCO, 2013; Kuh, 2008).
2. Astin’s (1999) belief was that it was “what the individual does, how he or she
behaves, that defines and identifies involvement” (p. 519).
3. The SRJC (2013b) Critical Analysis Institutional Learning Outcomes (see
Appendix J) were aligned with the ACRL (2000) Information Literacy
Standard Three performance indicator outcomes (see Appendix I) and LIR 10
course level outcomes (see Appendix K). This alignment indicated IL
requirement participation was the origin of critical analysis gains students
reported in the college’s longitudinal Student Surveys (SRJC, 2010; 2013d).
4. By successfully completing the IL requirement course, LIR 10, with a grade
of 2.00 or better, students demonstrated they had introductory critical
information evaluation knowledge and skills (SRJC, 2013c).
Survey methodology assumptions.
1. All members of the designated target population had access to email and the
Internet at the time of the study.
2. Participants were motivated to provide survey response data to assess the
student development impact of the college’s general education IL
requirement.
3. The participants in the online survey had successfully completed the IL
requirement.

72
4. The survey instrument, methodology, and analysis were valid and adequate
for measuring students’ self-reports of critical information evaluation behavior
changes and success.
5. Participants understood survey items and responded accurately and honestly
to the best of their ability.
6. The self-selected survey respondents were representative of the designated
target population.
Potential Limitations
The study of the college’s general education IL requirement posed several
limitations. Foremost was the uncertainty of attributing critical evaluation student
development to participation in the IL requirement. Indicators of IL information
evaluation behavior changes such as increased research confidence or increased
involvement in course discussions were a challenge to measure definitively. These
student success changes could be attributed to extraneous variables from other courses in
the general education curriculum having research related course learning outcomes (see
Appendix K). Also, students may have passed the IL requirement course, LIR 10, but
may not have fully integrated the range of IL information evaluation behaviors into their
practices. This requires a skills based assessment such as a pretest and posttest applied in
the IL requirement course, LIR 10.
The study’s use of a survey research design also posed limitations specific to that
methodology. Participants provided self-reported data that could not be substantiated.
Students taking a self-administered survey could have responded dishonestly or may not

73
have fully understood the closed-ended items. The survey asked students to identify some
individual characteristics. Because the surveys were anonymous, I was unable to verify
the accuracy of the self-reported student characteristic data using institutional records. A
limitation of the Information Literacy Requirement Impact Survey (see Appendix B) was
it measured how participants reported they behaved rather than how they actually
behaved. (Astin & Antonio, 2012). However, given that SRJC (2013e) values itself as a
learner-centered institution, an assessment of student self-reported behavior changes was
highly suitable and provided useful insight into the relationships I was studying.
The literature on the use of self-reported data was mixed. Kuh’s (2001) work with
the National Survey of Student Engagement (NSSE) survey led to national benchmarks
that were widely used in education to assess institutional effectiveness. The use of selfreported survey data reinforced by the literature showing the credibility of the method
and put forth the belief that many student engagement and learning outcomes cannot be
measured by skills-based tests. Pace’s (1985) seminal report on the credibility of selfreports introduced some common measurement errors, such as nonresponse bias. The
study also shared ways to improve the credibility of the methodology using question
scales and test-retest comparisons. Kuh and Pike (2011) advocated how the construct
validity of self-reported data can be supported if five conditions are met. The conditions
included that respondents know the information, the questions are clear, the measure is
conducted within a timely period, respondents consider the questions worthy of serious
answers, and the questions do not threaten privacy.

74
Other researchers including Bowman (2010) and Porter (2011) questioned the
evidence supporting self-reports as credible. Bowman’s study showed that cross-sectional
self-reported learning gains do not always align with longitudinal measurements of those
gains and that in some cases students did show the ability to estimate their own gains.
Porter also advocated for more objective measures, based on this review of the literature,
by arguing that the definition of validity has changed as survey methodology gained in
popularity. Astin and Antonio (2012) extended that the credibility of self-reported data
can be increased if multiple regression analysis is used as a way to control for differing
student Inputs and Environment independent variables when trying to explain the
variation in dependent Outcomes variables. Gonyea (2005) identified the usefulness of
self-reports but cautioned that they must be substantiated with objective measures. This
caution aligned with others that survey measurements can be trusted if care is given to the
design of the instrument (Kuh, 2001; Pace, 1985; Pike, 2011).
A constraint of survey methodology was the bias that could occur from failing to
obtain responses from all members of the sample. Self-selected survey respondents could
introduce bias because they may not have been entirely representative of the designated
target population. Bias can occur when the questionnaire response rate is not randomly
distributed, making the findings relevant only to the study (Creswell, 2012). This selfselection bias could result in a disproportionate number of members with specific
characteristics made generalizing the results to the overall target population questionable.
A limitation of survey research studies is that the results are not generalizable.
Survey research does not seek to generalize results to a larger population due to

75
restrictions such as time constraints and cost. Because of these restrictions, this study
cannot generalize results to the target population or to the larger California community
college setting. Despite this limitation, this project will elicit discussion among
administrative and faculty stakeholders
The cross-sectional aspect of the study limited it to the 12–24-month time frame
of enrollment in the IL requirement. This limited time frame could have potentially
affected the representation of the population in relation to the total population of students
enrolled at the college. Although I expected that the validity of the study would be
sufficient to provide confidence that the results were definitive, I did not have confidence
that I could generalize findings to other colleges in California and beyond. I could only
cautiously generalize findings from the participants in this study response sample to the
target group of individuals taking LIR 10 in the study’s designated semesters. I did not
have confidence generalizing the findings to the larger group of individuals taking LIR
10 in other semesters or to other higher education institutions with similar demographics
and situations. However, because the survey method was successful in obtaining
responses from an acceptable sample of students, the instrument could potentially be
reapplied in subsequent years to compare how outcomes change and how changes could
be applied to the LIR 10 course to help meet ongoing needs.
Similarly, generalizing survey results can be affected by the possible challenge of
a low response rate that could result in response bias. In general, email surveys have a
greater challenge with lower response rates than other methods (Lodico et al., 2010). All
members of the designated target population may not have had access to email at the time

76
of the study or their email addresses may not have been valid. I used several strategies to
encourage higher return rates including prenotification (see Appendix E), an anonymous
survey, a brief instrument, and a follow-up email (see Appendix G). However, I had no
guarantees that the response rate would be high or that those who responded were typical
(Creswell, 2012). Members of the target population might have had numerous reasons for
participating or not participating in an online survey, which could introduce bias from
respondents having particularly strong feelings or opinions one way or the other about the
research. A high response rate could lessen the potential for response bias, but even then
a survey study would not provide evidence of the causal relationships needed for making
generalizations.
A potential ethical limitation was my employment status at the institution where I
conducted the study. The instructional role I had during Summer 2013, Fall 2013, and
Spring 2014 semesters meant I could have had interaction with students in the population
through their enrollment in the classes I taught. At the time of the study, I had no
interaction with the students who participated nor was I involved in the identification of
students meeting eligibility criteria for the study. A possibility existed that I could present
a bias in writing up results in a manner that was more favorable to the college. I
circumvented that possibility by the use of an entirely quantitative study.
Variables (Scope) and Boundaries of the Study (Delimitations)
The scope of this study was a community college general education IL
requirement delivered via a 1-unit course, LIR 10, and its level of success in meeting
institutional objectives during the course of a single semester. I limited the study’s scope

77
narrowly to students in a single college based on Walden University’s project study
expectations. It was a study of the impact of a single course meeting a single general
education requirement in a single community college. A successful quantitative study
allows for the generalization of findings to the identified population. The limited scope of
this study affected the degree to which respondents represented all community college
students. Although the validity of the study was sufficient to provide confidence that the
results were definitive, local conditions will have to be considered before generalizing to
other colleges in California and beyond.
Delimiters included using an online survey composed of closed-ended items. This
limited scope in that it provided only a set number of response choices and measured
only students’ self-reports. The mixed academic level of the community college student
population was also a delimitation. The varied student information exposure level could
have potentially affected information evaluation abilities. An additional delimitation
included the fact that the study only included students who had taken a particular course
during a specific time frame and who were at least 18 years old at the time of the survey.
The scope of this study was a community college general education IL
requirement delivered via a 1-unit course, LIR 10, and its level of success in meeting
institutional objectives during a single semester. I limited the study’s scope narrowly to
students in a single college based on Walden University’s project study expectations. It
was a study of the impact of a single course meeting a single general education
requirement in a single community college. A successful quantitative study allows for the
generalization of findings to the identified population. The limited scope of this study

78
affected the degree to which respondents represented all community college students.
Although the validity of the study was sufficient to provide confidence that the results
were definitive, local conditions will have to be considered before generalizing to other
colleges in California and beyond.
Protection of Participants’ Rights
The study contained multiple layers of participant protection. I based these
protections on the ethical principles learned in the National Institutes of Health’s
Protecting Human Research Participants training (See Appendix H). The pilot and full
survey data collection was anonymous and the research procedures did not specifically
seek to include or exclude any members of a vulnerable group as membership in a
particular group was not relevant to the study. The study’s research procedures were
designed to protect adequately all individuals including those that might have been part
of any of the possibly vulnerable groups listed. No risks were identified, and data
collection contained several steps to protect participants’ and stakeholders’ welfare.
SRJC, the community partner, granted permission and provided a letter of cooperation.
The research design and anonymous survey instrument ensured data confidentiality. The
informed consent form described the study’s risks, benefits, contacts, and protections in
easy to understand language. No known risk of harm existed and participants could
withdraw at any time. The study was reviewed and approved by the Walden University
IRB. The Walden approval number is 04-23-15-0319952, and that approval expires on
April 22, 2016.

79
Confidentiality
The college’s OIR conducted participant recruitment, data collection, and
dissemination activities as outlined in a letter of cooperation. The OIR Department
removed all names and identifiers from the participant data. This data is being used,
secured, and protected in accordance with institutional regulations and Title 20 of the
United States Code of Federal Regulations § 1232g - Family Educational Rights and
Privacy Act (FERPA; Legal Information Institute, 2015). I could not identify any
applicable state laws that might be relevant. I followed all data collection procedures in
accordance with best practices from the CCCCO (2013).
The college’s OIR recruitment activities included generating and securely storing
a confidential list of the target population. The OIR used this list of the population only to
send survey emails at the designated times. I conducted data collection using Survey
Monkey to collect anonymous student responses. At the end of the study I compiled the
confidential Limited Data Set (LDS) comprised of anonymous survey response results
data in the SPSS format and aggregated anonymous institutional data about student and
Environment characteristics. Nothing in the file linked participant names to survey
responses. When I completed the study, the OIR destroyed the list of the target
population. All digital copies of the LDSs will be stored in secured files until no longer
needed and then destroyed. The anonymous survey design ensured that I do not know the
names or any identifiers of study participants. Thus, all published results of the study are,
and will remain, confidential.

80
Informed Consent
An informed consent notice appeared on the opening screen of the online survey
and in all recruitment emails. To begin the survey participants had to indicate that they
chose to continue, that they understood they were taking part in a research study, and
what was required of them.
The consent notice outlined the criteria for including participants and also the
survey’s purpose. The consent notice told students that participation was voluntary, and
that participants would remain anonymous. The consent notice notified potential
participants that the study had no known risks and offered them no direct benefits (except
for the opportunity to develop awareness of learning development). The notice also told
potential participants that the data collected would be handled as outlined above. I
included researcher contact information and also contact information for the Walden
University representative in case questions arose about the study or procedures. Potential
participants were able to decline to participate or quit at any time without penalty (Lodico
et al., 2010). Finally, the notice asked participants to acknowledge that they met
eligibility requirements, agreed to participate by entering the survey, and reminded
participants that they might print or save a copy for their records.
Protection from Harm
The informed consent notice communicated that no known risks or direct benefits
for participation were identified, beyond the chance to reflect on learning development.
The anonymous voluntary opt-in or out nature of the survey and secure data processing
also provided participants with privacy protections. The survey procedures included a

81
briefing on what the items would ask to minimize any potential psychological distress or
discomfort. The study allowed only adults (18-years old or above) to participate and did
not include any experimental treatments. In addition, the conditions outlined in the
college’s Letter of Cooperation ensured that participants received institutional and Family
Educational Rights and Privacy Act (FERPA) protections (Legal Information Institute,
2015).
Data Analysis Results
The following sections provide the results of the data analysis. Data analysis of
the study’s results consists of several steps aligned with the problem, conceptual and
theoretical framework, and research questions and hypotheses. Following the data
analysis plan the steps include cleaning the data set, determining the response rate of the
sample, conducting descriptive analysis, comparing the representativeness of the sample
to the target population, cross-tabulation analysis, chi-square for association analysis,
correlational analysis, and multiple regression analysis.
Data Clean-up
The full data set consisted of the N = 592 responses I received from the survey
data collection. I prepared the data for analysis using listwise deletion to omit n = 67
cases based on three criteria. The first criterion was to identify and remove all cases that
had a nonresponse for any item on the survey (n = 28). The second criterion was that
respondents needed to have used information evaluation skills in subsequent classes. I
removed all cases with a college attended (Item 1) response that indicated they had
attended no college/university since taking LIR 10 (n = 17). The third criterion was that

82
respondents needed to have taken an LIR 10 class. I removed all cases with a LIR 10
length (Item 11) response that reflected participation in a 1- week length class (n = 22),
which was a credit-by-exam test out option. I performed the detection using SPSS
Missing Values Analysis and a manual examination of all variables to determine which
cases met any of those three criteria. I made the assumption that the data had been
collected correctly and chose not to remove any outliers. The final cleaned data set
contained responses from N = 525 students who had completed the entire survey, had
taken an LIR 10 course, and who indicated they had attended additional college courses
after LIR 10. I used this data set for all data analysis.
I next recoded some variables to simplify the response options categories for
selected items by combining them into fewer categories. For example, I cleaned up the
multiple responses type variables by recoding these mark all that apply items into one
variable. These included three variables, college attended (Item 1), ethnicity (Item 4), and
English course level (Item 7). I recoded the variable, college attended (Item 1), into two
categories, SRJC or another 2-year College (n = 447) or 4-year College/Private (n = 78)
because of the small number of responses for the 4-year College/Private category when
they were separate. I prioritized the multiple mark all that apply categories for ethnicity
(Item 4) using the Student Characteristics Derived Data Elements Dictionary from the
Management Information System of the CCCCO (2008). All of the ethnicities except
white (n = 321) and Hispanic (n = 136) had very low responses numbers (American
Indian [n = 9], Asian [n = 26], African American [n = 14], Filipino [n = 7], Pacific
Islander [n = 3], Other [n = 9]). I recoded the ethnicity variable into two categories, Non-

83
Hispanic and Hispanic. I based this decision on the low number of respondents for the
various ethnicities other than white and Hispanic and the need to consider the
underprepared aspect of Hispanic learners in the context of the local problem. I
prioritized the multiple mark all that apply categories for English course level (Item 7)
using the designations from the CCCCO (2014a) Student Success Scorecard. The
Scorecard designated students as college prepared by if their lowest English course level
was at the transfer level and unprepared (remedial) if their lowest English course level
was at the below transfer level. To achieve consistency with the CCCCO designations, I
recoded the 18 English course multiple response combinations from the survey into four
categories based on the CCCCO designation for student’s college preparation level.
These categories included no English courses, below transfer, initially below transfer,
and transfer. I justified the focus on English course transfer level based on identification
of underprepared students as an issue within the context of the local and larger problem.
I recoded the two LIR 10 Environment variables. The LIR 10 format (Item 10)
had a very low response number for hybrid (n = 6). I combined the on-ground (n = 170)
and the hybrid (n = 6) responses into on-ground (n = 176). I justified combining the
responses based on the CCCCO (2008) distinction between on-ground and online
formats. The CCCCO designates that courses that are 51% or more taught at a distance
should be reported as online and below 51% as on-ground. The CCCCO does not list a
designation for a hybrid format. As the LIR 10 format, hybrid course had been taught at
below 51% online. I determined I could combine it with the on-ground format. The LIR
10 length (Item 11) variable contained three response categories, 6 weeks, 9 weeks, or 12

84
weeks. The college designated term length as courses that were more than 8 weeks (long)
and ones that were less than 8 weeks (short) courses (SRJC, 2015). I combined the LIR
10 Length 9 weeks (n = 142) and 12 weeks (n = 107) categories into one called more than
8 weeks (n = 249) to align with the college’s course length distinction.
I also computed the variables for the two items that needed combining into a
composite measure made up of the designated subitems. SRJC Critical Analysis (Item
12) contained two subitems (Items 12a and 12b), and SRJC Critical Analysis (Item 13)
contained six subitems (Items 13a through 13f). I added the numerical scores of the
subitem responses for each of the composites.
Response Rate
A limitation of anonymous survey research is the introduction of potential
nonresponse or self-selection bias given that participants decide for themselves whether
or not to respond. In deciding who should receive the survey, I could not assume a 100%
response rate, given the relatively low response rates generated from online surveys.
Laguilles, Williams, and Saunders (2011) noted it was unlikely an online survey will
receive a 100% response rate, and Creswell (2012) indicated the average response rate to
be somewhere between 10% and 20%. A higher response rate can increase confidence in
data generated by the survey (Creswell, 2012). Low response rates can also increase the
likelihood that the results will have some bias because respondents may not be
representative of the entire target population. Testing for representativeness of the sample
can detect the potential of the sample having this type of bias.

85
I calculated the response rate of the survey as the percentage of responses of the
target population. In May 2015, I sent the survey to 2012 students with a goal of
obtaining a 20% minimum response rate (n = 404) returns. At the end of 2 weeks I had
received only an 18% response rate (n = 371) so I sent a reminder email that extended the
deadline for an additional week. By the end of the 3-week deadline, I had received a 29%
response rate (N = 592). When I cleaned the data, I subtracted n = 67 responses that were
incomplete or did not meet the selection criteria. The response rate for resulting usable
surveys was 26% with a sample size of N = 525.
Inputs-Environment-Outcomes Assessment Framework
Astin and Antonio’s (2012) I-E-O model conceptually framed the study’s
assessment of the impact of the college’s IL requirement on student development and
defined what data would be needed. Therefore, I collected three distinct types of data
aligned with the I-E-O theoretical constructs, as shown in Figure 2, using the survey
instrument and aggregated institutional data. Survey Items 2–9 best described student
characteristics Inputs (IV). Survey Items 10–11 best described the LIR 10 program
characteristics Environment (IV), survey Items 12, 13, and, 14 best described the
affective behavior change and confidence level Outcomes (DV). I used these data in the
form of variables to specify the identified characteristics of students and the information
evaluation behavior changes that were called for to address the broadly written
hypotheses and answer the broadly written research questions.

86

Figure 2. Inputs-Environment-Outcomes model with study’s data points.
This I-E-O model visually showed how the Inputs (independent variables) can
affect both the Environment (independent variable) and the affective behavioral and
psychological Outcomes (dependent variables). For example, Figure 2 show how a
student’s Inputs characteristic of English course level (A) could affect the decision to
select the on-ground or the online format Environment (B). It could also influence the
frequency of information evaluation behavior change Outcomes (C) she or he achieves.
Equally, the student characteristic of ethnicity (A), for example, could have a direct effect
on the level of affective behavior change Outcomes (C) regardless of if the IL

87
requirement course she or he took was on-ground, hybrid, or online format Environment
(B). This model served as the basis for the cross-tabulation and correlation pairings, as
well as the regression analysis reported later in Section 2.
Questions and Hypotheses
The college’s gap in targeted assessment of its general education IL requirement
led to the study’s overarching question, which asked if a 1-unit general education
requirement was an effective IL education delivery method for students. I addressed the
overarching question using two researchable questions that focused on assessing
significant relationships among the student characteristics identified in the problem
(Inputs), participation in the IL requirement program (Environment), and changes in
information evaluation behavior or levels of confidence (Outcomes). I tested each of the
measurable research questions with null and alternative hypotheses.
The two research questions (RQ1 and RQ2) are similar in that they both look for
relationships among identified student Inputs characteristics and program Environment
characteristics (the IL requirement course format and length), but they differ in the
Outcomes they measure. RQ1 measures the Outcomes of information evaluation behavior
change and RQ2 measures the Outcome of the level of confidence. The research
questions are aligned directly with the I-E-O framework assessment model, and so they
become the lens through which I analyze the results. I used the data set of N = 525
responses from students who completed the survey to address the study’s research
questions.

88
Research Question 1
RQ1: What is the relationship between completion of the general education IL
requirement course with different formats and lengths and frequency of information
evaluation behavior changes among students with identified characteristics?
H01: There is no statistically significant relationship between identified
characteristics of students who completed the general education IL requirement course
with different formats and lengths in terms of information evaluation behavior changes.
Ha1: There is a statistically significant relationship between identified
characteristics of students who completed the general education IL requirement course
with different formats and lengths in terms of information evaluation behavior changes.
Research Question 2
RQ2: What is the relationship between completion of the general education IL
requirement course with different formats and lengths and how skills learned contributed
to information evaluation confidence in other courses among students with identified
characteristics?
H02: There is no statistically significant relationship between identified
characteristics of students who completed the general education IL requirement course
with different formats and lengths in terms of how skills learned contributed to
information evaluation confidence in other courses.
Ha2: There is a statistically significant relationship between identified
characteristics of SRJC students who completed the general education IL requirement

89
course with different formats and lengths in terms of how skills learned contributed to
information evaluation confidence in other courses.
Descriptive Analysis
To obtain a profile of the respondents, I conducted a descriptive analysis of the
survey’s variables from two perspectives. First, I examined descriptive statistics for the
total number of cases (N = 525) of the response sample using Range, Measures of Central
Tendency, Standard Deviation (SD), Variance (V), Skewness (SE = .10), and Kurtosis
(SE = .21). The measures of central tendency described the participant responses using
the mean (M) for interval type variables, the mode for nominal type variables, and the
median for ordinal type variables (Everitt & Skrondal, 2010). The measures of range
(spread) represented the maximum value minus the minimum value, standard deviation
indicated the amount the scores deviated from the mean, and the variance showed the
amount of variation around the mean. In comparison to a normal distribution, skewness
showed the symmetry of the distribution from the center point and Kurtosis showed the
height of the peak. Next, I examined the frequency of each survey item using counts (n)
and percentages (%) to summarize the response data for each category in more detail and
to assess potential univariate patterns (Blaikie, 2003). In this frequency analysis, I used
counts to provide the number of responses for the categories and percentages to provide
the relation of the values for each category to those of the entire response sample. Table 3
contains the descriptive statistics for the overall data set and the frequencies and
percentages for the variable categories for each survey item.

90
Table 3
Information Literacy Requirement Impact Survey Item Descriptive Analysis
F

%

1. College attended

Range
5

Mean

SD

1.66

1.61

Variance

Skewness

Kurtosis

2.57

2.04

2.29

2-year 447
85.1
4-year/Private
78
14.8
Inputs = student demographic and preparation characteristics (independent variables)
2. Age category
19 or younger
20–24
25–29
30–34
35–39
40–49
50 or older

95
189
81
43
24
52
41

18.1
36.0
15.4
8.2
4.6
9.9
7.8

Female
Male
Other

361
159
5

68.8
30.3
1.0

Non-Hispanic
Hispanic
5. Primary language
English
Not English
6. Terms attended
0 Terms
1–2 Terms
3–4 Terms
5–6 Terms
7–8 Terms
9–12 Terms
13+ Terms
7. English course
Below transfer
Initially below transfer
Transfer
No English courses
8. Preparedness self-concept
Super prepared
Somewhat prepared
Don’t know
Somewhat unprepared
Completely unprepared
9. Papers written
0 papers
1–2 papers
3–4 papers
5–6 papers
7–8 papers
9–12 papers
13+ papers

389
136

74.1
25.9

459
66

87.4
12.6

21
90
170
127
63
18
36

4.0
17.1
32.4
24.2
12.0
3.4
6.9

78
178
228
41

14.9
33.9
43.4
7.8

142
291
25
55
12

27.0
55.4
4.8
10.5
2.3

26
89
113
94
59
49
95

5.0
17.0
21.5
17.9
11.2
9.3
18.1

3. Gender

4. Ethnicity

6

3.06

1.87

3.49

0.87

-0.47

2

1.32

0.49

0.24

1.01

-0.38

1

0.26

0.44

0.19

1.10

-0.79

1

1.13

0.33

0.11

2.26

3.14

6

3.61

1.45

2.09

0.66

0.15

3

2.13

0.94

0.88

-0.83

-0.27

4

3.94

0.97

0.94

-1.19

1.09

6

4.14

1.86

3.44

0.22

-1.13

(table continues)

91
F

%

Range

Mean

SD

Variance

Skewness

Kurtosis

Environment = program characteristics (independent variables)
10. LIR 10 format
On-ground
Online

176
349

33.5
66.5

11. LIR 10 length
Less than 8 weeks
More than 8 weeks

276
249

52.6
47.4

1

1.66

0.47

0.22

-0.70

-1.52

1

2.47

0.5

0.25

0.10

-2.00

4.03

-0.60

-0.72

1.05

-0.81

-0.45

1.19

-0.39

-0.98

19.16

-0.36

-0.59

0.71

-0.36

-0.97

Outcomes = behavior change and level of confidence (dependent variables)
12. SRJC Critical Analysis
8
7.78
2.01
10 145
27.6
9
79
15.0
8 104
19.8
7
52
9.9
6
64
12.2
5
24
4.6
4
55
10.5
2
2
0.4
12a. Information
4
4.06
1.03
A lot 229
43.6
Some 158
30.1
A little
79
15.0
None
57
10.9
Don’t know/ Can’t answer
2
0.4
12b. Conclusions
4
3.72
1.09
A lot 157
29.9
Some 164
31.2
A little 111
21.1
None
88
16.8
Don’t know/ Can’t answer
5
1.0
13. ACRL Critical Analysis
24
24.14
4.38
30
78
14.9
29
32
6.1
28
43
8.2
27
34
6.5
26
41
7.8
25
22
4.2
24
63
12.0
23
30
5.7
22
22
4.2
21
21
4.0
20
28
5.3
19
20
3.8
18
82
15.6
17
3
0.6
16
2
0.4
12
2
0.4
11
1
0.2
6
1
0.2
13a. Relevance
4
4.10
0.84
A lot more frequently 204
38.9
Somewhat more frequently 174
33.1
No change 141
26.9
Somewhat less frequently
5
1.0

(table continues)

92
F
A lot less frequently
1
13b. Strategy
A lot more frequently 193
Somewhat more frequently 198
No change 129
Somewhat Less frequently
4
A lot less frequently
1
13c. Verify
A lot more frequently 202
Somewhat more frequently 166
No change 151
Somewhat Less frequently
4
A lot less frequently
2
13d. Criteria
A lot more frequently 221
Somewhat more frequently 160
No change 139
Somewhat Less frequently
4
A lot less frequently
1
13e. Evidence
A lot more frequently 225
Somewhat more frequently 137
No change 156
Somewhat Less frequently
5
A lot less frequently
2
13f. Discussions
A lot more frequently 109
Somewhat more frequently 132
No change 271
Somewhat Less frequently
10
A lot less frequently
3
14. Confidence
Super confident 197
Somewhat confident 204
Neutral 116
Somewhat unconfident
3
Completely unconfident
5
Students’ recommendation of timing
15. Helpful term
1–2 Terms 457
> 2 Terms
38
Terms doesn’t matter
30

%

Range

Mean

SD

Variance

Skewness

Kurtosis

4

4.10

0.81

0.65

-0.36

-0.81

4

4.07

0.86

0.73

-0.36

-0.86

4

4.14

0.85

0.72

-0.41

-1.01

4

4.10

0.89

0.79

-0.41

-0.98

4

3.64

0.85

0.72

0.36

-0.64

4

4.11

0.83

0.69

-0.68

0.32

5

1.36

1.17

1. 38

3.55

11.21

0.2
36.8
37.7
24.6
0.8
0.2
38.5
31.6
28.8
0.8
0.4
42.1
30.5
26.5
0.8
0.2
42.9
26.1
29.7
1.0
0.4
20.8
25.1
51.6
1.9
0.6
37.5
38.9
22.1
0.6
1.0

87.0
7.2
5.7

The data in Table 3 did not have any variables showing large standard deviations
(SD), with most values being under 1. The exceptions were the variables with more
response categories including college attended (SD = 1.61), age category (SD = 1.87),
number of terms attended (SD = 1.45), number of papers written (SD = 1.86), SRJC

93
Critical Analysis (SD = 2.01) with information (SD = 1.03) and conclusion (SD = 1.09),
ACRL Critical Analysis (SD = 4.38), and most helpful term to take LIR 10 (SD = 1.17).
The variance followed the same overall pattern with most values for the variables falling
under 1. The exceptions were the variables college attended (V = 2.57), age category (V =
3.49), number of terms attended (V = 2.09), number of papers written (V = 3.44), SRJC
Critical Analysis (V = 4.03) with information (V = 1.05) and conclusion (V = 1.19),
ACRL Critical Analysis (V = 19.16), and most helpful term to take LIR 10 (V = 1.38).
For the skewness (SE = 0.10) and symmetry of the distributions, most variables showed
values around 1. The exceptions were the variables college attended (2.04), primary
language (2.26), LIR 10 length (0.10), and the most helpful term (3.55). Most variables
had positive values indicating a tail to the right. The exceptions were the variables
English course, the preparedness self-concept, LIR 10 format, SRJC Critical Analysis and
its subitems, and ACRL Critical Analysis and its subitems except for discussion, which
all had lower and negative values indicating a small tail to the left. Kurtosis (SE = 0.21),
showed a greater variation in the values among the variables with the majority ranging
from close to zero to just under .50. Other variables clustered around 1, except for the
variables college attended (2.29), primary language (3.14), and most helpful term (11.21).
Most values were negative indicating flatter distribution. The variables with positive
values were primary language, terms attended, preparation self-concept, confidence, and
most helpful term.
Next I used the I-E-O framework to structure the item by item analysis shown in
Table 3. In this analysis, I examine the frequency of the response options (shown in

94
italic) for each of the survey items and categorize them by calculating the percentage that
formed the majority. Survey Items 1 and 15 were not aligned with the conceptual
framework or the research questions. However, I still provided descriptive analysis.
Item 1. The variable, college attended, was included in the survey to filter out
respondents who had attended no college/ university and thus did not meet the inclusion
criteria, and the item was not used to answer the research questions. The findings
indicated that the majority of 85% of the students taking the survey were still attending
classes at SRJC or another 2-year college (n = 447) with only 15% at 4-year
College/Private (n = 78).
Inputs - Demographic and Preparation Characteristics (Independent Variables)
I included four items on the survey that examined the demographic characteristics
of the respondents and four items that identified self-reported experiences that could
potentially indicate the preparation level of the respondents.
Item 2. For the age category variable, the three highest responses were
respectively, the age 20–24 (n = 189), 19 or younger (n = 95), and 25–29 (n = 81)
categories. The next age grouping in descending order was 40–49 (n = 52), 30–34 (n =
43), and 50 or older (n = 41). The 35–39 (n = 24) age group was the lowest. The majority
of 70% of the students were in the 29 and under (n = 365) age categories.
Item 3. The variable for gender showed the sample was predominately female (n
= 361), which comprised 69% of the students. Males (n = 159) represented 30% of the
respondents. The category for other (n = 5) had a very small number of respondents.

95
Item 4. The results for the variable indicating ethnic distribution were mixed, but
two groups dominated. The white (n = 321) ethnicity group was largest at 61%. The
Hispanic (n = 136) group was approximately 26% of the respondents. All of the other
ethnic groups Asian (n = 23), Black (African American; n = 14), American Indian (n = 7),
Filipino (n = 5), Pacific Islander (n = 4), and other (n = 19), were considerably less well
represented 13% combined (n = 72). Given the strong division between the white and
Hispanic ethnicities represented, the low frequencies for all the other ethnic groups, and
the increasing Latino (Hispanic survey response item) demographic, I recoded categories
of non-Hispanic (n = 389) and Hispanic (n = 136) and focused the examination on them.
The non-Hispanic group was 74%, and the Hispanic category was 26%.
Item 5. The variable primary language of the sample was predominately English
primary language (n = 459) which was a large majority at 87% while the Not-English
primary language (n = 66) category was 13%.
Item 6. The highest response frequencies for the variable number of terms
students had attended college were for 3–4 terms (3rd year of college; n = 170), 5–6
terms (3rd year of college; n = 127), 1–2 terms (1st year of college; n =90), 7–8 terms
(4th year of college; n = 63), 13+ terms (6th+ year of college; n = 36), and 0 terms (just
started college; n = 21) respectively. Respondents had predominately attended more
college terms with 3–4 terms being the highest response category value at 32%. Those
respondents with 3–4 terms and higher (n = 414) were combined to represent 79% of the
total while those under 3–4 terms (n = 111) represented 21%.

96
Item 7. For the variable, English course level, the largest number of students had
begun college at the transfer (n = 228) English course level. Next were those that started
at the initially below transfer (n = 178) English course level, and then those at the below
transfer (n = 78) English course level. The lowest response value was for the no English
courses (n = 41) level. The largest group, 77% of students, had taken transfer English (n
= 446) with 23% having taken only below transfer or no English courses (n = 119).
However, regarding the local and national concern for underprepared students, if the
below transfer and initially below transfer groups were combined (n = 256), that group
was the majority at 49%.
Item 8. This item asked respondents to provide their self-concept of how prepared
they were to evaluate information required to write papers or participate in discussions in
other courses prior to taking LIR 10. The highest frequency reported feeling somewhat
prepared (n = 291). The next group of respondents reported feeling super prepared (n =
142). A smaller number of students reported feeling somewhat unprepared (n = 55). The
next smaller group reported that they don’t know (n = 25), and an even smaller number
said they felt completely unprepared (n = 12). Overall, the largest majority of students,
83%, expressed the self-concept that they felt they were prepared at some level (n = 433).
Just 17% felt they didn’t know or were not prepared (n = 92).
Item 9.This item requested information about the number of college research
papers students had written that required them to evaluate information. Responses
reported the frequency distribution at 3–4 papers (n = 113), 13+ papers (n = 95), 5–6
papers (n = 94), 1–2 papers (n = 89), 7–8 papers (n = 59), 9–12 papers (n = 49), and 0

97
papers (n = 26). The largest number of students, 78%, had written 3–4 papers (n = 410)
or more. Only 22% wrote 1–2 or 0 papers (n = 115).
Environment - Program Characteristics (Independent Variables)
Two survey items produced Environment variables that could be used to assess
specific characteristics of the general education IL requirement program.
Items 10. This item measured the variable of the format (mode of instruction)
respondents chose for the general education IL requirement course, LIR 10. The online (n
= 349) format showed the highest frequency with 67% of the students taking it. The onground (n = 176) format represented 33% of the classes taken.
Item 11. This item measured the variable of the length of the chosen general
education IL requirement course, LIR 10. The greatest frequency of respondents, 53%,
indicated they had taken the shorter length course (6 weeks), represented by the category
less than 8 weeks (n = 276). Those indicating the longer length courses (9 or 12 weeks),
represented by the category more than 8 weeks (n = 249), were at 47% of the total.
Outcomes - Behavior Change and Level of Confidence (Dependent Variables)
Three survey items produced the study’s Outcomes variables. I reported
information evaluation behavior changes in two composite measures, one (Item 12)
derived from the college (SRJC Critical Analysis) and one (Item 13) from a national
library association (ACRL Critical Analysis). I reported information evaluation
confidence in one measure (Item 14) based on reports of how the IL requirement
contributed to the level of confidence in writing papers or participation in discussions in
other courses.

98
Composite-Item 12. I combined two survey subitems that asked students to rate
to what extent LIR 10 contributed to their critical analysis learning gains to create the
SRJC Critical Analysis variable. The frequencies for the scale of the composite showed
that 72% of the responses (n = 380) were in the top half of the scale (7, 8, 9, 10) and 28%
(n = 145) were in the bottom half of the scale (2, 4, 5, 6).
Subitem 12a. This subitem of the SRJC Critical Analysis composite focused on
locating, analyzing, evaluating, and synthesizing relevant information. The majority of
students, 89%, reported that they had experienced some level of positive learning gains (n
= 466). The highest number of the students, 44%, reported their learning gains were a lot
(n = 229). A smaller number, 30%, reported some (n = 158). A few students, 15%,
reported a little (n = 79) and an even smaller group, 11%, reported none (n = 57) or don’t
know/ can’t answer (n = 2) if they had experienced any learning changes.
Subitem 12b. This subitem of the SRJC Critical Analysis composite focused on
students’ ability to draw conclusions decision making and problem solving. The majority,
83%, of students (n = 432) collectively reported that LIR 10 had contributed positively to
their learning experience with these a lot (n = 157), some (n = 164), and a little (n = 111)
learning gains. A smaller number of students (n = 93) said they experienced none (n =
88) or don’t know/ can’t answer (n = 5), which represented 17%.
Composite-Item 13. The ACRL Critical Analysis variable was the combination
of six survey subitems that asked students to report how frequently they performed
specific information evaluation actions now compared to how often they did them before
taking LIR 10. I combined these subitems and the frequencies for the scale of the

99
composite showed that 70% of the responses (n = 365) were in the top half of the scale
(22, 23, 24, 25, 26, 27, 28, 29, 30) and 30% of the responses (n = 160) were in the bottom
half of the scale (6, 11, 12, 16, 17, 18, 19, 20, 21).
Subitem 13a.The first subitem of the ACRL Critical Analysis composite focused
on evaluating the relevance of information. The majority of students, 72%, indicated they
determined the relevance of information a lot more frequently (n = 204) or somewhat
more frequently (n = 174) after taking the LIR 10 course. A smaller number of the
students, 27%, reported they experienced no change (n = 141). A very small number
(1%) selected somewhat less frequently (n = 5) or a lot less frequently (n = 1).
Subitem 13b. The second subitem of the ACRL Critical Analysis composite
focused on reviewing search strategy. The majority of students, 75%, indicated they
reviewed their search strategy more frequently either somewhat more frequently (n = 198)
or a lot more frequently (n = 193) after taking the LIR 10 course. A smaller number,
25%, said they experienced no change (n = 129), or they did these information evaluation
actions somewhat less frequently (n = 4), or a lot less frequently (n = 1).
Subitem 13c. The third subitem of the ACRL Critical Analysis composite
focused on verifying information. The majority of students, 70%, indicated they did these
information evaluation actions more frequently after taking the LIR 10 course with the
highest category being a lot more frequently (n = 202) and then somewhat more
frequently (n = 166). Fewer students, 30%, said they experienced no change (n = 151),
and a very small number said they did this activity somewhat less frequently (n = 4) or a
lot less frequently (n = 2).

100
Subitem 13d. The fourth subitem of the ACRL Critical Analysis composite
focused on using criteria to evaluate information. The majority of the students, 73%, said
they did this behavior more frequently after taking the LIR 10 course with a lot more
frequently (n = 221) representing the largest group, then somewhat more frequently (n =
160). A smaller number, 27%, said they experienced no change (n = 139), or they did this
behavior somewhat less frequently (n = 4), or a lot less frequently (n = 1).
Subitem 13e. The fifth subitem of the ACRL Critical Analysis composite focused
on evaluating information for evidence. The majority of students, 69%, indicated they did
this behavior more frequently after taking the LIR 10 course with the category of a lot
more frequently (n = 225) the highest followed by somewhat more frequently (n = 137).
A smaller number, 31%, said they experienced no change (n = 156), or they did the
behavior somewhat less frequently (n = 5), or a lot less frequently (n = 2).
Subitem 13f. The sixth subitem of the ACRL Critical Analysis composite focused
on participating in discussions. About half of the students, 52%, said they experienced no
change (n = 271) after taking the LIR 10 course. The next highest group, 46%, said they
did this activity more frequently indicated by a lot more frequently (n = 109) and
somewhat more frequently (n = 132). A very small number, 2%, said they did this activity
somewhat less frequently (n = 10) or a lot less frequently (n = 3).
Item 14. This item asked students to report on changes in their level of
confidence in writing papers or participating in discussions in other courses based on the
information evaluation skills learned in LIR 10. Responses indicated that 76% of the
students said they were more confident with somewhat confident (n = 204) as the largest

101
group and super confident (n = 197) a close second. Those that said they were neutral (n
= 116), those that said they were somewhat unconfident (n = 10), and those completely
unconfident (n = 10) combined to represent 24% of the respondents.
Students’ Recommendation of Timing
Item 15. I included this item in the survey to gather potential data for future
research regarding students’ recommendations of in which terms they believe would be
most helpful to take the general education IL requirement course, LIR 10. A majority,
87%, of the respondents, reported LIR 10 would be most helpful if taken in the 1–2 terms
(1st year of college study; n = 457). I divided the remaining 13% of the responses from
six categories ranging from 2 terms to 12+ terms that I combined as > 2 terms (n = 38)
and the category term doesn’t matter (n = 30).
Descriptive Analysis Interpretation
I gained useful insights about the Inputs, Environment, and Outcomes aspects of
the response sample through inspection of the descriptive data. The inspection showed
that most of the students who took LIR 10 during the designated study period reported
feeling somewhat prepared. The majority of the students had attended 3–4 terms or more,
had taken a transfer level English course, identified with the self-concept that they felt
somewhat or super prepared with information evaluation skills before taking LIR 10, and
had written 3–4 papers or more. However, even though the students reported that they
came feeling somewhat prepared, for the Outcomes measured, the majority identified
that LIR 10 positively impacted the frequency of their SRJC Critical Analysis learning
gains changes, their ACRL Critical Analysis behavior changes, and their information

102
evaluation confidence for involvement in writing papers and participating in discussions
other classes. Also, a large majority of the students recommended that LIR 10 would be
most helpful if taken in the 1st or 2nd term.
Representativeness Analysis
Because the survey was anonymous, I was unable to verify the accuracy of the
self-reported data collected. However, I was able to compare whether the response
sample was representative of the designated target population of all students who took the
LIR 10 course that academic year, using institutional data containing the aggregated
demographic and program characteristics. The target population was comprised of all
students invited to take the survey. In this way, I was able to assess whether those who
completed the survey were representative of the larger group. To determine how well the
self-selected survey sample represented the distribution of the target population, I
examined demographic characteristics and experience with the IL requirement course.
In Table 4, I compared the frequencies and percentages of the demographic
characteristics of the N = 525 students who responded to the survey to those from the
target population of 2012 students who were invited to take the survey. I conducted a chisquare goodness-of-fit test to determine whether the sample respondents had statistically
significant differences in the age, gender, and ethnicity distributions compared to those in
the designated target population. The minimum expected frequencies for the variables
were age category (n = 21), gender (n = 1), and ethnicity (n = 150.7). The chi-square
goodness-of-fit test indicated that the demographics variables age category [χ2(6) =
38.39, p < .001], gender [χ2(2) = 475.36, p < .001], and ethnicity [χ2(1) = 89340.60, p <

103
.001] were not similarly distributed in the participants who responded to the study as in
the target population. The age category variable comparison showed a small variation of
the response sample having a slightly higher proportions of ages 18–19, 40–49, and 50 or
older and less of ages 20–24. The gender category showed a small variation of the
response sample having slightly higher proportions of females and slightly less of males.
The designated target population contained 62.8% females and 37.1% males. The
response sample had 68.8% females and 30.3% males. The ethnicity showed a slight
variation in the proportions of several ethnicities. However, when I conducted the chisquare goodness-of-fit test on the ethnicity variable for the non-Hispanic and Hispanic
categories, the proportions were highly similar. This indicated that the numbers of nonHispanic and Hispanic participants who responded to the study were not statistically
significantly different from the proportions found in the target population [χ2(2) = 2.01, p
= .157]. Therefore, the response sample can be considered representative for the ethnicity
categories, non-Hispanic and Hispanic, of the designated target population. Although the
age and gender comparison showed small differences between the groups, they did not
prove to be statistically significantly. For the purposes of this study, I made the
assumption that the response sample was meaningfully similar for the demographic
characteristics. I based this assumption on the observation of the statistically significant
ethnic distribution and the relatively small differences in the age and gender distributions.

104
Table 4
Comparison of Student Demographic Characteristics
Age category
19 or younger
20–24
25–29
30–34
35–39
40–49
50 or older
Total

Target population
N
%
314
15.6
901
44.8
331
16.5
170
8.5
81
4.0
129
6.4
86
4.2
2012
100.0

Response sample
N
%
95
18.1
189
36.0
81
15.4
43
8.2
24
4.6
52
9.9
41
7.8
525
100.0

Gender
Females
Males
Other
Total

N
1239
745
28
2012

%
62.8
37.1
.01
100.0

N
361
159
5
525

%
68.8
30.3
1.0
100.0

Ethnicity
American Indian
Asian
African American/Black
Filipino
Hispanic/Latino
Pacific Islander
White
Other (Unknown /Multi)
Total

N
10
88
45
0
578
6
1143
142
2012

%
.05
4.4
2.2
0.0
28.7
.03
56.8
7.1
100.0

N
9
26
14
7
136
3
321
9
525

%
1.7
5.0
2.7
1.3
25.9
.6
61.1
1.7
100.0

Non-Hispanic
Hispanic
Total

1434
578
2012

71.3
28.7
100.0

389
136
525

74.1
25.9
100.0

Note. Response data from the Information Literacy Requirement Impact Survey
conducted May 2015. Institutional data from the California Community College
Chancellor’s Office Data Mart (CCCCO, 2014c).

105
Next, I compared the frequencies and percentages of the program characteristics
variables of format and length between the response sample and the target population for
the IL requirement course, LIR 10, as shown in Table 5. I conducted a chi-square
goodness-of-fit test to determine whether the sample respondents reported the same
distributions of LIR 10 format and length variables as those in the target population. The
minimum expected frequency was 5.3. The chi-square goodness-of-fit test indicated that
the LIR 10 format and length [χ2 (2) = 541.77, p < .001] variables were not similarly
distributed in the participants who responded to the study as in the target population as
illustrated in Table 5. The differences indicated in the data set were that the target
population had a higher distribution for the on-ground 6 weeks (14.6% vs. 13.1%), onground 12 weeks (15.6% vs. 10.3%), and the online 6 weeks (54.2% vs 39.4%) formats
and lengths. The response sample was higher for the on-ground 9 weeks (9.0% vs 3.2%),
and the online 9 weeks (17.2% vs. 10.4%), and online 12 weeks (10.1% vs 1.0%) formats
and lengths. Based on these results, the distributions were considerably different, so I
could not consider the program characteristics of the response sample to be representative
of the target population of students invited to take the survey. A possible explanation for
this variation was that the students in the response sample may not have accurately
remembered the length of the class they took. More respondents indicated they had taken
longer course lengths than was possible compared to the baseline numbers of the target
population of what course lengths the college offered. For the purposes of this study, I
made the assumption that the response sample’s LIR 10 course length concept was
invalid and the program characteristics comparison was not meaningfully similar.

106
Table 5
Comparison of LIR 10 Formats and Lengths

Format and length
On-ground 6 weeks
On-ground 9 weeks
On-ground 12 weeks
Online 6 weeks
Online 9 weeks
Online 12 weeks
Hybrid 9 weeks
Total

Target population
LIR 10
N
%
14
3
15
52
10
1
1
98

Response sample
LIR 10
N
%

14.6
3.2
15.6
54.2
10.4
1.0
1.0
100.0

69
47
54
207
89
53
6
525

13.1
9.0
10.3
39.4
17.0
10.1
1.1
100.0

Note. Response data from the Information Literacy Requirement Impact Survey
conducted May 2015. Institutional data obtained from the SRJC.
Representativeness Analysis Interpretation
I conducted representativeness analysis to assess the data for potential response
bias limitations that survey research can introduce. Based on the results of the
representativeness analysis, I accepted the assumption that for this study the demographic
characteristics were meaningfully similar because the ethnicity variable was significant
statistically and the differences for the age and gender variables were small. However, I
did not accept that the program characteristics for this study were meaningfully similar
due to the discrepancies in LIR 10 lengths reported by the response sample. I did not
believe that the error in reporting for the LIR 10 course length variable limited the ability
to draw conclusions from the other findings. The findings for the representativeness of
the LIR 10 format program characteristic and the demographic characteristics were all
meaningfully similar. The findings from this study will not generalize to other groups,

107
but the representativeness analysis findings provided confidence that the response sample
was similar to the target population for the demographic characteristics.
Cross-Tabulation Analysis
I used cross-tabulation as a measure of association to examine relationships
between pairs of categorical variables including Inputs and Environment item responses.
The results of the univariate descriptive analysis guided the bivariate analysis of simple
cross-tabulation to examine frequency connections (Blaikie, 2003). I also conducted chisquare tests for associations between the nominal by nominal and the nominal by ordinal
Inputs and Environment independent variables to determine if any variables had
statistically significant relationships at the p <= .05 level. One of the assumptions of the
chi-square test was that no more than 20% of the cell frequencies within a pairing can be
less than 5, and none can have frequencies less than 1 (Blaikie, 2003).
The results of the chi-square tests showed that 12 pairings between independent
variables were positively significantly associated. The pairings included ethnicity and age
category [χ2(6) = 17.86, p = .007], ethnicity and primary language [χ2(1) = 136.65, p <
.001], ethnicity and terms attended [χ2(6) = 19.46, p = .003], ethnicity and English course
[χ2(3) = 9.26, p = .026], ethnicity and LIR 10 format and LIR 10 length [χ2(6) = 23.27, p
= .001], primary language and terms attended [χ2(6) = 20.58, p = .002], primary language
and LIR 10 format and LIR 10 length [χ2(6) = 20.32, p = .002], English course and age
category [χ2(18) = 42.36, p = .001], English course and primary language [χ2 (3) = 11.68,
p = .009], English course and terms attended [χ2(18) = 49.96, p < .001], English course

108
and papers written [χ2(18) = 77.48, p < .001], and English course and LIR 10 format and
LIR 10 length [χ2(18) = 22.30, p = .219].
Cross-Tabulation Analysis Interpretation
The results of the bivariate cross-tabulation analyses provided evidence regarding
which of the categorical type Inputs and Environment characteristics variables might
warrant further inspection in the correlation analysis. Based on the chi-square results, I
excluded the variable gender from the correlation analysis because it did not have any
significant pairings due to low cell frequencies for the category of other (n = 5). I
included the independent variable categories of age, ethnicity, primary language, number
of terms attended, English course level, number of papers written, LIR 10 format, and
LIR 10 length in the correlation analysis.
Correlational Analysis
I conducted a zero order multiple correlation test to study the effects of all of the
Inputs, Environments, and Outcomes variables simultaneously. The purpose of this
bivariate test was to determine whether any significant relationships existed among the
variables as a way to indicate which I should include in the regression analysis. I included
only the significantly correlated variables found earlier in the cross-tabulation analysis, so
gender was not included in the correlation. The assumptions of measurement scale,
linearity, no outliers or unusual points, and normality were met. I used the Pearson
product-moment (r) correlation coefficient in the multiple correlation test to show how
strongly and in what direction relationships existed between the pairs of variables (Astin
& Antonio, 2012). The r coefficient value range was from -1.0 to +1.0. Using a critical

109
value table (Chew, 2015) I obtained the expected correlation coefficient of r > 0.074 that
is appropriate for a sample size of N = 500. Also, I used Cohen’s (1992) standard for
assessing the multiple correlation effect size or strength of the relationships between
variables. A coefficient value close to 0 indicates no relationship. Coefficients between
.02 and .14 represent a small relationship, between .15 and .34 a medium relationship,
and above .35 a large relationship. Positive values mean when one variable gets larger,
the other also gets larger. Negative values mean when one variable gets larger, the other
gets smaller.
Table 6 showed 43 correlations that were statistically significant and were greater
or equal to r (524) = > 0.074, one-tailed. Given that all of the study’s remaining variables
correlated significantly with one or more of the other variables, I concluded that all
should be included in the multiple regression test. The significant variables included age
category (Item 2), ethnicity (Item 4), primary language (Item 5), number of college terms
attended (Item 6), English course level (Item 7), prior preparation self-concept (Item 8),
number of papers written (Item 9), LIR 10 format (Item10), LIR 10 length (Item 11),
SRJC Critical Analysis for gains in student information evaluation learning (Item 12),
ACRL Critical Analysis for changes in the information evaluation behaviors (Item 13),
and levels of confidence (Item 14).

110
Table 6
Means, Standard Deviations, and Zero-Order Correlations for all Variables
Variables
2. Age category
4. Ethnicity
6. Terms attended
7. English course
8. Preparedness self-concept
9. Papers written

M
3.06

SD
1.87

0.26

0.44

-.17***

---

1.45

.35***

-.16***

0.94

-.20***

-.03

.15***

---

.10

-.10**

-.21***

.11**

.22***

---

.17***

-.08*

.41***

.18***

.27***

---

-.15***

.00

.14***

.17***

-.02

---

.04

-.19***

---

3.61
3.13
3.94
4.14

1.85

2.

4.

6.

10. LIR 10 format

0.66

0.47

.00

11. LIR 10 length

0.47

0.50

.00

.05

12. SRJC Critical Analysis

7.78

2.01

.14**

.14**

4.38

.10*

.15**

13. ACRL Critical Analysis
14. Confidence
5. Primary language

24.14
4.11
0.13

7.

8.

9.

10.

11.

12.

13.

14.

5.

---

0.83

.09*

0.33

-.13**

* p < .05, ** p < .01, *** p < .001, one-tailed.

-.04
.51***

---

.03

-.02

-.01

-.01

-.12*

-.35***

-.15***

-.15**

.04

---

-.05

-.10*

-.38***

-.14**

-.15***

.05

.79***

---

.01

-.11*

-.04

.43***

.44***

---

-.03

-.10*

.08*

.14**

.09*

-.05

.06
-.16***

.04
-.07

.06
-.17***

---

111
Correlation Analysis Interpretation
Inputs. For the seven variables that represented the Inputs, several variables
existed with the largest positive correlations. These pairings included the variables
ethnicity with primary language (r = .51), the number of terms attended with the age
category (r = .35), and the number of terms attended with the number of papers written (r
= .41). The other variable pairings showed small or medium correlations. The
demographic variables age, ethnicity, and primary language all had small or medium
negative correlations with the other Inputs variables. However, age had small positive
correlations with all three Outcomes variables although ethnicity did not. The pairings for
the other preparation variables, showed no surprises. For example, the English course
variable showed a medium positive correlation with the preparedness self-concept
variable and a small negative one with the two behavior change Outcomes variables. I
would expect this, as higher-level English courses require more research papers, allowing
students to gain IL preparation and, therefore, not attribute that behavior change to the
LIR 10 class. A surprising result was that the confidence Outcomes variable showed
small positive correlations with all of the Inputs variables except ethnicity that showed a
small negative one.
Environment. The two variables that represented the Environment were
correlated with each other, which was no surprise. The variable LIR 10 length was only
correlated with 1 other variable and that was a medium negative correlation with LIR 10
format (r = -.19). The LIR 10 format variable was correlated with 3 Inputs variables.
Those correlations were a medium negative relationship with ethnicity (r = -.15), a small

112
positive relationship with English course (r = .14), and a medium positive relationship
with the variable preparedness self-concept (r = .17). The LIR 10 format variable also
had a small negative correlation with the confidence Outcomes variable (r = -.11).
Overall, these results were not a surprise given that the demographic analysis showed that
the majority of student respondents reported feeling somewhat prepared or super
prepared prior to taking LIR 10.
Outcomes. The three variables that represented the Outcomes showed very strong
positive correlations with each other. The positive pairings were ACRL Critical Analysis
with SRJC Critical Analysis (r = .79), confidence with SRJC Critical Analysis (r = .43),
and confidence with ACRL Critical Analysis (r = .44). The strong correlation between
the SRJC Critical Analysis variable with the ACRL Critical Analysis variable was not a
surprise given that the measures were shown to be aligned in Section 1. Other small
correlations between the Outcomes variables and the Inputs or Environment variables
existed. These correlations showed no surprises as all followed the results shown in the
demographic analysis.
Multiple Regression Analysis
I used a hierarchical multiple regression analysis to examine relationships among
the Inputs, Environment, and Outcomes variables. Astin and Antonio (2012) stated that
multiple regression is a multivariate statistical technique that allows for assessment of the
degree and character of the relationship among one dependent variable (criterion) and
several independent variables (predictors). They believed this analysis method can help
control for potential bias due to differences in student Inputs. It can provide a way to

113
study “naturally occurring variations in Environmental conditions and to approximate the
methodological benefits of true experiments by means of complex multivariate statistical
analyses” (p. 29). Astin and Antonio recommended using a blocked stepwise multiple
regression for I-E-O assessment. They suggested the purpose of using this hierarchical
type multiple regression was to be able to assess the changes to the variability of the
dependent variable from each of the added independent variables in the regression
models. The results of the multiple regression analysis provided data relevant to
answering RQ1 and RQ2. The Research Questions were written broadly to allow
measurement of multiple independent variables and dependent variables in the regression
analysis. I prepared the regression equation using all of the study’s variables except
gender (Item 2), as indicated from the correlation and cross-tabulation analyses results I
obtained previously. For each Outcomes variable, I entered the independent variables into
the regression in three blocks. The demographic Inputs block included the variables age
category (Item 2), ethnicity (Item 4), and primary language (Item 5). The preparation
Inputs block included the variables number of college terms attended (Item 6), English
course level (Item 7), prior preparation self-concept (Item 8), and number of papers
written (Item 9). The Environment block included the variables LIR 10 format (Item10),
and LIR 10 length (Item 11). The study design met all assumptions of linearity,
independence of errors, homoscedasticity, unusual points, and normality of residuals.
Research Question 1 Answered
I considered RQ1 using the results of two hierarchical multiple regression tests
conducted on two dependent variables, SRJC Critical Analysis (Item 12), and ACRL

114
Critical Analysis (Item 13). The first multiple regression analysis results for the SRJC
critical analysis dependent variable indicated that six independent variables were
significant. The hierarchical multiple regression coefficients predicting the SRJC Critical
Analysis variable from ethnicity, age category, primary language, self-concept of prior
preparation, number of papers written, and LIR 10 format are shown in Table 7. The
initial inclusion of the ethnicity variable to the prediction of SRJC Critical Analysis
(Model 1), led to a statistically significant increase in R2 of .02, F(1, 523) = 11.21, p <
.001. The addition of the age variable to the prediction of SRJC Critical Analysis (Model
2), led to a statistically significant increase in R2 of .05, F(1, 522) = 13.51, p < .001. The
addition of the primary language variable to the prediction of SRJC Critical Analysis
(Model 3), led to a statistically significant increase in R2 of .06, F(3, 521) = 10.37, p <
.001. The addition of the self-concept of prior preparation variable to the prediction of
SRJC Critical Analysis (Model 4), led to a statistically significant increase in R2 of .15,
F(4, 520) = 22.74, p < .001. The addition of the papers written variable to the prediction
of SRJC Critical Analysis (Model 5), led to a statistically significant increase in R2 of .16,
F(5, 519) = 19.10, p < .001. The addition of the LIR 10 format variable to the prediction
of SRJC Critical Analysis (Model 6), led to a statistically significant increase in R2 of .16,
F(6, 518) = 16.74, p < .001.; adjusted R2 = .15. These six predictors accounted for 16.2%
of the variance in the SRJC Critical Analysis variable. The significant independent
variables in the model accounted for a relatively low percentage of the variance in the
dependent variable.

115
Table 7
Regression Coefficients and Standard Errors for SRJC Critical Analysis
SRJC Critical Analysis
Model 1
Model 2
Model 3
Model 4
Variable
B
β
B
β
B
β
B
Constant
7.61**
7.01**
6.98**
9.80**
Ethnicity
.66**
.15
.80**
.17
.57**
.13
.30
**
Age category
.18
.17
.19**
.18
.14*
*
Primary language
.59
.10
.42
Preparation self-concept
-.65**
Papers written
LIR 10 format
R2
F
ΔR2
ΔF

.02
11.21
.02
11.21

Note. N = 525. * p <.05, ** p <.001.

.05
13.51
.03
15.51

.06
10.37
.01
3.94

.15
22.74
.10
56.52

β
.07
.13
.07
-.32

Model 5
B
9.92**
.30
.16**
.45
-.60**
-.09*
.16
19.10
.01
4.04

β
.07
.15
.07
-.29
-.09

Model 6
B
10.08**
.25
.16**
.44
-.57**
-.10*
-.36*
.16
16.74
.01
4.29

β
.06
.15
.07
-.28
-.09
-.09

116
The second multiple regression analysis results for the ACRL Critical Analysis
dependent variable indicated that four independent variables were significant. The
hierarchical multiple regression coefficients predicting the ACRL Critical Analysis
variable from age category, self-concept of prior preparation, and LIR 10 format are
shown in Table 8. The initial inclusion of the ethnicity variable to the prediction of
ACRL Critical Analysis (Model 1) led to a statistically significant increase in R2 of .02,
F(1, 523) = 11.27, p < .001. The addition of the age variable to the prediction of ACRL
Critical Analysis (Model 2) led to a statistically significant increase in R2 of .04, F(2,
522) = 9.78, p < .001. The addition of the self-concept of prior preparation variable to the
prediction of ACRL Critical Analysis (Model 3) led to a statistically significant increase
in R2 of .16, F(3, 521) = 31.78, p < .001. The addition of the LIR 10 format variable to
the prediction of ACRL Critical Analysis (Model 4) led to a statistically significant
increase in R2 of .16, F(4, 520) = 25.01, p < .001; adjusted R2 = .16. These four predictors
accounted for 16.1% of the variance in the SRJC Critical Analysis variable. The
significant independent variables in the model accounted for a relatively low percentage
of the variance in the dependent variable.

117
Table 8
Regression Coefficients and Standard Errors for ACRL Critical Analysis

Variable
Constant
Ethnicity
Age category
Preparation self-concept
LIR 10 format
R2
F
ΔR2
ΔF

Model 1
B
23.76**
1.45**

.02
11.27
.02
11.27

Note. N = 525. *p<.05, **p<.001.

ACRL Critical Analysis
Model 2
Model 3
β
B
β
B
**
22.82
29.74**
**
.15
1.67
.17
.84*
.29*
.12
.17
-1.61**

.04
9.78
.02
8.13

.16
31.78
.12
73.10

Model 4
B
30.06**
.08
.73
.07
.17
-.36
-1.55**
-.77*

β

.16
25.01
.01
4.13

β
.07
.07
-.34
-.08

118
Based on results of the two multiple regression tests I conducted for RQ1, I
accepted the null hypothesis (H01) for some variables and the alternative hypothesis (Ha1)
for others as shown in Table 9. I answered RQ1 sing the hypothesis testing results.
Table 9
Results of Hypotheses Tests for Research Question 1
H01 Accepted

Ha1 Accepted

There is no statistically significant relationship
between identified characteristics (gender,
terms attended, and English course) of students
who completed the general education IL
requirement course with different formats in
terms of information evaluation behavior
changes (SRJC Critical Analysis and ACRL
Critical Analysis).

There is a statistically significant relationship
between identified characteristics (ethnicity, age
category, primary language, preparedness, selfconcept, and papers written) of students who
completed the general education IL requirement
course with different formats in terms of
information evaluation behavior changes (SRJC
Critical Analysis and ACRL Critical Analysis).

RQ1: What is the relationship between completion of the general education IL requirement course with
different formats and lengths and frequency of information evaluation behavior changes among students
with identified characteristics?

The results of the regression analysis explained the statistically significant
relationship between the variables in two ways. This test showed which of the
independent variables were not needed in the multivariate context because others better
explained the variance in the dependent variable. The variance was explained first by the
elimination of nonsignificant independent variables (predictors) from the equation and
secondly by indicating the amounts of the variation explained by the specific predictor.
For example, the Environment variables related to completion of the general education IL
requirement course showed that the LIR 10 format was significant for explaining a small
portion of the frequency of the information evaluation behavior changes for the Outcome
variables SRJC Critical Analysis and ACRL Critical Analysis. LIR 10 length was not
significant. I did not include the variable LIR 10 length in Table 9 as in the original

119
hypothesis because the regression model results were not significant. However, because
length was tied to format in practice, in that all course formats have a length, it was not
practicable to consider length separately. Therefore, I do not believe that failing to find a
significant statistical relationship of length with other variables was meaningful. The
Environment variable of taking LIR 10 in the online format decreased the frequency of
information evaluation learning gains. In addition, several identified characteristics
existed that were significant and explained a portion of the variance. For example, the
results indicated that the demographic characteristics Inputs variables of being Hispanic
and being older increased the frequency of information evaluation behavior changes. The
preparation characteristics Inputs variables of being prepared as the prior preparedness
self-concept and having written more papers decreased the frequency of information
evaluation learning gains. All other variables did not have significant effects on the
variation of the dependent variable.
Research Question 2 Answered
I considered RQ2 using the results of one hierarchical multiple regression test
conducted on the dependent variable, confidence in writing papers or participating in
discussions in other courses (Item 14). The multiple regression analysis results for the
confidence dependent variable indicated that two variables were significant. The
hierarchical multiple regression coefficients predicting the confidence variable from age
category and LIR 10 format are shown in Table 10. The initial inclusion of the age
variable to the prediction of confidence (Model 1), led to a statistically significant
increase in R2 of .01, F(1, 523) = 3.92, p < .048. The addition of the LIR 10 format

120
variable to the prediction of confidence (Model 2), led to a statistically significant
increase in R2 of .02, F(2, 522) = 4.98, p < .007; adjusted R2 = .02. These two predictors
accounted for 1.50% of the variance in the confidence variable. The significant
independent variables in the model accounted for a relatively low percentage of the
variance in the dependent variable.
Table 10
Regression Coefficients and Standard Errors for Confidence
Confidence
Model 1
Model 2
Variable
B
β
B
**
Constant
3.98
4.12**
*
Age category
.04
.09
.04*
LIR 10 format
-.19*
R2
F
ΔR2
ΔF

.01
3.92
.01
3.92

β
.09
-.11

.02
4.98
.01
5.99

Note. N = 525. *p<.05, **p<.001.
Based on the results of the multiple regression tests I conducted for RQ2, I
accepted the null hypothesis (H02) for some variables and the alternative hypothesis (Ha2)
for others as shown in Table 11. I answered RQ2 using the hypothesis testing results.

121

Table 11
Results of Hypotheses Tests for Research Question 2
H02 Accepted

Ha2 Accepted

There is no statistically significant relationship
between identified characteristics (gender, ethnicity,
primary language, terms attended, English course,
preparation self-concept, and papers written) of
students who completed the general education IL
requirement course with different formats in terms of
how skills learned contributed to information
evaluation confidence in other courses.

There is a statistically significant
relationship between identified
characteristics (age category) of students
who completed the general education IL
requirement course with different formats
in terms of how skills learned contributed to
information evaluation confidence in other
courses.

RQ2: What is the relationship between completion of the general education IL requirement course with
different formats and lengths and how skills learned contributed to information evaluation confidence in
other courses among students with identified characteristics?

The results of the regression analysis explained the statistically significant
relationship among the variables. The Environment variables related to completion of the
general education IL requirement course showed that the LIR 10 format was significant
for explaining a small portion of the frequency of the information evaluation behavior
changes for the Outcomes variable of how skills learned contributed to information
evaluation confidence in other courses. LIR 10 length was not significant. I did not
include the variable LIR 10 length in Table 11 as it was in the original hypothesis as
indicated above. The Environment variable of taking LIR 10 in the online format
decreased the frequency of confidence. The Inputs variable age category was the only
significant identified characteristic that explained a portion of the variance. The older age
categories increased the level of information evaluation confidence in other courses. All
other independent variables did not have significant effects on the variance of the
dependent variable.

122
Multiple Regression Analysis Interpretation
Considering the study’s results, the college would do well to include the data in
determining how effectively the IL education program characteristics of the LIR 10
formats and lengths meet the needs of students, especially the underprepared students.
The college could also benefit from further consideration of the Inputs survey items
measuring the preparation characteristics. The preparation characteristics were measured
by the variables number of terms attended, preparation self-concept, English course level,
and number of papers written. In the instrument development stage, I may not have
included the best measures to indicate preparation. For example, the variable, terms
attended, could have been problematic because students may have attended a large
number of terms but not had classes that required much research. In the two regression
tests for RQ1 related to the frequency of behavior changes Outcomes variables, I had the
concern that the regression model for the preparedness self-concept variable was
inconsistent with the models of the other independent variables. In general, the Inputs
preparation characteristics variables of being prepared as the prior preparedness selfconcept and having written more papers decreased the frequency of information
evaluation learning gains. Practical implications of the regression analysis results for the
IL education experiences are that students’ self-reported feelings of prior preparation
affected the ability of the other independent variables to predict more variability of the
Outcomes variables.
Astin’s (1985) I-E-O framework assessment model was appropriate for answering
the research questions. The I-E-O framework constructs worked well to describe the

123
issues, frame the study, and measure the variables. In addition, I considered the results in
relation to the appropriateness of Astin’s (1985) theoretical assumption that highly
involved students are more likely to reach their educational goals. The college assumes
that its core courses provide a baseline for all of its students and that the general
education IL requirement helps ensure that students have the IL critical analysis skills
and abilities needed to effectively write papers and participate in class discussions. I
would recommend conducting additional quantitative studies for assessing the IL
requirement and all other general education requirements. However, I think the best
results would come from using the I-E-O assessment model in a longitudinal study to
allow for comparison of groups.
In terms of the methodology, I would restructure the survey items to collect data
using interval scale so that it could be used more easily with parametric tests. Also, I
would recommend that future study also attempt to gather experimental data to
triangulate with the survey data. An experiment examining LIR 10’s impact on grades
could show a deeper aspect of student involvement impact. An interesting finding was
how little the independent variables addressed the variation in the regression results. All
three multiple regression tests I conducted had large constant values and small r values.
The constant represented the predicted value of the dependent variable when all other
variables were 0 (UCLA: Statistical Consulting Group, 2015). The high constant value
indicated that a large proportion of the variance was not attributed to the independent
variables in the model. The results of all three multiple regression tests showed that the
independent variable predictors accounted for small percentages of the variance in the

124
dependent Outcomes variables (SRJC Critical Analysis R2 = 16.2%, ACRL Critical
Analysis R2 = 16.1%, Confidence R2 = 1.50%). These results of the regression analysis
indicate that I should be cautious about making deeper conclusions regarding the
meaning of the findings. It also relates to an important finding from the descriptive
analysis results, in which student responses indicated they strongly endorsed taking the
IL requirement course, LIR 10, in their 1st or 2nd term of college.
The low level of variation explained by the independent variables in the
regression analysis could have been caused by the blocked stepwise hierarchical
regression test used. Astin and Antonio (2012) recommend this type of regression test to
address the issue of having multiple Inputs independent variables correlated with the
dependent Outcomes variables. They believe that the blocked stepwise regression test
measures all of the independent variables as they are related to the theory constructs and
that this provides the best prediction of the variability of the Outcomes variable. Nau
(2015) further supports the stepwise regression option as being more powerful for finetuning the model than a single multiple regression option. Nau also cautions that stepwise
regression can result in a poor model based on the fact that predictors close to the cut-off
point may be excluded or, alternatively, may be included possibly resulting in a
completely different model.
The survey instrument and methodology were effective for gathering a large
learner-centered response sample. The regression analysis was valid and adequate for
measuring students’ self-reports of critical information evaluation behavior changes and
levels of confidence. Because the survey method was successful in obtaining responses

125
from an acceptable sample of students, the instrument could potentially be reapplied in
subsequent years to compare how Outcomes change and how changes could be applied to
the LIR 10 course to help meet ongoing needs. However, the limitations of the survey
methodology, such as response bias or inaccurate self-concepts, limits possible
application to other groups completing the IL requirement. Students reported feelings of
being prepared for research, but if experimental measures were done using grade data
different result may emerge. Astin and Antonio (2012) believe the complexity of
regression analysis allows for predictions from survey data, but the study’s regression
results did not prove reliable enough to consider using them in a predictive capacity.
Results Support a White Paper for Policy Recommendations
Astin’s (1985) I-E-O conceptual model using survey research methodology
proved to be an effective method of assessing the college’s general education IL
requirement. These survey study results directly addressed the college’s assessment gap
previously identified by providing specific learner-centered data about the general
education IL requirement. I believe the findings from this study provide relevant support
for the development of a white paper for policy recommendations. I can use the white
paper as a tool to communicate the evidence from the research literature showing the
background of the issues surrounding the problem, the data analysis results, and policy
recommendations to administrative and faculty and decision makers.
Summary
The cross-sectional survey design I used was appropriate for gathering
quantitative data. The study survey assessed LIR 10, the required course, to determine

126
whether a relationship existed between participation in the general education IL
requirement and critical information evaluation behavior changes or levels of confidence
among students. The closed-ended items investigated student respondents’ self-reported
behavior changes and levels of confidence as a result of participation in the college’s
general education IL requirement. The survey design provided a way to use learnercentered self-reports to conduct targeted assessment of the IL requirement. The setting of
a large community college in northern California provided an adequate target population
of more than 2,000 students who took the IL requirement in the designated academic
terms. From this target population, I could pull the random sample based on identified
eligibility criteria. The target population was representative of the college’s ethnicity
breakdown between non-Hispanic and Hispanic students. I designed a survey instrument
using selected items from SRJC’s (2013d) Student Survey, educational issues identified in
the literature, and the SRJC (2013b) Institutional Learning Outcomes critical analysis
outcomes (see Appendix J). I also included an item measuring information evaluation
behavior changes using the wording of selected ACRL (2000) Information Literacy
Standard Three (see Appendix I) performance indicator outcomes. Lastly, I developed
items to gather data on areas not covered by existing instruments. After peer expert
review, I made adjustments. A pilot test gave positive results ensuring that the instrument
was understandable and that respondents could easily complete the items. These survey
measures aligned with the study’s research questions and hypotheses that defined the
variables, how they would be scored, and the Inputs, Environment, and Outcomes data
that would be collected. I outlined the study’s assumptions and discussed survey

127
methodology and potential limitations of the general education IL requirement. The
study’s design also included multiple protections for participants including data
confidentiality, a comprehensive informed consent notice, and an anonymous design to
protect from harm. I collected data for three weeks and received N = 525 usable
responses. I then conducted data analysis using descriptive analysis, cross-tabulation,
correlation analysis, and multiple regression analysis. I presented the data analysis results
and provided interpretation noting any consistencies or inconsistencies. The multiple
regression results provided the data needed to address the hypotheses and answer the
research questions. Finally, I noted that the findings and research indicated that a white
paper with policy recommendations would be the most appropriate project genre to
communicate the results of this study to the college. In Section 3, I describe the project
that developed from the study’s findings and research. In Section 4, I reflect on the
project’s recommendations and implications for further research.

128
Section 3: The Project
Introduction
The problem that I addressed in this study was a gap in the college’s assessment
of the effectiveness of the general education IL requirement. In Section 3, I provide
information about the selection of a white paper as the project genre and include a
description of the project and its goals. I conducted a literature review that highlights the
practical basis for the use of a white paper and the research and findings as the basis for
the recommendations I made in the project. I outline the particulars of the project’s
implementation and evaluation plan. I also include a discussion of the projects study’s
local, state and national, and social change implications.
Description and Goals
I used Astin and Antonio’s (2012) I-E-O model to frame the project. This
theoretical model formed the basis of the literature review of issues surrounding the
problem, the development of the Information Literacy Requirement Impact Survey
collecting students’ self-reports, and the data analysis conducted to examine relationships
among demographic and preparation characteristics, program characteristics, and
information evaluation behavior and level of confidence.
Project Description
The project I created is a practice and policy recommendation white paper that I
will disseminate to the college’s stakeholders for use in making informed decisions. I
used a simple outline to structure the white paper focusing on the problem, the study, and
the solution. The content of the white paper is presented in a way that engages the

129
college’s key administrative and faculty stakeholders so they can thoroughly understand
and evaluate the research and findings of the study. A change in assessment practices will
not happen at the college if the stakeholders do not focus on the solutions presented in the
white paper. The project consists of an introduction, the problem section including
background information aligned with the three issues surrounding the college’s
assessment gap, the study section including the conceptual I-E-O assessment model, data
collection and data analysis results, the recommendation section including
recommendations for practice and for future research, the conclusion, and references.
Within the white paper, I combined the evidence derived from scholarly research
with the data collected from the study’s quantitative survey to suggest strategies the
college can use for examining existing assessment practices. The study examined
relationships among student characteristic Inputs, program characteristic Environment
and behavior and confidence Outcomes. The purpose of the white paper is to share with
stakeholders the highlights of the issues and relationships identified in the data analysis
relevant to the evaluation of the effectiveness of the IL general education requirement
delivery method. These policy and programmatic recommendations, informed by the
research and findings, are intended to form the basis for discussion and inquiry regarding
assessment practices within the college’s shared governance structure.
Project Goals
The white paper project had three goals. The project’s first goal was to
communicate the background of the college’s IL, general education requirement
assessment gap problem to administrative and faculty stakeholders. Providing

130
information in an easy to interpret way can influence readers to make decisions (Boktor,
2013; Gordon & Graham, 2003; Kantor, 2010; Mattern, 2013; Srikanth, 2002; Stelzner,
2010). The second goal was to educate stakeholders about how the study findings can be
useful for decisions regarding the effectiveness of the IL requirement. Through sharing of
the project, a third goal was the identification of individuals interested in continuing the
discussion of institutional change to improve assessment practices. It will be through
discussions that the college’s collaborative decision-making processes will determine the
effectiveness of the IL general education requirement.
Rationale
This study and the associated project are significant because a gap existed in the
required assessment of IL practices at the college. I chose the white paper as the
appropriate project because of the study’s goal to communicate details of the assessment
gap practices problem and to provide recommendations relevant to data-based decision
making. A white paper is an excellent forum for sharing recommendations grounded in
research literature and study findings (Gordon & Graham, 2003).
White Paper Connects Research to Recommendations
The literature provides support for the white paper project genre. Gordon and
Graham (2003) stated that white papers use facts and logic derived from literature and
research evidence to persuade and make policy recommendations. Mattern (2013) agreed
that white papers can influence stakeholders through informative content showing
problems as opportunities. Therefore, the white paper is an effective format to inform
educational stakeholders regarding a problem and possible solutions (Graham, 2015).

131
White Paper Connects Findings to Recommendations
The project genre of policy recommendations in the form of a white paper
evolved logically from the results of the survey study’s targeted assessment of the IL
requirement. The white paper can effectively communicate a high-level description of the
study’s findings and implications for the recommended policy and programmatic changes
related to the college’s assessment gap problem. The study had a high response rate
indicating that students were interested in the survey content. The descriptive findings
identified the sample population, the program details, and frequency of information
evaluation behavior and confidence changes. The correlation and multiple regression
findings of this study indicated significant relationships among student demographic and
preparation characteristics and the format and length of the IL requirement course in
relation to the behavior and confidence Outcomes. In addition to informing stakeholders
about the findings, the white paper can communicate the issues surrounding the problem
and the study, and provide charts or graphics to illustrate the data results.
White Paper Addresses the College’s Assessment Gap Problem
The college has not assessed its assumption that a relationship existed between
students’ IL requirement participation and development of critical information evaluation
behaviors. The content of the white paper directly addresses this problem of a gap in the
college’s assessment practices by providing a concise, research-based background of the
problem, an overview of the study, and recommendations based on data resulting from
the use of a theory-based assessment model. Astin and Antonio’s (2012) I-E-O
assessment model offered a relevant and learner-centered model for assessing general

132
education requirements. Boktor (2013) suggested that the value of white papers lies in
providing a way for research findings to be accessible and potentially useful for solving
educational problems. Using the I-E-O model can increase the effectiveness of current
assessment practices by promoting the use of data for decision-making. Presenting
findings in the white paper format provides information to those who can act on the
research results and make necessary changes to existing assessment practices. I addressed
the study’s problem through respondent self-reporting of student and program
characteristics, behavior change, and confidence level data. Broad assessment data
intended to measure the effectiveness of student IL learning gains is available from the
college. However, an assessment gap existed specific to the IL requirement. The general
education course, LIR 10, has not been evaluated using specific criteria. Through this
project, I can provide administrative and faculty stakeholders with quantitative student
self-reported data derived through a targeted assessments process. These data have the
potential to more fully inform and demonstrate that IL education, as indicated by student
respondents who had completed the LIR 10 course with a grade of 2.0 or better, had a
positive impact on information evaluation skills, including increased confidence. This
project serves to promote understanding of IL education practices that impact students’
success and provide administrative and faculty stakeholders with increased understanding
of how to best assess the effectiveness of the general education program. The white paper
may inform administrative and faculty stakeholders of the issues related to general
education requirement assessments and spur discussion around the timing of student

133
completion of the IL requirement. The white paper may also encourage reform efforts
including new approaches to general education requirement pathways.
Review of the Literature
This literature review reflects the appropriateness of a white paper to make policy
recommendations that address the problem of a gap in the college’s assessment practices.
I describe the white paper’s historical origins, structure, and benefits as an information
sharing format to present the research-based and theory-based recommendations that
evolved from the study’s findings. Those recommendations align the findings with the
issues surrounding the college’s assessment gap problem including underprepared
students, IL education delivery methods, and assessment of general education
requirements.
Historical Origins of the White Paper
A white paper is a research-based report that efficiently informs and persuades an
identified audience (Hoffman, 2006; Kemp, 2005; Mattern, 2013). Historically, white
papers were considered to be a form of grey literature and typically included research
findings (Juricek, 2009). Grey literature referred to publications not available through
normal channels, for example, unpublished works, reports, working papers, and
proceedings (Boekhorst, Farace, Frantzen, Boor, & Croon, 2004). The concept of a white
paper evolved in the federal government as a way to describe an authoritative and
informative report (Sakamuro, Stolley, & Hyde, 2015). Stelzner (2007; 2010) noted the
word was first used in the early twentieth century by the British government. White

134
papers provided officials with a format that allowed for timely assembling,
dissemination, and absorption of information.
Structure of the White Paper
Although no official standards exist for a white paper, the genre adheres to a
common format and structure (Graham, 2013; Mattern, 2013; Stelzner, 2010). Gordon
and Graham (2003) noted that white papers inform decision makers with a short, easy to
read, and authoritative tone based on well-researched facts. Both Mattern and Stelzner
emphasized the need for a compelling title to get the reader's attention and explain what
the paper contains. Graham and Mattern noted that white papers generally include an
introduction, a problem description, data, proposed solutions, and a conclusion.
Sakamuro et al. (2010) believed that clear headings allow the reader to scan effectively
through the document. Astin and Antonio’s (2012) I-E-O theoretical model framed the
study and the white paper’s problem description.
The literature stressed the importance of considering the intended audience while
structuring the white paper. Keys to an effective white paper include describing the
problem accurately, making technical terms and examples easy to read, understanding the
audience, and focusing on the interest of the reader (Graham, 2013; Kemp, 2005;
Mattern, 2013; McKeon, 2005). Kemp suggested using simple terminology to convince
decision makers of the need for change. Parker (2013) added the importance of
constructing a white paper that remains relevant for an extended period. Stelzner (2010)
argued that a key to successful writing of white papers is to focus on the needs of the
reader. White paper writers should simplify and explain complex information so the

135
intended audience can understand the problem and proposed solutions (Gordon &
Graham, 2003; McKeon, 2005).
Kantor (2010) stated that communications have become abbreviated, and decision
makers are not likely to read a long white paper. As a result, this type of report needs
concise and articulate writing. Stelzner (2007) and Graham (2013) recommended that
white papers should not exceed 12 pages in length to retain the audience’s interest.
Anderson (2013), Kantor, and Srikanth (2002) argued against using a traditional textheavy structure, noting how that format can be a disadvantage when trying to engage
time- and attention-challenged stakeholders. They advocated using elements such as
smaller blocks of text, color, images, graphics, charts, and callouts to create an engaging
structure and improve readability by bringing attention to important considerations.
Careful attention to the structure can provide an easily read format that tells a story.
Benefits of White Paper as an Information Sharing Format
The literature supported the choice of a white paper as an efficient method to
share the policy recommendations derived from the project study’s findings. Sakamuro et
al. (2015) described the white paper as a specialized tool for disseminating information
about an identified problem to a targeted audience. Graham (2015), McKeon (2015), and
Stelzner (2010) furthered how this tool can be useful to recommend timely and
trustworthy solutions to decision makers. White papers are likely to enlist support for
education change initiatives as they can convince decision makers that implementing a
recommended action could work (Graham, 2013). The white paper promotes change by

136
connecting Astin and Antonio’s (2012) I-E-O theory, the study’s findings, and the critical
research literature to inform the content of the recommendations
The recommendations shared in the white papers can encourage change, but for
the findings from the study to have value, decision makers need to read it and use it. The
white paper can provide background of the problem and potential solutions to readers in a
visual format (Gordon & Graham, 2003; Graham, 2013). Educational stakeholders must
read and process large amounts of information prior to reaching a decision. The white
paper can list policy recommendations in an easy to read, time-saving format (Graham,
2013). Stelzner (2007) argued that variety, accessibility, and breadth make the format
versatile and useful. The white paper is an appropriate selection for the project because I
can structure the research and study findings in an engaging information sharing format.
The project provides a strong message that encourages college stakeholders to recognize
the importance of the data and value of the policy change recommendations regarding an
assessment of the IL requirement.
Research, Theory, and Results Support Content of Project
White papers demonstrate how research and theories interconnect to guide
adaptation to change, thereby keeping pace with educational demands. Through the
study, I located research identifying the issues surrounding the college’s problem of a gap
in targeted assessment including student preparation, IL requirement education delivery,
and general education requirement assessment. Using the I-E-O assessment framework, I
connected that research with the data analysis results from the study to substantiate the

137
recommendations I made for college policy and programmatic changes encouraging
timely implementation at no cost.
Before I introduce the recommendations that evolved from the study, I must
revisit the limitations of using survey methodology that I previously identified. These
limitations can cause uncertainty by attributing self-reported behavior or confidence
changes to involvement in the IL requirement. In the case of this study, I gave careful
attention to the design of the Information Literacy Requirement Impact Survey instrument
to ensure it satisfied all these conditions and included the use of peer expert and pilot
testing. Also, a large sample size was obtained, and I compared multiple analysis
techniques. Given these conditions, I made the assumption that the study’s use of survey
methodology was carefully executed, and the data analysis results can provide
preliminary support for policy change recommendations.
Student Preparation
The data analysis identified significant positive relationships between students
completing the IL requirement and their self-reported behavior changes and confidence in
subsequent courses. It suggests a need to modify registration practices to ensure students
enroll in the IL course in their first or second term. The first policy recommendation
relates to adjusting enrollment practices that can help underprepared students.
1. Use the study’s learner-centered self-reports to develop a process prioritizing
that the IL requirement is completed early in the general education
requirement pathway.

138
The results of the study supported this recommendation. The multiple regression
results showed the independent Inputs variables of age category, ethnicity, preparation
self-concept, and papers written were all significant in explaining portions of the
variability of three Outcomes variables. Higher levels of preparation were positively
associated with other student characteristics and with information evaluation behavior
changes and confidence levels. In the descriptive results, student responses indicated 87%
reported that taking the IL requirement course, LIR 10, in the first or second term of
college study would be the most helpful for writing papers and participating in
discussions in other courses. The data showed the importance of students completing the
general education IL requirement at the beginning of their time at the college.
The literature supports the notion that underprepared students benefit from IL
education. Bronstein (2014) studied the self-concepts students had of their information
seeking behaviors using an online survey. The findings showed that respondents’
indications of high levels of self-efficacy were significant on three scales with a
correlation between outcomes and age. Mulvey (2009) highlighted the problem of how
academically underprepared students can have skill levels well below their classmates
and the importance of support courses to bridge the gap. Varlejs, Stec, and Kwon (2014)
studied nineteen high schools and found that library resources were limited. As a result,
students were not receiving the IL preparation needed to be successful in college courses.
Roscoe (2015) shared how the numbers of underprepared Latino students were increasing
and that they had unique academic support needs that colleges need to address. Tovar
(2015) concurred that support programs had an impact on Latino/a community college

139
student’s success. Johnston, Partridge, and Hughes (2014) identified challenges that ESL
students experience with IL academic expectations of reading and understanding
information sources and gave insights into how educators can improve their approaches.
Kuh and Gonyea (2015) conducted an exploratory study of undergraduates’ use of the
library using self-reported questionnaire data from more than 300,000 students. Their
findings showed students’ experiences were related to engaging in academically
challenging activities requiring critical thinking and having more interactions with
instructors but not necessarily information literacy. They furthered the idea that the
library contributed to positive learning for students, especially underprepared students.
They advocated that colleges acknowledge the valuable role IL plays in developing
student’s information evaluation skills. Frost, Strom, Downey, Schultz, and Holland
(2010) advocated for colleges to provide more learning communities that can integrate
learning for students by connecting academic services such as library instruction.
Schnee’s (2014) qualitative study also supported the value of learning communities for
underprepared students. Cho and Mechur Karp (2013) found that students who participate
in academic support courses in the first semester, especially those underprepared, were
more likely to take classes in subsequent semesters. Gurantz (2015) studied California
community college registration priorities and student characteristics. Findings showed
that newer students tended to take more units but can become lost if a class is not
available. Findings also indicated that colleges should review policies, funding, and
staffing allocations to ensure that they offer sufficient courses in areas with impacted
enrollment. Adjusting practices to ensure students enroll in the IL requirement courses

140
early in their college experience could empower students with the knowledge and skills
needed for research success throughout their educational experience at the college.
IL Requirement Education Delivery
The study provided background evidence regarding the different IL education
delivery methods used by California community colleges. SRJC instituted the method of
a general education requirement course, LIR 10. The program characteristic measured in
the study’s survey examined the students’ reports of the formats and lengths of LIR 10 in
which they had participated. The data suggests that even though students reported feeling
prepared when they began the LIR 10 course, the course still positively affected their
frequency of information evaluation behaviors and levels of confidence. The second
policy recommendation relates to continuing to support the IL requirement.
2. Use the study’s findings as an indicator that the IL general education
requirement course is an effective delivery method that should be supported
ensuring that formats and lengths of course offerings meet the needs of all
student demographics.
The results of the study supported this recommendation. The self-reported
descriptive data showed the majority of students attributed their higher frequencies of
information evaluation behaviors and increased confidence levels to IL requirement
course, LIR 10. The study’s cross-tabulation results supported that finding showing the
LIR 10 format and length was positively associated with ethnicity, primary language, and
the number of papers students had written. The correlational analyses also showed the
LIR 10 format had significant positive relationships with the Inputs variables of the

141
English course level students had achieved, the self-concept of students’ information
evaluation preparedness and confidence levels. The multiple regression results showed
the Environment variable LIR 10 format was significant in explaining portions of the
variability of the frequency of information evaluation behaviors and confidence
Outcomes variables. Based on those results, I concluded that students consider the IL
general education requirement to be an effective delivery method and that it should be
supported by the college. The data results also provided insight for making future course
offering program decisions that match the needs of all student demographics. The
descriptive results indicated the students who identified ethnicity as Hispanic showed a
preference for LIR 10 courses with longer lengths and the on-ground format. Analysis
indicated that the majority of students, 53%, had taken LIR 10 in the shorter length (6
weeks) course, and 67% (n = 349) of students had taken the online format. The findings
show that more study is needed to make conclusions regarding the impact of format.
The literature supports that consideration should be given to policy decisions
necessary for continuing the IL requirement. Mayer and Bowles-Terry (2013) shared
ways that IL instruction can engage students in subject specific research papers and
projects. Schroeder and Cahoy (2010) articulated the affective learning and critical
thinking competencies that IL students must use in completing research assignments in
other courses. Cahoy and Schroeder (2012) further noted that affective IL learning is
often challenging to measure and that self-reports can be valuable when observation
research is not possible. They advocated that IL affective learning outcomes be included
in IL standards to have the most impact. Bryan (2014) studied the ACRL standards and

142
demonstrated that IL instruction supported many of the university’s critical thinking
outcomes. Hicks and Sinkinson (2015) also found a connection between critical thinking
and IL. They noted that faculty and librarians should partner to develop pedagogical
strategies to maximize student learning. Radcliff and Wong (2015) provided pretest and
posttest research to support the role that critical thinking plays in the IL by incorporating
argument learning outcomes into information evaluation. Hofer, Townsend, and Brunetti
(2012) observed that librarians can use the concept of learning thresholds to help
struggling students integrate IL instruction to encourage engagement with IL skills such
as information evaluation needed in other classes. Seeber (2015) also supported using IL
threshold concepts noting that changes in online searching technologies require students
to critically evaluate strategies and information sources like never before.
The literature supports the college considering programmatic decisions regarding
the formats and lengths of IL requirement course offerings. Cho’s (2011) research
showed that interaction in an online course can have a large impact on student
satisfaction. Nicholson and Galguera’s (2013) study considered the role that the Internet
place in literacies of all kinds. They stressed that online skills are essential for
engagement in educational work. They examined the use of social media and other
interactive online tools used by educators and found that although most students were
able to use them, some had significant challenges. Rao, Cameron, and Gaskin-Noel
(2009) shared the positive learning results they received from incorporating core
competencies, such as critical thinking, into an online class. Machado and Afonso (2015)
also stressed that interactivity influenced satisfaction and successful learning outcomes in

143
their study of a standardized test delivered to more than 5,000 South American students
in the online format. Simpson (2013) noted that graduation rates can be 20% lower in
online classes and stressed the need for student support to ensure success.
Support existed in the literature for the value of online learning. In an early study,
Lim, Morris, and Kupritz, (2006) found no significant differences in learning outcomes
between online and a hybrid/blended delivery formats. Clark and Chinburg (2010)
studied the learning outcome differences among undergraduate students who had
received library instruction in the online and face-to-face formats. A citation analysis of
term papers also showed no significant differences in research performance between the
online and face-to-face students. Wolff, Wood-Kustanowitz, and Ashkenazi (2014) used
regression analysis to examine how different community college student characteristics
affected performance in an online and face-to-face biology course. They concluded that
preparation characteristics and format of delivery had significant effects on course
completion and retention. Virtue, Dean, and Matheson (2014) pointed out the increased
use of online learning objects in education and showed the value of assessing student
learning in the format. Their study’s results showed that 96% of the students surveyed
reported increased understanding using the online tutorial. Silk, Perrault, Ladenson, and
Nazione’s (2015) had similar findings from a study that compared the effectiveness of
library instruction in the online and in-person formats. Although they showed differences
in research learning outcomes, they did not note any significant differences between the
course formats.

144
General Education Requirement Assessment
The study provided background evidence of the college’s assessment gap and best
practices for general education requirement assessment. The study’s results provide
implications for policy changes and remediation of the assessment gap problem by
assisting stakeholders with institutional decision making related to the assessment of the
effectiveness of the IL requirement using the alternate approaches and practices identified
through this study.
3. Use the Information Literacy Requirement Impact Survey to conduct targeted
assessment of the IL general education requirement on a 3-year basis and
correlate the findings with the college’s triennial Student Survey results.
The data analysis results of the study supported this recommendation. The crosssectional results from this study can form baseline data for such assessment. This study
used theory-based research methods to develop a targeted quantitative survey instrument.
The study’s descriptive and correlation results data regarding student and program
characteristics and self-reported behavior changes and increases in confidence suggested
that the IL requirement course had an impact. The Information Literacy Requirement
Impact Survey instrument, based on Astin and Antonio’s (2012) I-E-O assessment model,
was successful in collecting self-reported data specific to the IL requirement program.
The descriptive data from the study showed that 72% of the student responses (n = 380)
indicated that participation in the IL requirement was the origin of their critical analysis
knowledge and abilities measured by the college in its broad institutional assessment
measurement (SRJC, 2013d). These results relate to the college’s general education IL

145
requirement assumption that its Critical Analysis Institutional Learning Outcomes (SRJC,
2013b; see Appendix J) aligned with the national ACRL (2000) Information Literacy
Standard Three performance indicator outcomes (see Appendix I) and the IL requirement
course, LIR 10, course level outcomes (SRJC, 2013c; see Appendix K). The
documentation of these three measures demonstrates that they all examine the same
outcomes. However, that alignment had not been previously tested. The college’s broad
assessment of the Critical Analysis Institutional Learning Outcomes (2013b) through its
triennial Student Survey (2013d) did not differentiate whether the high critical analysis
learning gains that students reported were due to participation in the IL requirement
through LIR 10 or were gained through other means such as participation in English 1A.
The study’s targeted assessment results fill that gap and show that the IL requirement has
an impact.
The literature supports that consideration should be given to the augmentation of
assessment practices that can be implemented in an easy and timely manner incurring
little or no cost to the college. Given concerns such as Pascarella’s (2001) questioning of
the validity of self-reported learning gains, I will recommend the college also consider
supplementing the survey assessment with more objective measures. Pace (1985)
suggested using question scales and test-retest comparisons. Pascarella and Astin and
Antonio (2012) advocated that a pretest and posttest assessment instrument be used when
students begin college and again at the end of their educational experience. Porter (2011)
identified time-use diaries as a credible approach but cautioned that it was an expensive
and time-consuming method. Gonyea (2005) encouraged triangulating self-reported

146
survey data using multiple data sources. I recommend consideration of a supplemental
study using college records, such as grade-point-averages, of students who have taken the
IL requirement course and those who have not (Anaya, 1999). Finally, I recommend that
the college consider using a standardized programmatic IL skills assessment instrument
so assessment results can be obtained immediately after course completion and compared
across course sections (Oakleaf, 2014). A lack of research literature specifically related to
an assessment of a general education IL requirement existed. These recommendations
will encourage additional study, which will add to the literature on the topic.
The purpose of this literature review has been to place the project genre and
policy recommendations in the scope of the existing literature. This information will be
used to structure the white paper and provide evidence for the recommendations. The
study provided background evidence of the extent of the problem and issues surrounding
the gap in general education requirement assessment practices and research support for
the recommendations. That background evidence combined with the recommendations
will be used to ground the white paper and educate stakeholders about the implications of
the study (Gordon & Graham,, 2003). The research literature, theory, and study results
support the use of a white paper for policy.
Saturation Reached in Literature Review
This literature review focused on the development of the format, content, and
recommendations made in the white paper. I conducted an extensive review of the
educational literature using EBSCO, Sage, ERIC subscription databases, and a free
database, Google Scholar, as well as searches of the open web using Google. The

147
searches focused on the primary topic of white paper and yielded a limited number of
scholarly research items and references on the topic. The secondary searches for this
literature review included the following keyword terms and phrases: executive summary,
policy recommendations, dissertation writing, gray literature, program evaluation,
community college, education, workplace, evaluation research, education delivery,
general education program assessment, student involvement and engagement, InputsEnvironment-Outcomes assessment, and quantitative analysis. I initially limited the
search criteria to the last 5 years of publication. For topics that did not yield results, I
expanded the criteria to include the last 10 years of publication. I used scholarly
academic journal articles when available. Due to the lack of scholarly journal articles on
the topic of the white paper, I used a free web search to locate references. I used the same
criteria whenever possible. The literature review searches related to the recommendations
focused on the items most important to recommendations. These included student success
courses, first-year programs, underprepared students, remediation, developmental
education/college preparation, face-to-face, on-ground, online, embedding IL instruction,
general education, and Latino/Hispanic students.
Project Implementation Description
The dissemination plan for the project includes the distribution of the project in
three venues: email and presentations at the college, potential presentations at library and
education conferences, and attempts to publish in related journals. I require few resources
to disseminate this project. Possible barriers to implementation of the dissemination plan
exist, but I have identified potential solutions that should make the process go smoothly.

148
The dissemination plan’s goal is to initiate discussion and education of stakeholders to
encourage them to consider the study’s research and findings to inform future decision
making.
Potential Resources and Existing Supports
The most important resources I will need for dissemination of the project are time
and access to the identified audiences. The first dissemination venue is the college. The
goal of presenting this project to administrative and faculty decisions makers requires few
resources and primarily the use of existing supports. The only resource I require for
reaching the college audience is email, to share the project and to communicate requests
for time on the various meeting agendas to present to the designated groups. The college
has a robust shared governance structure of committees and councils, and I will use that
existing support. As a member of the college community, I have knowledge of the local
stakeholder groups within the existing support structure. I will present at the regularly
scheduled meetings of the multiple groups relevant to general education, future
assessment practices, and potential registration pathway changes. Administrative and
faculty stakeholder recognition of value is an important first step to information
acknowledgment and putting the project’s recommendations to use. The college
stakeholders are aware of the assessment gap problem, and provided strong support for
the study. For the other two dissemination venues, email again will be the only resource I
require. Potential conference presentations or locating publishing opportunities will
require time, networking, and outreach via email.

149
Potential Barriers and Solutions
The major barrier to implementing the project dissemination plan at the college
would be sharing the white paper’s recommendations with administrative and faculty
stakeholders if I am unable to persuade them to include the presentation on the
appropriate meeting agendas. A possible solution would be to network in advance and
share an overview of the project with graphs in the email request I send to the committee
leaders in charge of the agendas. A more challenging barrier to project implementation
may be the potential refusal of the administrative and faculty stakeholders to consider the
white paper’s findings and recommendations valid because they are based on survey data.
Despite the evidence to the contrary included in the project, some administrative and
faculty stakeholders may not consider survey data to be as valid as experimental data. A
possible solution would be to remind stakeholders of the college’s learner-centered
values and how survey data provides a method for the student voice to be quantified. In
addition, I will share Astin’s (1985) long history of using the I-E-O assessment model of
survey assessment in education and how regression analysis techniques can increase the
depth of survey measurements. I could also encounter resistance to the recommendations
of the project related to making a change from the current assessment practices for
general education requirements or to the idea of making a change to require completion
of the IL requirement in the first or second term of registration. I could address these
barriers by further discussion of the issues and recommendations. At each presentation, I
will provide opportunities for stakeholders to ask questions and to supply them with
further data as necessary. These opportunities will allow stakeholders to voice concerns

150
or support that I can use to continue networking at the college to improve the chances of
implementation of the project’s policy recommendations. Finally, negativity may be
encountered in the conversation process from individuals who may not see the value of
the IL requirement and do not consider the study’s data as sufficient evidence to change
their views. O’Banion (1997) believed for the change process to be successful,
stakeholders must embrace a common value in the educational reform process. This
project is only the first step in the process. Decisions and change within the shared
governance structure of community colleges happen slowly. Developing institutional
support for this project requires understanding key players and the value of the support
they bring to the decision-making process and knowing who may assist in influencing
change within the college. Active involvement of all stakeholders with a shared goal will
be necessary to achieve any sustainable change in the college’s assessment practices.
Implementation Proposal and Timetable
The white paper is the format I will use to disseminate the results of the study and
project to key college administrative and faculty decision makers responsible for the
general education program. The success of this project relies on the ability to implement
effective communication of the recommendations to these stakeholders. Distribution of
white papers has expanded to include both hard-copy and digital forms (Stelzner, 2010).
Graham (2015) noted that dissemination methodology should align with the purpose of
the white paper to reach the intended audience. The college values sustainability, so the
dissemination plan uses only email to reach the identified administrators, department
chairs, and key faculty. I will electronically distribute the white paper project in the

151
Portable Document Format (PDF) format to ensure that no digital compatibility issues
would evolve with reading it. I will create a short project introduction email message
indicating the purpose of the white paper report and indicating the results are from a
doctoral study of a local problem. I will also include a brief description of the project
study. The organized, concise format of the white paper, as well as the research-based
recommendations presented, have the potential to catch the attention of administrative
and faculty stakeholders and to effect a change in the college’s current assessment
practices of general education requirements. At the very least, the comments or questions
from readers of the white paper will frame the dialog for such change.
The dissemination goals for the project include presenting in person or via email
to the college’s stakeholders. The key stakeholders are the college’s administrative and
faculty decision makers who are responsible for academic programs, including the
general education requirements and student service programs that interface with the
requirements. I will send the project introduction email directly to the administrative
decision makers including the Vice-President of Academic Affairs and the Academic
Affairs Deans. In addition to the administrators, I will distribute the email to the
presidents and chairs of the identified shared governance committees and councils with a
request to be put on a future meeting agenda to present the project to their groups. These
groups include the Faculty Academic Senate comprised of elected members representing
all academic departments, the EPCC consisting of members responsible for the
coordination of college academic planning, the Academic Calendar/Registration
Committee responsible for structuring the college’s priority registration process, and the

152
Department Chairs Council comprised of the elected department leaders. I will also send
the email request to present the project to the chairs of relevant department level
stakeholders including the Library & Information Resources Department, the English
Department, and the Counseling Department. A week before I am scheduled to present at
a designated meeting, I will email the white paper to all committee and council members
to encourage informed discussion. At the meetings, I will present an overview of the
identified problem, insights gained from the results of the study, and recommendations.
The project’s recommendations focus on identifying changes needed in
assessment and registration pathway practices that have the potential to maximize
students’ learning gains and ultimate completion success. This presentation format will
provide the best opportunity to promote the recommendations from the project most
directly to key stakeholders. These conversations with key stakeholders will allow for
exploration of the study and data analysis that can identify where the organization stands
on committing to an implementation plan. In addition, through these dialogs I may
identify other stakeholders who may be able to provide additional support to ensure
further dissemination of the project.
For implementation of the conference presentations and article publishing, I will
determine the dates and locations of relevant conferences and statewide meetings and
send email requests to present. Finally, I will write an article for publication based on the
problem, findings, and recommendations outlined in the project. I will send publishing
requests to relevant library and education publications such as the Journal of Academic
Librarianship, Reference Services Review, Journal of Information Literacy,

153
Communications in Information Literacy, Journal of Education, The Journal of
Educational Research, Academic Senate for California Community Colleges Rostrum,
and the Community College Journal.
The proposed timetable for implementation of the project at the college venue is
one academic semester, approximately 4 months. This timetable allows sufficient time
for administrative and faculty leaders to schedule presentations at designated meetings. I
plan to coordinate the presentations noted previously within one month and complete the
presentations as soon as possible, depending on the timing of monthly and quarterly
scheduled meeting dates. The semester timetable will ensure that I can present to all
relevant administrative and faculty stakeholders in a manner that will best generate
discussions of the problem, issues, and recommendations from the study and to allow for
follow-up after asking for feedback. For the conference and article publication, I may
need additional time to submit and get the proposals accepted and then make
presentations, depending on the timetables of conference schedules. I intend to complete
and submit an article for publication within 6 months of project completion but am
unable to define a timetable for when the article will be published.
Roles and Responsibilities of Student and Others
As the researcher, I have the primary role of managing the dissemination plan for
this project. I will handle all communication with stakeholders via email along with my
presence at meetings and conferences. My responsibilities include the above-mentioned
email communications, being accepted onto appropriate committee meeting agendas, and
identifying potential conferences to present and publications to submit the article. The

154
only others I depend on are the designated committee, council, and department chairs
who would schedule the project presentation on their agendas.
Project Evaluation Plan
The inclusion of a structured evaluation plan is a critical project component to
ensure that communication of recommendations occurs. I chose to use a goals-based
evaluation for the project (Lodico et al., 2010). The purpose of this evaluation revolves
around measuring the effectiveness of how well I achieve the dissemination goals
outlined for the implementation of the project. The process of evaluating practices is
ongoing for educational institutions (Spillane, 2012). The use of the project’s defined
goals for evaluation purposes will keep the process focused and manageable. I can also
use the evaluation to determine whether any adjustments are needed to improve the
communication goals outlined. By making the evaluation of dissemination goals part of
the process, I will be able to collect immediate feedback from participants at meetings or
by email. Spillane noted that working within an organizational routine, such as the
college’s shared governance process, can be an effective method to transform work
practices. The largest gain that communicating the project provides is the application of
research to practice. For example, communicating the project’s recommendations to key
stakeholders may lead to the college identifying and implementing changes in registration
practices that could require students to complete the general education IL requirement in
the first or second term.
The dissemination aspect of the project will be evaluated. The plan for
dissemination involves presenting the project at college meetings, potentially presenting

155
at conferences, and potential publication of an article in Library or education based
academic journals. One way I can evaluate the effectiveness of the project will be to ask
the stakeholders to respond to the white paper with questions or comments. I will use an
informal request for evaluation to do that. When I present the white paper including the
problem, the study, and the recommendations to administrators, committees, and
departments, I will end with a call for responses and questions that includes contact
information to encourage further discussion. The nature of the feedback will reveal
whether the white paper has served its purpose in effectively communicating the results
to the appropriate stakeholders. Further dialog could then occur regarding the next steps
that should be taken based on the feedback obtained. Working closely with the
stakeholders will also help evaluate whether more research is necessary or desired
regarding general education requirement assessment practices at the college. Finally,
should I be successful in presenting the white paper at an academic conference, I will
evaluate that project dissemination by taking questions at the end and will include contact
information for further inquiries from conference attendees. If I am able to publish the
project as an article, contact information will be included to encourage feedback.
Project Implications
The project, in the form of a white paper, highlights the components of a study
designed to examine an assessment gap practices at a community college. The study
provided data to determine the impact of the IL requirement on students’ information
evaluation behaviors and confidence levels. The white paper will make recommendations
for improvement of existing assessment practices and encourage further research.

156
Local Implications
The project will inform local decisions by filling the assessment gap. It has the
potential to provide data relevant for IL education at other community colleges. The
white paper addresses a specific need at the college, that of revising an assessment
practice to one that could provide targeted learner-centered data. The data from this study
provides information that can be used at the local institutional level to evaluate
assessment practices and program effectiveness. The data has broader implications for
stakeholders seeking to learn more about how the general education requirement as an IL
education delivery method contributes to student involvement or engagement. The study
results can provide the first stage of data-collection in a longitudinal assessment plan.
Project implications to the college include the project providing information and
research findings relevant to the issues surrounding the college’s assessment gap
problem. Issues include increasing numbers of underprepared students who may not
possess IL skills and abilities, the IL education delivery method options available, and the
need to regularly assess general education requirements. The results of this study can be
used to inform stakeholders of a theory-based assessment method for evaluating the
impact of the IL requirement on student success. The policy recommendations will
provide faculty and administrative stakeholders with data they can use to make informed
decisions as they determine the effectiveness of the IL requirement. Moreover, Astin and
Antonio’s (2012) I-E-O assessment model can be used to show the college is complying
with meeting educational accreditation standards relating to information literacy. This
model may also provide information that could be useful for decision makers in regards

157
to continuing to support the general education IL requirement and supporting a
requirement that students take the course in the first or second college term.
The findings from this project are the groundwork from which administrative and
faculty stakeholders may design, present, and encourage changes to assessment practices.
Sharing the data results of this study may encourage college leaders to appreciate the
value of assessing general education requirements in a targeted way. The
recommendation that the college consider the use of Astin and Antonio’s (2012) I-E-O
assessment model as a general education assessment method can aid in assuring and
improving higher education quality. Although this single survey study may not allow
results to be generalized, administrative and faculty stakeholders can utilize the I-E-O
model for insight into the college’s assessment practices.
State and National Implications
This project has the potential to extend benefits beyond the local college campus
to the statewide community college system and the national community college system.
The study’s data will add to the IL education literature regarding the student development
impact of a particular delivery method. California community colleges have the unfunded
mandate of delivering IL education to their students. Most do so using one-hour
instructional sessions or by infusing the instruction into English or other discipline-based
courses. Information about changes in reported student behavior related to the critical
evaluation of information resulting from a required IL course could be useful (Kuh,
2008). An assessment of an IL education delivery method aligned with the ACRL (2000)
Information Literacy Standard Three (see Appendix I) could provide data of value to

158
community colleges nationally. The data has the potential to go beyond the local need to
inform IL education delivery at other California community colleges, as well as
community colleges nationally. Through the study’s self-reported findings, the IL
requirement was shown to be a successful educational delivery method. More California
community colleges may want to consider implementing an IL component within their
associate degree requirements. The project study has broader impact potential in that it
will add to the IL literature regarding the student development impact of this method of
delivery on students’ IL information evaluation behaviors. Also, the use of the I-E-O
assessment model in general education programs may enhance understanding of the
academic quality of requirements such as the IL educational program and provide a
stronger sense of the student development impact. Educators may use this study and
project as a reference for survey research relating to quality standards and students’
perceptions of quality. The study and project may also be used to encourage assessment
of IL educational methods, specifically at community colleges. Contributions to the
research literature add to the knowledge base and positively impact social change.
Social Change Implications
Even larger implications of the impact that IL education can have on students
exist. Gainer (2012) noted that IL education has social change implications. The potential
benefit for society based on the increased awareness of the impact of IL education
delivery is the effect IL education can have on students’ future workplace information
evaluation performance. The self-reported findings of this study showed IL education had
significant relationships with changes in student’s critical evaluation behaviors and

159
confidence. These information evaluation behaviors include those associated with
workplace performance and the civic collaboration essential to informed members of
society. A broader change implication lies in the fact that effective IL education can
prepare students with the information evaluation abilities and confidence needed for the
problem-solving and decision-making needed to be informed members of a democratic
society in regards to civic participation. The recommendations put forth in this project
support positive social change. Increasing students’ critical analysis skills early on may
lead students to complete higher levels of education, which in turn will help produce
more highly educated participants for the workforce and society.
Summary
The project, including policy recommendations presented in the form of a white
paper, evolved from the identified problem and findings of this research. The data
gathered from this study provided details regarding relationships among community
college student and program characteristics, and information evaluation behaviors and
confidence levels. These findings addressed the assessment gap at the college and formed
the basis for discussion and inquiry into the effectiveness of the general education IL
requirement. The findings also provided an example of the I-E-O assessment model in
action through the gathering and analyzing of data to assess a general education
requirement. Evaluation and implementation of the study project ensure that the
recommendations reach key stakeholders. The project was designed to make policy
recommendations that would inform decision making and lead to a stronger long-term
data-driven assessment process. The implementation plan will bring awareness to the data

160
relationships from the project to gain administrative and faculty stakeholder support and
encourage discussion as to the possibility of future changes in assessment practices. This
study has local, state, and national as well as social change implications. Locally, the
study fills a community college’s need for assessment data regarding the student
development impact of a particular IL education delivery method. This data has the
potential to go beyond the local need and inform IL education delivery at other California
community colleges, as well as community colleges nationally. Lastly, an even larger
social change implication showed the impact of IL education delivery can affect students’
future workplace information evaluation performance and future levels of civic
participation. In Section 4, I reflect on the project’s strengths and limitations, its
importance, broad implications, and recommendations for practice and further research.

161
Section 4: Reflections and Conclusions
Introduction
The problem addressed in this study and project was a gap in the targeted
assessment practices of a community college related to the effectiveness of a general
education IL requirement. This section provides information regarding the project’s
strengths and limitations in addressing that problem including reflections on the
importance of this work and recommendations for alternative approaches to the problem.
This section also covers what I learned about scholarship, project development and
evaluation, and an analysis of myself as a practitioner, project developer, and leader. In
this section, I discuss the project’s potential implications for social change at the
individual, local, and societal levels. The section concludes with potential project
applications and recommendations for future research and study.
Project Strengths & Limitations
Creating a white paper was the project I selected for communicating policy
recommendations to the college. A white paper offers several strengths and few
limitations in addressing the problem of a gap in the targeted assessment of the college’s
general education IL requirement.
Strengths
A white paper can inform and initiate discussion among administrative and
faculty stakeholders regarding the college’s assessment gap problem and the
recommended solutions derived from the study. The structure of the white paper supports
that goal and can be used to help administrative and faculty stakeholders solve an

162
educational problem. The white paper can be an efficient method of informing
stakeholders about the three issues surrounding the problem of the college’s IL
requirement assessment gap, the findings of the study, and the policy recommendations.
The project’s clear and engaging blocks of text, images, and charts can provide
information in a time-saving format for stakeholders to get information about the
problem’s issues.
I based the project’s recommendations on learner-centered, assessment data. The
volume of these data reflected students’ willingness to participate in the survey research
study. As stakeholders seek to solve the assessment gap problem, they can potentially use
that data and the resulting recommendations in multiple ways for programmatic decisionmaking. For example, a substantial proportion of students, 26% of those eligible,
responded to the survey. Students’ self-reports regarding their learning experiences in the
IL requirement course can be useful in helping stakeholders determine the effectiveness
of the program. The white paper can show what students feel is working in the program
and what might need improvement. These self-reports about students’ successes in
developing the Outcomes of information evaluation behavior and confidence ultimately
provided support for the recommendations for policy solutions. Student feedback formed
the foundation for understanding the issues. For example, strong student feelings existed
regarding the most helpful time frame for being exposed to the critical information
evaluation knowledge attributed to the IL requirement course. The white paper can
provoke discussion about requirement pathways for student completion of the IL

163
requirement, thereby increasing the potential for student success in writing papers and
participating in discussions in other courses while students are at the college.
Presenting the research findings to college administrators brings their awareness
to the important part this course has played in promoting academic success and may lead
to modifying current assessment practices. This project serves to promote understanding
of IL education practices that impact students’ success and provides administrative and
faculty stakeholders with increased understanding of how to best assess the effectiveness
of the general education program. The white paper can educate stakeholders about the
usefulness of theory-based policy recommendations that address the college’s need for
regularly targeted assessment of the IL general education requirement. In addition, the
white paper draws attention to the researcher-developed survey instrument that can be
used to assist administrators with their required reassessment practices.
The project helps to bridge the communication gap with administrative and
faculty stakeholders and provides them with the opportunity to develop an understanding
of the purpose and need for regular, targeted, student based reassessment of general
education requirements. The information gained from the study’s research and results
data can motivate stakeholders to implement effective reassessment strategies revealed by
the analysis of the study’s data. Also, I can use the white paper as the foundation for an
article that shares the body of knowledge concerning assessment of a general education
IL requirement using students’ self-reports. Finally, this project has value because it
exemplifies how positive results can occur through careful research practices.

164
Limitations
The white paper has some limitations in its ability to inform and initiate
discussion among administrative and faculty stakeholders. The white paper structure of a
minimum of 12 pages may be too long for busy stakeholders to read. Even though the
content is broken up into chunks of focused text, some stakeholders may not take the time
needed to read the entire document. The project may not contain enough images and
charts for the reader who prefers visual information. Although the Portable Document
Format (PDF) format is compatible across computer platforms and useful for printing, it
can be cumbersome to use. Also, the PDF format is static, and some may prefer a more
interactive reading experience. Overall, the presentation structure chosen for the white
paper may be too passive to initiate excitement and discussion about the identified gap in
targeted assessment practices problem. Beyond the potential problems with the structure,
the writing itself could be a limitation. I wrote the background of the problem and the
policy change recommendations concisely and the information may not be detailed
enough for some readers to understand the context. The dissemination methods for
sharing the project could also pose some limitations. The plan to email the white paper
directly to administrative and faculty stakeholders could fail if they do not read the white
paper. I may be unable to persuade stakeholder committee chairs to include the
presentation on the appropriate meeting agendas. The possibility existed that none of the
stakeholders will respond with any comments or questions thus limiting the dialog for
discussing an institutional change.

165
Possible limitations of validity could arise if administrative and faculty
stakeholders question the self-reported learner-centered assessment data. The potential
refusal of the administrative and faculty stakeholders to consider the white paper’s
findings and recommendations as valid existed because they are based on survey data.
Some stakeholders may not consider survey data to be as valid as experimental data.
Also, the uncertainty existed of a researcher attributing student critical evaluation
development to participation in the IL requirement. Indicators of IL information
evaluation behavior changes such as increased research confidence or increased
involvement in course discussions are challenging to measure definitively. Those
developmental changes could be attributed to extraneous variables such as other courses
in the general education curriculum having related research course learning outcomes.
The study’s use of a survey research design posed limitations specific to that
methodology contributing to possible project limitations. Participants provided selfreported data that could not be substantiated. Students taking a self-administered survey
could have responded dishonestly or may not have fully understood the closed-ended
questions. A limitation inherent in survey research studies is that the results are not
generalizable. However, it was not the purpose of this study to generalize results to the
target population or the larger California community college setting. Nonresponse bias
can affect the process of generalizing results. Members of the target population might
have had numerous reasons for participating in an online survey that could introduce bias
from respondents having particularly strong opinions one way or the other regarding the
research. The high response rate lessened the potential for response bias, but even so, a

166
survey study will not provide evidence of the causal relationships needed for making
generalizations. The survey required students to identify a number of individual
characteristics. Because the surveys were anonymous, I was unable to verify the accuracy
of the self-reported student characteristic data using institutional records.
It is possible that the stakeholders may be resistant to changes in assessment or
registration practices such as those recommended by this project. Thompson (2010) noted
that the field of education undergoes constant reform that causes leaders of change to
have to work hard to get stakeholders attention. Decisions and change within the shared
governance structure of community colleges happen slowly. The proposed dissemination
timetable of four months for implementation of the project could be unrealistic and may
require adjustments if the dialog takes longer than expected.
Recommendations for Alternative Approaches
This study’s learner-centered survey assessment provided valuable data as an
approach to addressing the identified problem. I chose to use a survey collecting selfreported data as a method to assess the impact of participation in the IL requirement
program. This data could then be used by the college to judge the effectiveness of its IL
general education requirement. I conducted the study at the end of one 16-week semester
and included only the students enrolled during one calendar year.
I also considered alternative ways of addressing the problem. Alternative
methods could be to collect student feedback from a longer period or to increase the
number of participants invited to take the survey. For example, the college could develop
an alternative using a biyearly time frame for reassessment to include more students.

167
Another alternative could be for the college to use time-studies to allow students to
reflect on their learning experiences along the way. An additional alternative approach
could be to develop assessment instruments such as a pretest and posttest to obtain a
measurement of students’ IL skills and abilities. This approach would involve collecting
data from students early in their college journey and then again at the end. An assessment
such as this could more objectively measure student development. Another alternative
approach could be to for the college to compare the GPA of students in two groups, those
who took the IL requirement and those who did not.
Scholarship, Project Development, and Leadership and Change
The sections below present details regarding the personal learning and growth I
experienced specific to the research and development of the project. I reflect on the
change leadership experience I gained as a scholar, practitioner, and project developer.
Analysis of Self as Scholar
As an educator in the dynamic information studies field, it is vital to stay current
and informed on effective educational delivery practices. I found the learning I gained
through the review of literature invaluable. I built upon prior knowledge and broadened
the scope of what constitutes effective educational practice. Thompson (2010) observed
that the term scholar represents an individual who practices inquiry to use research and
theory to increase knowledge. Through interacting with a large number of peer-reviewed
scholarly works, I learned the importance of a disciplined academic study. Through
developing and conducting the study, I learned the importance of scholarship resulting
from a high-quality level of research on which to ground the resulting project.

168
This project study provided the opportunity to practice research skills. As I
explored potential education problems at the college, I immersed myself in research,
seeking out literature about information literacy, assessment, policy development, survey
methodology, evaluation practices, and related topics. After preliminary research, I
decided to design a study to assess student Outcomes of behavior and level of confidence
changes having the potential to provide assessment data related to the effectiveness of the
IL requirement program. I then collected relevant literature based on the issues of student
preparedness, IL delivery methods, and program assessment practices. I also used the
literature for learning about the statistical testing techniques needed for analyzing the data
I collected. I believe a high level of scholarship was essential to help identify the
significant relationships that existed between the dependent and independent variables
used in the study. It was necessary to use research-based literature to develop a white
paper project to communicate the research and findings of the study and to support the
resulting recommendations addressing the problem.
As I began to realize the degree to which the literature and data guide research,
the scope and direction of my scholarship changed. Steven-Long, Schapiro, and
McClintock (2012) noted that scholars experience multiple learning gains beyond
developing intellect as they seek to understand a problem and connect data to theories
and practices. I focused on applying my skills to understand more deeply the issues
influencing the assessment of student information evaluation learning gains, behavior
changes, and confidence levels. Steven-Long et al. stressed the importance of scholars
seeking multiple perspectives and utilizing theoretical concepts. Thompson (2010) agreed

169
that grounding scholarship in theory is key to understanding educational problems. I
learned that academic scholarship encouraged depth of research. I considered possible
descriptors and combination of terms to search the literature involved in this study and
project. The knowledge and vocabulary I gained through this process have further
developed my ability to communicate problems and solutions to stakeholders by
researching and communicating the best practices and most useful theories for assessing
student IL behavior changes.
Analysis of Self as Practitioner
As a practitioner, I have worked in the field of information literacy instruction for
many years as a librarian and an instructor. Thompson (2010) noted that the word
practitioner describes a person engaged in a profession who uses theories. I applied
practical experience with information literacy and educational theories as a foundation
for constructing the study. As a reflective practitioner, I have been able to consider a
problem, apply current knowledge, and develop new understandings that I can apply to
create change. Steven-Long et al. (2012) noted the value of combining practical
experience and research within a theoretical framework for influencing individual, local,
and societal change. I evolved professionally and as a practitioner as I actively engaged
in the process of identifying the targeted assessment gap problem at the college. That
evolution expanded as I learned the process of finding potential solutions to the problem
and began doing a study. The results of the survey I conducted provided the information I
needed to be a leader-practitioner. I used that knowledge to develop a focused set of
recommendations.

170
This project study is an example of my personal growth as a practitioner. I applied
what I learned through the literature review and data analysis to a real world problem
aligning practices with institutional needs. As a practitioner conducting research for this
study, I adhered to the highest code of conduct and strictly followed Walden IRB’s
guidelines and suggestions. I applied procedures that ensured that I gather valid, reliable
data to effectively answer the study’s research questions. I learned how to utilize data
collecting and statistical software to assist in analyzing large volumes of data.
Throughout this study, I dedicated myself to obtaining first-hand experience researching
and applying current research to investigate an educational problem in an area in which I
possess practical knowledge. Steven-Long et al. (2012) noted that being a scholarpractitioner involves being an active participant in research to improve educational
practices. Through the process of inquiry required as I conducted this study, I have
become skilled as a scholar-practitioner capable of leading change in the college’s
required assessment practices (Schultz, 2010). The process of researching literature and
using a theoretical framework have contributed to my ability to make reliable policy
change recommendations to the college administrative and faculty stakeholders.
According to Thompson (2010), combining research grounded in theory with practical
understanding of working in the field provides a scholar–practitioner with unique insights
as an educational leader. I believe this experience will be useful as I use the white paper
project to the motivate policy changes in general education assessment practices.

171
Analysis of Self as Project Developer
As a project developer, I utilized leadership skills throughout the study. A leader
makes informed decisions based on extensive research and practical experiences. Hattie
(2015) described a leader as a change agent who focuses on the impact of an educational
program, effectively communicates, challenges self and others, and embraces errors as a
way to learn. One of the most important facts that I learned about project development
was that a plan for change should derive from the research, theory, and findings. When
advocating for change in educational assessment practices, a practitioner needs to
determine that the change is possible so the new practice can become commonplace.
Thompson (2010) noted that effective project management for leading change requires
understanding and inclusion of the entire system in the plan. The study stressed that a
leader should consider how changes made in one program affect other programs.
I believe that effective project development depends on personal involvement in
sharing information and also self-evaluation. I was careful to incorporate only researchbased practices and cautiously analyzed findings into the policy recommendations. Hattie
(2015) observed that evidence is vital to demonstrate that a new practice can have an
impact on the effectiveness of the educational program. I designed the project using
theory and research to frame the problem, carefully collecting data to get learner-centered
reports, and then applying more research to support the project delivery method and
policy change recommendations. I learned that implementation of a project requires not
only analysis to determine if the project addresses the problem but also includes an
evaluation plan to assess its effectiveness in meeting the intended goals. The

172
recommendations in the project offer potential solutions that best fit the college system,
and that the college can implement with only slight modifications to the assessment
practices currently in place.
While developing the implementation plan for the project, I gained experience in
using a leadership approach. I determined how to convey the study’s findings to motivate
stakeholders to want to change assessment practices. I realized the importance of
obtaining institutional and colleague support to be able to motivate changes in assessment
practices. Thompson (2010) noted the importance of a leader creating a climate of trust
through communication to support stakeholders through policy changes they may choose
to implement. Thompson also stated that obtaining a commitment from stakeholders is
critical to sustaining any system changes. Completing this study and presenting the
findings to college stakeholders does not guarantee assessment practices will change.
Stakeholders may not want to let go of the time and experience they have invested in
mastering the old practices. As a scholar-practitioner, I will use a leadership approach to
show administrative and faculty leaders how the results of this study relate to the
importance of regular targeted assessment practices. Obtaining a commitment by sharing
research and data can help college leaders understand the problem and motivate them to
make informed policy decisions.
Reflection on the Importance of the Work
This project study experience has strengthened my intention to engage in research
efforts that positively impact student development. In my capacity as a scholarpractitioner and project developer, my research and leadership efforts will continue to

173
focus on assessing student learning experiences to advocate for practices that will
contribute positively to increasing students’ and the college’s academic success rates.
Providing reliable data that the college can use for assessing the effectiveness of a general
education requirement shows the importance of this study. Through this project study, I
became a scholar capable of contributing to social change. The research skills acquired
through this study and project helped to focus and coordinate my knowledge, experience,
and practice. As a result, I hoped to motivate college administrative and faculty
stakeholders to recognize the impact of IL education on students’ affective behavioral
changes and level of confidence growth in ensuing courses. This learner-centered survey
study provided quantitative research results that advanced practical application of the
resulting policy recommendations related to assessment practices in the college setting.
The project, as a product, can be used by administrative and faculty decision makers at
the local institutional level to evaluate the college’s assessment practices impacting
student achievement and ultimately program effectiveness. The data is of broader
importance to the scholarship of students related to how the IL requirement as an IL
education delivery method contributed to their engagement and success in other college
courses. Finally, the study has the potential to provide the first stage of data collection in
a long-term longitudinal assessment plan.
Implications, Applications, and Directions for Future Research
A scholar-practitioner can influence social change on many levels. The white
paper project has the potential for impacting positive social change at the individual,
organizational, and societal levels.

174
Social Change at the Individual Level
This study project experience has strengthened my intention to engage in research
efforts that positively impact student development. As a continuation of my role as a
scholar-practitioner and project developer, my research and leadership efforts will
continue to focus on assessing student learning experiences, In this way, I can inform and
support practices that positively contribute to increasing students’ academic success rates.
Emerging Internet technologies created an educational paradigm shift requiring
increased student capacity to analyze new information for quality and relevance.
Effective IL education helped students gain the information evaluation skills and abilities
needed for problem-solving and to make informed decisions (York, 2013). Students
recommended receiving IL education early to promote success in completing papers and
participating in discussions in other courses. A prospective outcome of this study is the
development of students who are better prepared to critically analyze the kinds of
information needed in both academic settings and the workplace setting.
Social Change at the Organizational Level
The primary implication for institutional social change resulting from this project
is the application of these findings, evidenced by implementing data-based decisionmaking processes relative to future assessments of general education requirements. In
light of the research and study data regarding IL education delivery methods and
student’s IL critical analysis behaviors, assessors of the effectiveness of delivery methods
need to take these behaviors into account. Using the white paper project, I have offered
the I-E-O assessment model and the Information Literacy Requirement Impact Survey

175
instrument as a solution to the college’s gap in practice in the hope that it will positively
impact future assessment practices at the college. Schultz (2010) noted that if
stakeholders can agree on implementing assessments that combine theory and practice,
the result will be thriving learning environments. The intended impact is greater than just
a one-time assessment. The additional value lies in educating stakeholders about the I-EO assessment model with the goal of shaping programmatic decisions, thus enhancing the
effectiveness of future assessment processes using student’s reports of success.
The study and white paper project provide data that the college can use to assess
the student development impact of its IL education program. College stakeholders were
considering dropping the general education IL requirement without considering data
indicating whether or not it had a positive impact on students’ critical analysis learning
gains. The self-reported data derived from this project study suggests that the general
education IL requirement delivery method is promising for impacting the IL critical
analysis behaviors and confidence of community college students. A data-driven targeted
assessment will ensure the college is presenting its students with the most effective
program possible to develop IL critical evaluation skills and knowledge. These IL critical
evaluation competencies help ensure that students can efficiently and effectively obtain
and use information to solve academic research problems. The findings will inform the
college’s development of any future IL education assessment projects. The data obtained
from this study and presented in the white paper can assist stakeholders in making
informed decisions about assessment practices.

176
Social Change at the Societal Level
The study and project have state and national implications and applications. The
study fills a community college’s need for assessment data regarding the student
development impact of a particular IL education delivery method. This data has the
potential to go beyond the local need to inform IL education delivery at other California
community colleges, as well as community colleges nationally. An even larger social
change implication exists as the impact of IL education delivery can affect students’
future workplace IL information evaluation performance and civic participation.
The study project’s data will add to the IL education literature regarding student
development impact of a particular IL education delivery method. Shipan and Volden
(2012) introduced the importance of the concept of policy diffusion and how policies
spread between institutions. They noted that scholar-practitioners participating in policy
advocacy at their community colleges were in a perfect position to encourage broad
policy change across the system between institutions. California community colleges
have the unfunded mandate of delivering IL education to their students. Most do so using
one-hour instructional sessions or by infusing the instruction into English or other
discipline-based courses. Kuh (2008) reports that information about changes in student
behavior resulting from a required IL course could be useful. An assessment of an IL
education delivery aligned with the ACRL (2000) Information Literacy Standard Three
(see Appendix I) provides data that can be of value to community colleges nationally.
Educators may use this project study as a reference for survey research using the
I-E-O model. The project study could also encourage ongoing assessment of IL

177
education, specifically at community colleges and related to quality standards and
students’ evaluation of information quality. Not all students come to the community
college prepared with information evaluation skills. Required IL education can provide a
way for all students to gain key critical analysis skills. This has the potential to provide
equality in the classroom and increase marginalized student success in other courses
requiring research (Schultz, 2010). Given the impact IL has on critical evaluation skills
and those associated with civic collaboration among informed members of society, IL
also has social change implications (Everett, 2015; Gainer, 2012; Monge, & FrisicaroPawlowski, 2014).This study project’s potential is multifaceted. It will inform local
decisions by effectively filling the assessment gap and could provide useful data for IL
education at other community colleges. It will also add to the IL literature regarding the
student development impact of this method of delivery on students’ IL information
evaluation behaviors.
Contributions to research literature in this area can add to the educational and
critical analysis knowledge base that can beneficially impact social change. The IL
literature contained numerous studies and reports addressing the importance of IL skills
and abilities in workplace settings beyond the community college. Bird, Crumpton, Ozan,
and Williams (2012) conducted a survey of alumni to determine what development of IL
competencies had on their performance after graduation. Their findings showed that
students identified advanced navigation and evaluation skills as the IL competencies most
relevant to them (Bird et al., 2011). Sokoloff (2012) conducted a study of workplace
managers who identified the ability to critically evaluate information and use it for

178
recommendations and decisions as most valuable. Travis (2011) studied transferability of
IL skills to the workplace and found that respondents credited their IL skills for obtaining
their jobs. Head’s (2012) PIL study on IL in the workplace included a focus group of
recent graduates from the college. The report’s findings indicated that graduates found
the IL workplace challenges to be urgent deadlines and vague research assignments with
minimum feedback (Head, 2012). Employers in the same study listed unmet expectations
as challenges, including the employee’s inability to conduct research as a member of a
team and lack of persistence in digging deeper for solutions (Head, 2012). Monge et al.
(2014) concluded IL skills are vital in information-based workplace environments. These
studies illustrate how positively students and employers view IL knowledge and skills
and the importance of including IL education at a community college as a community
benefit.
Methodological and Theoretical Implications
This study goes beyond simply adding to the literature regarding IL education. It
has the methodological implication of providing a carefully created and tested survey
instrument that can be used by future researchers to collect student self-reports of
information evaluation behavior and confidence changes. The study also has the
theoretical implication of adding to the literature showing a survey design based on Astin
and Antonio’s (2012) I-E-O assessment model to study students’ self-reported
information evaluation development.

179
Recommendations for Practice
This study’s research and findings have several practical applications for the
college. The white paper lists these applications as recommendations for practice. I
developed these recommendations for programmatic and policy changes based on the
issues identified in this study including student preparation, the IL requirement as an
education delivery method, and general education requirement assessment. The college
can apply these recommendations as solutions for its targeted assessment gap problem by
using the study’s findings to evaluate and enhance the effectiveness of its IL requirement.
1. Use the study’s learner-centered self-reports to develop a process prioritizing
that the IL requirement is completed early in the general education
requirement pathway.
2. Use the study’s findings as an indicator that the IL general education
requirement course is an effective delivery method that should be supported
ensuring that formats and lengths of course offerings meet the needs of all
student demographics.
3. Use the Information Literacy Requirement Impact Survey to conduct targeted
assessment of the IL general education requirement on a 3 year basis and
correlate the findings with the college’s triennial Student Survey results.
The IL requirement effectiveness research should not end with this project study.
More research is needed to evaluate the effects of the IL general education requirement
on students’ critical analysis learning outcomes.

180
Recommendations for Future Research
To fully investigate if the IL requirement positively contributed to student
development, additional research is needed. More assessment will be essential for
administrative and faculty stakeholders to evaluate the IL requirement’s effectiveness.
1. Conduct longitudinal or repeated survey research on the general education IL
requirement using the Information Literacy Requirement Impact Survey
instrument to increase the reliability and validity of this study’s results.
2. Use Astin and Antonio’s (2012) I-E-O assessment model to assess other
general education requirements.
3. Conduct additional data collection related to the timing of when in their
academic career students should complete the IL requirement to provide
insight into the optimal timing for delivery of IL education to increase the
requirement’s usefulness for students.
4. Conduct a case study of a research intensive course comparing students selfreports to instructor reports of information evaluation measures and
triangulating those results with the students’ grade data.
5. Use objective data collection methods such as comparing the grade-pointaverages of groups of students who took the IL requirement course with those
who did not.
6. Use a standardized IL skills assessment instrument such as a pretest and
posttest to collect assessment results immediately after course completion and
compare them across courses.

181
7. Conduct qualitative research such as interviews or focus groups to provide a
rich data set of learner-centered comments to validate whether or not the IL
requirement continues to meet students’ and the institution’s needs.
Continuing assessment of the IL requirement using the I-E-O assessment model
can help stakeholders understand how the independent variables of demographic and
preparation characteristics, and the effects of the IL education delivery method impact
students’ affective information evaluation behaviors and confidence in other courses.
Conclusion
This quantitative research study and white paper project outlined a local
educational problem; the lack of targeted assessment practices at a community college.
The study examined relationships between student characteristics, aspects of the general
education IL requirement, and subsequent frequency of student use of IL critical
information evaluation behaviors, confidence in writing papers, and participation in
discussions in other courses. The study and white paper summarized the research and
associated findings and made recommendations related to future targeted assessment of
the impact of the general education IL option for equipping students with the information
evaluation knowledge and skill set needed for successful involvement in other courses.
The study provided recommendations for future research and an assessment instrument
and method I recommended for future use. The relationships identified in the research
provided the basis for the recommendations I offered in the white paper.
The Information Literacy Requirement Impact Survey involved assessment of
student self-reports. The study’s findings provided supportive data that key stakeholders

182
can use to gauge the effectiveness of the general education IL requirement as an
educational delivery method. Using a survey aligned with the learner-centered mission of
the college, I obtained relevant findings supporting the effectiveness of the IL education
delivery method. Because the study was successful, the survey produced an instrument
that the college can reapply in subsequent years to see how Outcomes change and how
the college can apply changes to the LIR 10 course to help meet ongoing needs.
The study’s assessment produced results from three types of data analysis:
descriptive, correlation, and multiple regression. The subsequent findings provided data
to support a recommendations type project. The descriptive data provided a profile of the
students, and the univariate analysis showed that the majority of students reported that the
IL requirement had a favorable impact on their information evaluation behaviors and
confidence. The cross-tabulation and correlation analyses provided bivariate data that
showed all variables in the study had one or more significant relationships with other
variables measured, except for gender, and thus I used them in the regression analysis.
The multiple regression provided multivariate data that answered the study’s research
questions and hypotheses and showed that some student (Inputs) and program
(Environment) characteristic variables were significantly related to the Outcomes
variables. Results of data analysis justify the identification of the college’s assessment
gap problem and the need to share and further evaluate the target specific indicators of
the program goals and objectives. The overall results revealed that students reported
favorably on their IL learning experiences in the IL delivery system.

183
Astin and Antonio’s (2012) I-E-O assessment model framed the study and the
white paper by emphasizing the relationship patterns. It provided the background of the
problem showing the preparedness issues surrounding student characteristic Inputs, the
IL education delivery methods characteristic of the Environment, and the students’
critical analysis behavior and confidence Outcomes that will fill the college’s gap in
general education assessment practices. The study results were the center point of the
white paper providing informative background of how the data collection and analysis
results answered the research questions regarding the relationship between the student
characteristics, the IL requirement and students’ reported feelings of critical information
evaluation development. The recommendations are the white paper’s cumulating point.
This project builds upon the SRJC (2013b) Institutional Learning Outcomes
assessments currently practiced by the college but gives targeted information specific to
the IL requirement program that can inform decision makers about the contributions of
this specific general education requirement. The study’s results presented data that
stakeholders can use as a starting place for an improvement of the college’s assessment
practices. The study also provided theory-based evidence that the college can use to
assess the effectiveness of the general education IL requirement.
The findings from this research study and project have the potential to support
social change in the education community. A college that can offer effective information
literature education influences societal change by promoting student critical analysis
behaviors that impact students’ ability to think critically in their other courses, workplace,
and civically. Promoting student success and providing higher level skilled workers to

184
compete in a global workforce enhances the potential for positive societal change. If the
study’s findings are effective in garnering the college’s support, individuals and
educational organizations could benefit. This study offers a starting place for educational
decision makers to best approach future assessment practices. Stakeholder feedback will
be invaluable for assessing this project and informing future research. The education
community can be especially resistant to change, and this may prove to be true for the
changes in assessment practices recommended by this project. Through the practices of
feedback, process revisions, and success stories, the college can accept these changes.
My scholar-practitioner journey does not end with the completion of this study
project. Completing this project provided the opportunity to place research into practice
by promoting more focused assessment opportunities. These opportunities could
contribute to social change by helping students obtain timely skills needed to succeed
academically, and qualify and compete for higher-level jobs within the community.
Implementing program change requires leadership at all levels of the institution.
The leadership of a scholar-practitioner can benefit the college by identifying a problem
such as the need to assess how community colleges monitor general education
requirements and actively seeking solutions that can result in effective changes. Leaders
also campaign to obtain support for institutional change among key stakeholders. Taking
a leadership role will help monitor the outcome of changes to practices as well as provide
an opportunity to continue researching the effectiveness of the modified practices. As a
result of the work and data from this study and project, I have acquired the knowledge
required to lead a drive for change at the college, hence becoming a leader for change.

185
References
Academic Senate for California Community Colleges (ASCCC). (1998). Information
competency in the California community colleges. Retrieved from
http://asccc.org/node/174895
Accrediting Commission for Community and Junior Colleges (ACCJC). (2014).
Accreditation standards. Retrieved from http://www.accjc.org/wpcontent/uploads/2014/07/Accreditation_Standards_Adopted_June_2014.pdf
Anaya, G. (1999). College impact on student learning: Comparing the use of selfreported gains, standardized test scores, and college grades. Research in Higher
Education, 40(5), 499–526.
Anderson, M. (2013). Beyond the White Paper: Videos and slideshows are taking center
stage in presenting technical information. IEEE Spectrum. Retrieved from
http://spectrum.ieee.org/at-work/innovation/beyond-the-white-paper
Andrews, C. R. (2012). Libraries and general education: New strategies to enhance
freshman orientation, faculty collaboration, and curriculum development.
International Journal of Learning, 18(5), 109–131.
Artman, M., Frisicaro-Pawlowski, E., & Monge, R. (2010). Not just one shot: Extending
the dialogue about information literacy in composition classes. Composition
Studies, 38(2), 93–110.
Association of College & Research Libraries (ACRL). (1989). Presidential committee on
information literacy: Final report. Retrieved from
http://www.ala.org/acrl/publications/whitepapers/presidential

186
Association of College & Research Libraries (ACRL). (1998). A progress report on
information literacy: An update on the American Library Association presidential
committee on information literacy: Final report. Retrieved from
http://www.ala.org/acrl/publications/whitepapers/progressreport
Association of College & Research Libraries (ACRL). (2000). Information literacy
competency standards for higher education. Retrieved from
http://www.ala.org/acrl/sites/ala.org.acrl/files/content/standards/standards.pdf
Astin, A. W. (1985). Achieving educational excellence: A critical assessment. San
Francisco, CA: Jossey-Bass.
Astin, A. W. (1999) Student involvement: A developmental theory for higher education.
Journal of College Student Development, 40(5), 518–529.
Astin, A. W., & Antonio, A. (2012). Assessment for excellence: The philosophy and
practice of assessment and evaluation in higher education (2nd ed.) Lanham,
MD: Rowman & Littlefield Publishers.
Bird, N. J., Crumpton, M., Ozan, M., & Williams, T. (2012). Workplace information
literacy: A neglected priority for community college libraries. Journal of Business
& Finance Librarianship, 17(1), 18–33.
http://dx.doi:10.1080/08963568.2012.630593
Blaikie, N. H. (2003). Analyzing quantitative data: From description to explanation.
London, United Kingdom: SAGE Publications Ltd.
Boekhorst, A. K., Farace, D. J., Frantzen, J., (2004). Grey literature survey 2004: A
research project tracking developments in the field of grey literature. Retrieved

187
from http://www.nasponline.org/publications/spr/pdf/spr373gregory.pdf
Boktor, J. (2013). White Paper. Retrieved from
https://sites.google.com/site/hboktorportfolio/whitepaper
Bodi, S. (1988). Critical thinking and bibliographic instruction. Journal of Academic
Librarianship, 14(3), 150–153.
Bowles-Terry, M. (2012). Library instruction and academic success: A mixed-methods
assessment of a library instruction program. Evidence Based Library and
Information Practice, 7(1), 82–95.
Bowman, N. A. (2010). Can 1st-year college students accurately report their learning and
development? American Educational Research Journal, 47(2), 466–496.
Branson, C. M. (2008). Achieving organizational change through values alignment.
Journal of Educational Administration, 46(3), 376–395.
http://dx.doi:10.1108/09578230810869293
Bronstein, J. (2014). The role of perceived self-efficacy in the information seeking
behavior of library and information science students. Journal of Academic
Librarianship, 40(2), 101–106.
Brown, J. D. (2011). Likert items and scales of measurement. Statistics, 15(1), 10–14.
Bryan, J. E. (2014). Critical thinking, information literacy and quality enhancement
plans. Reference Services Review, 42(3), 388–402.
http://dx.doi.org/10.1108/RSR-01-2014-0001
Cabrera, A. S. P. (2014). First generation minority students: Understanding the influential
factors that contributed to their preparation and decision to pursue higher

188
education. PSU McNair Scholars Online Journal, 8(1), Article 2, n.p. Retrieved
from http://pdxscholar.library.pdx.edu/mcnair/vol8/iss1/2
Cahoy, E. S., & Schroeder, R. (2012). Embedding affective learning outcomes in library
instruction. Communications in Information Literacy, 6(1), 73–90.
California Community Colleges Chancellor's Office (CCCCO). (2008). Distance
education guidelines ~ 2008 omnibus version. Retrieved from
http://extranet.cccco.edu/Portals/1/AA/DE/de_guidelines_081408.pdf
California Community Colleges Chancellor's Office (CCCCO). (2011). Student success
task force final report. Retrieved from
http://www.californiacommunitycolleges.cccco.edu/Portals/0/Executive/StudentS
uccessTaskForce/SSTF_Final_Report_1-17-12_Print.pdf
California Community Colleges Chancellor's Office (CCCCO). (2013). Student success
initiative. Retrieved from http://www.californiacommunitycolleges.cccco.edu/
PolicyInAction/StudentSuccessInitiative.aspx
California Community Colleges Chancellor's Office (CCCCO). (2014a). California
community colleges student success scorecard: Santa Rosa Junior College.
Retrieved from http://scorecard.cccco.edu/scorecardrates.aspx?CollegeID=261
California Community Colleges Chancellor's Office (CCCCO). (2014b). Key facts about
California community colleges. Retrieved from
http://californiacommunitycolleges.cccco.edu/PolicyInAction/KeyFacts.aspx

189
California Community Colleges Chancellor's Office (CCCCO). (2014c). Management
systems data mart: Sonoma community college district. Retrieved from
http://datamart.cccco.edu/Students/Student_Term_Annual_Count.aspx
California Community Colleges Chancellor’s Office. Management Information System.
(2008). Student characteristics derived data elements. Retrieved from
http://extranet.cccco.edu/Portals/1/TRIS/MIS/Left_Nav/DED/Derived_Elements/
STD/STD10.pdf
Carifio, J., & Perla, R. J. (2007). Ten common misunderstandings, misconceptions,
persistent myths and urban legends about Likert scales and Likert response
formats and their antidotes. Journal of Social Science, 3, 106–116.
Chen, C., Pedersen, S., & Murphy, K. (2012). The influence of perceived information
overload on student participation and knowledge construction in computermediated communication. Instructional Science, 40(2), 325–349.
http://dx.doi:10.1007/s11251-011-9179-0
Chew, B. (2015) Table of critical values for Pearson correlation. Retrieved from
http://faculty.fortlewis.edu/CHEW_B/Documents/Table%20of%20critical%20val
ues%20for%20Pearson%20correlation.htm
Cho, S., & Mechur Karp, M. (2013). Student success courses in the community college:
early enrollment and educational outcomes. Community College Review, 41(1),
86–103. http://dx.doi:10.1177/0091552112472227
Cho, T. (2011). The impact of types of interaction on student satisfaction in online
courses. International Journal on E-Learning, 10(2), 109–125.

190
Clark, S., & Chinburg, S. (2010). Research performance in undergraduates receiving face
to face versus online library instruction: A citation analysis. Journal of Library
Administration, 50(5/6), 530–542. http://dx.doi:10.1080/01930826.2010.488599
Cohen, J. (1992). A power primer. Psychological bulletin, 112(1), 155. Retrieved from
http://drsmorey.org/bibtex/upload/Cohen:1992.pdf
Community College Survey of Student Engagement (CCSSE). (2013). High-impact
practices increase student engagement. Retrieved from
http://www.ccsse.org/survey/survey.cfm
Conteh-Morgan, M. (2002). Connecting the dots: Limited English proficiency, second
language learning theories, and information literacy instruction. Journal of
Academic Librarianship, 28(4), 191–196. http://dx.doi: 10.1016/S00991333(02)00282-3
Creswell, J. W. (2012). Educational research: Planning, conducting, and evaluating
quantitative and qualitative research (Laureate custom ed.). Boston, MA: Pearson
Education.
Detlor, B., Julien, H., Willson, R., Serenko, A., & Lavallee, M. (2011). Learning
outcomes of information literacy instruction at business schools. Journal of the
American Society for Information Science & Technology, 62(3), 572–585.
http://dx.doi:10.1002/asi.21474
Detmering, R., & Johnson, A. (2011). Focusing on the thinking, not the tools:
Incorporating critical thinking into an information literacy module for an

191
introduction to business course. Journal of Business & Finance Librarianship,
16(2), 101–107. http://dx.doi:10.1080/08963568.2011.554771
Dunn, K. (2002). Assessing information literacy skills in the California state university:
A progress report. Journal of Academic Librarianship, 28(1–2), 2–35.
Elkins, D. J., Forrester, S. A., & Noel-Elkins, A. V. (2011). Students' perceived sense of
campus community: The influence of out-of-class experiences. College Student
Journal, 45(1), 105–121.
ETS (2014). The iSkills assessment. Retrieved from https://www.ets.org/iskills/about
Everett, J. B. (2015). Public community colleges: Creating access and opportunities for
first-generation college students. Delta Kappa Gamma Bulletin, 81(3), 52–58.
Everitt, B., & Skrondal, A. (2010). The Cambridge Dictionary of Statistics. Cambridge,
MA: Cambridge University Press.
Finley, W., & Waymire, T. (2012). Information literacy in the accounting classroom: a
collaborative effort. Journal of Business & Finance Librarianship, 17(1), 34–50.
http://dx.doi:10.1080/08963568.2012.629566
Fitzpatrick, M. J., & Meulemans, Y. (2011). Assessing an information literacy
assignment and workshop using a quasi-experimental design. College Teaching,
59(4), 142–149. http://dx.doi:10.1080/87567555.2011.591452
Frost, R. A., Strom, S. L., Downey, J., Schultz, D. D., & Holland, T. A. (2010).
Enhancing student learning with academic and student affairs collaboration.
Community College Enterprise, 16(1), 37–51.

192
Gainer, J. (2012). Critical thinking: Foundational for digital literacies and democracy.
Journal of Adolescent & Adult Literacy, 56(1), 14–17.
http://dx.doi:10.1002/JAAL.00096
Gonyea, R. M. (2005). Self-reported data in institutional research: Review and
recommendations. New Directions for Institutional Research, 127, 73–89.
Retrieved from http://sshl.sysu.edu.cn/docs/2013-03/20130327172420936127.pdf
Gordon, M., & Graham, G. (2003). The art of the white paper. Retrieved from
http://www.gordonandgordon.com/downloads/art_of_the_white_paper_2003.pdf
Graham, G. (2013). White paper for dummies. Hoboken, NJ: Wiley.
Graham, G. (2015). How to plan a problem/solution white paper. Retrieved from
http://www.thatwhitepaperguy.com/getting-started-with-white-papers/how-toplan-a-problemsolution-white-paper/
Grimes, D., & Boening, C. (2001). Worries with the web: a look at student use of web
resources. College and Research Libraries, 62(1), 11–22.
Gross, M., & Latham, D. (2012). What's skill got to do with it?: Information literacy
skills and self-views of ability among first-year college students. Journal of the
American Society for Information Science & Technology, 63(3), 574–583.
http://dx.doi:10.1002/asi.21681
Gross, M., & Latham, D. (2013). Addressing below proficient information literacy skills:
Evaluating the efficacy of an evidence-based educational intervention. Library &
Information Science Research, 35, 181–190. http://dx.doi:
10.1016/j.lisr.2013.03.001

193
Gross, M., Latham, D., & Armstrong, B. (2012). Improving below-proficient information
literacy skills: Designing an evidence-based educational intervention. College
Teaching, 60(3), 104–111. http://dx.doi:10.1080/87567555.2011.645257
Gurantz, O. (2015). Who loses out? Registration order, course availability, and student
behaviors in community college. Journal of Higher Education, 86(4), 524–565.
Hattie, J. (2015). High-impact leadership. Educational Leadership, 72(5), 36–40.
Head, A., & Eisenberg, M. (2009). Finding context: What today's college students say
about conducting research in the digital age. Project Information Literacy at
University of Washington. Retrieved from
http://projectinfolit.org/images/pdfs/pil_progressreport_2_2009.pdf
Head, A. (2012). Learning curve: How college graduates solve information problems
once they join the workplace. Project Information Literacy at University of
Washington. Retrieved from http://ssrn.com/abstract=2165031
Head, A. (2013). Learning the ropes: How freshmen conduct course research once they
enter college. Project Information Literacy at University of Washington.
Retrieved from
http://projectinfolit.org/images/pdfs/pil_2013_freshmenstudy_fullreport.pdf
Heaney, A., & Fisher, R. (2011). Supporting conditionally-admitted students: A case
study of assessing persistence in a learning community. Journal of the
Scholarship of Teaching and Learning, 11(1), 62–78.

194
Hicks, A., & Sinkinson, C. (2015). Critical connections: Personal learning environments
and information literacy. Research in Learning Technology, 23, 1–12.
http://dx.doi:10.3402/rlt.v23.21193
Higher Education Research Institute (HERI). (2014). Cooperative Institutional Research
Program (CIRP). Retrieved from http://www.heri.ucla.edu/abtcirp.php
Hellenius, S. (2007). Information competency graduation programs: A survey of
methods. Retrieved from http://www.asccc.org/node/176738
Hofer, A. R., Townsend, L., & Brunetti, K. (2012). Troublesome concepts and
information literacy: Investigating threshold concepts for IL instruction. Portal:
Libraries and the Academy, 12(4), 387–405.
Hoffman, S. (2006). How to write a white paper: The five laws. Hoffman Marketing
Communication. Retrieved from
http://www.hoffmanmarcom.com/docs/White_paper_Five_Laws.pdf
Hogan, N., & Varnhagen, C. (2012). Critical appraisal of information on the web in
practice: Undergraduate students' knowledge, reported use, and behaviour.
Canadian Journal of Learning and Technology, 38(1), 1–14.
Horner, J., & Thirlwall, D. (1988). Online searching and the university researcher.
Journal of Academic Librarianship, 14(4), 225–230.
Jamieson, S. (2004). Likert scales: how to (ab)use them. Medical education, 38(12),
1217–1218.
Johnston, N., Partridge, H., & Hughes, H. (2014). Understanding the information literacy
experiences of EFL (English as a foreign language) students. Reference Services

195
Review, 42(4), 552–568. http://dx.doi.org/10.1108/RSR-05-2014-0015
Juricek, J. E. (2009). Access to grey literature in business: An exploration of commercial
white papers. Journal of Business & Finance Librarianship, 14(4), 318–332.
http://dx.doi:10.1080/08963560802365388
Kantor, J. (2010). Crafting white paper 2.0: Designing information for today's time and
attention-challenged business reader. Raleigh, NC: Lulu Press.
Kemp, B. E., & Nofsinger, M. M. (1988). Library/research skills for college-bound
students: Articulation in Washington State. Journal of Academic Librarianship,
14(2), 78–79.
Kemp, A. (2005). White paper writing guide: How to achieve marketing goals by
explaining technical ideas. Impact Technical Publications. Retrieved from
http://www.impactonthenet.com/wp-guide.pdf
Kim, J., & Bragg, D. D. (2008). The impact of dual and articulated credit on college
readiness and retention in four community colleges. Career & Technical
Education Research, 33(2), 133–158.
Kuh, G. D. (2001). The national survey of student engagement: Conceptual framework
and overview of psychometric properties. Bloomington, IN: Indiana University
Center for Postsecondary Research.
Kuh, G. D. (2008). High-impact educational practices: What they are, who has access to
them, and why they matter. Washington, DC: Association of American Colleges
and Universities.

196
Kuh, G. D., & Gonyea, R. M. (2015). The role of the academic library in promoting
student engagement in learning. College & Research Libraries, 76(3), 359–385.
Kuhlthau, C. (1991). Inside the search process: information seeking from the user's
perspective. Journal of the American Society for Information Science, 42(5), 361–
371.
Laguilles, J., Williams, E., & Saunders, D. (2011). Can lottery incentives boost web
survey response rates? Findings from four experiments. Research in Higher
Education, 52(5), 537–553. http://dx.doi:10.1007/s11162-010-9203-2
Legal Information Institute (LII). (2015). Title 20 U.S. Code § 1232g - Family
educational and privacy rights. Cornell University Law School. Retrieved from
https://www.law.cornell.edu/uscode/text/20/1232g
Lewis, D. W. (1987). Research on the use of online catalogs and its implications for
library practice. Journal of Academic Librarianship, 13(3), 152–157.
Lim, D. H., Morris, M. L., & Kupritz, V. W. (2006). Online vs. blended learning:
Differences in instructional outcomes and learner satisfaction. Paper presented at
the academy of Human Resources Development (AHRD) International
Conference, Columbus, OH. Retrieved from
http://184.168.109.199:8080/xmlui/bitstream/handle/123456789/2257/EJ842695.
pdf?sequence=1
Lodico, M., Spaulding, D., & Voegtle, K. (2010). Methods in educational research:
From theory to practice. (Laureate custom ed.). San Francisco, CA: John Wiley &
Sons.

197
Machado, E. A., & Afonso, L. E. (2015). Satisfaction and learning outcomes with
distance education in Brazil among students of business and accounting: is the
grass always greener on the other side? Congressos Anpcont, 19, 1–18. Retrieved
from http://congressos.anpcont.org.br/ix/anais/files/2015-05/epc285.pdf
Mattern, J. (2013). How to write a white paper. Directory Journal; Business Journal.
Retrieved from http://www.dirjournal.com/business-journal/how-to-write-awhite-paper/
Mayer, J., & Bowles-Terry, M. (2013). Engagement and assessment in a credit-bearing
information literacy course. Reference Services Review, 41(1), 62–79.
http://dx.doi.org/10.1108/00907321311300884
McBride, M. F. (2011). Reconsidering information literacy in the 21st Century: The
redesign of an information literacy class. Journal of Educational Technology
Systems, 40(3), 287–300.
McKeon, P. (2005). Eight rules for creating great white papers. Knowledgestorm: The
Content Factor. Retrieved from http://www.idemployee.id.tue.nl/
g.w.m.rauterberg/lecturenotes/Eight-Rules-for-Writing-Great-White-Papers.pdf
Meriam Library, California State University at Chico. (2010). Evaluating information:
Applying the CRAAP test. Retrieved from
http://www.csuchico.edu/lins/handouts/evalsites.html
Monge, R., & Frisicaro-Pawlowski, E. (2014). Redefining information literacy to prepare
students for the 21st century workforce. Innovative Higher Education, 39(1), 59–
73. http://dx.doi:10.1007/s10755-013-9260-5

198
Moore, D., Brewster, S., Dorroh, C., & Moreau, M. (2001). Information competency
instruction in a two-year college: One size does not fit all. Reference Services
Review, 30(4), 300–306. http://dx.doi:10.1108/00907320210451286
Mulvey, M. E. (2009). Characteristics of under-prepared students: Who are "the underprepared"? Research & Teaching in Developmental Education, 25(2), 29–58.
Nau, R. (2015). What’s a good value for R-squared? Retrieved from
http://people.duke.edu/~rnau/regstep.htm
Nicholson, J., & Galguera, T. (2013). Integrating new literacies in higher education: A
self-study of the use of Twitter in an education course. Teacher Education
Quarterly, 40(3), 7–26.
O’Banion, T. (1997). A learning college for the 21st century. Phoenix, AZ: American
Council on Education and Oryx Press.
Oakleaf, M. (2014). A roadmap for assessing student learning using the new framework
for information literacy for higher education. Journal of Academic Librarianship,
40(5), 510–514.
Orme, W. (2004) A study of the residual impact of the Texas information literacy tutorial
on the information-seeking ability of first year college students. College &
Research Libraries, 65(3), 205–214.
Pace, R. C., & Others. (1985). The credibility of student self–reports (ED 266 174 - TM
860 118). Washington, DC: national Institute of Education. Retrieved from
http://files.eric.ed.gov/fulltext/ED266174.pdf

199
Parker, R. (2013). 4 Keys to compelling content marketing with white papers. Retrieved
from http://contentmarketinginstitute.com/2013/08/keys-compelling-contentmarketing-white-papers/
Pascarella, E. T. (2001). Using student self-reported gains to estimate college impact: A
cautionary tale. Journal of College Student Development, 42, 488–492.
Pike, G. R. (2011). Using college students’ self-reported learning outcomes in scholarly
research. New Directions for Institutional Research, 150, 41–58.
Porter, S. R. (2011). Do college student surveys have any validity? Review of Higher
Education, 35(1), 45–76.
Porter, S. R. (2013). Self-reported learning gains: A theory and test of college student
survey response. Research in Higher Education, 54(2), 201–226.
Radcliff, S., & Wong, E. Y. (2015). Evaluation of sources: A new sustainable approach.
Reference Services Review, 43(2), http://dx.doi.org/10.1108/RSR-09-2014-0041
Radom, R., & Gammons, R. W. (2014). Teaching information evaluation with the five
Ws. Reference & User Services Quarterly, 53(4), 334–347.
Rao, S., Cameron, A., & Gaskin-Noel, S. (2009). Embedding general education
competencies into an online information literacy course. Journal of Library
Administration, 49(1–2), 59–73.
Reeves, W. W. (1996). Cognition and complexity: The cognitive science of managing
complexity. Lanham, MD: Scarecrow Press, Inc.
Ritzhaupt, A. D., Feng, L., Dawson, K., & Barron, A. E. (2013). Differences in student
information and communication technology literacy based on socioeconomic

200
status, ethnicity, and gender: Evidence of a digital divide in Florida schools.
Journal of Research on Technology in Education, 45(4), 291–307.
Robertson, S. N. (2013). General education: Charting a roadmap toward student success.
Peer Review, 15(2), 18–19.
Roscoe, J. L. (2015). Advising African American and Latino students. Research &
Teaching in Developmental Education, 31(2), 48–60.
Sakamuro, S., Stolley, K., & Hyde, C. (2010). White paper: Organization and other tips.
Purdue University Online Writing Lab. Retrieved from
https://owl.english.purdue.edu/owl/resource/546/02/
Sakamuro, S., Stolley, K., & Hyde, C. (2015). White paper: Purpose and audience.
Purdue University Online Writing Lab. Retrieved from
http://owl.english.purdue.edu/owl/resource/546/1/
Santa Rosa Junior College (SRJC). (2003). SRJC college catalog supplement 2003–2004.
Retrieved from http://www.santarosa.edu/schedules/college_catalog/
pdf/supplement2003–2004.pdf
Santa Rosa Junior College (SRJC). (2010). Student survey Fall 2010. Retrieved from
http://www.santarosa.edu/administration/planning/pdfs/2010%20Student%20Surv
ey%20-%20final%20draft.pdf
Santa Rosa Junior College (SRJC). (2013a). Institutional effectiveness assessment report:
Benchmarks for Santa Rosa Junior College. Retrieved from
http://www.santarosa.edu/administration/planning/pdfs/2013/IE Assessment
Report 2013.pdf

201
Santa Rosa Junior College (SRJC). (2013b). Institutional learning outcomes. Retrieved
from http://www.santarosa.edu/slo/institutional/
Santa Rosa Junior College (SRJC). (2013c). LIR 10 course outline as of fall 2013.
Retrieved from https://portal.santarosa.edu/SRweb/
SR_CourseOutlines.aspx?CVID=24532&Semester=20137
Santa Rosa Junior College (SRJC). (2013d). Student survey Fall 2013. Retrieved from
http://www.santarosa.edu/administration/planning/pdfs/2013%20Student%20Surv
ey%20-%20final%20draft.pdf
Santa Rosa Junior College (SRJC). (2013e). Vision, mission statement, values. Retrieved
from http://www.santarosa.edu/polman/1mission/1.1.pdf
Santa Rosa Junior College (SRJC), (2014–15). Associate degree graduation requirements
and general education option A, 2014–15. Retrieved from
http://www.santarosa.edu/for_students/student-services/articulation/pdf/AAAS_2014–15.pdf
Santa Rosa Junior College (SRJC). (2014). General education learning outcomes.
Retrieved from http://www.santarosa.edu/slo/general/
Santa Rosa Junior College. (2015). Schedule of classes. Retrieved from
https://portal.santarosa.edu/SRWeb/SR_ScheduleOfClasses.aspx
Santa Rosa Junior College (SRJC), Academic Senate. (2013). Minutes of April 3, 2013.
Retrieved from http://www.santarosa.edu/senate/archive/minutes/
Minutes%202013/minutes130403.doc

202
Santa Rosa Junior College (SRJC), Educational Planning & Coordinating Council
(EPCC). (2013). Minutes of March 14, 2013.Retrieved from
https://bussharepoint.santarosa.edu/committees/educational-planningcoordinating/Committee%20Documents/EPCC%20Minutes%203-14-13.pdf
Santa Rosa Junior College (SRJC), Office of Institutional Research (OIR). (2013). Fact
book 2013. Retrieved from
http://www.santarosa.edu/research/pdfs/FB%202012.pdf
Schnee, E. (2014). A foundation for something bigger: Community college students'
experience of remediation in the context of a learning community. Community
College Review, 42(3), 242-261. http://dx.doi:10.1177/0091552114527604
Schultz, J. R. (2010). The scholar-practitioner: A philosophy of leadership. ScholarPractitioner Quarterly, 4(1), 52–64.
Schroeder, R., & Cahoy, E. S. (2010). Valuing information literacy: affective learning
and the ACRL standards. Portal: Libraries and the Academy, 10(2), 127–146.
Seeber, K. P. (2015). Teaching “format as a process” in an era of Web-scale discovery.
Reference Services Review, 43(1), 19–30. http://dx.doi.org/10.1108/RSR-07-20140023

Sherman, M., & Martin, J. (2012). Information literacy in the workplace: Employer
expectations. Journal of Business & Finance Librarianship, 17(1), 1–17.
http://dx.doi:10.1080/08963568.2011.603989
Sherman, M., Martin, J., & An, X. (2012). The impact of library instruction on the quality
of student project performance in an advanced financial management case class.

203
Journal of Business & Finance Librarianship, 17(1), 51–76.
http://dx.doi:10.1080/08963568.2012.630646
Shipan, C. R., & Volden, C. (2012). Policy diffusion: Seven lessons for scholars and
practitioners. Public Administration Review, 72(6), 788–796.
http://dx.doi:10.111/j.1540-6210.2012.02610.x
Showman, A., Cat, L., Cook, J., Holloway, N., & Wittman, T. (2013). Five essential
skills for every undergraduate researcher. Council on Undergraduate Research
Quarterly, 33(3), 16–19.
Siefert, L. (2011). Assessing general education learning outcomes. Peer Review,
13/14(4/1), 9–11.
Silk, K. J., Perrault, E. K., Ladenson, S., & Nazione, S. A. (2015). The effectiveness of
online versus in-person library instruction on finding empirical communication
research. Journal of Academic Librarianship, 41(2), 149–154.
http://dx.doi:10.1016/j.acalib.2014.12.007
Simpson, O. (2013). Supporting students for success in online and distance education.
New York, NY: Routledge.
Sokoloff, J. (2012). Information literacy in the workplace: Employer expectations.
Journal of Business & Finance Librarianship, 17(1), 1–17.
http://dx.doi:10.1080/08963568.2011.603989
Sorey, K., & DeMarte, D. (2013). Assessment of general education: Adapting the
AAC&U value rubrics. Peer Review, 15(2), 27–28.

204
Soper, D.S. (2015). A-priori sample size calculator for multiple regression. Retrieved
from http://www.danielsoper.com/statcalc3/calc.aspx?id=1
Spillane, J. P. (2012). Data in practice: Conceptualizing the data-based decision-making
phenomena. American Journal of Education, 118(2), 113–141.
Srikanth, A. (2002). Effective business writing: The white paper. The Internet Writing
Journal. Retrieved from http://www.writerswrite.com/journal/sep02/srikanth.htm
Stevens-Long, J., Schapiro, S. A., & McClintock, C. (2012). Passionate scholars:
Transformative learning in doctoral education. Adult Education Quarterly, 62(2),
180. http://dx.doi:10.1177/0741713611402046
Stelzner, M. (2007). Writing white papers: How to capture readers and keep them
engaged. Poway, CA: WhitePaperSource Publishing.
Stelzner, M. (2010). How to write a white paper: A white paper on white papers.
Retrieved from
http://coe.winthrop.edu/educ651/readings/HowTo_WhitePaper.pdf
Taylor, A. (2012). A study of the information search behavior of the millennial
generation. Information Research, 17(1), 508. Retrieved from
http://InformationR.net/ir/17-1/paper508.html
Thompson, C. (2003). Information illiterate or lazy: How college students use the Web
for research. Portal: Libraries and the Academy, 3(2), 259–267.
Thompson, D. R. (2010). Foundations of change for the scholar-practitioner leader.
Scholar-Practitioner Quarterly, 4(3), 270–286.

205
Thurmond, V., & Popkess-Vawter, S. (2003). Examination of a middle range theory:
Applying Astin’s Inputs-Environment-Outcome (I-E-O) model to web-based
education. Online Journal of Nursing Informatics, 7(2). Retrieved from
http://ojni.org/7_2/thurmond.htm
Tovar, E. (2015). The role of faculty, counselors, and support programs on Latino/a
community college students’ success and intent to persist. Community College
Review, 43(1), 46–71. http://dx.doi:10.1177/0091552114553788
Travis, T. (2011). From the classroom to the boardroom: The impact of information
literacy instruction on workplace research skills. Education Libraries, 34(2), 19–
31.
UCLA: Statistical Consulting Group. (2015). SPSS annotated output regression analysis.
Retrieved from http://www.ats.ucla.edu/stat/spss/output/reg_spss.htm
University of Washington's iSchool. (2014). Project information literacy (PIL). Retrieved
from http://projectinfolit.org
Varlejs, J., Stec, E., & Kwon, H. (2014). Factors affecting students' information literacy
as they transition from high school to college. School Library Research, 171–23.
Virtue, A., Dean, E., & Matheson, M. (2014). Assessing online learning objects: Student
evaluation of a guide on the side interactive learning tutorial designed by SRJC
libraries. Interdisciplinary Journal of E-Learning and Learning Objects, 10, 93–
105. Retrieved from http://www.ijello.org/Volume10/IJELLOv10p093105Virtue0886.pdf

206
WASC Senior College and University Commission. (2013). Handbook of accreditation –
2013. Retrieved from http://www.wascsenior.org/content/2013-handbookaccreditation
Wieler, A. (2004). Information-seeking behavior in generation Y students: Motivation,
critical thinking, and learning theory. Journal of Academic Librarianship, 31(1),
46–53.
Wolf-Wendel, L., Ward, K., & Kinzie, J. (2009). A tangled web of terms: The overlap
and unique contribution of involvement, engagement, and integration to
understanding college student success. Journal of College Student Development,
50(4), 407–428.
Wolff, B. G., Wood-Kustanowitz, A. M., & Ashkenazi, J. M. (2014). Student
performance at a community college: Mode of delivery, employment, and
academic skills as predictors of success. Journal of Online Learning & Teaching,
10(2), 166–178.
York, C. (2013). Overloaded by the news: Effects of news exposure and enjoyment on
reporting information overload. Communication Research Reports, 30(4), 282–
292. http://dx.doi:10.1080/08824096.2013.836628
Zachery, I. (2010). The effect of information literacy competency on student learning and
success in three California community colleges. (Doctoral dissertation) Retrieved
from ProQuest Digital Dissertations. (Order No. 3421710, George Mason
University).

207
Appendix A: The Project

Recommendations for
Increasing the Impact
of the General
Education Information
Literacy Requirement
A White Paper by Phyllis Usina, Walden University
p. 1

Executive Summary
Santa Rosa Junior College (SRJC) has used a 1-unit general education course as
an information literacy (IL) requirement since 2002. In that time, the college has
conducted no targeted assessment of this delivery method. In response to several years
of budget constraints, college stakeholders considered ending the IL requirement in
2013. Critical Analysis Institutional Learning Outcomes data indicated students had IL
related learning gains, but an assessment gap existed regarding the impact the IL
requirement had on these gains.
This targeted doctoral capstone study used Astin and Antonio’s (2012) I-E-O
assessment model. Two research questions explored relationships among the Inputs of
student demographic and preparation characteristics, the Environment of IL
requirement course formats and lengths, and the Outcomes reports of frequency of
affective information evaluation behaviors and confidence in writing papers or
participating in discussions in subsequent courses. The anonymous Information
Literacy Requirement Impact Survey instrument was administered online in the Spring
2015 semester. Self-reports from 525 students age18 or over who had completed the IL
requirement course with a grade of 2.0 or better during the 2013–2014 academic year
were collected. Data analysis showed that relationships existed between the Inputs,
Environment, and Outcomes variables measured.

208
p. 2

Based upon the study’s research and findings, this report provides three policy
recommendations. The first recommendation involves using the study’s learnercentered self-reports to modifying registration pathways to support early completion of
the general education IL requirement. Students recommended the IL requirement be
completed in the first or second term of college attendance. A second recommendation
encourages continued support for the IL requirement as an effective delivery method
ensuring that formats and lengths of course offerings meet the needs of all student
demographics. The majority of students reported that the IL requirement had a
favorable impact on their information evaluation behaviors and confidence. Most
students reported completing the IL requirement in their third or fourth semester and
felt they were already somewhat prepared. However, they attributed a greater
frequency of information evaluation behaviors changes and higher levels of confidence
to participation in the Il requirement. A third recommendation suggests that the college
use the Information Literacy Requirement Impact Survey to conduct targeted
assessments of the IL general education requirement on a 3-year basis and correlate the
findings with the college’s triennial Student Survey results. The study’s response rate
of 26% fully completed surveys demonstrated student comfort with the survey and
supported the methodology’s alignment with the college’s learner-centered values.

These policy recommendations; modifying registration pathways, continuing
support for the IL requirement, and conducting targeted assessment on a 3-year basis,
form a foundation that can inspire future research. The successful use of the I-E-O
model provides stakeholders with a framework to fill the college’s assessment gap. The
study also provides theory-based data that can be used to gauge the effectiveness of the
general education IL requirement. Offering effective information literature education
influences societal change by promoting student information evaluation behaviors and
confidence. These behaviors are associated with improved workplace performance and
the civic collaboration essential to informed members of society.

209
p. 3

Introduction
More than a decade has
passed since Santa Rosa
Junior College (SRJC)
instituted a 1-unit general
education information
literacy (IL) requirement. In
that time, the college
conducted no targeted
assessment of this delivery
method. Data was needed to
determine if the IL
requirement equipped
students with effective
academic research abilities
impacting their critical
analysis gains. The college
could benefit from data
showing if the general
education IL requirement
delivery method is meeting
the changing needs of the
institution and its students.
The purpose of this
quantitative survey study
was to address the gap in
assessment using students’
reports of frequency of
critical information
evaluation behaviors and
confidence in academic
settings as a result of
participation in the IL
requirement course while
controlling for differing
student characteristics. Such
an assessment could assist
stakeholders in making
future decisions relating to
the program and ensure that

resources are being used to
best effect. The college, like
other colleges, struggles to
respond to demographic
shifts, budget restrictions,
accreditation standards, and
changing government
regulations. These
competing pressures make it
essential for colleges to
regularly examine the
relevance of required
general education courses to
students’ programs of study.
Data collection involved an
anonymous survey
instrument that measured
students’ demographic and
preparation characteristics,
IL course format and length,
and frequency of
information evaluation
behaviors, and levels of
confidence.
This study provided
targeted data college
decision makers can use to
assess the efficacy of the
general education IL
requirement delivery
method. It supplements the
college’s broad institutional
level assessment, currently
in use, with a targeted
survey component designed
to determine if the
requirement’s IL
development goals for
students are being met.

Contents
Executive Summary
Introduction

1
3

Problem and Study Design
Context of the Problem
Theoretical Framework
Research Questions
Data Collection
Data Analysis

4
5
5
6
7

Recommendations and Research
Student Preparation
IL Education Delivery
Assessment Gap
Future Research

8
11
14
18

Conclusion
References

19
21

210
p. 4

Context of the Problem
The problem prompting this study was a community college’s lack of targeted
assessment of the student development impact of its general education IL requirement. In
2002, the college’s faculty and administrators made the assumption that a general
education requirement would be the IL educational delivery method best suited to
developing student research competencies in other courses. In so doing, they signaled
their commitment to the development of IL critical evaluation skills and knowledge in
students. This commitment originated from the radical paradigm shift that Internet
technologies produced in how students conduct academic research. This shift necessitated
increased student capacity to critically evaluate information.
Concerns about students’ IL critical information evaluation performance dated
back to the 1980’s (Association & Research Libraries [ACRL], 1989, 1998; Kuhlthau,
1991) and research showing students’ deficiencies in IL abilities continued to affect
higher education up to the present (Chen, Pedersen, & Murphy, 2012; Gross & Latham,
2012; Head & Eisenberg, 2009; Head, 2013; Ritzhaupt, Feng, Dawson, & Barron, 2013;
Taylor, 2012). As a result of these concerns, the scope of IL education developed and
expanded. In the 2000s, the ACRL (2000) published its Information Literacy Competency
Standards for Higher Education. ACRL’s Information Literacy Standard Three included
critical thinking components related to analyzing information that enabled students to use
information to increase their knowledge. Multiple authors continued to acknowledge the
growing need for IL support in education. Effective IL education became necessary to
help students gain the information evaluation abilities needed for problem-solving and
making informed decisions (York, 2013).
The literature review revealed a lack of published studies assessing the efficacy of
a required general education course as the IL education delivery method that
compounded the college’s assessment gap. The college’s assessment of its Institutional
Learning Outcomes (2013b) uses a broad Student Survey (2013d) that reflects student’s
self-reported affective gains. Students report high learning gains for the critical analysis
outcomes that include IL abilities. However, the survey does not address the question of
whether participation in the IL requirement course, LIR 10, is responsible for those gains.
The college’s evaluation allows for a broad, standardized assessment of student
development but does not specifically assess the impact of the general education
requirement for IL education delivery. Given that no specific IL requirement assessment
process was implemented, local shared governance committees discussed the question of
discontinuing the IL requirement.
Three relevant issues surrounded the college’s IL requirement assessment gap
problem. These spanned the larger education context of the United States and the state of
California, and the local setting of a community college. These issues included increasing
numbers of underprepared students who may not possess IL skills and abilities, the IL
education delivery methods options available, and the need to regularly assess general
education requirements.

211
p. 5

Theoretical and Conceptual Framework
Astin and Antonio’s (2012) Inputs-EnvironmentOutcomes (I-E-O) assessment model evolved from
their work on large-scale longitudinal student surveys.

The Inputs-Environment-Outcomes (I-E-O) model shows how
Inputs can affect the educational Environment that can affect
Outcomes. Inputs can also directly affect Outcomes.

The I-E-O assessment model focused the
study’s problem, data collection, and
analysis investigations on three issues
surrounding the college’s IL requirement
assessment gap problem.

The I-E-O assessment model
highlighted the need to include
the preparedness issues
surrounding student
characteristic as Inputs, the
characteristic defining the IL
education delivery methods of
the Environment, and the
frequency of critical
information evaluation
behaviors and confidence
levels as affective Outcomes.
The I-E-O relationship patterns
also define what concepts
needed to be measured by the
self-developed survey
instrument used for data
collection, the techniques
selected for data analysis, and
the policy and programmatic
recommendations.

Research Questions
The use of the I-E-O assessment model to provide insight into the impact of SRJC’s IL
requirement program gave rise to one overarching and two research questions.

The identified student characteristics referred to in the research questions include age, gender, ethnicity,
primary language, terms attended, English course level, research preparedness, and number of papers.

212
p. 6

Data Collection
Considering the intersection
of the college’s IL
requirement assessment gap
and its learner-centered
values, survey research was
the ideal approach to explore
relationships between the
issues surrounding the
problem. Some may perceive
limitations in using survey
methodology and question
attribution of self-reported
behavior or confidence to
participation in the IL
requirement. In the case of
this study, however, careful
attention was given to the
design of the self- developed
Information Literacy
Requirement Impact Survey
instrument. Validity was
increased by adapting items
from the SRJC (2013d)
Student Survey instrument
and information evaluation
performance indicator
outcomes from ACRL’s
(2000) Information Literacy
Standard Three. In addition
peer expert review, pilot
testing, a large sample size,
item scales, and rigorous
analysis techniques were
used. Given these conditions
and that the study’s use of
survey methodology was
carefully executed, the data
analysis results can provide
preliminary support for
policy change
recommendations.

In Spring 2015, data was
collected online
anonymously in cooperation
with the college as a
community partner.
Participation was voluntary
and limited to students a
minimum of 18 years old
who completed the IL
requirement course with a
grade of 2.0 or better during
the 2013–2014 academic
year. All 2012 students in
the target population were
invited to engage the most
participants possible

The study’s total population
purposeful sample return
was 592 surveys, a 29%
response rate. Responses
that were incomplete or did
not meet the selection
criteria (n = 67) were
subtracted. The response
rate for resulting usable
surveys was 26% with a
sample size of N = 525.
The survey was
administered twelve to
twenty-four months after
program participation. This
timeframe was long enough

to allow students to
gain experiences in
other courses but was
not so long after
completion of the IL
requirement course,
LIR 10, that students
forgot learning
outcomes.
The last item on the
survey was students’
recommendations of
when would be most
helpful time to take the
IL requirement. The
college had no set
general education
requirement sequence
or pathway a student
must follow. As a
result, many students
completed the IL
requirement at the end
of their time at the
college. California’s
Student Success
Initiative (SSI) stressed
the importance of
orienting students early
to ensure all students
have the foundational
skills essential for the
achievement of a
degree, certificate, or
transfer (CCCCO,
2011). The concept of
timing had implications
for the IL education
needs of underprepared
students.

213
p. 7

Data Analysis
This study answered the research questions by testing for significant relationships
between the student demographic and preparedness characteristics as Inputs, IL
requirement course format and length as Environment characteristics, and critical
information evaluation affective behavior changes and confidence levels in relation to
writing papers and participating in discussions in other courses as Outcomes.
Descriptive analysis
described the sample using
frequencies, percentages,
measures of central tendency,
range, and standard deviation.

Cross-tab analysis used chisquare for association to
examine relationships between
pairs of categorical variables.

Correlation analysis
showed the strength and
direction of relationships.

Multiple Regression
analysis examined the
The I-E-O model visually shows how the Inputs (independent variables)
can affect both the Environment (independent variable) and the affective
behavioral and psychological Outcomes (dependent variables).

Representativeness of the Response Sample
This study conducted representativeness
analysis to assess the data for potential
response bias limitations. Based on the results,
this study accepted the assumption that the
demographic characteristics were meaningfully
similar because the ethnicity variable was
significant statistically, and the differences for
the age and gender variables were small.
However, the error in reporting for the LIR 10
course length variable did not limit the ability
to draw conclusions from the
representativeness of the LIR 10 format
program characteristic and the demographic
characteristics findings.

independent variables to
determine how much each
explained the variation in the
dependent variable.

214
p. 8

Inputs – Student Preparedness
Student Preparedness Issues Surrounding the Problem
The California Community College system’s open access mission provides
academic opportunities for students who may not otherwise be able to obtain degrees.
Many students arrive with a low level of academic preparation and are required to stay in
school longer to attain their degrees, increasing the chance they may not meet their
educational goals (Astin, 1999; Cabrera, 2014; Kim and Bragg, 2008). Head’s (2013)
study of freshmen research habits included a sample of the college students. This study
found students reported feeling “unprepared to deal with the enormous amount of
information they were expected to find and process for college research assignments”
(Head, 2013, p. 2).
The college’s ethnic demographic is predominately white with a Latino
(Hispanic) population that has expanded from “15% to 29%” (SRJC, Office of
Institutional Research [OIR], 2013, p. SD 6-7) within the last decade and continues to
grow at an ever faster rate. The college has also experienced an increasing number of
students enrolling who are underprepared for college-level work. These students lack
many of the skills necessary for academic success. This growing demographic of
underprepared students required development in reading and writing prior to enrolling in
college-level courses. The college’s student demographics were similar to national
statistics with regard to the increases in enrollment of students underprepared in the
critical information evaluation learning behaviors (CCCCO, 2014c). Several studies and
reports addressed the role of IL education in developing underprepared students to a level
where they are competitive with their peers in the classroom (Finley & Waymire, 2012;
Gross & Latham, 2012; Head, 2013; SRJC, 2013a).
This growth of underprepared students required SRJC to focus its mission more
tightly on the development of students (SRJC, 2013e). The college’s Scorecard data
showed that ESL students were not persisting to graduation as often as other ethnic
groups (CCCCO, 2014a). These students were found to lack the academic preparation
necessary to succeed in courses that require research. Increased pressures for assessment
of programs from accrediting bodies (ACCJC, 2014; WASC 2013) spurred the college’s
interest in assessing the effectiveness of its institutional programs. As community
colleges continue to incorporate new methods to meet the needs of underprepared
students, studying the effects of the IL requirement can offer data regarding student
development.

215
p.9

Recommendation Based on Research Evidence
Study findings suggested a need to modify registration practices to ensure
students enroll in the IL requirement course in their first or second term. The policy
recommendation calls for adjusting enrollment pathways for underprepared students
1. Use the study’s learner-centered self-reports to develop a
process prioritizing that the IL requirement is completed early
in the general education requirement pathway.
The results of the study and research supported this recommendation and showed
the importance of students completing the general education IL requirement at the
beginning of their time at the college.

In the descriptive analysis results,
87% of student responses indicated
that taking the IL requirement
course, LIR 10, in the first or
second term of college study would
be the most helpful for writing
papers and participating in
discussions in other courses.
Adjusting practices to ensure students enroll in the IL requirement courses early
in their college experience can equip students with the knowledge and skills need to cope
with academic research challenges throughout their educational experience at the college.
Cho and Mechur Karp (2013) found that students, especially underprepared students, who
took academic support courses in the first semester enrolled in classes in subsequent
semesters. Frost, Strom, Downey, Schultz, and Holland, (2010), and Schnee (2014)
advocated that colleges provide more learning communities to integrate underprepared
students’ learning by connecting academic services such as library instruction. Kuh and
Gonyea’s (2015) findings based on self-reported questionnaire responses involving more
than 300,000 students showed students’ educationally valuable activities included
engaging in academically challenging activities requiring critical thinking and having
more interaction with instructors. They concluded the library was conducive to positive
learning for students, especially underprepared students, and advocated for colleges to
acknowledge the valuable role IL plays in developing students’ information evaluation
skills. Varlejs, Stec, and Kwon (2014) found that limited library resources resulted in
students not receiving the IL preparation needed for college success.

216
p. 10

The findings from the study showed that the majority of the students had attended
3–4 terms or more, had taken a transfer level English course, identified with the selfconcept that they felt somewhat or super prepared with information evaluation skills
before taking LIR 10, and had written 3–4 papers or more. However, even though the
students reported that they came feeling somewhat prepared, for the Outcomes measured,
the majority identified that LIR 10 positively impacted the frequency of their SRJC
Critical Analysis learning gains changes, their ACRL Critical Analysis behavior changes,
and their information evaluation confidence for involvement in writing papers and
participating in discussions other classes.

The study’s data analysis showed that more than 25% of the response
sample identified as the Latino/ Hispanic ethnicity. This data was
significantly representative of the target population invited to participate
and the demographics of the college as a whole (SRJC, OIR, 2013).
Roscoe and Tovar (2015) reported that the increasing numbers of underprepared
Latino students have unique academic support needs that colleges must address to impact
Latino/a community college students’ success. Johnston, Partridge, and Hughes (2014)
identified challenges that ESL students experience with IL academic expectations of
reading and understanding information sources and gave insights into how educators can
improve their approaches. Mulvey (2009) highlighted the problem of academically
underprepared students having skill levels well below their classmates and the importance
of support courses to bridge the gap. In this study higher levels of academic preparation
were positively associated with other student characteristics and with information
evaluation behavior changes and confidence levels.

217
p. 11

Environment – IL Requirement Education Delivery Method
IL Requirement Education Delivery Method Issues Surrounding the Problem
In 2002, the college institutionalized general education requirement as its IL
education delivery method to address concerns about students who were underprepared to
meet academic critical evaluation expectations. The requirement was a 1-unit course, LIR
10, Introduction to Information Literacy. This course met the general education “Area I:
Information Literacy Requirement” (SRJC, 2014–15, p. 1) for the local associate degree.
The college’s IL requirement evolved from events occurring in the larger educational
setting. The Academic Senate for California Community Colleges (ASCCC; 1998)
resolved that all California community colleges should implement education programs to
ensure that graduating students meet IL competencies. To meet accreditation standards,
community colleges institutionalized and implemented IL education using different
delivery methods including a stand-alone credit course (required or optional), infusing IL
into a core research course, integrating library instruction sessions into courses, and selfpaced tutorials (Hellenius, 2007). The college was one of only a few California
community colleges that implemented a stand-alone course, general education
requirement as the delivery method for IL education (Hellenius, 2007; Zachery, 2010).
IL education develops critical information evaluation competencies in students,
allowing them to make connections needed for successful involvement in courses that
require research for writing. Detmering and Johnson (2011) noted the value of critical
thinking in academic courses. Kuh (2008) listed IL as a high impact practice affecting
student engagement. Studies by Bowles-Terry’s (2012) and Moore, Brewster, Dorroh and
Moreau (2001) showed relationships between participation in an IL course and higher
pass rates in subsequent composition classes or overall GPA. The California Community
Colleges Chancellor's Office (CCCCO; 2011) highlighted the need to equip academically
underprepared students early in the education process with college success skills thereby
increasing their chances of graduating.
The impact of the IL requirement became important during a time of budget
constraints when the college reduced the number of LIR 10 courses offered. This
reduction caused a delay for some students to complete degree requirements. The delay
raised questions about the efficacy of the general education IL requirement delivery
method and if it should be eliminating (SRJC, Academic Senate, 2013; SRJC,
Educational Planning & Coordinating Council [EPCC], 2013). Stakeholders discussed
enrollment pressures but did not consider the impact of the IL requirement on student
development. This underscored the importance of conducting targeted assessment of the
impact of the IL requirement course, LIR 10 that could be connected to the institution’s
broader Student Survey (2013d) assessment of IL critical analysis learning gains.

218
p.12

Recommendation Based on Research Evidence
Study findings indicated that the college instituted method of a general education
IL requirement course, LIR 10 was equipping students with information evaluation
behaviors and research confidence. The policy recommendation calls for continued
support of the IL requirement and to provide course offerings that meet student need.
2. Use the study’s findings as an indicator that the IL general
education requirement course is an effective delivery method that
should be supported ensuring that formats and lengths of course
offerings meet the needs of all student demographics.
The results of the study and research supported this recommendation. The selfreported descriptive data showed the majority of students attributed their higher
frequencies of information evaluation behaviors and increased confidence levels to IL
requirement course, LIR 10.
Assessment of
the LIR 10
format and
length showed
that more
online and
shorter
courses are
offed.
The study’s cross-tabulation results supported the finding showing that the LIR 10
format and length was positively associated with ethnicity, primary language, and the
number of papers students had written. The correlational analyses also showed the LIR
10 format had significant positive relationships with the Inputs variables of the English
course level students had achieved, and the self-concept of students’ information
evaluation preparedness as well as confidence levels. The multiple regression results
showed the Environment variable of LIR 10 format was significant in explaining portions
of the variability of the frequency of information evaluation behaviors and confidence
Outcomes variables. Based on those results, students consider the IL general education
requirement to be an effective delivery method and believe that it should be supported by
the college.
The literature supports that consideration should be given to policy decisions
necessary for continuing the IL requirement. Robertson (2013) used evaluation criteria to
keep a community college’s general education program relevant for student engagement
and removing requirements that no longer met the criteria. The college remains
committed to teaching information literacy to its students, suggesting the need for a

219
p.13

reasonable model of targeted assessment. Gurantz’s (2015) studied a California
community college’s registration priorities and student characteristics showing that
students could become lost if support courses were not available. Colleges should review
policies, funding, and staffing allocations to ensure that they offer sufficient courses in
areas with impacted enrollment. Mayer and Bowles-Terry (2013) shared ways that IL
instruction can engage students in subject specific research papers and projects.
Schroeder and Cahoy (2010) articulated the affective learning and critical thinking
competencies that IL students must use in completing research assignments. They
advocated that IL affective learning Outcomes should be included in IL standards to have
the most impact. Bryan (2014) studied the ACRL standards and demonstrated that IL
instruction supported many of the university’s critical thinking outcomes. Hicks and
Sinkinson (2015) also found a connection between critical thinking and IL. They noted
that faculty and librarians should partner to develop pedagogical strategies to maximize
student learning. Radcliff and Wong (2015) provided pretest and posttest research to
support the role that critical thinking plays in IL by incorporating argument learning
outcomes into information evaluation. Hofer Townsend, and Brunetti (2012) observed
that librarians can use the concept of learning thresholds to help struggling students
integrate IL instruction to encourage engagement with IL skills such as information
evaluation needed in other classes. Seeber, K. P. (2015) also supported using IL threshold
concepts noting that changes in online searching technologies require students to
critically evaluate strategies and information sources like never before.
The study and the literature supports the importance of considering the formats
and lengths of course offerings. Cho’s (2011) research showed how an online format can
have a large impact on student satisfaction. The descriptive results provided insight for
making future course offering programmatic decisions that meet the needs of all student
demographics. The results indicated students who identified ethnicity as Hispanic showed
a preference for LIR 10 courses with longer lengths and the on-ground format. Analysis
indicated that the majority of students, 53%, had taken LIR 10 in the shorter length (6
weeks) course, and 67% (n = 349) of students had taken the online format. The findings
show that more study is needed to make conclusions regarding the impact of format. The
program characteristic measured in the study’s survey examined students’ reports of the
formats and lengths of LIR 10 in which they had participated. The data suggests that even
though students reported feeling prepared when they began the LIR 10 course, the course
still affected their frequency of information evaluation behaviors and levels of
confidence. Wolff, Wood-Kustanowitz, and Ashkenazi (2014) examined how student
preparation characteristics and format of delivery had significant effects on completion.
Simpson (2013) noted the need for student support because graduation rates can be 20%
lower in online classes. Clark and Chinburg (2010) agreed with Lim, Morris, and
Kupritz, (2006) that between online and traditional formats no significant differences in
learning outcomes were found. Silk, Perrault, Ladenson, and Nazione’s (2015) had
similar findings in their study of the effectiveness of online and in-person formats.

220
p. 14

Outcomes – IL Requirement Assessment Gap
IL Requirement Assessment Issues Surrounding the Problem
The college needed a targeted assessment process to determine if the goals of its IL
education delivery method were being met and if they conformed to the changing needs of
the institution and its students. The college supports the IL requirement delivery method but
has not specifically assess the student development impact of this method. The competing
pressures of demographic shifts, budget restrictions, accreditation standards, and changing
government regulations brought the college’s attention to the need to examine the relevance
of its required general education courses.
Assessment of general education requirements ensures students are learning what
the institution has placed as its highest priorities (Andrews, 2012; Robertson, 2013; Siefert,
2011; Sorey et al., 2013). Siefert (2011) introduced the Valid Assessment of Learning in
Undergraduate Education (VALUE) as an assessment model of general education learning
outcomes including written communication, inquiry, critical thinking, and information
literacy. Robertson (2013) noted the importance of using evaluation criteria to keep the
community college general education program relevant, thereby increasing student
engagement. This study also recommended removing general education courses that no
longer met the criteria. The college remains committed to teaching information literacy to
its students, suggesting the need for a reasonable model of targeted assessment.
The college’s Institutional Effectiveness Assessment Report (SRJC, 2013a) cited
these high gains for the critical analysis outcomes assessment from the Student Survey
(SRJC, 2013d) as a benchmark measure of educational effectiveness. The broad assessment
of SRJC’s (2013b) Critical Analysis Institutional Learning Outcomes through its triennial
Student Survey (2013d) did not differentiate whether the high critical analysis learning gains
students reported were attributed to participation in the IL requirement course, LIR 10, or
were gained through other means. Despite these IL related gains, the college’s shared
governance committees responded in 2013 to several years of statewide budget cuts with
discussions of eliminating the IL requirement (SRJC, Academic Senate, 2013; SRJC,
Education Planning and Coordination Council [EPCC], 2013). These discussions showed
the need to conduct targeted assessments specific to the impact of the general education IL
requirement course, LIR 10.
The college’s assessment gap was compounded by the lack of published literature
regarding similar requirements at the community college level. Sorey and DeMarte’s (2013)
study illustrated the importance of evaluating general education requirements for student
development. Gonyea (2005) encouraged triangulating self-reported survey data using
multiple data sources. Zachery (2010) reported anecdotal findings that IL education
positively influenced student performance on research papers and stressed the need for the
development of a quantifiable assessment instrument to determine the extent to which IL
affects student development.

221
p.15

Recommendation Based on Research Evidence
Study findings suggested that the I-E-O assessment model in the form of the
learner-centered Information Literacy Requirement Impact Survey was a successful
assessment tool for the IL requirement. The policy recommendation calls for conducting
ongoing targeted assessment and comparing it to broader institutional measures.
3. Use the Information Literacy Requirement Impact Survey to
conduct targeted assessment of the IL general education
requirement on a 3 year basis and correlate the findings with the
college’s triennial Student Survey results.
The results of the study and research supported this recommendation and showed
the validity of using a survey of student self-reports as a targeted assessment instrument.
The Information Literacy Requirement Impact Survey instrument, based on Astin and
Antonio’s (2012) I-E-O assessment model, was successful in collecting self-reported data
specific to the IL requirement program.
The descriptive analysis showed students reported high frequencies of SRJC
Critical Analysis learning gains, ACRL Critical Analysis behavior changes, and levels of
information evaluation confidence for involvement in writing papers and participating in
discussions other classes. The correlation analysis showed the English course variable
had a medium positive correlation with the preparedness self-concept variable and a
small negative one with the two behavior change Outcomes variables. It would be
expected that higher level English course levels require more research papers so students
would gain IL preparation. The regression analysis was valid and adequate for measuring
students’ self-reports of critical information evaluation behavior changes and levels of
confidence. Using the I-E-O assessment model can promoting the use of targeted data for
decision-making. These data have the potential to more fully inform and demonstrate that
the general education IL requirement education delivery method had a positive impact on
information evaluation skills, including increased confidence.
The high preparation
levels students reported
also affected the amount
of variability that
multiple regression
models were able to
explain for the Outcomes
variables.

222
p.16

For the information evaluation Outcomes item, the majority of students, 89%,
reported they had experienced some level of positive learning gains. Only 11% reported
none or don’t know/ can’t answer. The drawing conclusions item was mixed.

These results
indicate the IL
requirement
had an impact
on students’
critical
analysis
learning gains.
The results on the information evaluation item from the SRJC (2013d) Student
Survey showed that 85% of the student responses indicated learning gains. The results
from the Information Literacy Requirement Impact Survey showed 89% of students
reported that they had experienced some level of positive learning gains after taking the
IL requirement. The similar percentages of students in this comparison indicates that
more research should be done to confirm that participation in the IL requirement was the
origin of students’ gains in critical analysis knowledge and abilities.
The SRJC Critical Analysis survey items
used the exact structure as those on
SRJC’s (2013d) Student Survey. This
alignment would be useful in future
longitudinal assessments to allow results
from the two instruments to be
compared. As discussed SRJC’s broad
Student Survey assessment results do not
provide data specific to the general
education IL requirement. These results
show how the study’s targeted assessment
results could allow for comparison. It is
to be noted these results are for
illustrative purposes only and cannot be
considered valid because the surveys
were conducted at different times.
The regression analysis showed a statistically significant relationship among the
ethnicity, age category, primary language, preparedness, self-concept, and papers written
characteristics of students, the IL requirement course LIR 10 formats, and information
evaluation behavior changes (SRJC Critical Analysis and ACRL Critical Analysis).

223
p.17

Even though the students reported that they came to LIR 10 feeling somewhat
prepared, descriptive analysis showed the majority reported the IL requirement had a
positive impact on their six ACRL (2000) information evaluation behaviors selected from
ACRL (2000) Information Literacy Standard Three performance indicator outcomes. The
discussion outcome was the only one that did not score high. These ACRL information
evaluation behaviors included confidence in applying information evaluation criteria,
comparing new with prior knowledge to determine contradictions; understanding
information through discourse with others, and determining if search query should be
revised to improve results.

Even though the students reported that they came to LIR 10 feeling somewhat
prepared, the descriptive analysis showed the majority reported the IL requirement had a
positive impact on their level of confidence in writing papers or participating in
discussions in other courses based on the information evaluation skills learned in LIR 10.
Responses indicated that 76% of the
students said they were more confident
with somewhat confident (n = 204) as the
largest group and super confident (n =
197) a close second. Those that said they
were neutral (n = 116), those that said
they were somewhat unconfident (n = 10),
and completely unconfident (n = 10)
combined to be 24% of the respondents.
The regression analysis showed a statistically significant relationship among the
age category of students, the IL requirement course LIR 10 formats, and information
evaluation confidence in other courses.

224
p. 18

Recommendations
Recommendations for Practice

Recommendations for Future Research

This study’s research and findings have
several practical applications for the
college. These recommendations for
programmatic and policy changes were
based on the issues identified in this
study including student preparation, the
IL requirement as an education delivery
method, and general education
requirement assessment. The college can
apply these recommendations as
solutions for its targeted assessment gap
problem by using the study’s findings to
evaluate the effectiveness of its IL
requirement

1. Conducting longitudinal or repeated survey
research on the general education IL
requirement using the Information Literacy
Requirement Impact Survey instrument will
increase the reliability and validity of this
study’s results.
2. Given the success of the Information
Literacy Requirement Impact Survey for
gaining targeted assessment data, the college
should consider using the Astin and
Antonio’s (2012) I-E-O assessment model to
assess other general education requirements.
3. Additional data collection related to the
timing of when in their academic career
students took the IL requirement will
provide insight into the optimal timing for
delivery of IL education via the IL
requirement, thereby increasing the
requirement’s usefulness for students.
4. A case study of a course requiring research
could be done comparing students selfreports on information evaluation measures
to instructor reports of those same measures
and triangulating with the students’ grade
data.
5. Future research could include using
objective data collection methods. An
example would be comparing the gradepoint-averages of groups of students who
took the IL requirement course with those
who did not.
6. The college could consider using a
standardized IL skills assessment instrument
such as a pretest and posttest so assessment
results could be obtained immediately after
course completion and compared across
courses.
7. Qualitative research methods such as
interviews or focus groups could provide a
rich data set of learner-centered comments
that might validate whether or not the IL
requirement was meeting students’ and the
institution’s needs.

1. Use the study’s learner-centered selfreports to develop a process
prioritizing that the IL requirement is
completed early in the general
education requirement pathway.
2. Use the study’s findings as an
indicator that the IL general education
requirement course is an effective
delivery method that should be
supported ensuring that formats and
lengths of course offerings meet the
needs of all student demographics.
3. Use the Information Literacy
Requirement Impact Survey to conduct
targeted assessment of the IL general
education requirement on a 3 year
basis and correlate the findings with
the college’s triennial Student Survey
results.

225
p. 19

Conclusion
This quantitative research study and white paper project outlined a local
educational problem; the lack of targeted assessment practices at a community college.
The study examined relationships between student characteristics, aspects of the general
education IL requirement, and subsequent frequency of student use of IL critical
information evaluation behaviors, confidence in writing papers, and participation in
discussions in other courses. The study summarized the research and associated findings
and made recommendations related to future targeted assessment of the impact of the
general education IL option for equipping students with the information evaluation
knowledge and skill set needed for successful involvement in other courses. The study
provided recommendations for future research.
The Information Literacy Requirement Impact Survey involved assessment of
student self-reports. The study’s findings provided supportive data that key stakeholders
can use to gauge the effectiveness of the general education IL requirement as an
educational delivery method. This study obtained relevant findings supporting the
effectiveness of the IL education delivery method using a survey aligned with the
learner-centered mission of the college. Because the study was successful, the survey
produced an instrument that the college can reapply in subsequent years to see how
Outcomes change and how the college can apply changes to the LIR 10 course to help
meet ongoing needs.
The study’s assessment produced results from three types of data analysis:
descriptive, correlation, and multiple regression. The subsequent findings provided data
to support a recommendations type project. The descriptive data provided a profile of the
students, and the univariate analysis showed that the majority of students reported that
the IL requirement had a favorable impact on their information evaluation behaviors and
confidence. The cross-tabulation and correlation analyses provided bivariate data that
showed all variables in the study had one or more significant relationships with other
variables measured, except for gender, and thus this study used them in the regression
analysis. The multiple regression provided multivariate data that answered the study’s
research questions and hypotheses and showed that some student (Inputs) and program
(Environment) characteristic variables were significantly related to the Outcomes
variables. Results of data analysis justify the identification of the college’s assessment
gap problem and the need to share and further evaluate the target specific indicators of
the program goals and objectives. The overall results revealed that students reported
favorably on their IL learning experiences in the IL delivery system.
Astin and Antonio’s (2012) I-E-O assessment model framed the study by
emphasizing the relationship patterns. It provided the background of the problem
showing the preparedness issues surrounding student characteristic Inputs, the IL
education delivery methods characteristic of the Environment, and the students’ critical
analysis behavior and confidence Outcomes that will fill the college’s gap in general
education assessment practices.

226
p. 20

The study results provide informative background of how the data collection and
analysis results answered the research questions regarding the relationship between the
student characteristics, the IL requirement and students’ reported feelings of critical
information evaluation development. This study builds upon the SRJC (2013b)
Institutional Learning Outcomes assessments currently practiced by the college but gives
targeted information specific to the IL requirement program that can inform decision
makers about the contributions of this specific general education requirement.
The study’s results presented data that stakeholders can use as a starting place for
an improvement of the college’s assessment practices. The study also provided theorybased evidence that the college can use to assess the effectiveness of the general
education IL requirement. The findings from this research may encourage social change,
especially within the education community. A college that can offer effective
information literature education influences societal change by promoting student critical
analysis behaviors that impact students’ ability to think critically in their other courses,
workplace, and civically. Promoting student success and providing higher level skilled
workers to compete in a global workforce enhances the potential for positive societal
change. If the study’s findings are effective in garnering the college’s support,
individuals and educational organizations could benefit.
This study offers a starting place for educational decision makers to best approach
future assessment practices. Stakeholder feedback will be invaluable for assessing this
project and informing future research. The education community can be especially
resistant to change, and this may prove to be true for the changes in assessment practices
recommended by this project. The practices of feedback, process revisions, and success
stories, provide opportunities for the college to accept these changes as useful tools. By
completing this project study This study placed research into practice by promoting more
focused assessment opportunities. These opportunities could contribute to social change
by assisting college leadership in helping students obtain the skills they need in a timely
manner, succeed academically, and qualify and compete for higher-level jobs within the
community.

227
p. 21

References
Academic Senate for California Community Colleges (ASCCC). (1998). Information competency in the California community
colleges. Retrieved from http://asccc.org/node/174895
Accrediting Commission for Community and Junior Colleges (ACCJC). (2014). Accreditation standards. Retrieved from
http://www.accjc.org/wp-content/uploads/2014/07/Accreditation_Standards_Adopted_June_2014.pdf
Andrews, C. R. (2012). Libraries and general education: New strategies to enhance freshman orientation, faculty collaboration, and
curriculum development. International Journal of Learning, 18(5), 109–131.
Association of College & Research Libraries (ACRL). (1989). Presidential committee on information literacy: Final report.
Retrieved from http://www.ala.org/acrl/publications/whitepapers/presidential
Association of College & Research Libraries (ACRL). (1998). A progress report on information literacy: An update on the American
Library Association presidential committee on information literacy: Final report. Retrieved from
http://www.ala.org/acrl/publications/whitepapers/progressreport
Association of College & Research Libraries (ACRL). (2000). Information literacy competency standards for higher education.
Retrieved from http://www.ala.org/acrl/sites/ala.org.acrl/files/content/standards/standards.pdf
Astin, A. W. (1999) Student involvement: A developmental theory for higher education. Journal of College Student Development,
40(5), 518–529.
Astin, A. W., & Antonio, A. (2012). Assessment for excellence: The philosophy and practice of assessment and evaluation in higher
education (2nd ed.) Lanham, MD: Rowman & Littlefield Publishers.
Bowles-Terry, M. (2012). Library instruction and academic success: A mixed-methods assessment of a library instruction program.
Evidence Based Library and Information Practice, 7(1), 82–95.
Bryan, J. E. (2014). Critical thinking, information literacy and quality enhancement plans. Reference Services Review, 42(3), 388-402.
http://dx.doi.org/10.1108/RSR–01-2014-0001
Cabrera, A. S. P. (2014). First generation minority students: Understanding the influential factors that contributed to their preparation
and decision to pursue higher education. PSU McNair Scholars Online Journal, 8(1), Article 2, n.p. Retrieved from
http://pdxscholar.library.pdx.edu/mcnair/vol8/iss1/2
California Community Colleges Chancellor's Office (CCCCO). (2011). Student success task force final report. Retrieved from
http://www.californiacommunitycolleges.cccco.edu/Portals/0/Executive/StudentSuccessTaskForce/SSTF_Final_Report_1-1712_Print.pdf
California Community Colleges Chancellor's Office (CCCCO). (2014a). California community colleges student success scorecard:
Santa Rosa Junior College. Retrieved from http://scorecard.cccco.edu/scorecardrates.aspx?CollegeID=261
California Community Colleges Chancellor's Office (CCCCO). (2014c). Management systems data mart: Sonoma community college
district. Retrieved from http://datamart.cccco.edu/Students/Student_Term_Annual_Count.aspx
Chen, C., Pedersen, S., & Murphy, K. (2012). The influence of perceived information overload on student participation and
knowledge construction in computer-mediated communication. Instructional Science, 40(2), 325–349. http://dx.doi:10.1007/s11251011-9179-0
Cho, S., & Mechur Karp, M. (2013). Student success courses in the community college: early enrollment and educational outcomes.
Community College Review, 41(1), 86–103. http://dx.doi:10.1177/0091552112472227
Cho, T. (2011). The impact of types of interaction on student satisfaction in online courses. International Journal on E-Learning,
10(2), 109–125.
Clark, S., & Chinburg, S. (2010). Research performance in undergraduates receiving face to face versus online library instruction: A
citation analysis. Journal of Library Administration, 50(5/6), 530–542. http://dx.doi:10.1080/01930826.2010.488599
Detmering, R., & Johnson, A. (2011). Focusing on the thinking, not the tools: Incorporating critical thinking into an information
literacy module for an introduction to business course. Journal of Business & Finance Librarianship, 16(2), 101–107.
http://dx.doi:10.1080/08963568.2011.554771

228
Finley, W., & Waymire, T. (2012). Information literacy in the accounting classroom: a collaborative effort. Journal of Business &
Finance Librarianship, 17(1), 34–50. http://dx.doi:10.1080/08963568.2012.629566
Frost, R. A., Strom, S. L., Downey, J., Schultz, D. D., & Holland, T. A. (2010). Enhancing student learning with academic and
student affairs collaboration. Community College Enterprise, 16(1), 37–51.
Gonyea, R. M. (2005). Self-reported data in institutional research: Review and recommendations. New Directions for Institutional
Research, 127, 73–89. Retrieved from http://sshl.sysu.edu.cn/docs/2013-03/20130327172420936127.pdf
Gross, M., & Latham, D. (2012). What's skill got to do with it? Information literacy skills and self-views of ability among first-year
college students. Journal of the American Society for Information Science & Technology, 63(3), 574–583.
http://dx.doi:10.1002/asi.21681
Gurantz, O. (2015). Who loses out? Registration order, course availability, and student behaviors in community college. Journal of
Higher Education, 86(4), 524–565.
Head, A., & Eisenberg, M. (2009). Finding context: What today's college students say about conducting research in the digital age.
Project Information Literacy at University of Washington. Retrieved from
http://projectinfolit.org/images/pdfs/pil_progressreport_2_2009.pdf
Head, A. (2013). Learning the ropes: How freshmen conduct course research once they enter college. Project Information Literacy at
University of Washington. Retrieved from http://projectinfolit.org/images/pdfs/pil_2013_freshmenstudy_fullreport.pdf
Hellenius, S. (2007). Information competency graduation programs: A survey of methods. Retrieved from
http://www.asccc.org/node/176738
Hicks, A., & Sinkinson, C. (2015). Critical connections: Personal learning environments and information literacy. Research in
Learning Technology, 231–12. http://dx.doi:10.3402/rlt.v23.21193
Hofer, A. R., Townsend, L., & Brunetti, K. (2012). Troublesome concepts and information literacy: Investigating threshold concepts
for IL instruction. Portal: Libraries and the Academy, 12(4), 387–405.
Johnston, N., Partridge, H., & Hughes, H. (2014). Understanding the information literacy experiences of EFL (English as a foreign
language) students. Reference Services Review, 42(4), 552–568. http://dx.doi.org/10.1108/RSR-05-2014-0015
Kim, J. & Bragg, D. D. (2008). The impact of dual and articulated credit on college readiness and retention in four community
colleges. Career & Technical Education Research, 33(2), 133–158.
Kuh, G. D. (2008). High-impact educational practices: What they are, who has access to them, and why they matter. Washington,
DC: Association of American Colleges and Universities.
Kuh, G. D., & Gonyea, R. M. (2015). The Role of the Academic Library in Promoting Student Engagement in Lear ning. College &
Research Libraries, 76(3), 359–385.
Kuhlthau, C. (1991). Inside the search process: information seeking from the user's perspective. Journal of the American Society for
Information Science, 42(5), 361–371.
Mayer, J., & Bowles-Terry, M. (2013). Engagement and assessment in a credit-bearing information literacy course. Reference
Services Review, 41(1), 62–79. http://dx.doi.org/10.1108/00907321311300884
Moore, D., Brewster, S., Dorroh, C., & Moreau, M. (2001). Information competency instruction in a two-year college: One size does
not fit all. Reference Services Review, 30(4), 300–306. http://dx.doi:10.1108/00907320210451286
Mulvey, M. E. (2009). Characteristics of under-prepared students: Who are "the under-prepared"? Research & Teaching in
Developmental Education, 25(2), 29–58.
Radcliff, S., & Wong, E. Y. (2015). Evaluation of sources: A new sustainable approach. Reference Services Review, 43(2).
http://dx.doi.org/10.1108/RSR-09-2014-0041
Ritzhaupt, A. D., Feng, L., Dawson, K., & Barron, A. E. (2013). Differences in student information and communication technology
literacy based on socioeconomic status, ethnicity, and gender: Evidence of a digital divide in Florida schools. Journal of Research on
Technology in Education, 45(4), 291–307.
Robertson, S. N. (2013). General education: Charting a roadmap toward student success. Peer Review, 15(2), 18–19.
Roscoe, J. L. (2015). Advising African American and Latino students. Research & Teaching in Developmental Education, 31(2), 48–60.

229
Santa Rosa Junior College (SRJC). (2013a). Institutional effectiveness assessment report: Benchmarks for Santa Rosa Junior College.
Retrieved from http://www.santarosa.edu/administration/planning/pdfs/2013/IE Assessment Report 2013.pdf
Santa Rosa Junior College (SRJC). (2013b). Institutional learning outcomes. Retrieved from
http://www.santarosa.edu/slo/institutional/
Santa Rosa Junior College (SRJC). (2013d). Student survey Fall 2013. Retrieved from
http://www.santarosa.edu/administration/planning/pdfs/2013%20Student%20Survey%20-%20final%20draft.pdf
Santa Rosa Junior College (SRJC). (2013e). Vision, mission statement, values. Retrieved from
http://www.santarosa.edu/polman/1mission/1.1.pdf
Santa Rosa Junior College (SRJC), (2014–15). Associate degree graduation requirements and general education option A, 2014–15.
Retrieved from http://www.santarosa.edu/for_students/student-services/articulation/pdf/AA-AS_2014-15.pdf
Santa Rosa Junior College (SRJC), Academic Senate. (2013). Minutes of April 3, 2013. Retrieved from
http://www.santarosa.edu/senate/archive/minutes/Minutes%202013/minutes130403.doc
Santa Rosa Junior College (SRJC), Educational Planning & Coordinating Council (EPCC). (2013). Minutes of March 14, 2013.
Retrieved from https://bussharepoint.santarosa.edu/committees/educational-planningcoordinating/Committee%20Documents/EPCC%20Minutes%203-14-13.pdf
Santa Rosa Junior College (SRJC), Office of Institutional Research (OIR). (2013). Fact book 2013. Retrieved from
http://www.santarosa.edu/research/pdfs/FB%202012.pdf
Schnee, E. (2014). A foundation for something bigger: Community college students' experience of remediation in the context of a
learning community. Community College Review, 42(3), 242–261. http://dx.doi:10.1177/0091552114527604
Schroeder, R., & Cahoy, E. S. (2010). Valuing information literacy: affective learning and the ACRL standards. Portal: Libraries and
the Academy, 10(2), 127–146.
Seeber, K. P. (2015). Teaching “format as a process” in an era of Web-scale discovery. Reference Services Review, 43(1), 19–30.
http://dx.doi.org/10.1108/RSR-07-2014-0023
Siefert, L. (2011). Assessing general education learning outcomes. Peer Review, 13/14(4/1), 9–11.
Silk, K. J., Perrault, E. K., Ladenson, S., & Nazione, S. A. (2015). The effectiveness of online versus in-person library instruction on
finding empirical communication research. Journal of Academic Librarianship, 41(2), 149–154.
http://dx.doi:10.1016/j.acalib.2014.12.007
Simpson, O. (2013). Supporting students for success in online and distance education. New York, NY: Routledge
Sorey, K., & DeMarte, D. (2013). Assessment of general education: Adapting the AAC&U value rubrics. Peer Review, 15(2), 27–28.
Taylor, A. (2012). A study of the information search behavior of the millennial generation. Information Research, 17(1) 508.
Retrieved from http://InformationR.net/ir/17-1/paper508.html
Tovar, E. (2015). The role of faculty, counselors, and support programs on Latino/a community college students’ success and intent
to persist. Community College Review, 43(1), 46–71. http://dx.doi:10.1177/0091552114553788
Varlejs, J., Stec, E., & Kwon, H. (2014). Factors affecting students' information literacy as they transition from high school to college.
School Library Research, 171–23.
York, C. (2013). Overloaded by the news: Effects of news exposure and enjoyment on reporting information overload.
Communication Research Reports, 30(4), 282–292. http://dx.doi:10.1080/08824096.2013.836628
WASC Senior College and University Commission. (2013). Handbook of accreditation – 2013. Retrieved from
http://www.wascsenior.org/content/2013-handbook-accreditation
Wolff, B. G., Wood-Kustanowitz, A. M., & Ashkenazi, J. M. (2014). Student performance at a community college: Mode of delivery,
employment, and academic skills as predictors of success. Journal of Online Learning & Teaching, 10(2), 166–178.
Zachery, I. (2010). The effect of information literacy competency on student learning and success in three California community
colleges. (Doctoral dissertation) Retrieved from ProQuest Digital Dissertations. (Order No. 3421710, George Mason University).

230
Appendix B: Survey Instrument
Information Literacy Requirement Impact Survey












Purpose: This survey explores the impact the Area I Information Literacy requirement (LIR 10) had on how you
evaluate the information you need for research in other classes.
Anonymous: No name, email, or IP address will be linked to your answers in any way.
Required Time: The survey takes approximately 5–10 minutes to complete.
Inclusion Criteria: a minimum of 18 years of age and to have successfully completed SRJC’s IL requirement
course, LIR 10, with a grade of 2.00 or better in Summer 2013, Fall 2013, or Spring 2014.
Participation is Voluntary: You were invited because you successfully completed a LIR 10 class. You may
refuse to participate or quit at any time without penalty.
How Results Will Be Used: The results of this study will help the research determine the effectiveness of this
general education requirement. The study research will be published in a doctoral study, an education journal, and
presented at professional conferences.
Format: You will answer 15 quick multiple-choice items about yourself, your information evaluation behaviors,
and confidence after completing LIR 10.
Risks: There are no known risks involved in participating in this study.
Benefits: A benefit from participation is the chance to reflect on learning development and the potential
realization of a gain in confidence. Future students may benefit from research on information evaluation. No
compensation will be offered.
Confidentiality: The online survey is anonymous and all results will be summarized and stored securely without
any identifying information. Data will be kept for a period of at least 5 years, as required by the university.
Contact information: The primary researcher is Phyllis Usina. You may already know me as a Librarian at Santa
Rosa Junior College, but this study is separate from that role, as I am conducting the study as a Walden University
student. If you have items about the study or the procedures, you may contact her at phyllis.usina@waldenu.edu.
If you want to discuss your rights as a participant privately, you can contact Dr. Leilani Endicott at
irb@waldenu.edu. Walden University’s approval number for this study is 04-23-15-0319952 and it expires on
April 22, 2016.

1. *Where have you taken classes since you completed LIR 10? (Mark all that
apply)
SRJC or another 2-year college
4-year college/university
Private college/university
No college/university
2. How old were you when you took LIR 10?
19 or younger
20–24
25–29
30–34
35–39
40–49
50 or older
3. What is your gender identification?
Female
Male
Other

231
4. What is your racial / ethnic background? (Mark all that apply)
American Indian
Asian
Black (African American)
Filipino
Hispanic
Pacific Islander
White
Other
5. Is English your primary language?
Yes
No
6. How many terms had you attended college before you took LIR 10? Please
include all terms, semesters, or quarters, at all college ever attended.
0 Terms (just started college)
1–2 Terms (1st year of college)
3–4 Terms (2 nd year of college)
5–6 Terms (3rd year of college)
7–8 Terms (4th year of college)
9–12 Terms (5th year of college)
13+ Terms (6th+ year of college)
7. Up to and including the semester you took LIR 10, had you EVER taken any of
the following courses? (Mark all that apply)
Any College Skills English courses
Any English as a Second Language (ESL) courses
English 302 or 305
English 100
English 1A
English 5
No English course
8. Before taking LIR 10 how prepared were you to evaluate the information
required to write papers or participate in discussions in other courses?
Super prepared
Somewhat prepared
Don’t know
Somewhat unprepared
Completely unprepared

232
9. How many college research papers, that required you to evaluate information,
had you written before you took LIR 10?
0 papers
1–2 papers
3–4 papers
5–6 papers
7–8 papers
9–12 papers
13+ papers
10. What format was your LIR 10 class in?
On-ground (Face-to-face)
Online
Hybrid
11. What length was your LIR 10 class?
1 week (Credit by Exam)
6 weeks
9 weeks
12 weeks
12. To what extent do you think taking LIR 10 contributed to your knowledge,
skills, and abilities in the following areas:
A lot
Some A
None Don’t know/
little
Can’t answer
a. Locating, analyzing, evaluating,
and synthesizing relevant
information.
b. Drawing reasonable conclusions
in order to make decisions and
solve problems.
13. How frequently do you do these actions now compared to how often you did
them before you took LIR 10?
Action
A lot
Somewhat No
Somewhat A lot less
more
more
Change Less
frequently
frequently frequently
frequently
a. I now determine
whether the information
satisfies my research need.
b. I now review my search
strategy and incorporate

233
additional concepts as
necessary.
c. I now determine
whether the information
contradicts or verifies
information used from
other sources.
d. I now compare
information from various
sources in order to
evaluate reliability,
validity, accuracy,
authority, timeliness, and
point of view or bias.
e. I now select
information that provides
evidence for the topic.
f. I now participate in
classroom and other
discussions.
14. After taking LIR 10 what is your level of confidence in writing papers or
participating in discussions in other courses based on the information evaluation
skills you learned?
Super confident
Somewhat confident
Neutral
Somewhat unconfident
Completely unconfident
15. Which terms do you recommend as the most helpful to take LIR 10?
1–2 Terms (1st year of college study)
3–4 Terms (2 nd year of college study)
5–6 Terms (3rd year of college study)
7–8 Terms (4th year of college study)
9–12 Terms (5th year of college study)
13+ Terms (6th+ year of college study)
Term taken does not matter
To submit your answers and close the survey you must use the "Click to Submit" button below.
Thank you for your time!
Your input will be extremely valuable to the research of information literacy education.

234
Appendix C: Peer Expert Review
I am a student in the Ed.D. Higher Education Leadership program at Walden
University. In my doctoral research study titled, Impact of a California Community
College’s General Education Information Literacy Requirement, I will conduct a survey
of Santa Rosa Junior College (SRJC) students who successfully completed the general
education information literacy requirement. Permission to conduct this research has been
conditionally obtained from SRJC pending Walden University Institutional Review
Board (IRB) approval.
You have been selected as a peer expert to review the Information Literacy
Requirement Impact Survey instrument. Your participation is completely voluntary and
your identity will not be published. Your assessment is crucial to help establish the
content validity of my instrument. Please review the survey items and fill in the form
below.
Thank you for your participation. If you have any questions, please contact me.
Phyllis Usina
How long did it take you to complete the survey?

Survey Item
1.
2.
3.
4.
5.
6.
7.
8.
9.
10.
11.
12.
13.
14.
15.
16.
17.
18.
19.
20.

Start time: ______________ End time: ______________

Appropriate
level

Easy to
understand

Complete

Yes
Yes
Yes
Yes
Yes
Yes
Yes
Yes
Yes
Yes
Yes
Yes
Yes
Yes
Yes
Yes
Yes
Yes
Yes
Yes

Yes
Yes
Yes
Yes
Yes
Yes
Yes
Yes
Yes
Yes
Yes
Yes
Yes
Yes
Yes
Yes
Yes
Yes
Yes
Yes

Yes
Yes
Yes
Yes
Yes
Yes
Yes
Yes
Yes
Yes
Yes
Yes
Yes
Yes
Yes
Yes
Yes
Yes
Yes
Yes

No
No
No
No
No
No
No
No
No
No
No
No
No
No
No
No
No
No
No
No

No
No
No
No
No
No
No
No
No
No
No
No
No
No
No
No
No
No
No
No

No
No
No
No
No
No
No
No
No
No
No
No
No
No
No
No
No
No
No
No

Use the
Item in the
Survey
Yes No
Yes No
Yes No
Yes No
Yes No
Yes No
Yes No
Yes No
Yes No
Yes No
Yes No
Yes No
Yes No
Yes No
Yes No
Yes No
Yes No
Yes No
Yes No
Yes No

ACRL
Standard
alignment
Yes No
Yes No
Yes No
Yes No
Yes No
Yes No
Yes No
Yes No
Yes No
Yes No
Yes No
Yes No
Yes No
Yes No
Yes No
Yes No
Yes No
Yes No
Yes No
Yes No

SRJC
outcome
alignment
Yes No
Yes No
Yes No
Yes No
Yes No
Yes No
Yes No
Yes No
Yes No
Yes No
Yes No
Yes No
Yes No
Yes No
Yes No
Yes No
Yes No
Yes No
Yes No
Yes No

Comments

235
Appendix D: Pilot Test Invitation Email
From: Randomly Selected SRJC LIR 10 Instructor Name on behalf of Phyllis Usina
To: [selected students who meet pilot study’s eligibility criteria]
Subject: Invitation to pilot test LIR 10 Requirement Survey
Dear LIR 10 Graduate,
You are invited to participate in a pilot study of my doctoral research about the
impact of LIR 10 at Santa Rosa Junior College (SRJC). My name is Phyllis and I am
conducting this pilot study in my role as a doctoral student at Walden University. My
doctoral research is about information evaluation behavior changes and confidence levels
of SRJC students who completed LIR 10. I am also a Librarian at Santa Rosa Junior
College.
You were invited to participate in this pilot study because you are 18 years of age
or older and to have successfully completed SRJC’s IL requirement course, LIR 10, with
a grade of 2.00 or better during Spring 2015.
In the pilot study I am testing a survey instrument I created to assess information
evaluation behavior changes and confidence levels. Your feedback is important and is
needed. Your assessment of the pilot survey items will help determine how well SRJC
students will understand the survey items.
This pilot survey is short, 10–15 minutes of your time, and has easy to answer
multiple choice items. Your participation is completely voluntary and will be anonymous.
To begin the online pilot survey now click on this link:
https://www.surveymonkey.com/s/LIR10surveyFeedback.
If you don't have time to take the pilot survey immediately that’s okay, it will be
available for 48 hours.
Thank you in advance for helping me to learn more about the impact of LIR 10. If
you have questions about the study or the procedures, you may contact me at
phyllis.usina@waldenu.edu. If you want to discuss your rights as a participant privately,
you can contact Dr. Leilani Endicott at irb@waldenu.edu. Walden University’s approval
number for this study is 04-23-15-0319952 and it expires on April 22, 2016.
Phyllis Usina
Walden University
707-778-2425 – phyllis.usina@waldenu.edu
This project has been reviewed and approved by the Walden University Institutional Review Board and the
SRJC Office of Institutional Research.

Note: These additional items appeared under items being tested in the pilot study of the
survey instrument.
Easy to
Understand?
Yes
No

Comments: if you answered no, please explain. For example, tell what you think was
confusing or why you did not understand the objective of the item.

Use the Item in
the Survey?
Yes
No

236
Appendix E: Prenotification of Invitation to Participate in Upcoming Study

From: SRJC Office of Institutional Research on behalf of Phyllis Usina
To: [selected students]
Subject: LIR 10 Impact Study Begins Next Week
Dear LIR 10 Graduate,
You are one of the students invited to voice your opinion about the impact of LIR
10 at Santa Rosa Junior College (SRJC) in an online survey to start in one week.
My name is Phyllis and I am conducting this study in my role as a doctoral
student at Walden University. My doctoral study research is about information evaluation
behavior changes and confidence levels of SRJC students who successfully completed
LIR 10.
Your feedback is important and is needed. Your views about how you now
evaluate information in other courses will help show the impact of the Area I general
education requirement that is met by LIR 10.
You were invited to participate in this study because you are 18 years of age or
older and you have successfully completed SRJC’s IL requirement course, LIR 10, with a
grade of 2.00 or better in Summer 2013, Fall 2013, or Spring 2014.
This survey is short, 5–10 minutes of your time, and has easy to answer multiple
choice items. Your participation is completely voluntary and will be anonymous.
Next week you will get an email with the link to the survey. The survey will be
available for two weeks, from Friday, May, 8, 2015 until Friday, May, 22, 2015.
If you have questions about the study or the procedures, you may contact me at
phyllis.usina@waldenu.edu. If you want to discuss your rights as a participant privately,
you can contact Dr. Leilani Endicott at irb@waldenu.edu. Walden University’s approval
number for this study is 04-23-15-0319952 and it expires on April 22, 2016.
Thank you in advance for helping me to learn more about the impact of LIR 10.
Phyllis Usina
Walden University
707-778-2425 – phyllis.usina@waldenu.edu
This project has been reviewed and approved by the Walden University Institutional Review Board and the
SRJC Office of Institutional Research.

237
Appendix F: Invitation to Participate in Study
From: SRJC Office of Institutional Research on behalf of Phyllis Usina
To: [selected students]
Subject: LIR 10 Impact Study Starts Today
Dear LIR 10 Graduate,
Remember me? I am Phyllis Usina, the Walden University student doing the
doctoral study about LIR 10.
As I emailed last week, you are invited to participate in my doctoral study about
the impact of LIR 10 at Santa Rosa Junior College (SRJC).
My doctoral study research is about information evaluation behavior changes and
confidence levels of SRJC students who successfully completed LIR 10. I am also a
Librarian at Santa Rosa Junior College.
Your views are vital to the study. Only you know how LIR 10 has influenced the
way you evaluate information you use for writing papers and participating in class
discussions.
You were invited to participate in this study because you are 18 years of age or
older and you have successfully completed SRJC’s IL requirement course, LIR 10, with a
grade of 2.00 or better in Summer 2013, Fall 2013, or Spring 2014.
Participation is completely voluntary. The survey is anonymous and confidential.
Take the online survey now. It is quick, only 5–10 minutes and easy. To begin
click on this link: https://www.surveymonkey.com/s/LIR10survey.
If you don't have time to take the survey immediately that’s okay, it will be
available for two weeks, from Friday, May 8, 2015 until Friday May, 22, 2015.
Thanks for your help. This study is only useful if everyone who gets the survey
sends it in. If you have questions about the study or the procedures, you may contact me
at phyllis.usina@waldenu.edu. If you want to discuss your rights as a participant
privately, you can contact Dr. Leilani Endicott at irb@waldenu.edu. Walden University’s
approval number for this study is 04-23-15-0319952 and it expires on April 22, 2016.

Phyllis Usina
Walden University
707-778-2425 – phyllis.usina@waldenu.edu
This project has been reviewed and approved by the Walden University Institutional Review Board and the
SRJC Office of Institutional Research.

238
Appendix G: Reminder to Participate in Study
From: SRJC Office of Institutional Research on behalf of Phyllis Usina
To: [selected students]
Subject: LIR 10 Impact Study Ends Friday
Dear LIR 10 Graduate,
Almost two weeks ago I sent an e-mail asking for your views on an online survey
about the LIR 10 course you completed in Summer 2013, Fall 2013, or Spring 2014.
My name is Phyllis and I am conducting this study in my role as a doctoral
student at Walden University. I am also a Librarian at Santa Rosa Junior College.
A big thanks if you submitted the survey and sorry for the extra email. The survey
is anonymous so I can’t filter out emails of people who already responded.
If you have not turned in the survey yet, please take 5–10 minutes and do it now
by clicking on this link https://www.surveymonkey.com/s/LIR10survey.
I made taking the survey super easy by using multiple-choice type items to get
data about your experiences. It will only be available for a few more days, until Friday,
May, 22, 2015.
Please help, I need to get as many responses as possible to have enough data to
make the study significant. You are the only one who can show if the Area I general
education requirement that is met by LIR 10 is working.
If you have questions about the study or the procedures, you may contact me at
phyllis.usina@waldenu.edu. If you want to discuss your rights as a participant privately,
you can contact Dr. Leilani Endicott at irb@waldenu.edu. Walden University’s approval
number for this study is 04-23-15-0319952 and it expires on April 22, 2016.
Thanks so much for taking the time out of your day to help with the study.
Phyllis Usina
Walden University
707-778-2425 – phyllis.usina@waldenu.edu
This project has been reviewed and approved by the Walden University Institutional Review Board and the
SRJC Office of Institutional Research.

– Deadline Extended
Last call. I have extended the survey deadline to May, 29, 2015 because I did not
received enough surveys to have enough data to make the study significant. I really need
your help.

239
Appendix H: Protecting Human Research Participants Training Certificate

Certificate of Completion
The National Institutes of Health (NIH) Office of Extramural Research certifies
that Phyllis Usina successfully completed the NIH Web-based training course
“Protecting Human Research Participants.”
Date of completion: 06/21/2013
Certification Number: 1202188

240
Appendix I: ACRL Information Literacy Standard Three
http://www.ala.org/acrl/sites/ala.org.acrl/files/content/standards/standards.pdf

Association of College and Research Libraries (ACRL) Information Literacy
Competency Standard for Higher Education
Standard Three
The information literate student evaluates information and its sources critically and incorporates selected
information into his or her knowledge base and value system.

Performance Indicators:
1.

The information literate student summarizes the main ideas to be extracted from the information
gathered.
Outcomes Include:
a.
b.
c.

2.

Reads the text and selects main ideas
Restates textual concepts in his/her own words and selects data accurately
Identifies verbatim material that can be then appropriately quoted

The information literate student articulates and applies initial criteria for evaluating both the
information and its sources.
Outcomes Include:
a.
b.
c.
d.

3.

Examines and compares information from various sources in order to evaluate reliability,
validity, accuracy, authority, timeliness, and point of view or bias
Analyzes the structure and logic of supporting arguments or methods
Recognizes prejudice, deception, or manipulation
Recognizes the cultural, physical, or other context within which the information was created
and understands the impact of context on interpreting the information

The information literate student synthesizes main ideas to construct new concepts.
Outcomes Include:
a.
b.
c.

4.

Recognizes interrelationships among concepts and combines them into potentially useful
primary statements with supporting evidence
Extends initial synthesis, when possible, at a higher level of abstraction to construct new
hypotheses that may require additional information
Utilizes computer and other technologies (e.g. spreadsheets, databases, multimedia, and audio
or visual equipment) for studying the interaction of ideas and other phenomena

The information literate student compares new knowledge with prior knowledge to determine the value
added, contradictions, or other unique characteristics of the information.
Outcomes Include:
a.

Determines whether information satisfies the research or other information need

241
b.
c.
d.
e.
f.
g.

5.

Uses consciously selected criteria to determine whether the information contradicts or verifies
information used from other sources
Draws conclusions based upon information gathered
Tests theories with discipline-appropriate techniques (e.g., simulators, experiments)
Determines probable accuracy by questioning the source of the data, the limitations of the
information gathering tools or strategies, and the reasonableness of the conclusions
Integrates new information with previous information or knowledge
Selects information that provides evidence for the topic

The information literate student determines whether the new knowledge has an impact on the
individual’s value system and takes steps to reconcile differences.
Outcomes Include:
a.
b.

6.

Investigates differing viewpoints encountered in the literature
Determines whether to incorporate or reject viewpoints encountered

The information literate student validates understanding and interpretation of the information through
discourse with other individuals, subject-area experts, and/or practitioners.
Outcomes Include:
a.
b.
c.

7.

Participates in classroom and other discussions
Participates in class-sponsored electronic communication forums designed to encourage
discourse on the topic (e.g., email, bulletin boards, chat rooms)
Seeks expert opinion through a variety of mechanisms (e.g., interviews, email, listservs)

The information literate student determines whether the initial query should be revised.
Outcomes Include:
a.
b.
c.

Determines if original information need has been satisfied or if additional information is
needed
Reviews search strategy and incorporates additional concepts as necessary
Reviews information retrieval sources used and expands to include others as needed

242
Appendix J: Institutional Learning Outcomes
http://www.santarosa.edu/slo/institutional/

Institutional Learning Outcomes (ILOs)
Through their experiences at Santa Rosa Junior College, students will bring into the college
community the following set of skills and values:
4. Critical Analysis


Locate, analyze, evaluate, and synthesize relevant information



Draw reasonable conclusions in order to make decisions and solve problems

243
Appendix K: Information Literacy Requirement Course Outline
COURSE CONTENT
Student Learning Outcomes: Upon completion of the course, students will be able to:
1. Analyze a research need
2. Find information effectively and efficiently by using a variety of search techniques
3. Access needed information in multiple publication formats
4. Evaluate the quality and relevance of information sources
5. Recognize several ethical and legal issues related to the use of information
Objectives:
Upon completion of the course, students will be able to:
A. Analyze a research question:
1. Articulate a research need
2. Determine the scope of a research need
3. Broaden or narrow a research need to fit the scope of a lower-division undergraduate research
assignment
B. Find information effectively and efficiently by using a variety of search techniques:
1. Identify various types of information sources, such as reference works, popular periodicals, scholarly
journals, etc.
2. Choose appropriate sources based upon the research need
3. Identify major concepts from the research need to be used as keywords
4. Use basic search techniques, such as keywords, Boolean operators, search limiters, etc.
5. Use advanced search techniques, such as field searching, truncation, wildcards, etc.
6. Evaluate search success and modify search strategies accordingly
C. Access needed information in multiple publication formats:
1. Use the item record to determine the means of access
2. Retrieve information from digital sources
3. Locate print sources in the library
D. Evaluate the quality and relevance of information sources:
1. Assess the quality of information sources based upon authority, objectivity, purpose and scope
2. Determine the importance of the publication date in the context of the research need
3. Determine the appropriateness of information based upon its relevance to a research need
E. Identify several ethical and legal issues related to the use of information:
1. Describe differences between summarizing, quoting, paraphrasing and plagiarizing information
2. Document sources in accordance with an academic style guide (APA or MLA)
3. Describe the role of copyright in relationship to sources, including digital media
4. Identify elements in a bibliographic citation
Topics and Scope
Topics will include:
I. Analysis of a research question
A. Context of a research need (personal, academic, discipline-specific, course-specific)
B. Refinement of a research need
C. Scope of a research need

244
II. Finding information effectively and efficiently by using a variety of search techniques
A. Types of information resources (popular, scholarly, primary, secondary, tertiary, etc. both online and in
print)
B. Selection of appropriate resources (reference books, periodicals, monographs, etc., both online and in
print)
C. Keywords and major concepts
D. Basic search techniques (keywords, Boolean operators, search limits, etc.)
E. Advanced search techniques (controlled vocabulary, truncation, wildcards, nesting, field searching,
phrase searching, etc.)
F. Modification of search strategies based upon the success of a search (using appropriate tools, altering
keywords)
III. Accessing needed information in multiple publication formats
A. Using the item record to determine means of access
B. Information retrieval from digital sources
C. Locating print resources by using Library of Congress call numbers
IV. Evaluating the quality and relevance of information sources
A. Authority
B. Objectivity
C. Scope
D. Purpose
E. Currency and context of research need
F. Relevance
V. Identifying several ethical and legal issues related to the use of information
A. Summary, quotations, paraphrasing, citing, plagiarism
B. In-text citations and works cited/reference list
C. Copyright and online sources
D. Elements of bibliographic entries
Assignments:
Representative assignments:
1. 1–2 homework assignments to assess the application of skills for each of the learning outcomes (5–10
assignments)
2. 1–2 class exercises tied to each of the learning outcomes (5–10 class exercises)
3. 1 term-long project, such as an annotated bibliography, a research journal or similar indicator of
engagement and skill in the research process
4. Quizzes, midterm and/or final exam

