Scaffolding Students’ Information Literacy Skills
with an Online Credibility Evaluation Learning Tool

by

Christopher Alan Leeder

A dissertation submitted in partial fulfillment
of the requirements for the degree of
Doctor of Philosophy
(Information)
in the University of Michigan
2014

Dissertation Committee:
Professor Karen Markey (Chair)
Professor Xun Ge, University of Oklahoma
Associate Professor Christopher Quintana
Associate Professor Soo Young Rieh

UMI Number: 3672736

All rights reserved
INFORMATION TO ALL USERS
The quality of this reproduction is dependent upon the quality of the copy submitted.
In the unlikely event that the author did not send a complete manuscript
and there are missing pages, these will be noted. Also, if material had to be removed,
a note will indicate the deletion.

UMI 3672736
Published by ProQuest LLC (2015). Copyright in the Dissertation held by the Author.
Microform Edition © ProQuest LLC.
All rights reserved. This work is protected against
unauthorized copying under Title 17, United States Code

ProQuest LLC.
789 East Eisenhower Parkway
P.O. Box 1346
Ann Arbor, MI 48106 - 1346

Acknowledgements

This dissertation builds on longstanding interests in the importance of Information
Literacy beginning with earning my MLIS degree at the University of Albany and continuing
through the doctoral program at the University of Michigan School of Information. Professors at
both universities inspired my interests through their teaching and mentorship. My advisor,
Professor Karen Markey, provided the opportunity to work on the three-year BiblioBouts project,
which gave me an exceptional experience to participate in building, testing and deploying an
online learning game, as well as conducting focus groups and running pilot tests. The project also
provided the opportunity to work with talented and motivated team members: Andrew Calvetti.
Brian Jennings, Greg Peters, Soo Young Rieh, Beth St. Jean, Fritz Swanson, Victor Rosenberg,
and Michelle Wong. Our librarian liaisons Averill Packard and Anita Dey (Saginaw Valley State
University), Catherine Johnson (University of Baltimore), Alyssa Martin (Troy University
Montgomery Campus), and Gabrielle Toth (Chicago State University) all contributed to the
success of the research. Special thanks to the Institute for Museum and Library Services for their
generous funding of the BiblioBouts research.
In completing this dissertation research, I drew on the expertise of my dissertation
committee: chair Professor Karen Markey and committee members Professor Roo Young Rieh,
Chris Quintana, and Professor Xun Ge from the University of Oklahoma. Classes taught by
Professor Rieh on credibility evaluation and Professor Quintana on designing learning software
were critical components in the development of this dissertation research. Professor Ge
generously agreed to be a remote member of the committee, and made time to meet online and
provide helpful feedback. Thanks to my committee and chair for reading drafts and providing
comments generously. In particular, I would like to thank my chair, Professor Karen Markey, for
her ongoing support and guidance.
Several other members of the UMSI learning community made important contributions to
this research. Professor Cliff Lampe provided access to the students in his course and allowed my
to conduct my study with them, as well as being a personal mentor. Professor Kristin Fontichiaro
ii

connected me with master’s students in her Information Literacy classes and School Media
Librarians from the community, all of whom provided helpful feedback on the prototype designs
for InCredibility. UMSI master’s students Tim Kiser and Priya Kumar did exceptional work on
coding the qualitative study data. The UM Center for Statistical Consulting and Research
provided valuable support for the data analysis. Without all of these contributions this research
project would not have been possible.

iii

Table of Contents
Acknowledgments........................................................................................................................... ii
List of Tables ................................................................................................................................ vii
List of Figures ................................................................................................................................ ix
List of Appendices ...........................................................................................................................x
Abstract .......................................................................................................................................... xi
Chapter 1. Introduction ....................................................................................................................1	  
1.1. Background .......................................................................................................................... 1	  
1.2. Problem statement................................................................................................................ 3	  
1.3. Purpose and research questions ........................................................................................... 7	  
1.4. Theoretical models ............................................................................................................... 8	  
1.5. Methods.............................................................................................................................. 10	  
1.6. Overview of this dissertation ............................................................................................. 12	  
Chapter 2. Literature Review .........................................................................................................14	  
2.1. Information Literacy .......................................................................................................... 14	  
2.1.1. History of IL ................................................................................................................14	  
2.1.2. Definitions and standards.............................................................................................20	  
2.1.3. Theoretical models .......................................................................................................23	  
2.1.4. Student information literacy skills ...............................................................................26	  
2.1.5. Benefits of IL training..................................................................................................28	  
2.1.6. Challenges to teaching IL ............................................................................................29	  
2.1.7. Implications for the research ........................................................................................33	  
2.2. Credibility evaluation......................................................................................................... 34	  
2.2.1. Theoretical models .......................................................................................................36	  
2.2.2. Credibility criteria ........................................................................................................36	  
2.2.3. Student credibility evaluation practices .......................................................................41	  
2.2.4. Collaborative online evaluation ...................................................................................44	  
2.2.5. Implications for the research ........................................................................................47	  
2.3. Computer-Supported Collaborative Learning.................................................................... 48	  
2.3.1. Learning theories .........................................................................................................50	  
2.3.2. Metacognition ..............................................................................................................54	  
iv

2.3.3. Instructional scaffolds ..................................................................................................60	  
2.3.4. Implications for the research ........................................................................................68	  
2.4. Literature Review Summary .............................................................................................. 69	  
Chapter 3: Prototype Design and Development.............................................................................72	  
3.1. Design-Based Research ..................................................................................................... 72	  
3.2. Theoretical models ............................................................................................................. 74	  
3.2.1. Information literacy model ..........................................................................................74	  
3.2.2. 5Ws model ...................................................................................................................76	  
3.2.3. Scaffolding model ........................................................................................................77	  
3.3. Initial prototype.................................................................................................................. 82	  
3.3.1. Structure of the prototype ............................................................................................84	  
3.3.2. Initial evaluation of prototype ......................................................................................96	  
3.4. Pilot-testing ........................................................................................................................ 97	  
3.4.1. Tutorial usability questions ..........................................................................................98	  
3.4.2. Tutorial content questions ..........................................................................................100	  
3.4.3. Metacognition test......................................................................................................103	  
3.4.4. Prototype pilot test .....................................................................................................104	  
3.5. Chapter summary ............................................................................................................. 108	  
Chapter 4: Research design ..........................................................................................................109	  
4.1 Purpose and research questions ........................................................................................ 109	  
4.2. Experimental design......................................................................................................... 112	  
4.3. Subject recruitment .......................................................................................................... 115	  
4.4. Data collection ................................................................................................................. 116	  
4.4.1. Credibility criteria test ...............................................................................................117	  
4.4.2. Metacognition test......................................................................................................119	  
4.5. Data analysis .................................................................................................................... 120	  
4.6. Qualitative Coding ........................................................................................................... 122	  
Chapter 5: Findings ......................................................................................................................128	  
5.1. Subject recruitment .......................................................................................................... 128	  
5.2. Randomized assignment of subjects ................................................................................ 129	  
5.3. Analysis of sources found by subjects ............................................................................. 134	  
5.3.1. Comparison of sources contributed by group ............................................................134	  
5.3.2. Summary of contributed sources ...............................................................................146	  
5.4. Data analysis .................................................................................................................... 148	  
v

5.5. Understanding of expert credibility criteria ..................................................................... 148	  
5.6. Evidence-based source characteristics ............................................................................. 153	  
5.7. Metacognitive awareness ................................................................................................. 161	  
5.8. Student skills .................................................................................................................... 167	  
5.8.1. Experience level and prior IL instruction ..................................................................168	  
5.8.2. Self-reported evaluation strategies .............................................................................168	  
5.8.3. Self-reported learning ................................................................................................171	  
5.8.4. Self-evaluation of skills .............................................................................................173	  
5.9. Summary of findings ....................................................................................................... 175	  
5.10. Limitations of the study ................................................................................................. 176	  
Chapter 6: Discussion ..................................................................................................................178	  
6.1. Summary of Research Question findings ........................................................................ 178	  
6.2. Additional findings .......................................................................................................... 180	  
6.2.1. Subjects’ contributed sources ....................................................................................181	  
6.2.2. Subjects’ self-evaluation of their skills ......................................................................183	  
6.3. Implications for the InCredibility tool ............................................................................. 184	  
6.4. Implications for study design ........................................................................................... 189	  
6.5. Implications for online IL instruction .............................................................................. 193	  
6.6. Contributions ................................................................................................................... 196	  
6.7. Significance...................................................................................................................... 200	  
6.8. Future research ................................................................................................................. 203	  
Appendices...................................................................................................................................207	  
References ....................................................................................................................................228	  

vi

List of Tables
Table 1. Data analysis methods ...........................................................................................................12	  
Table 2. Comparison of evaluative criteria ......................................................................................... 37	  
Table 3. Criteria used in specific IL games ........................................................................................ 38	  
Table 4. Comparison of two early checklists ...................................................................................... 39	  
Table 5. Examples of question prompts .............................................................................................. 64	  
Table 6. Scaffolding Design Framework (Quintana et al., 2004) ....................................................... 67	  
Table 7. Scaffolding Design Framework applied to IC tool design ................................................... 81	  
Table 8. InCredibility student workflow ............................................................................................. 85	  
Table 9. Prototype Notebook question prompts ................................................................................. 92	  
Table 10. 5Ws correspondence to expert terminology ....................................................................... 94	  
Table 11. Reflective prompts .............................................................................................................. 96	  
Table 12. Revised Notebook question prompts ................................................................................ 106	  
Table 13. Data analysis methods ...................................................................................................... 111	  
Table 14. Standardized IL tests ......................................................................................................... 117	  
Table 15. Credibility criteria post-test .............................................................................................. 118	  
Table 16. Adapted metacognition test questions .............................................................................. 120	  
Table 17. Coding rubric for student knowledge of credibility criteria ............................................. 121	  
Table 18. Coding rubric for student open-ended questions .............................................................. 122	  
Table 19. Potential responses and scores to definition responses ..................................................... 123	  
Table 20. Potential responses and scores to importance questions ................................................... 124	  
Table 21. Components of understanding score ................................................................................. 124	  
Table 22. Rubric for scoring post-test............................................................................................... 125	  
Table 23. Categories of credibility evaluations (Markey et al., 2014) ............................................. 125	  
Table 24. Initial condition assignment (193 subjects) ...................................................................... 129	  
Table 25. Final participant distribution (84 subjects) ....................................................................... 130	  
Table 26. Final participant gender by group ..................................................................................... 131	  
Table 27. Experience with searching the Internet ............................................................................. 132	  
Table 28. Prior IL instruction............................................................................................................ 132	  
Table 29. Prior IL instruction included quality evaluation ............................................................... 133	  
Table 30. T1's most frequently contributed sources ......................................................................... 136	  
Table 31. T2's most frequently contributed sources ......................................................................... 141	  
Table 32. CTRL's most frequently contributed sources ................................................................... 144	  
Table 33. Sources shared between groups ........................................................................................ 145	  
Table 34. Contributed sources by genre ........................................................................................... 147	  
Table 35. Data analysis methods ...................................................................................................... 148	  
Table 36. Coding rubric for student criteria responses ..................................................................... 149	  
Table 37. Examples of scored criteria responses .............................................................................. 150	  
Table 38. Understanding scores for credibility criteria .................................................................... 151	  
Table 39. Reflection prompts ............................................................................................................ 154	  
Table 40. Length of responses between groups ................................................................................ 155	  
Table 41. Coding categories (adapted from Markey et al., 2014) .................................................... 156	  
Table 42. Examples of coded reflection responses ........................................................................... 156	  
Table 43. Reflective prompt #1: How did you decide whether a webpage was credible? ............... 159	  
Table 44. Reflective prompt #2: What criteria did you use to help you evaluate credibility? ......... 159	  
Table 45. Total responses to prompts #1 and #2 .............................................................................. 160	  
vii

Table 46. Metacognitive statements by type..................................................................................... 162	  
Table 47. Average scores of metacognitive statements by group ..................................................... 162	  
Table 48. Highest average scores of metacognitive statements ........................................................ 163	  
Table 49. Group average metacognition scores from high to low .................................................... 165	  
Table 50. T1 sources categorized by metacognition score ............................................................... 166	  
Table 51. Post-test average self-evaluation scores ........................................................................... 174	  
Table 52. Adapted rubric prompts .................................................................................................... 185	  
Table 53. Adapted tip prompts.......................................................................................................... 186	  
Table 54. Adapted metacognitive prompts ....................................................................................... 187	  

viii

List of Figures
Figure 1. Model of cognitive and metacognitive behaviors ........................................................... 55	  
Figure 2. Overlap between IL and metacognitive skills (from Wolf, 2007)................................... 58	  
Figure 3. Theoretical model for online learning tool ...................................................................... 70	  
Figure 4. Tutorial opening page ...................................................................................................... 86	  
Figure 5. Example tutorial question ................................................................................................ 87	  
Figure 6. Tutorial tip text ................................................................................................................ 87	  
Figure 7. Tutorial correct answer .................................................................................................... 88	  
Figure 8. Tutorial wrong answer and tip ......................................................................................... 88	  
Figure 9. Tutorial second wrong answer......................................................................................... 89	  
Figure 10. Headquarters page ......................................................................................................... 90	  
Figure 11. Notebook expanded ....................................................................................................... 91	  
Figure 12. Investigate stage ............................................................................................................ 92	  
Figure 13. Question stage (part 1)................................................................................................... 93	  
Figure 14. Question stage (part 2)................................................................................................... 94	  
Figure 15. Solve stage ..................................................................................................................... 95	  
Figure 16. Experimental design .................................................................................................... 114	  
Figure 17. Study timeline.............................................................................................................. 115	  
Figure 18. Blog search results ....................................................................................................... 137	  
Figure 19. Commercial search results ........................................................................................... 138	  
Figure 20. Video search result ...................................................................................................... 139	  
Figure 21. Experts presented by TV channel website................................................................... 140	  
Figure 22. Research report search result 1 .................................................................................... 141	  
Figure 23. Research report search result 2 .................................................................................... 142	  
Figure 24. Article search result ..................................................................................................... 143	  
Figure 25. Research report search result 3 .................................................................................... 144	  
Figure 26. Google search engine results ....................................................................................... 146	  

ix

List of Appendices
Appendix 1. Kapoun’s five criteria for evaluating Web pages (1998) ........................................207	  
Appendix 2. CRAAP Test............................................................................................................209	  
Appendix 3. Kathy Schrock’s 5Ws ..............................................................................................211	  
Appendix 4. University of Michigan Library 5Ws ......................................................................212	  
Appendix 5. Prototype Tutorial Questions, Tips and Answers ...................................................214	  
Appendix 6. Pilot study metacognition test results......................................................................216	  
Appendix 7. Online Form for Control Groups.............................................................................218	  
Appendix 8. Schraw and Dennison Metacognitive Awareness Inventory (1994) .......................220	  
Appendix 9. Raes et al. adapted Metacognitive Awareness Inventory (2012) ............................222	  
Appendix 10. Demographics Chi Square Tests ...........................................................................224	  
Appendix 11. RQ1 Chi Square Tests ...........................................................................................227	  

x

Abstract
This research explored how to effectively teach today’s students Information Literacy (IL)
and credibility evaluation skills in the online information environment. In light of students’
reliance on the Internet, their general lack of IL skills, limited critical evaluation practices, and the
lack of consistent institutional IL training, new pedagogical methods are needed to teach effective
online IL skills. Specifically, there is a need for IL training that is customized to the online
information environment and relevant to the research habits of today’s students. To address this
problem, an online learning tool incorporating scaffolding and metacognitive support was
prototyped and built. The tool decomposes credibility evaluation into a structured set of stages,
giving students repeated practice in each stage while providing scaffolded support for learning
and metacognitive reflection, and integrating the instruction into the online information
environment.
An experimental study was conducted to test the tool's effectiveness, with a total of 84
students randomly assigned to three experimental conditions to allow for statistically valid
analysis of the results. The findings show that use of the online credibility evaluation tool
significantly increased subjects' understanding of credibility criteria. The results did not show a
significant difference between groups in the application of evidence-based source characteristics
as the basis for their credibility evaluations, or in metacognitive awareness of the evaluation
process, although descriptive trends suggest some improvement in the treatment group. Along
with these three research questions, the study also examined the types of sources that students
used in their research, showing that they relied on blogs and other hybrid online genres that do not
conform to the traditional genres often covered by IL instruction. The study also solicited selfreports of student learning, with students reporting that they learned that online credibility
evaluation is more complex than they thought, involving asking systematic questions and using
critical thinking.
Overall, this research demonstrates that IL instruction needs to address the specific
challenges of online credibility evaluation, and that scaffolding and metacognitive support in the
form of an online learning tool can effectively integrate IL instruction into the online information
environment where students actually do their research.
xi

Chapter	  1.	  Introduction	  

1.1.	  Background	  	  
Information literacy (IL) has been called a survival skill in the Information Age (ALA,
1989; Eisenberg, 2010) and “a prerequisite for participation in society and the work force” (US
21st Century Workforce Commission, 2000). It has also been described as the critical literacy of
the 21st century and the foundation of learning in our contemporary environment of continuous
technological change (Bruce, 2004). IL has been defined by the American Library Association
as: “To be information literate, a person must be able to recognize when information is needed
and have the ability to locate, evaluate, and use effectively the needed information" (ALA,
1989), which has become the widely accepted definition in academic libraries. Research shows
that IL instruction has a positive impact on student skills, performance and academic
achievement (Gross & Latham, 2007; Ren, 2000; Selegean, Thomas & Richman, 1983; Smalley,
2004; Van Scoyoc, 2003; Wang, 2006). Students will need these critical skills in their lives
outside of academia, as they are key to preparing students for life-long learning (ALA, 1989;
Daugherty & Russo, 2011).
IL skills have increasingly been recognized as critical to success in today’s economy and
society, with several professional organizations including IL skills into their official standards.
The Partnership for 21st Century Skills’ “Framework for 21st Century Learning” describes the
“skills, knowledge and expertise students should master to succeed in work and life in the 21st
century” (Partnership for 21st Century Skills, 2011), among which are includes information
literacy and critical thinking. Another professional organization, the International Society for
Technology in Education (ISTE), developed the National Educational Technology Standards
(NETS), described as “the standards for learning, teaching, and leading in the digital age” (ISTE,
2012), which include “Research and Information Fluency” and “Critical Thinking, Problem
Solving, and Decision Making.” A report from the Georgetown University Center on Education
1

and the Workforce states that competencies such as critical thinking, active learning, and
complex problem-solving are required for success in STEM (Science, Technology, Engineering,
Mathematics) occupations, which are critical to our nation’s continued economic
competitiveness (Carnevale, Smith, & Melton, 2011).
IL is closely aligned with critical thinking (Akyol & Garrison, 2011), with critical
thinking sometimes taken as a central aspect of IL (Lorenzo & Dziuban, 2006). The two terms –
“critical thinking” used primarily in education, “information literacy” used primarily in library
instruction – may in fact merely be disciplinary terminology for the same set of skills (Allen,
2008). Reece states that “information literacy is a form of critical thinking applied to the realm of
information” (2005, p. 488) and Doyle suggests “while critical thinking skills provide the
theoretical basis for the process, information literacy provides the skills for practical, real world
application” (1994, p. 4). In this context, IL can be understood as an embodiment of critical
thinking in the context of information seeking activities.
Information literacy can also be seen as inherently metacognitive in that it encourages
individuals to become aware of their search and evaluation skills and apply them to specific
information needs (Booth, 2011). Metacognition involves “planning cognitive tasks, monitoring
one’s progress to meet goals, taking appropriate steps to solve problems, and reflecting on past
performance for future improvement” (Quintana et al., 2005, p. 2360). Metacognitive abilities
are categorized as knowledge of cognition and regulation of cognition (Schraw & Dennison,
1994). The Information Literacy Competency Standards for Higher Education aim to support
students in building a “metacognitive approach to learning” through gaining control over their
interactions with information and through making explicit the criteria for gathering, analyzing,
and using information (ACRL, 2000, p. 6). The Middle States Commission on Higher Education
in their Guidelines for Information Literacy in the Curriculum echo the ACRL, stating that “one
of the highest and best uses of information literacy is as a metacognitive device for students to
better manage the learning process” (MSCHE, 2003). Any training in IL skills should not only
equip students with guidelines to help them assess the credibility of websites, but should also
encourage them to reflect on the process of evaluation (Madden et al., 2011).
Learning software applications can systematically support metacognition through the use
of instructional scaffolding. Azevedo (2005) defined scaffolding in computer-based learning
environments as “instructional support in the form of guides, strategies, and tools that are used
2

during learning to support a level of understanding that would be impossible to attain if the
students learned on their own” (p. 199). (Azevedo also describes human tutors as providing
scaffolding, however the research focuses on what he calls “embedded procedural scaffolds.”)
These instructional scaffolds can help students to work through a difficult task and attain a
higher level of proximal development that would be beyond their unassisted efforts (Ge & Land,
2004). With the assistance of scaffolds, learners can bridge the gaps between their current
abilities and intended learning goals that would be unachievable through their unassisted effort
alone (Rosenshine & Meister, 1992). The use of metacognitive support in the form of scaffolding
can help students to develop strategies to be more critical in their evaluation of the credibility of
web sources (Iding et al. 2008). Since students do not spontaneously engage in metacognitive
thinking unless they are specifically encouraged to do so, it is important to include metacognitive
support in learning environments (Lin, 2001).

1.2.	  Problem	  statement	  	  
Evaluating the credibility of online information sources may be difficult for today’s
students due to the volume and diversity of sources, and the lack of conventional quality control
mechanisms and indicators of authority from traditional print-based formats (Rieh, 2002; Gasser
et al., 2012; Metzger et al., 2010). Historically, markers of credibility in the print-based paradigm
were maintained by professional gatekeepers such as editors and reviewers (Rieh & Danielson,
2007). However, the fast-changing information environment of the Internet is transforming
familiar genres, combining them and creating new genres which resist easy classification
(Markey et al., 2014). When print-document genres are adapted to the Internet, they can appear
to be shuffled, disassembled and reassembled in new and sometimes confusing ways (Crowston
et al., 2010). Information sources on the web often lack the filters and markers of institutional
credibility and authority which promote reliability in traditional print sources (Burbules, 2001;
Mackey & Jacobsen, 2011). Overall, web pages typically offer few reliable cues to credibility
that students can use in their evaluations (Iding et al., 2008).
While IL instruction programs attempt to instill evaluation skills, research shows that
college students rarely evaluate the quality of information sources that they find online (Becker,
2003; Brand-Gruwel et al., 2005; Julien & Barker, 2009; Kolowich, 2011; Metzger, 2007;
3

Parker-Gibson, 2005; Walraven et al., 2009). This is a particularly urgent problem since for
many of today’s students the Internet is the starting point when searching for information
(Becker, 2003; Costello et al., 2004; Curtis, 2000; Swanson 2005). Studies consistently show that
college students overwhelming rely on Google to the exclusion of many other academic search
tools (Hargittai et al., 2010; Head & Eisenberg, 2011; Kim & Sin, 2011; Kolowich, 2011;
OCLC, 2002; Van Soyoc & Cason, 2006). Overall, students are unsophisticated information
seekers in academic contexts (Julien & Barker, 2009; Thomas, 2004).
While stakeholders in higher education and in professional societies agree that IL is
necessary to students’ success in their education and afterward in their work and personal lives,
there are significant challenges to the implementation of universal IL instruction programs. One
obstacle is the “faculty problem”: the perception of librarians that faculty are either apathetic or
outright obstructive towards efforts to collaborate on IL instruction (McCarthy 1985). Faculty
may assume that IL instruction happens in introductory courses, and may feel that it is not their
responsibility to teach it themselves (Saunders 2012). Librarians and faculty may have very
different expectations of the content and desired emphasis of IL instruction (Cunningham, Carr,
& Brasley, 2011). The onus is on librarians to initiate and sustain discussions with faculty about
IL instruction and to proactively build collaborative relationships (Saunders 2012). However,
even when librarians successfully work with faculty to bring IL instruction into the classroom,
the reality is that there is very little time available in the faculty member’s curricula to include IL
content (van Meegen & Limpen, 2010). Another obstacle is the inconsistent accreditation
standards across the United States (Owusu-Ansah 2004; Gross & Latham, 2007). Only a small
percentage of higher education institutions with first-year experience programs include a
required information literacy component (Boff & Johnson 2002). These difficult conditions mean
that broad integration of IL into undergraduate education remains an aspiration rather than a fully
realized ideal (McGuiness 2006).
Traditional IL training methods (one-shot sessions, tutorials, worksheets) are often
simplistic, not customized to the online information environment, and rely on a traditional
classroom-based pedagogical model, and thus may not connect effectively with today’s students
(Costello et al., 2004; Manuel, 2002; Gibson, 2008; Leach & Sugarman, 2005). These brief
training sessions may be the only explicit and focused exposure to IL that most students receive,
and the limited time and contact with students make it difficult for librarians to keep students
4

interested and engaged (Smale 2011). This lack of opportunity to engage and motivate students
means that students get bored quickly when IL lessons do not trigger them to pay attention
(Doshi 2006). When learning new skills, today’s students often prefer active involvement in the
learning process, and a networked, participatory learning environment (Davidson & Goldberg,
2009; Halse & Mallinson, 2009; Thomas & Brown, 2011). Overall, one-shot instruction sessions
cannot provide students with the engagement and sustained practice required to learn, apply and
master IL competencies (Mokhtar et al. 2008, Mery et al. 2012).
Although metacognition is an important part of learning, most IL instructional models
position reflection at the very end of the process or simply take it for granted (Markless, 2009).
This approach is not likely to enable the development of the metacognitive strategies necessary
to perform problem-solving with information, particularly when students are working
independently online (Markless, 2009). Studies show that students often lack the ability or
awareness to monitor and regulate their cognitive processes while engaged in problem-solving
(Artz & Armour-Thomas, 1992), which likely relates to the lack of metacognitive skills (Ge &
Land, 2004). Novice learners usually have weak metacognitive skills, which impede their ability
to engage in complex practices like online inquiry (Quintana et al., 2005). These students are
unlikely to be successful in completing complex web-based tasks, such as online credibility
evaluation (Kauffman et al., 2008).
Learning software applications can systematically support metacognition through the use
of instructional scaffolding. Wood, Bruner and Ross (1976) defined scaffolding as a “process
that enables a child or novice to solve a problem, carry out a task or achieve a goal which would
be beyond his unassisted efforts” (p. 90). Essentially, scaffolds change the task in some way so
that learners can accomplish what would otherwise be out of their reach (Reiser, 2004). With the
assistance of scaffolds, learners can bridge the gaps between their current abilities and intended
learning goals that would be unachievable through their unassisted effort alone (Rosenshine &
Meister, 1992). During this process, scaffolds enable learners to reflect in action (Hung, 2001),
providing “opportunities for students to deepen their understanding by externalizing and
comparing their knowledge and beliefs with those of their peers” (Sharma & Hannafin, p.43).
Social software tools are “increasingly being recognized as essential scaffolds and learning
tools” (McLoughlin & Lee, 2008, p. 649) because their affordances support participatory

5

knowledge creation through networking, socialization, communication and engagement with
communities of learning (McLoughlin & Lee, 2008).
In light of students’ reliance on the Internet, their general lack of IL skills, limited critical
evaluation practices, and the lack of consistent institutional IL training, new pedagogical models
are needed to teach effective online IL skills. Specifically, there is a need for IL training that is
customized to the online information environment and relevant to the research habits of today’s
students. If students are to effectively evaluate the credibility of online information sources, they
must learn the specific criteria on which to judge the credibility of these sources, and the
evidence necessary to support their evaluations (Metzger, 2007; Harris, 2008; Iding et al., 2009).
They must also learn to base their judgments on evidence-based source characteristics rather than
relying on subjective judgments based on intuition or projection (Markey, Leeder & Rieh, 2014).
Guidance in learning these skills should be provided through structured scaffolding and
metacognitive prompts that support students in reflecting on their learning. Developing students’
metacognitive skills regarding credibility evaluation, and understanding IL as a structured
process requiring practice, planning and reflection, will help students become critically aware
users of online information, and will prepare them for lifelong learning.
Although there is a significant amount of literature on case studies of IL instruction, there
is little empirical research on its effectiveness beyond surveys, pre/post-tests and outcomes
evaluation (Barclay, 1993; Coupe, 1993; Rockman, 2002; Orme, 2004). Scaffolding and
metacognition have been studied in other fields, e.g. education and psychology (Kauffman, 2004;
Iding, 2008; Pifarre & Cobos, 2010), educational media (Bannert, Hildebrand & Mengelkamp,
2009), pharmacy (Ge, Planas & Er, 2010; Ge, 2013), science (Qunitana, et al., 2004; Azevedo,
2005; Quintana, Zhang & Krajick, 2005; Raes, 2012; Tanner, 2012), and specific domains such
as reading comprehension and writing skills (Lin, 2011). However, there has been little research
on the application of scaffolding and metacognitive support to teaching students IL and
credibility evaluation skills (Gorrell et al., 2009; Bannert & Mengelkamp, 2013), and in online
learning environments (Akyol & Garrison, 2011). These gaps in the literature are addressed by
the research through conducting an experimental study on the learning impact of an IC tool that
incorporates scaffolding and metacognitive support.

	  

6

1.3.	  Purpose	  and	  research	  questions	  

The purpose of the research is to investigate the effects of scaffolding and metacognitive
support on student learning of online credibility evaluation skills. The study tested whether a
custom-built online credibility evaluation learning tool that incorporates scaffolding and
metacognitive supports increased students’ understanding of the expert criteria that constitute
credibility evaluations, their application of evidence-based source characteristics in making
credibility evaluations, and the metacognitive awareness of the online information evaluation
process. These outcomes are defined as follows:
•
•

•

•

“Understanding” was defined as the ability to 1) accurately define the criteria and 2)
articulate their importance.
The expert credibility criteria that students learned are the concepts of authority,
relevance, reliability, currency, and purpose (based on a scaffolded model of “who, what,
where, when and why”) and their definitions.
The evidence-based source characteristics students learned to examine are evidence used
for credibility evaluations such as author credentials, main ideas, references/links, site
domain, contact information, date, and About and purpose statements.
The metacognitive strategies students learned are increased use of planning, monitoring,
and reflecting on their evaluation practices.

The research design uses an experimental study to address these research questions:
RQ1: Do students who use the online credibility evaluation learning tool demonstrate
greater understanding of expert credibility criteria in the process of evaluating online
sources compared to groups of students who use a tutorial and an online form, or those
who use only an online form?
RQ2: Do students who use the online credibility evaluation learning tool demonstrate
greater application of evidence-based source characteristics as the basis for their
credibility evaluations compared to groups of students who use a tutorial and an online
form, or those who use only an online form?
RQ3: Do students who use the online credibility evaluation learning tool demonstrate
greater metacognitive awareness compared to groups of students who use a tutorial and
an online form, or those who use only an online form?

7

1.4.	  Theoretical	  models	  	  
The design of the online credibility evaluation learning tool titled “InCredibility” (IC)
was inspired by learning theory and its application to online learning tools. Constructivist
learning theory, which sees learning as a social process in which students play an active role in
building knowledge (Woodard, 2003; Gibson, 2008), and Vygotsky’s Zone of Proximal
Development model of bridging students’ current knowledge toward more advanced practice
(Vygotsky, 1978; Azevedo, 2005; Zhang & Quintana, 2012), both informed the pedagogical
model of the prototype learning tool.
Constructivism provides a theoretical framework for scaffolding and for providing
metacognitive support, and guides the design and development of the prototype IC tool. Through
instructional supports that structure an otherwise haphazard sequence of actions, and visual
representations that structure what had previously been just a series of uncoordinated events, the
scaffolds embedded in the prototype learning tool enhance the development of the student’s
metacognitive and self-regulation processes (Azevedo, 2005; Quintana et al., 2005; Stahl, 2006).
Metacognitively-aware learners have been shown to be more strategic and perform better than
unaware learners, through planning, sequencing, and monitoring their learning in a way that
directly improves performance (Schraw & Dennison, 1994). Effective searching of the web is a
complex process of reasoning and decision making (Todd, 2000), and strong self-regulation
ability and metacognitive awareness are necessary in order to be successful in web-based
learning (Raes et al., 2012).
The design of the tool follows Quintana et al.’s Scaffolding Design Framework of
supporting sensemaking, process management, and reflection and articulation (2004). Learning
is scaffolded by the structured decomposition of tasks into discrete units, and the segmentation of
the learning goal into stages. Since novice learners usually have weak metacognitive skills,
which are important for engaging in complex practices like online credibility evaluation, the
prototype learning tool provides needed practice and reinforcement of these important skills
(Quintana et al., 2005). In addition, the situated, just in time, web-based nature of the tool
facilitates active involvement in the learning process by Millennial students who value
collaboration and peer-based learning (Manuel, 2002; Head & Eisenberg, 2010).

8

The design of the tool is also informed by theories of library-related information seeking
behavior (ISB). One of the most influential models in IL theory is Kuhlthau’s Information
Search Process (ISP), which broke new ground by exploring the cognitive and affective aspects
of the information search process, rather than merely the mechanistic steps of information
retrieval (Kuhlthau, 1991). The cognitive processes involved in Kuhlthau’s ISP (critical thinking,
decision making, problem solving) relate closely to the fundamental skills of IL (Thomas, 2004).
Drawing on constructivist pedagogical and cognitive learning theory, Kulthau crafted a model
that identified “zones of intervention” based on Vygotsky’s Zone of Proximal Development
which focus on providing effective assistance and coaching when it is most needed by students.
The ISP model created a new vocabulary and role for school librarians (Behrens, 1994). Since it
is empirically tested, the ISP model was an important milestone in research on students’
information-seeking behavior and has served as the basis for much library instruction (Thomas,
2004).
Three other important theoretical models influenced the design of tool. The first is
“library anxiety” (Mellon, 1986), a sense of powerlessness which students may feel when they
begin an information search that requires using the library, involving feeling lost, fearful of
library staff, and unable to navigate the library. The IC tool addresses these anxiety barriers by
situating library instruction in the online context where students normally do their research,
rather than relying on placing them physically in the library, or bringing a librarian physically to
the classroom. The second theoretical model is the Principle of Least Effort (PLE) (Zipf, 1949;
Rosenberg, 1974; Mann, 1993) and the related term “satisificing” (Simon, 1956; Agosto, 2002;
Thomas, 2004; Gibson, 2008; Warwick et al., 2009) both of which suggest that students often
accept the first satisfactory alternative over the best possible alternative when searching for
information. The tool’s design structures the information-searching process through a series of
measured steps, each involving self-review and reflection on the part of students, as well as
providing them a process map of the overall task and progress monitors of completion and gives
them repeated opportunities to practice new skills. The third theoretical model is Competency
Theory, which suggests that students who lack information literacy skills do not realize it and
therefore are unlikely to seek out instruction (Gross & Latham, 2007). The three stages of the
tool give students repeated, structured practice in evaluating their own work (Investigate) as well
as the evaluating the quality of other students’ evaluations (Question) and making comparative
9

evaluation judgments (Solve). Through this structured skills practice, students learn to evaluate
their own skill level more realistically and compare their own skills to others based on shared
performance.
Together, the IL models of Kuhlthau’s Information Search Process, Library Anxiety, the
Principle of Least Effort, Satisficing, and Competency Theory provide context and background
to efforts to teach effective IL skills to college students. These theories help researchers
understand how students approach – and avoid – evaluating online information, and illustrate the
importance of designing learning tools that can ameliorate the challenges students feel while still
providing effective skills training.
Credibility evaluation criteria are a crucial component of the tool’s theoretical design.
The literature on online credibility evaluation provides extensive insight into the actual practices
students use, but without providing suggested methods to improve students’ evaluation skills.
Incorporating a research-based understanding of actual student credibility evaluation behavior
into the design of a learning tool situated in the online information environment was one of the
fundamental motivations for this research project. Former school media librarian Kathy
Schrock’s criteria model called “5Ws” (Who, What, Where, Why, and When) employs nonexpert language that is appropriate to the intended audience of the learning tool: incoming
college students, and potentially high school and community college students. In the learning
tool’s later stages, the 5Ws are mapped to more sophisticated credibility criteria language
(authority, relevance, reliability, currency, and purpose), providing scaffolding to bridge the gap
between students’ unsophisticated understanding of online information evaluation and the more
sophisticated models of evaluation criteria used by experts. While the 5Ws model of website
evaluation is widely used in IL education, especially in high schools, it has not been empirically
tested (Schrock, 2013) so this study represents the first research into the effectiveness of the 5Ws
model for teaching website evaluation criteria.

1.5.	  Methods	  	  
The research involves an experimental study of college undergraduates using a two
treatment group and a control group to compare the performance of subjects under three learning
10

conditions. The Treatment 1 group completed the IC tutorial and used the IC tool to conduct
credibility evaluations; Treatment 2 completed the IC tutorial but did not use the IC tool, instead
using an online form to conduct credibility evaluations; and the Control Group will only use the
online form to conduct credibility evaluations, without using either the IC tutorial or the IC tool.
Comparison between the Treatment 1 group and Control Group measured the imapct on learning
outcomes of using the IC tutorial and IC tool. Comparison between the Treatment 2 and Control
Group measured the impact of the using the IC tutorial only. Thus, the independent variable is
exposure to the structured sequence of scaffolded instruction and guided practice that constitutes
the design of the IC tool. The dependent variable is the students’ demonstrated use of specific
evidence to evaluate credibility criteria, and metacognitive awareness of the steps of the
evaluation process.
Both quantitative and qualitative data were collected during this study. Qualitative data
included student responses entered to credibility evaluation prompts, comments on other
students’ sources, and students’ responses to the credibility criteria and metacognition post-tests.
Quantitative logfile data on the use of the tool, including time spent on tasks, use of prompts, and
overall level of participation (e.g., did they exceed quotas) were automatically recorded. Logfile
data is a relatively unobtrusive method for gathering data and provides fine-grained, detailed,
time-referenced markers of student actions and allows the researcher to examine patterns among
student learning strategies (Perry & Winne, 2006).
After completing their treatment/control group activities, all subjects completed two posttests: a test of credibility criteria and a metacognition test. Both instruments were pilot tested
prior to the start of the experiment, and were modified as needed based on results from cognitive
interviews with pilot test subjects. The custom credibility criteria post-test developed for this
study asks 10 specific questions about the criteria for evaluating the credibility of online
information, based on the expert terminology and definitions introduced in the Question stage (as
opposed to the novice-level terminology of the tutorial). The open-ended questions ask students
to demonstrate their knowledge of both the criteria themselves and the strategies for evaluating
the criteria. The custom metacognition test developed for this study asks 12 questions
customized specifically to the process of evaluating the credibility of online information.
Response options were on a 4-point Likert scale (strongly disagree, disagree, agree, strongly

11

agree). Both post-test were conducted as online surveys, with questions assigned randomly to
reduce order effects.
The study’s research questions were answered based on analyses of both qualitative and
quantitative data, which provided a multi-dimensional understanding of student behavior and
strengthened findings and conclusions regarding the impact of the online participatory learning
tool. Open-ended responses were analyzed through coding for the presence of credibility criteria
and evidence-based source characteristics. Metacognitive prompt responses were coded for the
presence of planning, monitoring, and reflection on students’ learning. Results of the credibility
criteria and metacognition post-tests were scored numerically. Table 1 below lists the data
analysis methods that were used to answer each research question:
Table 1. Data analysis methods
Research question

Data source

Analysis method

RQ1

Credibility criteria post-test

Coding and statistical

RQ2

Reflective prompts

Content analysis

RQ3

Metacognition post-test results

Scoring and statistical

After all data are analyzed, inter-group statistical tests were conducted to compare results
between the Treatment groups and Control Group and look for evidence of statistically
significant differences. Scores on the credibility criteria and metacognition post-tests were
analyzed using one-way analysis of variance (ANOVA) with treatment condition as the
independent variable and scores as dependent variables to analyze patterns of performance.
Statistical analysis was conducted using the SPSS statistical software package.

1.6.	  Overview	  of	  this	  dissertation	  	  
This dissertation consists of six chapters: a literature review, a description of the
prototype design of the IC tool, the experimental research design, the findings, and discussion
chapters. The literature review (Chapter 2) covers three disparate areas of research that together
form the theoretical basis of the IC tool: Information Literacy, online credibility evaluation, and
Computer-supported Collaborative Learning. Chapter 3 covers the design and development of
12

the prototype version of the IC tool, the results of initial pilot testing, and the plan for finalizing
the tool. Chapter 4 describes the three research questions, the experimental design for
experimentally testing the impacts of the tool on student learning, the tests to be used, and the
data collection and assessment plan. Chapter 5 describes the findings of the experiment,
including an analysis of the sources found by students during the experiment, the results of the
three research questions, and additional findings regarding student skills. Chapter 6 summarizes
the findings, discusses the implications of the findings for the design of the IC tool, the study
design, and for online IL instruction. The final chapter closes with a discussion of the
contributions of the study and suggestions for future research.

13

Chapter	  2.	  Literature	  Review	  
This literature review covers three fields of research which informed the design of the IC
tool: Information Literacy (IL), online credibility evaluation, the concepts of metacognition and
scaffolding from the field of Computer-supported Collaborative Learning (CSCL). Because the
research addresses student IL skills and methods to teach evaluation skills, this literature review
(2.1) examines theoretical models in the library literature that apply to IL skills, research into
student IL skills, the benefits and challenges of IL instruction, and instructional methods
currently used to teach IL. Because the research addresses what evidence students use to judge
credibility of online information, this literature review (2.2) examines research into student
credibility evaluation practices, terminology of credibility criteria, and models of the evaluation
process. Because the research addresses techniques to best support students in learning
credibility evaluation skills, this literature review (2.3) examines CSCL learning theories, in
particular metacognition and scaffolding. The last sections summarizes the impacts of the
literature on the design of the tool.

2.1.	  Information	  Literacy	  	  
The research addresses student IL skills and methods to teach evaluation skills. This
section reviews the history and definitions of IL, theoretical models of IL, research into student
evaluation skills, the benefits and challenges of IL instruction, the benefits and challenges of IL
instruction, and instructional methods currently used to teach IL.
	  
2.1.1.	  History	  of	  IL	  
The history of the concept of IL begins with the traditional models of “library
instruction,” “library skills,” “user education,” and “bibliographic instruction,” or the practice of
educating the library user to locate and use library resources (Arp, 1990; Shapiro & Hughes,
14

1996; Thomas, 2004). Academic and school librarians were responsible for developing the
original agenda and principles behind information literacy (Lorenzo & Dziuban, 2006). These
practices focused on educating library users about access to books and physical print resources,
as well as orientation to the structural aspects of libraries such as facilities, classification and
organization (Behrens, 1994). The term “information literacy” gained acceptance during the
1980’s (Sundin, 2008) and in the 1990’s a new focus emerged on enhancing student skills
(Markless & Streatfield, 2007). This shift in focus has been characterized as a move from the use
of particular information artifacts towards shaping user behavior (Sundin, 2008), from the
practice of teaching tool-based skills toward teaching competencies (Špiranec & Zorica, 2010),
and from the traditional definition of librarian as collector and curator of resources to
identification as information specialist and teacher (Thomas, 2004). In 1998, Langford
commented on the debates over the definition of IL:
Is it a concept or a process? Is it an embodiment of essential skills that
have only had name changes over the decades? Or is it a new literacy that
has been transformed from existing literacies to complement the emerging
technologies for which the Information Age students must be skilled?
(p.2).
While there has been much discussion of these questions, even today a general consensus has not
been reached on the answers. Gibson (2008) describes the IL model as a “reform movement”
with broad educational goals. He identifies the information literacy model as “a coherent,
planned, program-level set of research skills and learning outcomes” and “a programmatic,
curriculum-integrated, and pervasive and sustained placement of information and research skills
throughout the curriculum” (p. 12). This definition emphasizes the context of IL training as
integrated into the broader educational mission rather than as a collection of isolated skills. In
contrast, Gibson characterizes bibliographic instruction as an instructional movement focused on
teaching students about the tools, resources and strategies for using a specific library’s
information resources in the context of a particular assignment given by faculty.
The concept of IL has evolved over the past 30 years, developing through different
phases each with its own definitions and implementations. In general, however, two definitions
issued by professional organizations of libraries have set the generally-accepted standard. In
1989, the American Library Association (ALA) released its Presidential Committee on
15

Information Literacy report stating “To be information literate, a person must be able to
recognize when information is needed and have the ability to locate, evaluate, and use effectively
the needed information" (ALA, 1989). This report characterized IL as a “means of personal
empowerment” and a “survival skill.” In 2000, the Association of College and Research
Libraries (ACRL), a division of the ALA, issued their own definition of IL in their Information
Literacy Competency Standards for Higher Education. Referring back to the original ALA
definition, the ACRL definition added that IL is “a set of abilities requiring individuals to
"recognize when information is needed and have the ability to locate, evaluate, and use
effectively the needed information" “ (ACRL, 2000). These standards described IL as an “area of
competence” and “an intellectual framework.” This statement marked an important shift of focus
from defining a type of a person to identifying a set of abilities. As described by Kuhlthau
(1999):
Information literacy incorporates both library skills and information skills, but
adds the critical component of understanding the process of learning in
information-rich environments. Information literacy extends library skills beyond
the use of discrete skills and strategies to the ability to use complex information
from a variety of sources to develop meaning or solve problems (p. 11).
Breivik and Jones (1993) stated that information literate students:
…become sophisticated users of these resources and technologies as they:
(1) gather needed information from all sources; (2) test the validity of
information as it remains constant and as it changes from discipline to
discipline; (3) place information into various contexts that ultimately will
yield its pertinent meaning; and (4) remain skeptical about information
and discriminate fact from truth (p. 26).
IL is closely aligned with critical thinking (Akyol & Garrison, 2011), with critical
thinking sometimes taken as a central aspect of IL (Lorenzo & Dziuban, 2006). As defined by
the Foundation for Critical Thinking:
Critical thinking is the intellectually disciplined process of actively and
skillfully conceptualizing, applying, analyzing, synthesizing, and/or
evaluating information gathered from, or generated by, observation,

16

experience, reflection, reasoning, or communication, as a guide to belief
and action (Scriven & Paul, 1987).
Allen (2008) states that:
Critical thinking involves the conceptualization, analysis, synthesis,
evaluation, and ultimate application of information that the learner has
experienced combined with previous knowledge. (p. 21)
These definitions clearly share similarities with definitions of information literacy. In fact, the
two terms – “critical thinking” used primarily in education, “information literacy” used primarily
in library instruction – may in fact merely be disciplinary terminology for the same set of skills
(Allen, 2008). Reece states that “information literacy is a form of critical thinking applied to the
realm of information” (2005, p. 488) and Doyle suggests that “while critical thinking skills
provide the theoretical basis for the process, information literacy provides the skills for practical,
real world application” (1994, p. 4). In this context, IL can be understood as an embodiment of
critical thinking in the context of information seeking activities.
Some have argued for an even broader interpretation of the IL mission. Tuominen et al.
(2005) describe IL as a sociotechnical practice with much broader implications outside
education. Shapiro and Hughes argue that IL should not be conceptualized and taught as just a
collection of technical skills that teach people to be “effective information consumers” but
should also enable people to “think critically about the entire information enterprise and
information society”(1996, n.p.). They argue that IL should in fact be seen as a new liberal art
which embraces a broad perspective of social, cultural and even philosophical context. This call
is echoed by Breivik and Jones who argue that IL should be incorporated into undergraduate
education as a part of a revived liberal education philosophy, aimed at helping students “to better
illuminate their understandings” and develop the “full range of abilities that will be absolutely
necessary for future professional flexibility and successful citizenship” (Breivik & Jones, 1993,
p. 29).
Digital media and social computing have uncoupled credibility and authority, shifting the
burden of evaluation onto the individual information seekers (Metzger et al., 2010). Today’s
young people are leaving behind – or have already left - the traditional world of print, flocking to
all forms of digital media and developing new forms of online communication and new modes of
17

interaction with information (Todd, 2008). Traditional techniques of credibility evaluation
(granting credibility to representatives believed to provide reliable information or granting it
based on credentials) only works when there are a limited number of sources and when there are
high barriers for access to public dissemination of information (Metzger et al., 2010).The
Internet has become a site of information exploration, creation and exchange, a “virtual place for
knowledge production rather than a collection of stable documents” (Sundin & Francke, 2009,
n.p.). Traditional library-based research models, based on information scarcity and expert
authority, have been problematized.
The rapid evolution of information technology and the information environment has
impacted the nature of IL skills. Historically, markers of information credibility in the printbased paradigm were maintained by professional gatekeepers such as editors and reviewers (Rieh
& Danielson, 2007). Physical formats were often reliable proxies for authority, as publishers
limited the amount of sources produced and readers had little interaction with authors. The
guidelines for traditional credibility evaluation promoted by bibliographic instruction were
primarily based on the material format of documents (Sundin & Francke, 2009). However, the
Internet has caused an “erosion of information contexts” where all search results have almost
identical look and feel (Tuominen, 2007, p. 2). One of the chief differences between the web and
traditional sources of information is that it lacks the filters, the markers of institutional credibility
and authority, which promote reliability in many print sources (Burbules, 2001, Mackey &
Jacobsen, 2011). Web pages typically offer few reliable cues to credibility (Iding et al., 2008).
For instance, newsfeeds and RSS aggregators strip webpages of cognitive authority clues and
present all information uniformly (Tuominen, 2007). This the lack of conventional quality
control mechanisms and indicators of authority can make it difficult for students to make
credibility judgments about online information (Rieh, 2002). The stability and permanence of a
book as an information artifact, which helped users make credibility, does not apply in today's
online environment (Tuominen, 2007).
Since the basic model of library skills and bibliographic instruction pre-dates the Internet,
the traditional IL instruction model must be adapted to the current digital environment of
pervasive information and social media (Mackey & Jacobsen, 2011). Much of traditional IL
tended to be focused on library resources and technical searching skills, relying on laborious
sequential steps (Markless, 2009). Marcum (2002) argues that traditional IL practice is too
18

grounded in text and overly concerned with basic skills to address the current information
environment. Holman argues that librarians have traditionally relied on information retrieval
systems with complex interfaces and less intuitive search strategies based on mental models of
print-based research (Holman, 2011). Librarians have wanted to push students towards advanced
searching, but students are not interested because they are already satisfied with their search
results (Godwin & Parker, 2008). Even though they may not actually possess strong search
skills, students are still not likely to want or appreciate IL instruction that simply teaches
advanced search techniques (Holman, 2011). Instead, IL instruction should help students refine
their searches rather than trying to make them into "advanced" searchers (Godwin, 2008).
Echoing this point, Tuominen (2007) argues:
"The most important goal of IL education should be to increase users'
conceptual understanding of their information environment. In this sense, the
tricks of information retrieval like truncation or Boolean logics are not so
significant. Who even uses Boolean searches anymore?" (p. 8)
Holman suggests that rather than teaching students more effective search syntax, IL instructors
should focus on developing critical thinking and evaluative skills (Holman, 2011). Farkas argues
that:
“students will need to evaluate information in more nuanced ways than
they are currently taught at most colleges and universities. Information
literacy needs to be increasingly focused on teaching evaluative skills to
students, skills that go well beyond determining whether or not
something is peer-reviewed (p. 90)
Orme echoes this caution:
“Instructional librarians might wish to consider whether they wish to train
students to focus on doing things correctly or whether they might wish to
train students more generally to navigate an information landscape and
recognize what is correct when they encounter it.” (p. 213, italics in
original)
Given the changes in today’s information environment, a new pedagogical approach to IL
instruction is needed. The abundance of information sources available online, and their lack of
traditional markers of authority, present new challenges to both information seekers and IL
instructors.
19

2.1.2.	  Definitions	  and	  standards	  
The first documented use of the term “information literacy” was by Paul Zurkowksi,
president of the Information Industry Association (IIA), in a proposal to the National
Commission on Libraries and Information Sciences (NCLIS) that stated: “People trained in the
application of information resources to their work can be called information literates”
(Zurkowski, 1974). The report discussed the needs of workers in emerging information
technology environments and raised policy questions regarding the relationship between libraries
and the private sector (Behrens, 1994). The report suggested that NCLIS establish the goal of
achieving national information literacy in the following decade. In a later report, Eugene
Garfield, founder of the Institute for Scientific Information (ISI) which was a partner in creating
the IIA, presented a broader version of Zurkowski’s definition:
“The IIA defines an ‘information literate’ as a person who knows the techniques
and skills for using information tools in molding solutions to problems” (Garfield,
1979, p. 210).
This report characterized the content of such training as “methods of information handling” (p.
210), indicating the professional, work-related orientation of this document. Note that this
definition defines the characteristics of a person rather than a set of skills, as does the later ALA
definition.
The 1983 report by the National Commission on Excellence in Education, titled A Nation
at Risk, decried the contemporary state of American education, but ignored the role of libraries in
education (Behrens, 1994). This oversight in part prompted the American Library Association
(ALA) to create their own charter document. In 1989, the ALA Presidential Committee on
Information Literacy produced a report detailing the concept of information literacy, stating that
“To be information literate, a person must be able to recognize when information is needed and
have the ability to locate, evaluate, and use effectively the needed information” (ALA, 1989).
This authoritative definition became a springboard for our current understanding of the concept
(Eisenberg et al., 2004). The report identified specific skills required for students to be
considered information literate, and also expanded the concept to lifelong learning. The
recommendations of the Presidential Committee final report led to the creation of the National
20

Forum on Information Literacy later that year. This forum, which continues to meet regularly,
promotes the adoption of IL across all professions (Eisenberg et al., 2004).
In 2000, the Association of College and Research Libraries (ACRL), a division of the
ALA, issued their Information Literacy Competency Standards for Higher Education, providing
“a framework for assessing the information literate individual” (ACRL, 2000). Referring back to
the original ALA definition, the ACRL definition added that IL is “a set of abilities requiring
individuals to recognize when information is needed and have the ability to locate, evaluate, and
use effectively the needed information.” The ACRL document has become a common starting
point for librarians in higher education who are designing IL instruction (Bobish, 2010; Allen,
2008). It is the most widely adopted definition in use by universities and academic libraries
(Fitzpatrick & Meulmans, 2011).
In the field of primary school librarianship, the American Association of School
Librarians (AASL), a division of ALA which focuses on K-12 librarians, and the Association for
Educational Communications and Technology (AECT), created a set of standards focused on
teaching IL skills to primary school students. Information Power: Building Partnerships for
Learning (AASL & AECT, 1998) redefined the role of the library media specialist as an
instructor actively engaged in education efforts (Eisenberg et al., 2004). The AASL/AECT
standards included “Information Literacy Standards for Student Learning” which state that an
information literate student “accesses information efficiently and effectively” (Standard 1),
“evaluates information critically and competently” (Standard 2), and “uses information
accurately and creatively” (Standard 3). These criteria clearly echo the original ALA definition.
In 2009, AASL issued a new Standards for the 21st-Century Learner which extends the criteria
to a broader educational framework focused on skills, dispositions, responsibilities, and selfevaluation strategies. The IL elements are now described as “applying critical-thinking skills
(analysis, synthesis, evaluation, organization) to information and knowledge in order to construct
new understandings, draw conclusions, and create new knowledge” (AASL, 2009).
Another national standards initiative relevant to IL skills is the Common Core State
Standards Initiative, led by the National Governors Association Center for Best Practice and the
Council of Chief State School Officers (NGACPB, 2010). The educational standards described
in Common Core have achieved a widespread acceptance as a national baseline for student
achievement and have been implemented by 45 states and 3 territories (see
21

http://www.corestandards.org/in-the-states). The Common Core Standards for English Language
Arts and Literacy in History/Social Studies, Science, and Technical Subjects contains “Writing
Standards” with a sub-section “Research to Build and Present Knowledge” which includes
“assess the credibility and accuracy of each source” (Grade 6-8), “assess the usefulness of each
source in answering the research question” (Grade 9-10), and “assess the strengths and
limitations of each source in terms of the specific task, purpose, and audience” (Grade 9-10)
(NGACPB, 2010). The Grade 11-12 Writing Standards also include “gather relevant information
from authoritative print and digital sources, using advanced searches effectively” and “integrate
information into the text” (NGACPB, 2010). Overall, these different standards incorporate
substantially similar definitions of IL skills. The consistency of these standards and their linking
to accreditation for schools and universities helped normalize IL as part of the broader
educational mission.
Other professional and commercial organizations have developed similar standards. The
Association of American Colleges and Universities identifies IL as one of the essential learning
outcomes for all students in the 21st century (NLCLEAP 2007). The Partnership for 21st
Century Skills, a national organization that “advocates for 21st century readiness for every
student,” has issued its Framework for 21st Century Learning which describes the “skills,
knowledge and expertise students should master to succeed in work and life in the 21st century”
(Partnership for 21st Century Skills, 2011). These standards include “Learning and Innovation
Skills” (creativity and innovation, critical thinking and problem solving, communication and
collaboration) and “Information, Media and Technology Skills” (Information Literacy, Media
Literacy, ICT Literacy). The organization has also published a crosswalk between their standards
and the Common Core standards.
Another professional organization, the International Society for Technology in Education
(ISTE), a professional association for educators using technology in PK–12 classrooms, has
developed their National Educational Technology Standards (NETS). Described as “the
standards for learning, teaching, and leading in the digital age” (ISTE, 2012). NETS includes
sub-sections on “Research and Information Fluency” (apply digital tools to gather, evaluate, and
use information) and “Critical Thinking, Problem Solving, and Decision Making” (use critical
thinking to plan and conduct research) The multiplicity of these standards and the specificity of
their multiple criteria, while potentially confusing for instructors to implement, also testifies to
22

the increasing normalization and integration of IL concepts into mainstream education and
society.
2.1.3.	  Theoretical	  models	  
Many researchers have investigated what factors most influence students during the
library research process. One of the most influential models in IL theory is Kuhlthau’s
Information Search Process (ISP), which broke new ground by exploring the cognitive and
affective aspects of the information search process, rather than merely the mechanistic steps of
information retrieval (Kuhlthau, 1991). The cognitive processes involved in Kuhlthau’s ISP
(critical thinking, decision making, problem solving) relate closely to the fundamental skills of
IL (Thomas, 2004). Based on a longitudinal series of studies, initially focusing on high school
students researching term paper topics, Kuhlthau documented students’ thoughts, feelings, and
cognitive processes as they proceeded through the research process. The students’ emotional
states of confusion or confidence changed based on their stage in the process. Students often
experienced uncertainty and anxieties at the initial stages of the ISP. Drawing on constructivist
pedagogical and cognitive learning theory, she crafted a model that identified “zones of
intervention” based on Vygotsky’s Zone of Proximal Development which focus on providing
effective assistance and coaching when it is most needed by students. Since it is empiricallytested, the ISP model was an important milestone in research on students’ information-seeking
behavior and has served as the basis for much library instruction (Thomas, 2004). The ISP model
created a new vocabulary and role for school librarians (Behrens, 1994). Throughout a long
career, Kuhlthau has continued to contribute major theoretical works to IL research and practice,
including the model of inquiry-based information seeking or Guided Inquiry, which integrates
the librarian into a teaching team and seeks to create an environment that motivates students to
learn through constructing their own meaning and developing deep understanding (Kuhlthau et
al., 2007).
Of the more theoretical models and theories of information seeking behavior (ISB), some
that apply specifically to students and their library research habits include Library Anxiety, the
Principle of Least Effort, Satisficing, and Competency Theory. The concept of “library anxiety”
was originally described by Mellon (1986) as a sense of powerlessness when beginning an
23

information search that required using the library. In her research, students reported feeling lost,
fearful of library staff, and unable to navigate the library. Onwuegbuzie, Jiao & Bostick (2004)
elaborated a structural equation model of the situational antecedents to library anxiety. As
students transition from the high school library to the academic research library, they may be
affected by unfamiliarity with new surroundings and general anxiety about success. Students also
report the associated emotions of perceived inadequacy, embarrassment and intimidation in the
library environment (Van Scoyoc, 2003). Measurement scales have been developed to test the
impact of library anxiety on students’ behavior and performance. In the age of easily-accessible
online information, students who experience these emotions and feelings have even less
incentive to visit the library and may avoid doing so.
The Principle of Least Effort (PLE), originating in the work of George Zipf, suggests that
most people will chose easily available sources of information, even when they are clearly of
lesser quality than other, harder to find sources (Zipf, 1949; Rosenberg, 1974; Mann, 1993). The
literature on PLE shows that most researchers, even experienced scholars, rely on information
access systems that are perceived as easy to use (Mann, 1993). Related to PLE is the concept of
“satisficing,” a term coined by Herbert Simon (1956), which suggests that information seekers
rarely conform to the idealized model of effective, motivated searchers who will follow all the
steps of an ISB model, but instead accept the first satisfactory alternative a higher-quality
alternative. Students often reduce the cognitive of reviewing information through shortcuts such
as relying on familiar sites, search engine descriptions, skimming, and ending searches as soon
as an acceptable result is found (Thomas, 2004). Students conduct “cost-benefit analyses” of
how much time and attention to expend on searching and research (Gibson, 2008). The concepts
of PLE and satisificing correlate with findings that emphasize efficiency and ease of finding
results as primary motivations for much student research behavior, rather than the scholarly goals
that faculty and librarians might hope for. In a longitudinal study of undergraduate information
seeking behavior, Warwick et al. (2009) found that students completed information seeking tasks
with the minimum amount of effort judged necessary. Using “strategic satisificing,” the subjects
“estimated what the minimum literature requirements were and chose specific goals that they
could fulfill easily and quickly with their existing skills” (p. 2412). These students avoided the
library, relied on familiar strategies to find satisfactory information with a minimum of effort,
and were unwilling to move beyond their current skill level. New skills were only adopted when
24

immediately required by an assigned task, and students “deployed considerable ingenuity in
finding ways to avoid or limit complexity” (Warwick et al., 2009, p. 2414). This avoidant
behavior stands in stark contrast to the idealized model of ISB in which the motivated seeker
follows a directed process of information seeking to the best result.
Competency Theory suggests that students who lack information literacy skills do not
realize it and therefore are unlikely to seek out instruction (Gross & Latham, 2007). In a study
investigating the relation between standardized Information Literacy Test scores and Library
Anxiety Scale scores of college undergraduates, the researchers found that low-skilled students
are unlikely to self-identify as lacking skills in either a classroom or library context, and at the
same time are unable to accurately assess the skill levels of others (Gross & Latham, 2007). Selfteaching or learning from friends were the most frequently reported methods reported by these
students for learning research skills. The authors noted that “students who are unaware of a
deficit in their IL skills are unlikely to seek skill remediation on their own or to engage with
instruction when forced to take it” (p. 334). Students with low level skills hold inflated views of
their own competence in information seeking, do not know their own weaknesses, and often
overestimate their abilities to find and evaluate online information (Manuel, 2002). Today’s
college students may perceive their own fluency with technology to be so thorough that they fail
to see value in learning IL skills (Brown, Murphy & Nanny, 2003). While these “digital natives”
can often multitask, learn systems without consulting manuals, and surf the web confidently,
they also frequently lack the critical thinking and metacognitive skills necessary for college
research (Lippincott, 2005).
Together, the IL models of Kuhlthau’s Information Search Process, Library Anxiety, the
Principle of Least Effort, Satisficing, and Competency Theory provide context and background
to efforts to teach effective IL skills to college students. These theories help us understand how
students approach – and avoid – evaluating online information, and illustrate the importance of
designing learning tools that can ameliorate the challenges students feel while still providing
effective skills training.
	  

25

2.1.4.	  Student	  information	  literacy	  skills	  

Researchers consistently find that college students are not learning how to think critically
about the information they find (Kolowich, 2011). In fact, there is evidence that many students
are information illiterate when they enter college (Gross & Latham, 2007).The critical evaluation
skills of high school students were studied by Julien and Barker (2009), who found that
“understanding of critical evaluation criteria such as authority, accuracy, objectivity, currency,
and coverage was not evident” (p. 15). Studies show that most undergraduates are confused by
what college-level research entails, do not understand what quality research resources are or how
to locate them on the Internet, are unable to narrow down topics to make them manageable, and
are overwhelmed by the plethora of available resources (Head, 2007). Incoming college students
frequently don’t have an understanding of what is required for an academic research paper in
terms of authority, content, relevance, or other ways of evaluating information (Parker-Gibson,
2005). While searching for information, students do not effectively evaluate the sources they
found, and predominantly viewed the trustworthiness of information in terms of the site or
resource rather than by evaluation of the content. Parker-Gibson states that “Overall, students
gave less emphasis to the process of finding information than to the end product of the search”
(2009, p. 15). This general lack of IL skills in today’s students presents a critical problem for IL
instructors.
Jackson (2008) suggests that the critical thinking skills described in the IL standards may
rely on higher levels of cognitive development that are only reached by senior students.
Undergraduates may still be at earlier levels of cognitive development, either dualistic or
multiplistic, whereas true critical thinking requires reaching the relativistic stage of development
where students are aware they that are active makers of meaning (Jackson, 2008). If students still
believe that authorities hold absolute truth and/or take a surface approach to learning, they have
may not be able to engage in higher-level evaluative skills. Also, Jackson notes that stress,
anxiety and confusion can prompt students to regress to earlier stages when confronted with a
problem (for example, library anxiety or the stress of a difficult assignment). Level of domain
expertise is another important factor of student information seeking competence. Domain
novices tend not to employ the search strategies of experts (author searching, citation searching,
26

footnote chasing, journal runs, and known-item searching) but instead tend to rely on non-expert
strategies such as subject and keyword searching (Drabenstott, 2003). Drabenstott suggests that
“librarians and instructors have an important role to play to transition students from non-domain
to expert-domain strategies” (p. 85) and advocates for scaffolding domain novices from their
usual strategies into the strategies characteristic of experts.
Students’ lack of IL skills can in part be attributed to the primacy of the Internet in
college students’ lives, and their unsophisticated use of online search tools. Rieger et al. (2004)
states that a growing number of students do not understand the difference between information
that is offered by library resources and by internet search engines The web is the first choice of
information source for most students (Herring, 2011). College students overwhelming rely on
Google to the exclusion of many other academic search tools (Hargittai et al., 2010; Head &
Eisenberg, 2011; Kim & Sin, 2011; Kolowich, 2011; OCLC, 2002; Van Soyoc & Cason, 2006).
Most college students instinctively start their research by using public Internet sources (Curtis
2000; Mizrachi 2010). Costello et al. (2004) note that students with an “information-age
mindset” rely almost exclusively on the web for all their information needs. When
conceptualizing online searching, “students see Google as being ‘the’ Internet, and they use these
two terms interchangeably, seeing them to be one and the same thing” (Julien & Barker, 2009, p.
14). This often means that users make very little use of advanced search facilities which are
available (Spink, Wolfram, Jansen & Saracevic, 2001). Instead, students use tools that require
little skill, and “appear satisfied with a very simple or basic form of searching” and assume that
“search engines ‘understand’ their queries” (Rowlands et al., 2008, p. 297). In addition, students
tend to demonstrate inflated views of their own IL skills, especially students with lower level
skills whose lack of skill hinders their ability to accurately assess their own performance or to
recognize expertise in others (Gross & Latham 2007). This demonstrates the increasing
importance of IL skills in the contemporary online information environment. Thomas (2004)
argues that many of the problems that students encounter while researching online electronic
reflect their lack of basic literacy skills:
“…unless youngsters are taught and also expected to appraise critically the
resources they find on the Internet and pursue research questions rather
than fact-finding tasks, the potential for inspiring the development of
27

higher-order thinking skills represented by the activity of Internet-based
searching will remain largely unrealized” (Thomas, 2004, p. 136)
Given that the web is such an important part of many students’ lives, it is unfortunate that little
time in school is dedicated to teaching students how not only to be web users, but web learners
(Herring, 2011). While web users may have a superficial, technical understanding of the web,
their use involves little or no reflection. Web learners develop effective search strategies,
critically evaluate what they find, select relevant information, use information ethically and
effectively, and learn from reflection on their use (Herring, 2011). Web learners are information
literate users of the web. Madden et al. (2011) argue that any training in information literacy
skills should aim to equip students with guidelines to help them assess the credibility of websites,
while encouraging them to reflect on the nature of their search task and to apply the guidelines
accordingly.
Overall, students have trouble evaluating information and do not have a critical attitude
towards information on the web (Brand-Gruwel et al, 2005). When they do evaluate online
information, they often utilize criteria that are entirely different from those promoted in IL
training (Hargattai et al., 2010). Given the consistency of these findings, Julien and Barker
(2009) noted that “despite clear evidence that sophisticated information literacy skills are
beneficial to academic success, students are generally unsophisticated information seekers in
academic contexts” (p. 12). The academic requirements for credible, relevant sources to support
college research papers remain a mystery to many college students, presenting both a challenge
and an opportunity for IL instructors.
2.1.5.	  Benefits	  of	  IL	  training	  	  
Research shows that IL instruction has a positive impact on student skills, performance
and academic achievement. College students who participate in information literacy classes
report significantly less library anxiety (Van Scoyoc, 2003) and that high achieving students are
more likely to report experiencing formal information literacy instruction (Smalley, 2004; Gross
& Latham, 2007). Wang (2006) found statistically significant differences in grades between
college students who took a library credit course and students who did not, and those who had
taken the instruction in library skills received higher grades on their papers and in their courses.
28

Selegean, Thomas & Richman (1983) also found a statistically significant improvement in the
academic performance of those students who had completed the library instruction course over
those students who had not. Ren (2000) found that receiving library instruction significantly
increased college students’ self-efficacy in electronic information searching.
Daugherty & Russo (2011) showed that students who received IL training continued to
use the materials and skills taught in the course throughout their college careers for both course
work and personal research, suggesting that a library IL course establishes a foundation for lifelong learning. Wong, Chan & Chu (2006) found that students who took an IL course felt that
they retained the skills they were taught and also felt confident in doing library research.
Stamatoplos & Mackoy (1998) showed that students who received IL training felt increased
overall satisfaction with the library. School library studies have also shown IL’s positive effect
on student attitudes and achievement. Goodin (1991) showed that IL instruction makes a
significant impact on students' attitudes and performance and helps prepare high school students
for college; Lance et al. (2000) showed that school library programs increased student reading
scores; and Todd et al. (1992) demonstrated positive impacts on students’ learning processes and
outcomes. These studies demonstrate that IL training is beneficial to academic success, and
indicate that wider access to IL skills would offer opportunities for success to even more
students.
Overall, there is little empirical research into the impact of IL instruction on student
learning. This research makes a contribution to the field by conducting a well-designed
experimental study that provided measurable outcomes based on between-group comparisons
(see Chapter 4 for discussion of the research design.)
2.1.6.	  Challenges	  to	  teaching	  IL	  
While stakeholders in higher education and in professional societies agree that IL is
necessary to students’ success in their education and afterward in their work and personal lives,
there are multiple obstacles and challenges to implementation of universal IL instruction
programs. One obstacle is the “faculty problem”: the perception of librarians that faculty are
either apathetic or outright obstructive towards efforts to collaborate on IL instruction (McCarthy
1985). This may arise from several factors. Faculty may not view librarians as educational
29

partners but may regard them as support staff and providers of support services (Owusu-Ansah
2004; Manuel, Beck and Molloy 2005; McGuiness 2006). Faculty may feel that librarians are not
qualified to be teachers (Saunders 2012). They may also be unwilling to cede valuable in-class
time to librarians (Hardesty 1995; Breivik & Jones 1993; Owusu-Ansah 2004; Hrycaj and Russo
2007). The language of standards and outcomes used by librarians may not connect with faculty,
who may view IL as an administrative or bureaucratic imposition (Bell 2011). Faculty and
librarians may have very different expectations of the content and desired emphasis of IL
instruction (Cunnignham, Carr, & Brasley, 2011). Given these challenges, the onus is on
librarians to initiate and sustain discussions with faculty about IL instruction and to proactively
build collaborative relationships (Saunders 2012).
Another obstacle may be faculty perceptions of IL training itself. While faculty appear to
value IL competencies, they do not necessarily agree on how students should be taught these
skills (Saunders 2012). Some instructors seem to expect that students will acquire the necessary
competencies on their own by doing research projects, through working with other students, or
simply through exposure to resources (McGuinness 2006). Faculty tend to focus only on their
own disciplinary content and assume that IL skills will be addressed in other circumstances
(Hardesty 1995). They may assume that IL instruction happens in introductory courses, and may
feel that it is not their responsibility to teach it themselves (Saunders 2012). Over all, faculty tend
to rely on coursework and assignments as the primary vehicle for students to learn IL skills, and
do not actively integrate it into their curriculum in a systematic way (McGuinness 2006,
Saunders 2012). Even when librarians successfully work with faculty to bring IL instruction into
the classroom, the reality is that there is very little time available in the faculty member’s
curricula to include IL content (van Meegen & Limpen, 2010).
Harris identifies three structural challenges to teaching credibility evaluation in public
schools: the variation among curricula and school requirements between states, and even among
counties; the proliferation of high-stakes testing; and the culture of filtering and blocking online
information to protect children, with the unintended result of limiting opportunities to teach
authentic credibility (Harris, 2008). Some schools may also focus on teaching information and
communication technology (ICT) skills strictly as tool literacies, in which the technology itself is
the object of instruction, a practice which has its root in the traditional business school
30

curriculum where students are taught the computer applications which they will need to succeed
in the workforce (Harris, 2088). An additional obstacle is that IL instruction tends to be located
on the periphery of the curriculum in many academic institutions, and getting buy-in from
administrators for programs can be a struggle (Branston, 2006). Since IL is applicable in all
disciplines, it doesn’t belong specifically to any one discipline but belongs to all of them (Weiner
2010). IL may be perceived as something that is library-oriented and not part of the general
curriculum (Langford, 1998).
Despite the advocacy of librarians for comprehensive, campus-wide, for-credit IL
courses, accreditation standards across the United States are inconsistent in their inclusion of IL
requirements (Owusu-Ansah 2004). K-12 schools are inconsistent in requiring IL training for
their students (Gross & Latham, 2007). In most high schools, schools, there seems to be an
assumption that students will learn IL skills, but there is little empirical evidence that this is true
(Herring, 2011). Only a small percentage of higher education institutions with first-year
experience programs include a required information literacy component (Boff & Johnson 2002).
The inclusion of IL in curricula relies on the advocacy of individual librarians (Weiner 2012).
However, librarians lack political leverage within the academic community, making it difficult
for librarians to create change (McGuiness 2006). These difficult conditions mean that broad
integration of IL into undergraduate education remains an aspiration rather than a fully realized
ideal (McGuiness 2006). This lack of consistent, uniform program implementation into the
curriculum creates a tension between the official standards and competencies models developed
by library leaders, library associations, and national committees, and the practical experience of
librarians who deliver the instruction.
IL instruction is often conducted in a “one-shot” format, which requires that only the
most basic topics can be covered in one in-class session. However, these one-shot instruction
sessions are often “reactive, limited, place-bound, and constrained in terms of wider impact”
(Gibson, 2008, p. 12) whereas IL aims to have a broader impact on student success and lifelong
learning (Bruce, 2004). One-shot instruction sessions cannot provide students with the sustained
practice required to learn, apply and master IL competencies (Mokhtar et al. 2008, Mery et al.
2012). These brief training sessions may be the only explicit and focused exposure to IL that most
students receive, and the limited time and contact with students make it difficult for librarians to keep

31

students interested and engaged (Smale 2011). Since research tools and IL concepts are not
intrinsically interesting to most students, catching the attention of highly technologically and

visually oriented students is a challenge for librarians (van Meegen & Limpen, 2010). This lack
of opportunity to engage and motivate students means that students get bored quickly when IL

lessons do not trigger them to pay attention (Doshi 2006). Studies show that students rank
required, for-credit classes as their least preferred means of getting library instruction, compared
with individual instruction conducted at the point of need while students are actively seeking
information (Davidson, 2001). Manuel (2002) argues that the lecture format is an especially
ineffective instructional technique for Gen Y students because these students prefer more active
learning environments. Costello et al. (2004) state that delivery of library instruction through the
traditional lecture method is ineffective and does not engage today's students:
The literature clearly indicates that the traditional library instruction
session organized around a print handout, a lecture, and a demonstration
of library resources fails to adequately meet the needs of the current
generation of students. Instruction librarians must embrace new
technologies to assist them in their teaching mission (p. 453).
This criticism relates back to Gibson’s characterization of traditional BI one-shot sessions as
“reactive, limited, place-bound, and constrained in terms of wider impact” (2008, p. 12).
Information literacy needs to be reinforced over a longer period of time with appropriate
scaffolding and guidance (Chu, et al., 2011).
Other formats for teaching IL skills may be more appealing to students. In a study
comparing different methods of IL instruction, students specifically mentioned that they
preferred an online format since it allowed them to actually use the skills that they were learning
about (Anderson & May, 2010). Building IL skills training into the online environment that
today’s students are accustomed can potentially make IL more relevant to them by giving them
practice in evaluating real world sources that they might find in their own information searching
process. As information technology continues to reshape contemporary education, new forms of
IL instruction continue to emerge, including online, interactive tools and use of social media.
Teaching philosophy and pedagogy in librarianship has moved from the early concept of the
traditional “sage on the stage” model to the contemporary model of “guide on the side” (Doyle,
1994), embracing cognitive, constructivist and inquiry-based models of learning (Stripling,
32

2010). Librarians have shifted their educational goals from teaching students how to locate
materials for particular assignments using specific library tools to teaching students how to deal
with information in any format located anywhere (Thompson, 2002). Overall, there has been a
substantial shift in the status of librarians away from traditional roles as transmitters of
knowledge towards “the innovative role of facilitator of independent learning” (Lorenzo &
Dziuban, 2006, p. 11).
The expanded role of librarians has developed from simply teaching retrieval skills to
incorporating “a more total research environment in the course of finding and using
information/knowledge” (Owusu-Ansah, 2004, p. 5). Librarians now focus less on selecting and
presenting appropriate tools and resources for students’ use, instead focusing on to helping
students develop their own decision-making skills based on context-specific needs (Schrum &
Berenfeld, 1997). A primary motivation of these changes was (and continues to be) the rapidly
evolving technological infrastructure of the information environment, and the resultant changing
requirements for effective information seeking and use skills (Behrens, 1994). Despite these
transformations, however, the fundamental mission of IL education has remained the same: to
help people learn to help themselves by becoming more effective, critical-thinking information
users (Grassian, 2004).
2.1.7.	  Implications	  for	  the	  research	  
As the field of IL instruction has evolved over time from its origin in bibliographic and
user education to the a broader conceptualization of a set of skills and competencies, approaches
to teaching IL have changed as well. The contemporary mission and philosophy of IL instruction
envisions developing crucial 21st century skills in all learners. However, the content of much IL
instruction remains rooted in the print-based paradigm of source evaluation. The reliance on
traditional physical formats and cues is evident in many online IL tutorials, i.e., distinguishing
between popular magazines and scholarly journals by comparing physical formats. In today’s
online information environment, explicit markers of authority relied upon in print materials often
do not exist or may be difficult to identify without sustained practice. Today’s students may not
even recognize traditional physical formats such as magazines and scholarly journals, since
individual articles are typically available online as disaggregated items not clearly associated
33

with a physical source, and students must learn to make their own judgment of source credibility
based on synthesizing a variety of evidence-based source characteristics. Due to these factors, IL
instruction may seem irrelevant to today’s students, who are comfortable with web searching and
assume that search engines insure credibility. IL instruction needs to embrace online information
sources and integrate learning skills into the authentic context in which students actually do the
majority of their research: on the Internet.
Given research that demonstrates the lack of student IL skills, as well as the benefits of
IL training in preparing students for future success, a new pedagogical approach to IL instruction
is needed. This research aims to test an approach to integrating IL instruction into the online
searching environment, customized to online evidence-based source characteristics, and
incorporating interactivity and participation. This research addresses several of the challenges
identified in the literature review: it address the faculty problem and the deficiencies of the oneshot IL instruction mode by integrating IL instruction into coursework and real-life classroom
assignments. It addresses Library Anxiety by removing IL instruction from the context of the
library and into the immediate context of online searching where students feel comfortable. It
addresses the Principle of Least Effort and Satisficing by giving students repeated practice in a
structured process of IL evaluation. It addresses Competency Theory by giving students practice
in evaluating both their own evidence-based judgments as well as comparing their work to that
of their peers.
Overall, this research explores a new model of online IL instruction that is grounded
firmly in the canonical definition of IL as “a set of abilities requiring individuals to recognize
when information is needed and have the ability to locate, evaluate, and use effectively the
needed information" while exploring new techniques for teaching and learning these important
21st century skills.

2.2.	  Credibility	  evaluation	  	  
The research addresses what evidence students use to judge credibility of online
information. This section reviews research into student credibility evaluation practices,
terminology of credibility criteria, and models of the evaluation process. The study of how
people make credibility judgments about information sources is broad and highly
34

interdisciplinary, extending across several disciplines and areas of inquiry (Flanagin & Metzger,
2008). Credibility has been studied across a number of fields from communications to
information science, to psychology, and using different approaches, goals and conflicting views
(Rieh & Danielson, 2007). With multiple methodological approaches and theoretical models, the
field of research is complex and multifaceted (Wathen & Burkell, 2002). Generally, credibility is
considered to be part of the larger study of relevance evaluation, which is classified as an
element of human information behavior (Lazar, 2007; Rieh & Danielson 2007). Information
science researchers investigate the phenomena of credibility along multiple trajectories (Wathen
& Burkell, 2002). Rieh and Danielson (2007) reviewed credibility studies across multiple
disciplines and developed a comprehensive framework of credibility, organized around five
topics: 1) the construct of credibility as a chief element of information quality; 2) orientation
toward the targets of credibility evaluation; 3) credibility evaluation processes; 4) situational
aspects of credibility evaluation; and 5) the evaluator’s background and domain knowledge.
They argue that while credibility has traditionally been conceived of as limited to a criterion of
relevance judgments, it is important enough to be researched on its own.
There are no universal rules to judge credibility (Hilligoss & Rieh, 2007; Swanson, 2005)
and no definitive definition of the term itself (Flanagin & Metzger, 2008). Evaluation of
information is often subjective, relative, and situational rather than objective, absolute, and
universal (Harris, 2008). However, it is generally accepted among researchers that the primary
component of credibility is “believability,” which is itself made up of “trustworthiness” and
“expertise” (Hilligoss & Rieh, 2007; Flanagin & Metzger, 2008). Many studies have focused on
the issue of trust relating to websites (Madden et al., 2011). Trustworthiness can be defined as
information that is “reliable, unbiased, and fair” (Hilligoss & Rieh, 2007, p. 1469), while
expertise can be described as “the perceived knowledge, skill, and experience of the source”
(Fogg et al, 2003). These two primary components of credibility both relate to characteristics of
authors and originate in print-based media. Rieh and Danielson (2007) point out that historically
the credibility of information has been maintained by gatekeepers such as editors, reviewers,
publishers, and librarians. However, in the online environment, such professional knowledge
workers are no longer the acknowledged gatekeepers, and making credibility evaluations of
digital media calls for different practices of evaluation than those developed throughout the
history of traditional print-based media.
35

2.2.1.	  Theoretical	  models	  
Models describing the process of credibility evaluation help us understand how
information seekers make decisions about the quality of information sources that they find
during searches. Fogg et al. four types of online information credibility judgments: 1) presumed
(based on prior assumptions), 2) reputed (based on endorsement by other sources), 3) surface
(based on visual and structural cues), and 4) earned (based on experience with the information
source) (2003). Flanagin and Metzger (2008) describe two types of credibility evaluations: 1)
conferred (by another source’s reputation), and 2) tabulated (measured by numerical peer
ratings) (2008). Rieh (2002) studied the factors influencing users’ judgments of information
quality and cognitive authority on the web, and describes a process-oriented model of credibility
evaluation in which users make active judgments about information quality and authority based
on their own knowledge and experience. Two types of judgment are involved: predictive (based
on prior knowledge and expectation), and evaluative (based on opinions formed while
encountering the information). These judgments are iterative, repeated by information seekers
until the evaluative judgments meet the expectations of predictive judgments (Rieh & Hilligoss,
2008).
Researchers argue that most information seekers’ credibility judgments are based on
heuristics, cognitive rules of thumb employed to quickly make evaluations. Defined as
“situation-dependent dimensions and criteria for evaluation” (Rich & Danielson, 2007, p. 344345) and “evolved generalizations stored in one’s knowledge base” (Sundar, 2008, p. 78),
heuristics allow searchers to save the time and energy of an in-depth analysis of quality for every
information source they find. Quick judgments are made based on past patterns of experience,
which allow the user to conserve cognitive energy in the face of a large number of possible
information sources (Sundar, 2008). The use of heuristic cognitive processing is more practical
and efficient than the use of a detailed checklist approach to credibility evaluation, and is a
common means of coping with information overload (Metzger et al. 2010). 	  
	  
2.2.2.	  Credibility	  criteria	  
Researchers often focus on studying how specific features of websites effect users’
credibility judgments and what criteria they apply when making these judgments (Fogg et al.,
36

2003; Hargattai et al., 2010). Since the criteria for judging the trustworthiness of information are
subjective and open to interpretation, there are multiple and sometimes conflicting typologies of
the specific criteria which make up credibility judgments. Based on a review of “digital literacy”
literature, Metzger identified five fundamental criteria for evaluations of online information
credibility: accuracy, authority, objectivity, currency, and coverage (Metzger, 2007). These terms
were defined by Metzger as: lack of errors and verifiability (accuracy); identifiability of
authorship and the author's credentials (authority); presence of bias, opinion, or conflict of
interest (objectivity); whether the information is up to date (currency); and the
comprehensiveness or depth of the information provided on the site (coverage) (Metzger, 2007).
Rieh (2002) developed a credibility evaluation model, based on the work of Taylor
(1986) and Wilson (1983), which focuses on two factors: information quality and cognitive
authority. Wilson’s concept of cognitive authority relates to second-hand knowledge about
whether sources are considered to be worthy of belief. Rieh found that these decisions were
related to whether a source is judged to be trustworthy, credible, reliable, scholarly, official, and
authoritative (Rieh, 2002). Given the difficulty of establishing authority of much online
information, the criteria for this factor are less relevant to the present study than the criteria for
information quality, which is based on Taylor’ “value-added” model. Taylor focused on users’
judgments about the value of information, which Rieh found to be related to whether a source is
judged to be good, accurate, current, useful, and important (Rieh, 2002). A comparison of
different sets of criteria is shown in Table 2 below. The only terms shared by all the examples
are accuracy and currency.
Table 2. Comparison of evaluative criteria
Taylor (1986)
Value-added model
Accuracy
Currency
Comprehensiveness
Reliability
Validity

Rieh (2002)
Information Quality model
Accuracy
Currency

Metzger (2007)
Digital literacy literature
Accuracy
Currency

Goodness
Importance
Usefulness
Authority
37

Coverage/Scope
Objectivity
For comparison, a recent directory of IL games (McDevitt, 2011) includes several cases studies
that include the criteria used in different games (see Table 2). For the games that listed their
specific criteria, a comparison is shown in Table 3. Again, the only terms shared by all the
examples are accuracy and currency. Two terms were shared by three cases: authorship/authority
and objectivity. Audience was shared by two cases.
Table 3. Criteria used in specific IL games
IL Game #1
Accuracy
Currency
Authorship
Audience
Purpose

IL Game #2
Accuracy
Currency
Authority
Objectivity
Audience

IL Game #3
Accuracy
Currency
Authority
Objectivity

IL Game #4
Accuracy
Currency
Objectivity

Coverage
Expertise
Relevance
These sets of criteria show some overlap but also a great deal of variation in the specific
concepts that are considered to be part of credibility evaluation. While some terms may be
generally related (comprehensiveness and coverage, expertise and authority, objectivity and
purpose) there is still a great deal of variability between the specific sets of criteria used in
different cases.
Some instructional approaches enlist criteria guidelines to give students a basic checklist
of features to investigate when evaluating sources. However, there are arguments against this
type of checklist approach. Checklists can be reductive, forcing students into making simplistic
“yes” or “no” responses when complete information may be unavailable or difficult to find,
especially since authorship can often be difficult to determine online (Harris, 2008). Making
such a binary choice may be difficult for inexperienced students who may have more qualified
responses. Overall, a checklist approach may encourage a mechanistic process to decision
making that does not encourage critical thinking (Meola, 2004), and can result in superficial or
even false analyses (Harris, 2008). In addition, it may be unrealistic to expect students to
38

painstakingly apply all five criteria, i.e., accuracy, authority, objectivity, currency, and coverage,
in their daily research habits. The basic checklist approach can be defended, however, since
students generally lack the requisite experiences and practice to make informed evaluations of
online information on their own (Doyle & Hammond, 2006). Breaking credibility down into
specific components may help the student learn the steps involved in making an informed
evaluation.
Checklists are a popular model for teaching the evaluation of websites in libraries (Myhre
2012). An early example of a specifically web-based checklist was developed by Jim Kapoun
(1998), consisting of five evaluation criteria: accuracy, authority, objectivity, currency, and
coverage (Appendix 1). Each criteria includes a set of questions that the student answers to help
determine if the website meets a particular criterion. Many academic libraries use some variation
of Kapuon’s original criteria in their online instruction materials (Myhre, 2012). A more recent
version of this checklist that is widely used is the CRAAP (Currency, Relevance, Authority,
Accuracy, Purpose) test created by a librarian at California State University, Chico (Blakeslee,
2004). The CRAAP test was not specifically designed for web evaluation, although it contains
two additional questions marked as “for Web” (Appendix 2). On this checklist, Kapuon’s criteria
of objectivity and coverage have been replaced with relevance and purpose. See Table 4 for a
comparison of the criteria and their questions prompts.
Table 4. Comparison of two early checklists
Criteria
Accuracy

Kapoun (1998)
• Who wrote the page and can you
contact him or her?
• What is the purpose of the document
and why was it produced?
• Is this person qualified to write this
document?

Authority

• Who published the document and is
it separate from the "Webmaster?"
• Check the domain of the document,
what institution publishes this
document?
• Does the publisher list his or her
qualifications

Blakeslee (2004)
• Where does the information come from?
• Is the information supported by evidence?
• Has the information been reviewed or
refereed?
• Can you verify any of the information in
another source or from personal knowledge?
• Does the language or tone seem unbiased
and free of emotion?
• Are there spelling, grammar or
typographical errors?
• Who is the author/publisher/source/sponsor?
• What are the author's credentials or
organizational affiliations?
• Is the author qualified to write on the topic?
• Is there contact information, such as a
publisher or email address?
• Does the URL reveal anything about the

39

author or source? examples: .com .edu .gov
.org .net (for Web)
Objectivity

Currency

Coverage

Relevance

Purpose

• What goals/objectives does this page
meet?
• How detailed is the information?
• What opinions (if any) are expressed
by the author?
• When was it produced?
• When was it updated?
• How up-to-date are the links (if
any)?

• When was the information published or
posted?
• Has the information been revised or
updated?
• Does your topic require current information,
or will older sources work as well?
• Are the links functional? (for Web)

• Are the links (if any) evaluated and
do they complement the documents'
themes?
• Is it all images or a balance of text
and images?
• Is the information presented cited
correctly?
• Does the information relate to your topic or
answer your question?
• Who is the intended audience?
• Is the information at an appropriate level
(i.e. not too elementary or advanced for your
needs)?
• Have you looked at a variety of sources
before determining this is one you will use?
• Would you be comfortable citing this source
in your research paper?
• What is the purpose of the information? Is it
to inform, teach, sell, entertain or persuade?
• Do the authors/sponsors make their
intentions or purpose clear?
• Is the information fact, opinion or
propaganda?
• Does the point of view appear objective and
impartial?
• Are there political, ideological, cultural,
religious, institutional or personal biases?

These questions make explicit the procedure that experts use when evaluating the credibility of
websites, and organize the procedure that students will follow in their own credibility evaluation
process. They have been developed by content-domain experts, in this case instructional
40

librarians and educators, to provide expert modeling by showing students the questions they
should use to guide their thinking during problem solving (Ge & Land, 2004).
Myhre (2012) conducted a small study (N=14) of the use of a version of the CRAAP test
modified to specifically address online sources, and found that participants’ scores improved
from pre-test to post-test, as did their ability to explain why a website met the criteria. However,
qualitative responses suggests that students needed help articulating how they reached their
conclusions to these questions (Myhre 2012). This reinforces a weakness of the checklist
approach and underscores the importance of embedding a criteria checklist into a process of
learning credibility evaluation practices.
Former school media librarian Kathy Schrock developed a simplified version of these
criteria that is widely used in IL education, especially in high schools. Instead of using the expert
terminology of the previous checklists, Schrock’s list uses five easy questions (Who, What,
Where, Why, and When) to structure the evaluation process (Appendix 3). This language may be
more accessible for high schools students with little exposure to IL training, and the mnemonic
format of 5Ws may help these students adopt the criteria. An example of adoption of this model
by an academic library is the University of Michigan Library’s handout used in IL classes
(Appendix 4). Schrock’s 5Ws model is discussed in section 3.2.2.

2.2.3.	  Student	  credibility	  evaluation	  practices	  
For many students, the web has become the starting point when searching for information
(Becker, 2003; Costello et al., 2004; Curtis, 2000; Swanson 2005). However, given the volume
and diversity of online information sources, traditional credibility evaluation strategies may be
difficult for students to apply (Metzger et al., 2010). In this environment of information
abundance, students often look for other standards of quality. To many young people, search
engines themselves are the new de facto gatekeepers (Metzger et al., 2010). Many students seem
to credit search engine relevancy rankings with a kind of omniscience (Harris, 2008). Students
assume that search results are recommendations of credibility and rely on search engine brands
as endorsement of quality (Hargittai et al., 2010). This type of name-brand recognition has
41

become a new component of credibility judgments. Students often rely on the top items in a list
of search results as recommendations of the best sources and barely go beyond the first few
results pages (Lankes, 2008; Hargittai et al., 2010; Spink, Wolfram, Jansen & Saracevic, 2001),
even when the abstracts of the search results were less relevant than other results (Pan et al.,
2007).
Overall, students verify information they find online significantly less than do older
adults (Metzger et al., 2003). Motivated mainly by time constraints, students may compromise
information credibility for the sake of speed and convenience, and demonstrate willingness to
satisfice (Warwick et al., 2009; Connaway et al., 2011). Students prefer starting a new search
session rather than conducting the more difficult and time-consuming tasks of verification, and
might change their question or search strategy in order to avoid having to evaluate the results
(Warwick et al., 2009). Thus, while many students depend on the Internet to provide
information, they rarely take the necessary steps to verify the traditional criteria of credibility for
the information that they find (Metzger et al., 2003). Young people generally have simplistic and
unsophisticated mental maps of what the Internet is, often not understanding its networked
structure, and seeing Google itself as the entire Internet and not understanding how search
engines function (Large, 2005; Pan et al., 2007, Rowlands et al., 2008; Julien & Barker, 2009).
Since undergraduate students are much less likely to possess the skills to apply sophisticated
evaluation strategies, understanding how they actually evaluate online information is important
(Leckie, 1996; Warwick et al., 2009).
Studies have investigated the reality of how students perform online credibility
evaluations, and what criteria they actually employ. Results consistently show that young people
are unlikely to exert much effort in making credibility judgments (Metzger, 2007). Speed and
convenience are highly valued by students, who tend to be pragmatic and opportunistic when
searching for sources, and not overly concerned about quality (Flanagin & Metzger, 2008;
Connaway, et al. 2011). Strategies requiring more effort and initiative are less frequently
employed, with verification of author credentials the least used, presumably because it requires
the most effort to locate this information on many sites, particularly sites with no identified
author or corporate authorship (Metzger et al., 2003). However, students do frequently utilize
currency as an evaluation criterion since dates are relatively concrete and easy to locate in online
information sources (Metzger et al., 2003). Substantive content-specific criteria such as accuracy
42

and objectivity may occasionally be verified, but infrequently, in part because they are more
time-consuming, and possibly because they require greater disciplinary knowledge than students
may possess.
In general, students spend little time or effort evaluating search results, information and
sources (Walraven et al., 2009) and do not employ any systematic strategies for judging website
legitimacy (Mizrachi, 2010). Instead, they are more likely to simplify website evaluation tasks
and make judgments based primarily on site design and surface features rather than content
(Harris, 2008). Novelty, "coolness" of the interface, and professional site design can be crucial
elements of credibility decisions (Sundar, 2008). Even basic presentational factors such as color
can influence students in their credibility evaluations (Agosto, 2002). Technological affordances
may also impact how content is experienced and evaluated, with students showing a
predisposition toward sites that are novel and interactive (Sundar, 2008).
The field of credibility research has produced much data that describes how users
determine credibility, but there has been much less research into teaching effective credibility
evaluation (Harris, 2008). Traditional credibility evaluation models assume that students will be
thorough and meticulous in their evaluation behavior, and follow a checklist completely and
diligently. Instead, college students often use a “risk-averse strategy” for information-seeking
and base their decisions on efficiency and predictability (Head & Eisenberg, 2010, p. 3).
Although instructors and librarians may be loath to admit it, students are often not motivated by
learning but are more concerned with the pragmatic motivations of passing the course, finishing
the paper, and getting a good grade (Head & Eisenberg, 2010). Rather than viewing evaluation as
a process of reflection and judgment, students may see it instead as merely a procedural step to
be cursorily completed (Julien & Barker, 2009). Thus, a change in behavior is required, which is
more likely to be addressed through sustained practice of skills rather than a one-shot session.
Mizrachi (2010) argues that:
“librarians and educators must continue to stress the importance of critical
thinking and the development of evaluation strategies for determining the
reliability of sources found on the worldwide web.” (p. 574).

43

Information professionals and librarians can play an interventionist role in facilitating students’
judgments about credibility, developing critical thinking skills, and helping students make
informed judgments about others’ knowledge claims (Rieh & Danielson, 2007).
2.2.4.	  Collaborative	  online	  evaluation	  
The Internet has introduced a new paradigm of information credibility, radically different
than the traditional print-derived model, instead embracing a model of “multiple distributed
authorities based on information abundance and networks of peers” (Metzger et al., 2010, p.
415). Millennial students have grown up surrounded by interactive digital technology and are
accustomed to information sharing and remixing. Young people today are actively involved in a
culture of distributed social networks which shape the production and distribution of knowledge
(Ito et al., 2008). In contemporary learning environments, evaluating information found online
often involves collaborative support from others and is “far from being a solitary task” (Head &
Eisenberg, 2010, p. 14). Young people often use social and group-based means to evaluate
credibility (Metzger et al., 2010). These new modes of collaborative knowledge creation and
participatory evaluation are reshaping contemporary practices of credibility evaluation (Lankes,
2008). Farkas notes that “bringing students together to discuss ideas and solve problems
collaboratively helps them to co-create an understanding of information literacy that is greater
than what any of them could have developed alone” (Farkas , 2012, p. 92).
Research consistently shows that students’ information seeking and credibility judgments
are becoming inherently social, and relative to social context, as students often consult friends
and classmates when making evaluations (Metzger et al., 2003; Rieh & Hilligoss, 2008; Head &
Eisenberg, 2010). When today’s students search for information, they are less likely to use a
systematic information seeking process than relying on interest groups, peer web pages, or social
bookmarking (Markless, 2009). Traditional credibility models assumed that individuals work in
isolation to form credibility evaluations, but current research shows that users often rely on
others when making judgments (Metzger et al., 2010). Credibility evaluations are more likely to
be determined by synthesizing multiple sources of judgment, than by employing the print-based
concepts of authority and hierarchy (Lankes, 2008). The strategies that students employ when
searching for information and the criteria which they use in deciding what information to use is
44

“deeply influenced by others with whom they feel socially close and with whom they share
common ground” (Rieh & Hilligoss, 2008, p. 65). Since they respect one another’s authority
online, students are often more motivated to learn from their peers than from adults (Ito et al.,
2008).
The sociotechnical model conceptualizes information seeking and credibility judgments
as activities mediated by tools and shaped in a social setting (Tuominen et al., 2005; Sundin &
Francke, 2009). In this theoretical framework, information seeking and evaluation are considered
to be embedded in social practices (Sundin, 2005). Rather than embodying abstract, hierarchical
standards of authority, credibility criteria are seen as negotiated, situated in context and learned
from communities (Tuominen et al., 2005; Sundin & Francke, 2009). In these situated practices,
students learn to recognize what is considered to be reliable knowledge and what is regarded as
an uncertain source in the educational context (Sundin & Francke, 2009). This approach to
understanding credibility as situated in the online context and conducted collaboratively means
that the online tools and social media that students actually use in their daily lives, and utilize
practices they are already familiar. Since many students already use Web 2.0 technologies,
incorporating these tools into IL instruction can help engage them with the library (Click & Petit,
2010).
Utilizing familiar technology may make credibility training more relevant to Millenials
(Flanagin & Metzger, 2008). For today’s students, group engagement in the information
evaluation process informs decision making and is “crucial to credibility construction and
assessment” (Flanagin & Metzger, 2008, p. 10). This approach focuses on educating youth to
assess credibility in participatory ways (Lankes, 2008) and incorporates social evaluation
through collective intelligence (Metzger et al., 2010). Harris states that "Some of the best
practices in credibility evaluation instruction are those that occur over time, in the context of
application, and, in the best cases, provide collaborative and apprenticeship-like experiences"
(2008, p. 167). Digital media and networked online learning provide opportunities to build
collaborative credibility evaluation tools that support students as they search for and critically
evaluate information (Harris, 2008).
In today’s online information environment, credibility is often not determined by the
individual, but “within a community engaged in a larger conversation" (Lankes 2008, 114).
Collaborative filtering and peer-review systems such as recommender or reputation systems
45

allow users to pool their intellectual and experiential resources, transforming credibility
evaluation into a collaborative rather than an individual effort (Metzger, 2007). Not only the
criteria for evaluation but also the processes by which credibility evaluation is taught need to
reflect the realities of online information seeking. For example, practices of collaborative inquiry
can encourage students to participate in online verification strategies to assess author credentials
(Metzger et al., 2003) or to share and compare sources through organized, collaborative online
searches (Todd, 2000). To reinforce the real-world value of these practices, collaborative
learning opportunities should be employed in the context of real classroom assignments (Harris,
2008) and embrace the Internet as “a means of creating communities and fostering collaboration”
(Rieh & Danielson, 2007, p. 350). However, these approaches require “a cultural shift and a
certain openness to experimentation and social exploration that is generally not characteristic of
educational institutions” (Ito et al., 2008, p. 35).
Gross and Latham argue that “more research is also needed to develop innovative
strategies for providing new kinds of information literacy education…. Students with low-level
skills may benefit from working collaboratively to create knowledge” (2007, 350). There are
many opportunities for credibility education to incorporate the use of new information
technology and social media that can help users assess the credibility of online information.
Educators can better reach students and promote effective credibility evaluation skills by taking
advantage of “the way young people think and work” (Harris, 2008, p. 172). However, the social
factors involved in learning are rarely explored in IL teaching and learning (Walton & Hepworth,
2011). Much research on youth credibility evaluation does not address the “newer behaviors
emerging in digital environments” (Rieh & Hilligoss, 2008, p. 50). This gap in the literature is
addressed by the research.
One example of an experimental study that investigated the impact of online peer
interaction on learning IL skills is Walton and Hepworth's (2011) study of students ability to
critically evaluate source material. Students in the treatment condition evaluated sample web
pages then summarized their own evaluation criteria in postings to a discussion forum in the
Blackboard Learning Management System (LMS). Other students responded to these comments
in discussion threads. Control groups in the study did not use the discussion forum. Results
showed that the experimental group used better quality sources, demonstrated better
understanding of a wider range of evaluation criteria, and applied IL evaluation criteria in a more
46

sophisticated way. The treatment group appeared to have internalized their new knowledge by
thinking critically about information, and also demonstrated the higher cognitive states of
analysis, synthesis, and evaluation in their reflections on their own learning. The authors state
"Students genuinely appear to like the fact that they can see what others have written which
appears to add to their own knowledge and promotes a reduced sense of uncertainty" (p. 463).
These results highlight the value of the social constructivist aspect of becoming information
literate via discussion and collaboration rather than working alone or passively being lectured.
Walton & Hepworth argue that the asynchronous nature of online discussion threads has two
advantages over face-to-face settings: maintaining a complete record of conversations, which
allows students to review and re-read, and providing more time and "cognitive space" to
formulate responses. This practice of writing and posting material to a wider audience than just
an instructor also introduces students to the idea of being part of an academic community of
practice and makes students producers as well as consumers of information (Walton &
Hepworth, 2011). The results of this study, while limited to the context of a discussion forum in
an LMS, suggests that online peer interaction can be a successful venue for teaching IL skills.

2.2.5.	  Implications	  for	  the	  research	  
Research into online credibility evaluation often focuses on identifying the criteria and
heuristics of how people evaluate information, and theorizing models of credibility judgments.
While the results of credibility research are often applied to website designers as a technique of
increasing perceived credibility, these results are rarely applied to teaching students better
evaluation skills. Less work has been done on studying the best techniques to teaching effective
online credibility evaluation practices in students. Credibility research also rarely studies the
collaborative, networked online tools that today’s students use to evaluate credibility. Group
evaluation through ratings and recommendations are common online practice, as is participation
in discussion and chat forums. In the online information environment, users determine credibility
by synthesizing multiple sources of credibility judgments. Bottom-up evaluations of information
quality are often constructed online through collective or community efforts enabled by social
media, which in some cases allow information consumers to bypass traditional authorities
47

altogether. Networked online tools offer new possibilities for integrating credibility evaluation
instruction with the collaborative and social practices of today’s students, and situating learning
in authentic contexts is important.
This research explores a contextual approach that situates credibility evaluation education
in the online environment. It aims to incorporate findings from online credibility evaluation
research with the evaluation practices of today’s students into a learning tool that teaches
systematic web credibility evaluation practices situated in an online, networked environment.
The use networked social interaction as a medium for teaching credibility evaluation skills has
not been widely studied, which is one of the contributions this research makes to the field.

2.3.	  Computer-­‐Supported	  Collaborative	  Learning	  
The research addresses techniques to best support students in learning credibility
evaluation skills. This section reviews CSCL learning theories, in particular metacognition and
scaffolding. The field of Computer-Supported Collaborative Learning (CSCL) provided the
initial inspiration for the design and development of the prototype learning tool. While
collaboration is not an explicit variable studied in this research, the importance of collaborative
learning to today’s students provides an important context to this research. Although the tool
does not support explicitly collaborative actions by students, the exposure to other students’
evaluations of sources and the comparison of different sources and their evaluations is intended
to provide a learning environment that builds a sense of collaborative, group learning.
The focus of research in the field of CSCL has been characterized as the study of how
technology can be used as a mediational tool within collaborative methods of instruction
(Koschman, 1996), how students can collaborate on problem solving with the help of interactive
technologies (Stahl et al., 2006), how technology can support learning in groups, both co-located
and distributed (Ludvigsen & Mørch, 2010), and how collaborative learning supported by
technology can enhance peer interaction and facilitate knowledge building (Lipponen, 2002).
These characterizations share a common emphasis on interaction and mediation as key
components, with computer technology serving not just as an instructional tool but as a
connective medium between both student/student and student/teacher. Thus, the focus of CSCL
research is on process rather than outcomes, through studying how students use mediational
48

technology as a part of learning, and understanding this process from the participant/learner’s
point of view (Koschman, 1996).
Software implementations of CSCL principles have been given a variety of names,
including computer-supported intentional learning environments (Scardamalia et al 1989);
constructivist learning environments (Wilson, 1996); knowledge integration environments (Bell,
Davis & Linn, 1995); web-based learning environments (Hung, 2001); networked learning
environments (Lipponen 2002); collaborative knowledge-building environments (Scardamalia &
Bereiter, 1989); and technology-enhanced learning environments (Wang & Hannafin, 2005).
Wilson (1996) defined the constructivist learning environment as “a place where learners may
work together and support each other as they use a variety of tools and information resources in
their guided pursuit of learning goals and problem-solving activities” (p. 5). This early definition
was echoed by DeCorte (1996), who stated that “Powerful learning environments should embed
students’ constructive acquisition process as much as possible in authentic, real-life contexts that
have personal meaning for the learners, that offer ample opportunities for distributed learning
through social interaction and cooperation, and that are representative of the tasks to which
students will have to apply their knowledge and skills in the future” (p. 39).
As the focus of CSCL research is often interdisciplinary, researchers often employ
mixed-method research techniques. These methodological orientations are characterized by Stahl
et al. as experimental (“coding and counting” interactions and outcomes), descriptive (“exploring
and understanding” variables that support meaning-making) and iterative design (continuous
improvement of mediational artifacts) (Stahl et al., 2006). Also described as ethnographic,
descriptive and observational (Lipponen, 2002), the techniques employed by CSCL researchers
focus on investigating how learning can be supported and enhanced through the use of
interactive technologies, and on creating educational software in which the computer facilitates
in helping learners collaborate and construct knowledge (Sawyer, 2006).
This literature review discussed learning theories in the field, and then focuses on two
key themes from CSCL that are important to the research: metacognition and scaffolding.
	  
	  
	  

	  

49

2.3.1.	  Learning	  theories	  
It is generally agreed that CSCL’s origins began in the developmental psychology
research of Piaget and Vyogtsky, who both investigated the processes by which children learn.
While both researchers studied social activity as the basic unit of analysis, they theorized very
different models of how knowledge develops. Piaget’s socio-cognitive approach emphasized the
intellectual conflict a child experiences when new information based on different points of view
and conflicting perspectives interacts with prior knowledge (Dillenbourg et al., 1996), and how
this conflict is ultimately resolved through a process of assimilation (Koschman, 1996). These
conflicts give rise to learning through cognitive restructuring, with social interaction as the
context and catalyst for change (Dillenbourg et al., 1996). However, while Piaget considered
learning to take place within the individual’s head, Vygotsky’s socio-cultural approach
emphasized social interaction as a crucial element of learning. In this model, mutual engagement
forms the basis for co-construction of knowledge (Lipponen, 2002), placing learners in a
reciprocal relationship with each other. Social interactions are internalized by the individual and
cause cognitive change (Dillenbourg et al., 1996). Vygotsky saw the mind as socially
constructed, with consciousness derived from the culture in which it was constructed (Stahl,
2011). Thus, the two models view the phenomena of learning through contrasting lenses. Sociocognitive experiments generally involve two subjects of approximately the same age or
developmental level and focus on outcomes, while socio-cultural experiments study pairs of
different developmental levels and focus on interactions (Dillenbourg et al., 1996).
While Piaget’s basic cognitive research helped lay the groundwork for CSCL, Vygotsky
made a significant theoretical contribution with his conception of the “Zone of Proximal
Development” (ZPD). Described by Vygotsky as “the distance between the actual developmental
level as determined by independent problem solving and the level of potential development as
determined through problem solving under adult guidance, or in collaboration with more capable
peers” (Vygotsky, 1978, p. 86) , this model of a bridgeable gap in experiential knowledge
underlies the design of much interactive learning technology. The ZPD suggests a site of
intervention where theoretically-based support and guidance can enhance the capabilities of
learners and help them to expand their knowledge and skills (Koschman, 1996). CSCL
applications often employ some version of the ZPD model, with the computer in the role of
50

guide, in addition to or in place of the skilled peer or teacher. Scardamalia et al. state that
“cognitive research provided the scientific basis to design programs that actually helped students
learn how to learn, learn how to set cognitive goals, learn how to apply effective strategies for
comprehension, self-monitoring, and organization of knowledge” (1989, p. 51). These skills are
all important elements of the CSCL learning model, and researchers have developed specific
programmatic techniques to support students in gaining experience in using these strategies.
The two main traditions within the learning sciences, cognitive psychology and
sociocultural, share a common denominator in the Vygotskian paradigm that social interactions
precedes learning and cognition at the level of the individual (Ludvigsen & Mørch, in press).
While the traditional didactic pedagogy model was based on the transmission of
decontextualized knowledge from the instructor to the student, there has been a change “from
teacher controlled, prescriptive and didactic modes to learner-driven social, collaborative and
participatory approaches to task design and learner engagement” (McLoughlin & Lee, 2008, p.
648). Most researchers in the field today understand learning as “a process whereby the social
and cognitive are fundamentally intertwined” (Teasely, 2011, p. 131). The development of
constructivist learning theories, based on active participation in constructing knowledge, was
foundational to both library science and CSCL (Sawyer, 2006). While research in instructional
technology historically relied on psychological theories of learning (either behavioral or
cognitive), social science research emphasizing the social and cultural context of learning has
been integral to CSCL (Koschman, 1996). Instead of a behaviorist model emphasizing teaching
methods, or a cognitive model emphasizing interiorized mental development, the constructivist
model emphasizes “meaningful, authentic activities that help the learner to construct
understandings and develop skills relevant to solving problems” (Wilson, 1996, p. 3).
Sociocultural studies began to incorporate the complex social environment in which the
construction of learning occurs (Koschman, 1996).
The constructivist model of learning is characterized by reciprocity, an egalitarian
environment where “participants feel they can both produce and evaluate knowledge and
culture” (Ito et al., 2008, p. 39). This learning model emphasizes the acquisition of higher order
thinking and problem-solving skills, and de-emphasizes the assimilation of isolated facts
(Woodard, 2003). Constructivist theory sees learning as a social process in which students play
an active role in building knowledge, discovering relationships among facts, and constructing
51

conceptual frameworks that explain those relationships (Woodard, 2003). In this view of
learning, students create their own meanings and are best guided in learning through coaching
and scaffolding of new information rather than being the passive recipients of information
through the transmission model of lecture and occasional classroom discussion (Gibson, 2008, p.
16). This trend echoes the movement in IL instruction away from the “sage on the stage” model
to the “guide on the side” model.
The development of Web 2.0 technologies has influenced the growth of pedagogy based
on social constructivism (Farkas, 2012). Constructivist pedagogy views students as active
participants in learning, who are able to construct knowledge based on their own existing
understanding in interaction with peers and instructors. Farkas calls this “Pedagogy 2.0” and
characterizes it as a “learning ecology that unlocks the benefits of participatory technologies”
(2012, p. 87). Networked social media and participatory learning have the potential to
fundamentally impact the field of education. McLoughlin and Lee (2008) suggest that utilizing
these new tools may “assist us in breaking away from the highly centralized industrial model of
learning” (p. 641). Kapitzke (2003) argues that the traditional educational model of rote learning,
memorization and basic functional literacy was created to produce a standardized labor force for
the 19th century industrial economy. The MacArthur Foundation report “Imagining the Future of
Learning Institutions” states that the traditional educational model is out of sync with how
today's students actually learn. Most academic institutions are “stuck in an epistemological
model of the past” characterized by conventional models of learning that “tend to be passive,
lecture driven, hierarchical, and largely unidirectional from instructor to student" (Davidson &
Goldberg, 2009). This model of learning is disconnected from the way that students actually
learn in their social lives, setting a dangerous precedent in which education in general can appear
to be irrelevant to students.
To today’s students, academic learning may seem disconnected from the “peer-to-peer
distributed systems of collaborative work characteristic of the new internet age” (Davidson &
Goldberg, 2009). Many students today collaborate in producing and sharing information in
participatory online environments. Social interactions in online learning environments allow
youth to participate in a conversation through collaboration with others, and therefore become
involved in the process of credibility verification and knowledge creation (Lankes 2008).
Information seekers become information producers in these contexts. These principles of
52

participation and collaboration can be seen as particularly relevant to today’s students, the
“digital natives” or Millennials (Manuel, 2002; Palfrey & Gasser, 2008; Tapscott, 2009).
Networked social computing adds elements that Millenial students value: collaboration and peerbased learning (Manuel, 2002; Head & Eisenberg, 2010). These students expect to work in
groups, sharing knowledge, and interact socially with peers (Flanagin & Metzger, 2008).
These learners are empowered by situating their learning within a familiar socio-technical
environment of social media tools that they are accustomed to using, such as blogs, chat,
discussion forums, collaborative online editing, digital media creation, social networking (Halse
& Mallinson, 2009). These media can be utilized as next generation e-learning tools that
facilitate favored learning styles with “a balance among experiential learning, guided mentoring,
and collective reflection” (Dede 2005, p. 7). Farkas notes that “participatory technologies have
great potential for use in education as they have the potential to create a more engaging learning
environment” (84). The connection-forming, just in time, reflective, and learner-centered
characteristics of these networked tools facilitates active involvement in the learning process,
which is crucial to effectively educating today’s students (Halse & Mallinson, 2009). Social
networks and tools offer the educational potential to facilitate self-monitoring, problem-based
and collaborative activities through providing students the ability to “navigate and participate on
the web and use it to actively solve problems” (Dalsgaard, 2006., n.p.). Another benefit of
integrating social software into learning environments is that they can facilitate and strengthen
relationships between students through the mutual sharing of work and engaging in group
discussions (Dalsgaard, 2006). Together, these affordances offer new opportunities to engage
today’s college students in new forms of learning.
Davidson and Goldberg argue that “participatory learning” is a key term in thinking
about these emergent shifts in education. This parallels the discussion in both IL and CSCL
pedagogy about a change of focus from hierarchical, teacher-focused instruction to collaborative
student-centered learning. Instead of “passive, lecture driven, hierarchical, and largely
unidirectional from instructor to student” (Davidson & Goldberg, 2009, p. 20), participatory
learning embraces how today’s learners “use new technologies to participate in virtual
communities where they share ideas, comment on one another’s projects, and plan, design,
implement, advance, or simply discuss their practices, goals, and ideas together” (Davidson &
Goldberg, 2009, p. 12). This can be seen as part of the move from granting presumed authority to
53

traditional forms of print media towards new practices of online credibility evaluation. The
Internet, along with offering unprecedented access to information sources, also demands that the
learner shoulder the responsibility of evaluate the quality of information sources, and today’s
learning tools must support those skills. Training in these skills must be a major part of the future
of learning, in which students must develop “methods, often communal, for distinguishing good
knowledge sources from those that are questionable” or “collective credibility” (Davidson &
Goldberg, 2009, p. 27-28).
2.3.2.	  Metacognition	  	  
Along with the cognitive abilities of active construction of knowledge, learning also
involves metacognition, which involves “planning cognitive tasks, monitoring one’s progress to
meet goals, taking appropriate steps to solve problems, and reflecting on past performance for
future improvement” (Quintana et al., 2005, p. 2360). Metacognitively aware students follow a
series of metacognitive activities while completing a task: planning, strategizing, making
connections with prior knowledge, monitoring, regulating, and evaluating their own progress
(Flavell,1979). Metacognition has also been described as the process of planning, monitoring,
goal-setting, problem-solving and other higher-order skills (Scardamalia et al, 1989); the ability
to reflect upon, understand, and control one’s learning (Schraw & Dennison, 1994); an
awareness of student’s own knowledge and regulation of their cognition (Kaufman, 2004); the
ability to set goals, plan, monitor, and control cognition, motivation, and behavior (Pifarre &
Cobos, 2010); as awareness, knowledge, and control of cognition achieved through planning,
monitoring, and regulating (Pintrich et al., 1991); as task understanding and planning, monitoring
and regulation, and reflection (Quintana et al., 2005); and as the ability to “plan activities to meet
goals, anticipate obstacles, monitor their own progress, approach information critically, evaluate
information during the problem solving process and by these means to develop a personal
information style” (Markless & Streatfield, 2007, p. 22).
Pintrich et al. define metacognition as composed of three dimensions: metacognitive
knowledge, metacognitive monitoring, and self-regulation (Pintrich et al., 2000). Akyol and
Garrison apply this construct to the online learning environment, and describe it as consisting of
three interdependent dimensions: knowledge of cognition, monitoring of cognition, and
54

regulation of cognition (Akyol & Garrison, 2008). Knowledge of cognition consists of pre-task
states such as knowledge of the inquiry process, and knowledge of critical thinking and problem
solving. Monitoring of cognition consist of states such as commenting about self and others’
understanding, and making judgments about the validity of content. Regulation of cognition
consists of states such as procedural planning, setting goals, and questioning progress (Akyol &
Garrison, 2008). Schraw and Dennison defined the components of metacognition as knowledge
of cognition (consisting of awareness of one’s strengths and weaknesses, knowledge about
strategies, and why and when to use them) and regulation of cognition (consisting of planning,
implementing, evaluating, and monitoring strategies) (Schraw & Dennison, 1994).
Artz and Armour-Thomas stated that cognition involves doing, whereas metacognition
involves choosing and planning what to do and monitoring what is being done (Artz & ArmourThomas, 1992). The authors categorized a set of problem-solving activities (read, understand,
analyze, explore, plan, implement, verify, watch, and listen) as either cognitive or metacognitive
(or both), and developed a model that shows the interactions between these states. The model
developed by the authors is shown in Figure 1.
Figure 1. Model of cognitive and metacognitive behaviors

(from Artz & Armour-Thomas, 1992)

55

In essence, metacognition is thinking about thinking – understanding one’s own thought
process and their understanding of the task (Flavell, 1979). Higher levels of metacognitive ability
facilitate the transfer of acquired knowledge and skills to new learning tasks and problems (De
Corte, 1996). Thus, students who are learning unfamiliar content should be significantly helped
by developing metacognitive skills, which can compensate for low domain knowledge and
limited strategy repertoire (Bruning, et al., 2004). Metacognitively-aware learners have been
shown to be more strategic and perform better than unaware learners, through planning,
sequencing, and monitoring their learning in a way that directly improves performance (Schraw
& Dennison, 1994). Good learners are also highly aware of their own thinking and memory and
use that information to regulate their learning (Bruning, et al., 2004). The purpose of
metacognitive training is to help students think about their learning, how they approach specific
tasks, and the success of their strategies (Akyol & Garrison, 2011). Acquiring these skills is an
important objective of education (Scardamalia et al., 1989) and “a hallmark of effective
learning” (Kaufman, 2004, p. 142).
However, studies show that students often lack the ability or awareness to monitor and
regulate their cognitive processes while engaged in problem-solving (Artz & Armour-Thomas,
1992), which likely relates to the lack of metacognitive skills (Ge & Land, 2004). Novice
learners usually have weak metacognitive skills, which are important for engaging in complex
practices like online inquiry (Quintana et al., 2005). These students are unlikely to be successful
in completing complex web-based tasks, such as online credibility evaluation (Kauffman et al.,
2008). Bruning (1994) noted that college students:
“…show little awareness of their own thought processes and do not regulate
themselves in a strategic manner. They are unable to compensate for their
weaknesses and capitalize on their cognitive strengths, to select important topics
for study, or to plan their approaches to problems effectively. To help them
acquire these cognitive skills, we need to be prepared to model the kinds of
thinking required by our fields (and) create the kind of classroom social
communities that stimulate overt expressions of thought and generate peer
feedback.” (p. 18)
Supporting college students in modeling the metacognitivc strategies for online credibility
evaluation, and facilitating reflection through peer evaluations, is a focus of this research project.

56

Metacognition is not just an internal practice, but is socially situated and includes
communicating, explaining, and justifying one’s thinking to others (Akyol & Garrison, 2011).
The social constructivist learning model suggests that peers mediate each others’ learning
through effective dialogue, such as asking questions and providing examples (Ge & Land, 2004).
Lajoie and Lu (2012) argue that a “key mechanism in improving metacognition or self-regulation
is the ability to observe and listen to other perspectives” (p. 46). Peer interaction in the learning
process through asking for help, clarifying ideas and responding to feedback can enhance selfregulated learning through reflection on key task-solving processes (Pifarre & Cobos, 2010).
Interpersonal monitoring and regulating of members goal-directed behaviors can be encouraged
by small group problem-solving (Artz & Armour-Thomas, 1992). Through participating in
regulating each other’s work on the social level, peers in networked collaborative-learning
environments can help students became more aware of their own learning process (Pifarre &
Cobos, 2010). Social interaction and communication through mediated tools can help structure
self-regulation of behavior through reflection in action (Hung, 2001). When students compare
their work to that of others or are exposed to multiple perspectives from other students they
engage in spontaneous reflection (Lin, 2001). As students frame and resolve problems through
social interaction, they not only develop content knowledge but practice critical thinking and
analysis (Oliver, 2000). By contrasting their own hypothesis and evidence with those generated
by their peers, and evaluating peer theories, students have the opportunity to reflect on what they
do and do not understand (Lin, 2001). The act of explaining, questioning, clarifying, justifying,
or collaboratively developing strategies can help students become metacognitively mature
(Akyol & Garrison, 2011). Supportive social discourse is also regarded as an important aspect of
metacognitive development (Lin, 2001).
Information literacy can be seen as inherently metacognitive in that it encourages
individuals to become aware of their search and evaluation skills and apply them to specific
information needs (Booth, 2011). The Information Literacy Competency Standards for Higher
Education aim to support students in building a “metacognitive approach to learning” through
gaining control over their interactions with information and through making explicit the criteria
for gathering, analyzing, and using information (ACRL, 2000, p. 6). The Middle States
Commission on Higher Education in their Guidelines for Information Literacy in the Curriculum
echo the ACRL, stating that “one of the highest and best uses of information literacy is as a
57

metacognitive device for students to better manage the learning process” (MSCHE, 2003).
Walton & Hepworth (2011) argue that IL should be regarded as "a metacognitive tool which
provides a self-regulatory framework within a subject based programme" (p. 453).	  Any training
in IL skills should not only equip students with guidelines to help them assess the credibility of
websites, but should also encourage them to reflect on the process of evaluation (Madden et al.,
2011).
It has been argued that the ACRL IL standards themselves support a constructivist
learning approach since they advocate for “student-centered learning environments where
inquiry is the norm, problem solving becomes the focus, and thinking critically is part of the
process” and helping students “construct a framework for learning how to learn” (Bobish, 2010;
Allen, 2008). The convergence between IL and metacognition, specifically self-regulation, can
be described as an intersecting Venn diagram, with overlapping or shared skills in the center. See
Figure 2 below for a diagram.
Figure 2. Overlap between IL and metacognitive skills (from Wolf, 2007)

58

This diagram shows that self-regulated learners monitor while information-literate learners
evaluate (Wolf, 2007). While important components of both concepts lay outside the overlapping
area, the commonalities between the two fields are significant. Students in the center area
possess the skills and strategies to be self-regulated information seekers and evaluators.
Many, if not all, of the specific skills in both concepts can be applied directly to the task
of evaluating online information. Effective searching of the web is a complex process of
reasoning and decision making (Todd, 2000). In the online environment, a successful student has
to continuously decide where to go next and constantly has to evaluate how the information they
retrieve is related to their actual learning goals (Bennert & Mengelhamp, 2013). Strong selfregulation ability and metacognitive awareness are necessary in order to be successful in webbased learning (Raes et al., 2012). Iding et al. (2008) state:
“The incorporation of a metacognitive framework and the use of
metacognitive prompts… allows students to evaluate their own credibility
determinations regarding Web-based information and decide whether their
judgments are effective or not. It would also provide a basis for
internalizing effective credibility criteria” (Iding et al. 2008, p. 79) .
Students with metacognitive skills stay focused and can better assess the legitimacy of online
information (Garrison & Akyol, 2013). The use of metacognitive prompts can help students to
develop strategies to be more critical in their evaluation of the credibility of web sources (Iding
et al. 2008).
However, Markless (2009) points out that most IL models position reflection at the very
end of the process or simply take it for granted. This approach is not likely to enable the
development of the metacognitive strategies necessary to perform problem-solving with
information, particularly when students are working independently online (Markless, 2009).
Since they are part of an iterative process of learning, metacognitive strategies cannot simply be
inserted into a linear model , but must be integrated into the IL framework (Markless, 2009).
While metacognition is frequently studied in education and psychology research, it is
infrequently researched in IL studies (Gorrell et al., 2009). This research explores integrating IL
and metacognition instruction, and one of the outcomes that was measured in the experiment is
the effect on metacognitive awareness of students using the IC tool.

59

2.3.3.	  Instructional	  scaffolds	  
	  
Metacognition can be systematically supported by learning software applications through
the use of instructional scaffolding. Wood, Bruner and Ross (1976) defined scaffolding as a
“process that enables a child or novice to solve a problem, carry out a task or achieve a goal
which would be beyond his unassisted efforts” (p. 90). Azevedo (2005) defined scaffolding as
“instructional support in the form of guides, strategies, and tools that are used during learning to
support a level of understanding that would be impossible to attain if the students learned on
their own” (p. 199). Scaffolding consists essentially of rationalizing those elements of the task
that are initially beyond the learner's capacity, thus enabling the learner to focus on and
accomplish those elements that are within his range of competence. This definition of scaffolding
corresponds to Vygotsky’s concept of the Zone of Proximal Development, in which a
knowledgeable adult or peer provides support to help learners accomplish tasks that are beyond
their current level of proficiency (Vygotsky, 1978; Azevedo, 2005; Zhang & Quintana, 2012).
Essentially, scaffolds change the task in some way so that learners can accomplish what would
otherwise be out of their reach (Reiser, 2004). This assistance for the novice in performing a
specific task operationalizes Vygotsky’s concept of the relationship between instruction and
psychological development (Sharma & Hannafin, 2007). Instructional scaffolds can help students
to work through a difficult task and attain a higher level of proximal development that would be
beyond their unassisted efforts (Ge & Land, 2004). With the assistance of scaffolds, learners can
bridge the gaps between their current abilities and intended learning goals that would be
unachievable through their unassisted effort alone (Rosenshine & Meister, 1992). Reiser argues
that scaffolding supports student learning through two complementary mechanisms: 1)
structuring the learning task, guiding learners through key components and supporting their
planning and performance, and 2) shaping students’ performance and understanding of the task
in terms of key disciplinary content and strategies  (Rieser,  2004).  This scaffolding can
supplement the learner’s own metacognition, help them use epistemically appropriate practices
and products, and help them engage in more productive cognitive activities (Quintana et al.,
2004).
Metacognitive support aims to increase students’ learning competence by means of
systematic instruction in order to improve learning performance (Bennert & Mengelhamp, 2013).
60

CSCL software often incorporates automated prompts and questions which encourage student
reasoning and justification of ideas, providing students structure for collaboratively discussing
ideas, justifying them with evidence, or revising them after reflection on peer feedback (Oliver,
2000). This model of iterative, incremental learning emphasizes that students build knowledge
via “the continual improvement of ideas through intentional interactions with one another” (Lai,
2006, p. 130). Social software tools are “increasingly being recognized as essential scaffolds and
learning tools” (McLoughlin & Lee, 2008, p. 649) because their affordances support
participatory knowledge creation through networking, socialization, communication and
engagement with communities of learning (McLoughlin & Lee, 2008). Scaffolds can be both
social and technological, ranging from a human tutor to embedded software prompts (DBRC,
2003). Scaffolding can take the form of instructional supports which provide explicit structures
for an otherwise haphazard sequence of uncoordinated events (Azevedo, 2005; Stahl, 2006).
These type of scaffolds can promote deep learning, especially if they are tailored to the learner’s
needs (Sawyer, 2006). These needs are rooted in the unique challenges and obstacles novices
typically face when attempting to learn new disciplinary skills: superficial understanding, a focus
on results rather than process, unfamiliar discourse practices, and unfamiliar strategies used by
experts that are often tacit (Reiser, 2004). Thus, effective scaffolds help students expand their
understanding from prior knowledge and misconceptions to more advanced understanding,
challenging superficial beliefs and exposing students to expert discourses and strategies (Reiser,
2004; Sawyer, 2006). During this process, scaffolds enable learners to reflect in action (Hung,
2001), providing “opportunities for students to deepen their understanding by externalizing and
comparing their knowledge and beliefs with those of their peers” (Sharma & Hannafin, p.43).
Another form of scaffolding is guiding students to compare their work with others’ or expose
them to multiple perspectives, which can trigger spontaneous reflection (Lin, 2001). Research
has shown that students asking each other questions and self-questioning constituted successful
scaffolds, because while regulating each others’ work on the social level helped students to selfregulate (Pifarre & Cobos, 2010). By contrasting their hypotheses and evidence with those
generated by peers, students have the opportunity to reflect on what they do and do not
understand (Lin, 2001).
The concept of scaffolding has been adopted in research on technological supports for
learning, which have become increasingly important in pedagogical design (Quintana et al.,
61

2004). In computer applications, mechanisms of scaffolding can structure tasks in stages,
decompose complex tasks into component parts, and monitor progress (Reiser, 2004). The
procedural and rationalized structure of most educational software can make the implicit
elements of metacognition more explicit to learners (Quintana et al., 2005). For example, the
turn-based process of asynchronous online communication provides greater time for reflection
on each message and allows less confident students to learn by observing others’ conversations
(Dillenbourg et al., 1996). Reflection is scaffolded by displaying, prompting, and modeling one’s
own and others’ learning processes, as well as providing a forum for reflective social discourse
(Lin, 2001). Students are encouraged to use their existing self-regulated learning strategies,
including self-monitoring by automated instructional supports in the form of problem-solving
prompts (Kauffman et al., 2008). By using visual representations, planning tools, and reflection
support, learning software can help make the implicit nature of metacognition more explicit to
learners (Quintana et al., 2005). Effectively using these metacognitive skills is central to the
benefits of computer-supported learning environments (Pifarre & Cobos, 2010). It is important to
provide carefully structured and sequenced instructional scaffolding in constructivist learning
environments, because cognitive overload may be a challenge for novice learners (Ge & Er,
2005). Since students do not spontaneously engage in metacognitive thinking unless they are
specifically encouraged to do so, it is important to include metacognitive support in learning
environments (Lin, 2001).
Design principles that support metacognition include: providing frequent opportunities
for students to self-assess what they know and do not know, helping students articulate their own
thinking, fostering a shared understanding of goals, and developing knowledge of the self-aslearner in particular community (Lin, 2001). Specific techniques for supporting metacognition
include the structured decomposition of learning tasks into discrete units and the segmentation of
the learning goal into stages. Tools such as graphical organizers, progress monitors, behavioral
prompts and online discussion structure and make visible metacognitive processes.
Scaffolds can be simple progress monitors, visualizations that give the student both an
overview of the entire process and guidelines for achieving the individual component parts,
through graphical organizers, inquiry maps and planning grids (Quintana et al., 2005). They can
also include time and effort planning supports such as monitoring mechanisms that display a list
of goals, marking goals that have not yet been completed, and indicating the time remaining
62

(Azevedo, 2005). These visual representations introduce explicit structures to what had
previously been just a series of uncoordinated events (Stahl 2011), and are ultimately
mechanisms for enhancing the development of the student’s self-regulation processes (Pifarre &
Cobos, 2010).
Scaffolds can also take the forms of question prompts, which can include procedural
prompts, elaboration prompts, and reflection prompts (Ge & Land, 2004). Procedural prompts
guide learners step by step through the entire process of a specific problem-solving task;
elaborative prompts guide students to articulate their thoughts and elicit explanations; and
reflection prompts encourage reflection on their learning on a meta-level that students do not
generally consider (Ge, 2013). Prompts can be defined as measures to induce and stimulate
cognitive, metacognitive, motivational, and/or cooperative behaviors during learning, which vary
from hints, suggestions, reminders, and sentence starters (Morris et al., 2000). Metacognitive
prompts are instructional measures integrated in the learning context that ask students to carry
out specific metacognitive activities (Bannert & Mengelkamp, 2013). Question prompts have
been shown to be beneficial in developing learners’ metacognitive awareness and self-regulatory
abilities (Ge et al., 2010). Metacognitive prompts require students to explicitly reflect, monitor,
and revise their learning process (Bennert & Mengelhamp, 2013). Confidence judgments which
ask students about their performance shortly after completing a task is also an effective approach
to encouraging metacognitive skills, by invoking self-monitoring of the student’s own
performance (Kauffman et al., 2008). Confidence judgments are a good predictor of selfmonitoring behavior, and self-monitoring is related positively to metacognitive knowledge
(Kauffman et al., 2008).
Studies have shown that question prompting is an effective instructional strategy that
guides students to the most important aspects of a problem, and encourages planning,
monitoring, and self-reflection (e.g., Palincsar & Brown, 1984; Scardamalia & Bereiter, 1989).
Procedural prompts help learners complete specific tasks, while elaboration prompts help
learners articulate thoughts and elicit explanations (Ge & Land, 2004). Giving students prompts
in the form of repeated guiding questions requires them to evaluate and elucidate their
understanding (Iding et al. 2008). Ge and Land (2003) found that students who were prompted
by questions increased showed increased efforts to seek and identify relevant information for
their problem. Bixler and Land (2011) found that question prompts significantly improved
63

student problem-solving in a study of college students using a web-based learning environment.
Another study by Ge and Er (2005) showed that question prompts helped students organize their
thoughts, guided them through the complex problem-solving process, reminded them of the
problem-solving steps and the specifics which they might not have thought about explicitly.
Prompts can effectively teach students to generate critical metacognitive questions about
learning tasks and to construct a deeper understanding of the domain (Lin, 2001). Question
prompts play an important role in engaging learners in metacognitive actions such as selfexplanation, self-questioning, and self-monitoring and reflection (Ge & Er, 2005). Such
metacognitive scaffolds can “assist students in assessing their state of understanding, reflect on
their thinking, and monitor their problem-solving processes” (Kim & Hannafin, 2011, p. 408).
Table 5 lists examples of types of question prompts.
Table 5. Examples of question prompts
Author
Tanner (2012)

Kauffman et al.
(2008)

Ge & Land
(2004)

Prompts
Planning
What are all the things I need to do to successfully accomplish this task?
What resources do I need to complete the task?
Monitoring
What strategies am I using that are working well or not working well to
help me learn?
What other resources could I be using to complete this task?
Evaluating
To what extent did I successfully accomplish the goals of this task?
What worked well for me that I should use next time?
Problem solving
What do you see as the primary problem?
What are possible solutions to the problem?
Reflection
How certain are you that you have identified the primary problem?
Elaboration
What do you think are the primary factors of this problem?
Do you have evidence to support your solution?
Reflection
What are the pros and cons of the solution?
What could have been done differently?

Research has shown the effectiveness of using prompts in improving student
performance. Encouraging planning for and reflection on activities through self-monitoring
64

prompts results in students demonstrating more integrated knowledge (Davis, 2000). Students
who are prompted to monitor their progress achieve higher than do students who are not
prompted to self-monitor (Kauffman, 2004). Studies have demonstrated that question prompts
facilitates students’ problem-solving processes, specifically in problem representation, making
justifications, developing solutions, and monitoring and evaluating performance (Ge & Land,
2003; Ge & Land, 2004; Ge, Chen & Davis, 2005). In a study of 54 undergraduates using a webbased learning environment, Kauffman et al. found that students who received automated
problem-solving prompts and reflection prompts provided better answers to questions than
students who did not receive prompts (Kauffman et al., 2008). In a study of 96 undergraduates
using a web-based cognitive support system, Ge at al. found that students who received problemsolving prompts and compared their own answers to their peers’ significantly outperformed those
who did not (Ge et al., 2010). These question prompts were also shown to be beneficial in
developing learners’ metacognitive awareness and self-regulatory abilities (Ge et al., 2010). In
an experimental study with 119 students, those who received self-monitoring prompts showed
significant improvement on achievement tests over those who did not (Kauffman, 2004). Bixler
and Land (2011) studied 79 undergraduates and found that students who received metacognitive
scaffolds during the study performed significantly better than those who did not, with moderate
to high effect sizes.
Raes et al. (2012) conducted an experimental study on the effectiveness of technologyenhanced scaffolding to support both knowledge acquisition and metacognitive awareness. They
studied 347 secondary school students (grades 9-10) in science classrooms conducting a webbased collaborative inquiry project. Their results showed that technology-enhanced scaffolding is
effective for producing learning gains and improved metacognition. However, the authors also
note:	  
“Despite the widespread recognition of the need to scaffold students during
web-based inquiry learning, the understanding of how students’ metacognitive awareness can be supported in authentic classroom settings is
rather limited. Especially, more insight is needed in how to foster students’
web-based information problem solving skills, a pivotal 21st century skill
which is required in everyday life in and out of the classroom. The Internet
brings up-to-date scientific findings in the reach of everyone, yet searching
and finding relevant, credible, and scientifically substantiated information
on the Internet is a challenging task. Consequently, an important question

65

that arises is how to support the information problem solving skills of a
variety of students.” (p. 90)
This question motivates the present research to incorporate instructional scaffolds into the online
credibility evaluation in learning tool.
Ge (2013) reviewed the literature on question prompts and found that they play four main
functions in facilitating self-monitoring and self-regulation by:
1. Directing students to important information that they may have missed
2. Guiding students to elaborate their thoughts, make justifications, and generate
arguments
3. Facilitating reflection, metacognition and knowledge integration
4. Encouraging students’ self-monitoring during problem-solving
Online learning tools often use these types of scaffolding and prompts. Quintana et al.
(2005) describe several examples of effective utilization of prompts in learning environments.
The Digital Ideakeeper tool is an online scaffolded notebook, which automatically frames a
webpage with structured notetaking fields. The notebook is divided into three tabbed sections
titled Skim, Read, and Summarize, which serve as a process visualization of the structured steps
that the learner needs to complete. Each step is decomposed into sub-steps scaffolded with
textual prompts. Another scaffolded inquiry tool, Symphony, provides a visual process map that
represents the necessary steps of the research process, along with a planning grid that allow
students to create and modify their research plans. By making the entire process explicit and
visible, students are reminded that inquiry process involves a range of activities, not just the one
foremost in their mind. A third scaffolded tool, Artemis, supports students in searching for and
organizing information, and sharing questions and websites with each other. These tools make
the inquiry process explicit and visible, which can help learners see the “big picture” and help
them monitor and regulate their work (Quintana et al., 2005). These examples of scaffolded tools
to support online inquiry through a structured process of explicit steps were inspirational for the
design of the prototype IC tool.
Quintana et al. noted a lack of an empirically grounded consensus about successful
scaffolding methods, and developed a design framework to define and evaluate scaffolding
approaches for software tools (Quintana et al., 2004). Their Scaffolding Design Framework

66

synthesizes prior design efforts, theoretical arguments, and empirical work into a systematic set
of guidelines and strategies. Situated within the domain of science inquiry learning, this
framework is organized around the components of scientific reasoning: sensemaking, process
management, and reflection and articulation. Sensemaking refers to the basic operations of
science inquiry such as generating hypotheses, collecting observations, analyzing data, making
comparisons, and constructing interpretations. Process management involves engaging in and
managing new disciplinary processes. Reflection and articulation involves self-assessment
through reviewing, evaluating and synthesizing one’s work. The specific scaffolding strategies
within each component of the Scaffolding Design Framework are shown in Table 6 below.
Table 6. Scaffolding Design Framework (Quintana et al., 2004)
Scaffolding guidelines
Sensemaking

Process management
Reflection and articulation

Scaffolding strategies
Use representations and language that bridges learners’
understanding onto expert practice
Organize tools and artifacts around disciplinary strategies
Use representations that viewers can evaluate, such as graphs and
charts.
Provide structure for complex tasks and functionality
Embed expert guidance
Automate nonsalient and routine tasks
Provide prompts to facilitate planning, monitoring, and
sensemaking

This Scaffolding Design Framework provides detailed discussion of these guidelines and
provides examples of successful implementations in learning software. The goal of the
framework is to provide a basis to develop an integrated theory of pedagogical support for
complex learning with software, and to provide general principles for evaluating what
pedagogical approaches are effective in supporting learners (Quintana et al., 2004). This
Scaffolding Design Framework was used in developing the pedagogical model and design
features of the IC tool learning tool (see Chapter 3 for discussion of the incorporation of
scaffolds into the design and development of the IC tool prototype).
	  

67

2.3.4.	  Implications	  for	  the	  research	  
Although there is a significant amount of literature on case studies of IL instruction, there
is little empirical research on its effectiveness beyond surveys, pre/post-tests and outcomes
evaluation (Barclay, 1993; Coupe, 1993; Rockman, 2002; Orme, 2004). Scaffolding and
metacognition have been studied in other fields, e.g. education and psychology (Kauffman, 2004;
Iding, 2008; Pifarre & Cobos, 2010), educational media (Bannert, Hildebrand & Mengelkamp,
2009), pharmacy (Ge, Planas & Er, 2010; Ge, 2013), science (Qunitana, et al., 2004; Azevedo,
2005; Quintana, Zhang & Krajick, 2005; Raes, 2012; Tanner, 2012), and specific domains such
as reading comprehension and writing skills (Lin, 2011). However, there has been little research
on the application of scaffolding and metacognitive support to teaching students IL and
credibility evaluation skills (Gorrell et al., 2009; Bannert & Mengelkamp, 2013), and in online
learning environments (Akyol & Garrison, 2011). These gaps in the literature are addressed by
the research through conducting an experimental study on the learning impact of an IC tool
which incorporates scaffolding and metacognitive support.
The design of the IC tool was inspired by CSCL learning theory and its application to
online learning tools. The constructivist model of learning and Vygotsky’s Zone of Proximal
Development were fundamental inspirations. Constructivist theory, which sees learning as a
social process in which students play an active role in building knowledge, and the ZPD model
of bridging students’ current knowledge toward more advanced practice, both informed the
pedagogical model of the prototype learning tool. The importance of collaborative and
participatory learning to today’s students also provides an important context to this research.
The metacognitive skills of planning, monitoring and reflecting on the IL process were
supported by structuring tasks into stages, decomposing complex tasks into component parts, and
monitoring progress. The design of the tool follows Quintana et al.’s Scaffolding Design
Framework of supporting sensemaking, process management, and reflection and articulation.
Learning is scaffolded by the structured decomposition of tasks into discrete units, and the
segmentation of the learning goal into stages. The Digital Ideakeeper notebook created by
Quintana et al. was an inspiration for the browser-based notebook in The IC tool. Tools such as a
graphical organizer for online notetaking, a visual representation of the 3-stage IL process, a
progress monitor of the student’s relative completion of each stage, tips and question prompts
regarding the credibility criteria and the evaluation process are built on the literature showing
68

their effectiveness in supporting student learning. These tools will help make visible the
metacognitive processes required for effective online credibility evaluation. The key point for
designers is to explicitly depict online inquiry tasks to students (Quintana et al., 2005).
The social constructivist model of learning provides a theoretical framework for
scaffolding the online credibility evaluation process and for providing metacognitive support.
This theoretical model has guided the design and development of the prototype IC tool. Through
instructional supports that structure an otherwise haphazard sequence of actions, and visual
representations that structure what had previously been just a series of uncoordinated events, the
scaffolds embedded in the prototype learning tool enhance the development of the student’s
metacognitive and self-regulation processes. Since novice learners usually have weak
metacognitive skills, which are important for engaging in complex practices like online
credibility evaluation, the prototype learning tool provides needed practice and reinforcement of
these important skills. In addition, the situated, just in time, web-based nature of the tool
facilitates active involvement in the learning process by Millennial students who value
collaboration and peer-based learning.

2.4.	  Literature	  Review	  Summary	  
This research intends to fill a gap in the literature that exists at the intersection of three
fields: information literacy, online credibility evaluation, and CSCL learning models. While
research in each of these fields is divergent and generally not connected, they share similar
principles: developing models of online information evaluation processes, and investigating real
world evaluation practices. Overall, IL research and practice remains focused on practical
teaching practices and is segregated from research into credibility evaluation, despite the
commonalities in their content and models. This separation reflects the origins of the two
practices in library science and information science respectively. IL research often consists of
simple cases studies and self-report, while credibility evaluation practices have often been
empirically studied through observation and diary studies in the online environment. The results
of credibility research are often applied to website designers as techniques of increasing
perceived website credibility, but are rarely applied to teaching students better evaluation skills.
While CSCL approaches to support learning - metacognition, scaffolding, and participatory
69

learning – have been shown to be effective, these techniques are usually applied in STEM
(science, technology, engineering, and mathematics) fields and have not been utilized to teach IL
and credibility skills instruction. CSCL applications are often domain-specific, with tools being
built on specific disciplinary expert practice, primarily in the sciences (mathematics, physics,
engineering). Most CSCL applications are not generalizable to any other content area of
discipline. Thus, a tool for a different discipline must be built from scratch. However, IL and
credibility evaluation skills can be applied to information in any discipline, particularly in the
online information environment.
While LIS practitioners develop guidelines and checklists for IL training, they are usually
not informed by research into online credibility evaluation practices, but are based in the
traditional model of IL as bibliographic instruction. In reality, however, that model has lost much
of its meaning in the contemporary online information environment. Information literacy is
inherently metacognitive, and IL instruction can benefit from the application of CSCL learning
theory, in particular, constructivist learning models, participatory learning, and the use of
scaffold and prompts. While these three fields of research do not often interact, there are
significant areas of conceptual overlap, as shown in Figure 3 below.
Figure 3. Theoretical model for online learning tool

This intersection point has not been explored in the existing literature. Combining
theoretical backgrounds and findings from these three fields offers a new approach to teaching
70

effective online IL and credibility evaluation skills. Such an approach situates IL instruction in
the real-world information environment of the Internet which students rely on for finding
information, gives students practice in performing credibility evaluations in the online
information environment using specific criteria, uses instructional scaffolds to support learning,
teaches a structured and systematic process for evaluation, provides reflection and monitoring
support metacognition, and employs the participatory learning functions that students are
accustomed to. This study tests a pedagogical method that combines these elements into an
online credibility evaluation learning tool. The purpose of the research is to investigate the
effects of scaffolding and metacognitive support on student learning of online credibility
evaluation skills. The study tests if the IC tool incorporating scaffolding and metacognitive
support increases students’ knowledge of the expert criteria that constitute credibility
evaluations, the evidence-based source characteristics used in making credibility evaluations, and
the metacognitive strategies used while evaluating online information. Developing students’
metacognitive skills regarding credibility evaluation, and their understanding of IL as a
structured process requiring practice, planning and reflection, will help students become
critically aware users of online information, and will prepare them for success in their academic
and professional careers.

71

Chapter	  3:	  Prototype	  Design	  and	  Development	  	  

This chapter describes the design and development of the prototype IC tool, including the design
methodology, theoretical models, initial design documentation, and the development of the
prototype. Section 3.1 describes the overall research approach. Section 3.2 discusses the
theoretical models underlying the tool’s design. Section 3.3 describes the initial design and
development of the prototype tool. Section 3.4 describes the pilot testing of the prototype.
Results from the pilot testing are described, and conclusions drawn from pilot testing are
discussed, followed by the current plan for completing the final tool.

3.1.	  Design-­‐Based	  Research	  
The IC tool prototype was developed using the methodology of design-based research
(DBR), defined as “the study of learning in context through the systematic design and study of
instructional strategies and tools” (DBRC, 2003, p. 5). Sandoval and Bell define DBR as “a
means for studying innovative learning environments, often including new educational
technologies or other complex approaches, in classroom settings” (Sandoval & Bell, 2004, p.
200). The DBR model emphasizes the inter-relationship between theory, design and practice,
creating “a systematic but flexible methodology aimed to improve educational practices through
iterative analysis, design, development, and implementation, based on collaboration among
researchers and practitioners in real-world settings, and leading to contextually-sensitive design
principles and theories” (Wang & Hannafin, 2005, p. 6-7). Situating research in its social context
is crucial, as gathering input from both learner-subjects and teacher-partners is important to
shaping the design of tools. A clearly shared focus of all DBR researchers is the integration of
design and research, which is important for establishing a collaborative context (Hoadley, 2002).
DBR can be seen as a blend of empirical educational research with theory-driven designs
72

(DBRC, 2003), or a hybrid methodology (Wang & Hannafin, 2005) combining a theory-driven
approach with inductive design processes (Quintana et al., 2004). Researchers draw from
multiple disciplines, including developmental psychology, cognitive science, learning sciences,
anthropology, and sociology (Sandoval & Bell, 2004). To evaluate DBR tools, inductive
qualitative approaches and quantitative and quasi-experimental approaches are used (Fishman, et
al., 2004), employing both formative evaluation (Wang & Hannafin, 2005) and comparative
analysis (Shavelson, et al., 2003).
The work of DBR researchers is “grounded in real-world contexts where participants
interact socially with one another, and within design settings rather than in laboratory settings
isolated from everyday practice” (Wang & Hannafin, 2005, p. 9). Following the model of
constructivism, learning is seen as socially constructed and situated in an interactive,
interpersonal context. DBR employs “theoretically framed, empirical research of learning and
teaching based on particular designs for instruction” (Sandoval & Bell, 2004, p. 200). Real-world
educational contexts are used as natural laboratories to study the effectiveness of learning
environments on learning and teaching practices, simultaneously developing learning tools and
studying their effects (Sandoval & Bell, 2004). Researchers work closely with teachers and
students to design, develop, implement, and evaluate innovations in real classroom settings
(Fishman, et al., 2004). These partners all share the goals of conducting rigorous and reflective
inquiry, testing and refining innovative learning environments, and defining new design
principles based on previous research (Ludvigsen & Morch, in press). The participation of
practitioners helps to produce meaningful changes in the actual contexts of practice (Wang &
Hannafin, 2005). Since the emphasis is on understanding real world practices in their naturalistic
settings (Barab & Squire, 2004), the methodological orientation of DBR is pragmatic, grounded,
and contextual (Wang & Hannafin, 2005).
The focus of DBR on the social context of learning and its constructivist nature indicates
clear correspondences to Vygostsky’s learning theory (Hung, 2001). Following the Zone of
Proximal Development model, web-based learning environments create links between novices
and more capable peers, connecting learners with varying levels of expertise within the
knowledge building community in a continuum of participation structured through mediated
discourse (Hung, 2001). Students are immersed in participatory contexts in which authentic

73

activities are conducted, allowing learners to begin developing an understanding of the domain
(Quintana et al, 2006).
The DBR method is iterative, consisting not simply of research producing a final product,
but of research informing a cycle of development and refinement. This approach of continuous
refinement aims to improve the way a design operates in practice (Collins et al., 2004), through
an ongoing cycle of design, enactment, analysis, and redesign (DBRC, 2003). Instructional
activities and artifacts developed through this “design-analysis-redesign” process become the
subject of further research into their the impact on the reasoning and thinking displayed by
learners (Shavelson, et al., 2003). Testing these interventions in context can uncover
unanticipated outcomes or consequences, which then subsequently shape the further
development of the artifact and of the learning theory informing it (Hoadley, 2002). As opposed
to laboratory or experimental research, DBR methods respond to emergent features of the setting
(Wang & Hannafin, 2005). Overall, DBR methods are process focused, interventionist,
collaborative, multileveled, utility oriented, and theory driven (Shavelson, et al., 2003). The
outcomes of DBR are often specific “design principles” to guide, inform, and improve both
practice and research in educational contexts (Anderson & Shattuck, 2012). These can take the
form of evidence-based heuristics to inform the development and implementation decisions of
future DBR researchers (Herrington et al., 2007). A second outcome of DBR is the production of
“designed artifacts” which may range from software packages to professional development
programs (Herrington et al., 2007). These outcomes demonstrate the practical, real-world
orientation of the DBR method.

3.2.	  Theoretical	  models	  
  

The prototype design of the tool was based on a synthesis of three related but segregated

theoretical models: IL and credibility evaluation research findings and computer-supported
collaborative learning (CSCL) principles. This section describes the fundamental concepts from
each field that informed the design of the tool.
3.2.1.	  Information	  literacy	  model	  
  
  

Underlying the IC tool’s design is a 3-stage model that is the ACRL’s definition of
74

information literacy—“a set of abilities requiring individuals to ‘recognize when information is
needed and have the ability to locate, evaluate, and use effectively the needed information”
(ALA, 2000). The tool represents each step of “locate, evaluate, and use” through a division into
three stages identified as Investigate, Question, and Solve. Working through each stage, students
apply credibility and relevance criteria to online information sources. Each stage builds upon the
previous, providing students with multiple opportunities to apply the credibility and relevance
criteria. The tool reinforces the three-part structure through a process map represented as a 3-part
progress bar that highlights the current stage that the user is on and gives a visual depiction of the
user’s progress.
The design of the tool is also informed by theories of library-related information seeking
behavior (ISB) discussed in Chapter Two. The first is “library anxiety” (Mellon, 1986), a sense
of powerlessness which students may feel when they begin an information search that requires
using the library, involving feeling lost, fearful of library staff, and unable to navigate the library.
Unfamiliarity with new surroundings and general anxiety about success may affect students as
they transition from the high school library to the academic research library (Onwuegbuzie, Jiao
& Bostick, 2004). Students may also feel inadequate, embarrassed or intimidated in the library
environment (Van Scoyoc, 2003). IC tool addresses these anxiety barriers by situating library
instruction in the online context where students normally do their research, rather than relying on
placing them physically in the library, or bringing a librarian physically to the classroom.
Additionally, the online participatory learning tool incorporates social media components that
will help students feel they are in a familiar environment.
The design of the tool also seeks to addresses the Principle of Least Effort (PLE) (Zipf,
1949; Rosenberg, 1974; Mann, 1993) and “satisificing” (Simon, 1956; Buczynski, 2005), both of
which suggest that students often accept the first satisfactory alternative over the best possible
alternative when searching for information. Research consistently shows that students accept the
first answers they find when searching online (Lankes, 2008; Hargittai et al., 2010), and rely on
familiar strategies to find satisfactory information with a minimum of effort as well as be
unwilling to move beyond their current skill level (Warwick et al., 2009). The tool’s design
structures the information-searching process through a series of measured steps, each involving
self-review and reflection on the part of students, as well as providing them a process map of the

75

overall task and progress monitors of completion and gives them repeated opportunities to
practice new skills.
The tool also aims to address Competency Theory, which suggests that students who lack
information literacy skills do not realize it and therefore are unlikely to seek out instruction
(Gross & Latham, 2007). Low-skilled students also hold inflated views of their own competence
in information seeking, do not know their own weaknesses, and often overestimate their abilities
to find and evaluate online information (Manuel, 2002). The IC tool is designed to address this
issue by using student performance on the tutorial to classify students as lower- or higher-skilled
and providing a question asking-and-answering functionality that allows the higher-skilled
students to assist the lower-skilled through threaded discussions. The three stages of the tool give
students repeated, structured practice in evaluating their own work (Investigate) as well as the
evaluating the quality of other students’ evaluations (Question). Through this structured skills
practice, students learn to evaluate their own skill level more realistically and compare their own
skills to others based on shared performance.
3.2.2.	  5Ws	  model	  
  
  

Credibility evaluation criteria are a crucial component of the tool’s design. The literature

on online credibility evaluation provides extensive insight into the actual practices students use,
but without providing suggested methods to improve students’ evaluation skills. Incorporating a
research-based understanding of actual student credibility evaluation behavior into the design of
a learning tool situated in the online information environment was one of the fundamental
motivations for this research project.
As discussed in Chapter 2, the checklist model for teaching the evaluation of websites is
popular in academic libraries (Myhre, 2012). During the development phase of the prototype, the
research team reviewed several checklists for information evaluation used in IL instruction and
chose two that are widely used: the CRAAP test and Kathy Schrock’s 5Ws model. The CRAAP
acronym (Currency, Relevance, Authority, Accuracy, Purpose) is a play on words that helps gain
the attention in interest of college students (Myhre 2012), although its use would be considered
inappropriate for high school, as suggested by several SMLs interviewed for this research.
Schrock’s simplified 5Ws model (Who, What, Where, Why, and When) employs non-expert
language that is appropriate to the intended audience of the learning tool:(incoming college
76

students, and potentially high school and community college students (Appendix 3). These
models, along with other examples found by searching the web for academic library IL teaching
materials, were adapted and synthesized to create a set of credibility questions and prompts
contained in the IC tool’s Notebook, the browser-based plugin that students use to gather
information about their sources during the initial Investigate stage (Appendix 5). In the learning
tool’s later stages, the 5Ws are mapped to more sophisticated credibility criteria language
(authority, relevance, reliability, currency, and purpose), providing scaffolding to bridge the gap
between students’ unsophisticated understanding of online information evaluation and the more
sophisticated models of evaluation criteria used by experts.
Although the checklist approach is popular with IL librarians, it does have its detractors
who consider it reductionist and inflexible. For this reason, the IC tool does not rely on the
question prompts alone but involves students in repeated practice of their use, comparison to the
evaluations of other students, and reflection on their evaluation process to help them understand
evaluation as a process, not merely a list of criteria. The questions in the Notebook are anchored
in the student’s Internet browser and are connected to the specific website being evaluated, not
presented as an abstract checklist.
While Myhre (2012) conducted a small study (N=14) of the use of a version of the
CRAAP test, the 5Ws model of website evaluation has not been empirically tested (Schrock,
2013) so this study represents the first research into the effectiveness of the 5Ws model for
teaching website evaluation criteria.

3.2.3.	  Scaffolding	  model	  
  
  

The tool’s design employs basic CSCL design principles. The tutorial utilizes elements of

the cognitive tutor model of required sequential steps, while the three activity stages use a social
constructivist model of encouraging active participation and knowledge construction through
engaging students in problem-solving activity. To support students in building cognitive links
between prior knowledge (experience searching online, using Google and Wikipedia) and new
knowledge (the critical thinking skills used in evaluation of sources), the structure of the learning
tool begins with simple concepts in a familiar context and gradually elaborates on them through
prompts and hints that introduce higher-level concepts. To encourage student motivation,
77

learning is embedded in a realistic setting that students are accustomed to (online searching) and
in a relevant context (a real class assignment). To encourage interaction and collaboration with
other learners and teachers, social features such as comment threads and peer question answering
are integrated into the tool. Process maps, regular feedback, progress monitors, and reflection on
the students’ own work build metacognition about the process of evaluation and the informationseeking process.
The IC tool’s design is informed by CSCL principles of learner needs. Reiser (2004)
described the obstacles that novice learners face due to their lack of knowledge of and
experience with the topics they are learning. Overcoming these fundamental principles informed
the design of the IC tool:

•

Unfamiliar strategies

•

Superficial understanding and unfamiliar discourse

•

Lack of motivation

To address the unfamiliar strategies of online credibility evaluation, learners need step-by-step
guidance in following a structured process. Novice learners are not familiar with the steps
involved in evaluating sources, what criteria to use or where to find evidence that supports the
claim that a source is credible and relevant. The IC tool addresses this obstacle by providing
learners with process management tools, including a process map that visually summarizes the
sequential steps that must be followed: Investigate (find evidence for evaluation), Question
(make decisions about quality) and Solve (compare and synthesize multiple sources); progress
bars which visually demonstrate each student’s progress through the stages; a dynamic checklist
showing the student’s progress through the sub-steps of each stage; the sequential steps of the
notebook guiding the student through the 5Ws (criteria for credibility evaluation); and the
sequential steps of evaluation in the Question stage, which follows and reinforces the notebook
sequence.
To address their superficial understanding and unfamiliar discourse, the IC tool gradually
introduces students to the unfamiliar terminology of credibility evaluation and reinforces it to
students throughout the process of using the tool. Novices tend to use sources without critically
analyzing them, often using the first search results that they find. They are not familiar with the
78

terms such as “credibility” and “relevance” and may not know how to apply them to information
sources. The Notebook and the Investigate stage of the tool enlists a simplified framework of
“5W’s” (who, what, where, when, why), with basic descriptions and tips for finding answers. In
subsequent stages, higher-level credibility terminology (accuracy, relevance, reliability,
currency, purpose) is introduced and defined.
To addresses students’ lack of motivation, the tool places IL skills training in the online
information environment where today’s students do their research, ties it directly to in-class
research assignments, and utilizes components of social media that are familiar to students such
as threaded conversations and peer commenting. Novices need to understand why they are
learning skills. Today’s students may not see the point of the learning IL skills, since they feel
they are already experts in searching and can get what they need from Google and Wikipedia.
The tool is situated in the real life context of a class assignment.
These three obstacles can successfully be overcome by bridging learners’ prior
knowledge - starting them with what they already know and then connecting it to more
sophisticated concepts. Thus, InCredibility begins with basic online searching and introduces
students to the criteria of credibility and relevance, while giving them practice on how to find
evidence, make decisions, and synthesize the results of online source evaluation. It builds student
motivation by situating learning in the realistic online environment and encouraging peer
collaboration and discussion.
Along with a basis in learner needs, InCredibility is designed around principles of
scaffolding learning. Quintana, et al. (2004) described the basic techniques by which successful
scaffolding supports learning:

•

Sensemaking: Use representations and language that bridge learners’ understanding

•

Process management: Provide structure for complex tasks and functionality

•

Articulation and reflection: Facilitate ongoing articulation and reflection

To support sensemaking, educational scaffolding must aid students in developing new skills by
building on current knowledge (Quintana, et al. 2004). To bridge learner’s prior experience with
Internet searching, the first part of the tool is an interactive tutorial that gives examples of where
to look on websites for evaluation criteria. The tutorial provides hints based on student
79

performance; when students answer questions incorrectly more hints are provided. As they
progress through the three stages of the tool, students build cognitive links between prior
knowledge (experience searching online, using Google and Wikipedia) and new knowledge (the
critical thinking skills used in evaluation of sources). The terminology of credibility evaluation is
gradually introduced, starting with the simple version of 5Ws, and transitioning to higher-level
terminology (authority, relevance, etc.) as students’ expertise develops.
To support learning process management, educational scaffolding must decompose tasks
to simple, easy-to-understand units (Quintana, et al. 2004). The Notebook tool serves to break
down each task for students during the Investigate and Question stages by placing each
individual credibility and relevance criteria (5 Ws) on a separate page. To help students monitor
their progress, a process map on the Home page visually represents the three stages (Investigate,
Question, and Solve) as a visual conceptual organizer. As students complete each stage, these
bars progressively fill in with a new color to indicate completion of the tasks. Lastly, each stage
has a quota set by the classroom teacher. For example, students are required to find and
investigate a certain number of sources independently. Then, students must evaluate a certain
number of sources added by peers. Finally, students select a specific set of sources to use during
the Solve phase. Time-based reminders when students are close to the end and have not met
goals. These features structure the complex task of evaluating online information into more
manageable steps.
To support articulation and reflection, educational scaffolding must guide students in
reviewing their own understanding and making it explicit (Quintana, et al. 2004). During the
Investigate stage of the IC tool, students are prompted to enter comments on each source they
evaluate, explaining why they rated it as they did. At the end of the stage, they are prompted to
review their work and make any changes they feel necessary. A prompt asked “How confident in
your answers?” to encourage self-reflection. During the Question stage, other students can see
these comments and respond to them. Students receive comments on their own sources and make
comments on others. During the Solve stage, students are prompted to choose between two
sources and explain their rationale for choosing which is better, and then articulate their own
understanding of the credibility criteria that they used. A confidence judgment was prompted.
Because reflection is difficult for novices, they are supported through the use of sentence starters
and drop-down menus.
80

These principles of scaffolding are incorporated into the design of the IC tool through
gradually introducing new concepts and terminology of credibility evaluation that build on
students prior understanding of web searching, providing structure for complex tasks (process
maps, progress bars, prompts) and facilitating ongoing articulation of and reflection on their
understanding through comments and discussion. Scaffolds support students in externalizing and
comparing their knowledge and beliefs with those of their peers (Sharma & Hannafin, 2007).
Scaffolds also support students in developing their metacognitive skills. Through introducing
explicit procedural structure to what had previously been just a series of uncoordinated events, a
student’s self-regulation and self-evaluation processes are enhanced (Ge & Land, 2004; Pifarre
& Cobos, 2010). The IC tool supports students in the important metacognitive skills of planning
their tasks, monitoring their progress toward meeting goals, taking appropriate steps to solve
problems, and reflecting on past performance (Quintana et al., 2005, p. 2360). At each stage of
the IC tool, the unfamiliar and challenging process of evaluating the credibility of online
information is structured as a process of planning, monitoring, and problem-solving, scaffolding
the common activity of online searching with higher-order metacognitive skills. As students
complete each stage of the learning tool and gain repeated practice in each activity, they learn
how to regulate their online searching behavior and reflect on their own skills and understanding
and to reflect on their own thinking. Specific examples of scaffolds mapped to Quintana et al.’s
Scaffolding Design Framework are shown in Table 7 below.
  

Table 7. Scaffolding Design Framework applied to IC tool design  
Scaffolding guidelines
Sensemaking

Process management

Reflection and articulation

Scaffolding strategies
Tutorial
Hints
Process map
5Ws mapped to expert terminology
Notebook with 5Ws tabs
5W Question prompts
Automatic saving of URLs and screenshots
Progress bar for each stage
Process map
Quotas and deadlines for each stage
Confidence prompts
Self-evaluation prompts
Comparison prompts
Reflection questions

81

  

  

By providing a mediated tool through which students are not learning individually but are
interacting and communicating, the IC tool structures self-regulation of behavior through
reflection in action (Hung, 2001). The learning tool encourages students to participate in
regulating each other’s work on the social level, helping them to became more aware of their
own learning process (Pifarre & Cobos, 2010). Peer interaction in the learning process is
enhanced through asking for help, clarifying ideas and responding to feedback, structuring selfregulated learning through reflection on key task-solving processes (Pifarre & Cobos, 2010). The
social experience of this online learning tool differentiates it from traditional learning software,
which focuses on individual learning, and on existing learning environments, which are usually
offline and based on physical co-location.
  

3.3.	  Initial	  prototype	  	  
  

The initial inspiration for InCredibility came from the researcher’s experience on the

design team of the BiblioBouts information literacy game (Markey et al., 2012). The game
provided a model for teaching students the process of evaluating sources through an online
interface that structured the evaluation process into a step-by-step process and guided students
toward learning to use the library resources to create scholarly bibliographies. However, research
shows that today’s students overwhelming rely on online web-based sources when searching for
information, rather than using library sources (Herring, 2011; Kolowich, 2011). Game mechanics
were not employed because of the complexity of the design challenges, the expense of hiring a
professional programmer, and the amount of time required for development. Games are also not
suitable for all students. While some students thrive on competition and mastery, educational
games run the risk of discouraging other types of students: non-gamers who have little or no
experience with digital games, and players who do not enjoy competition or fear failure at an
unfamiliar task (Magerko, Heeter, and Medler, 2010). Based on these factors, the researcher felt
it was important to design and test a tool that focused on online sources and was integrated into
an Internet browser, to locate the evaluation process where students actually do their information
seeking.

82

The IC tool prototype was developed through an iterative DBR process. Based on the
work of Reiser (2004) and Quintana et al. (2004) on learner needs and goals, an initial design
document and storyboard was drafted that identified basic functionality, artifacts created,
information needed to do the work, and conceptual scaffolding requirements for each stage of
tool. Functionality included every specific action that students would complete in that stage;
artifacts created included concrete outcomes of the actions either within the game or outside;
information needed included both domain knowledge and task knowledge necessary to
successfully complete that stage; and conceptual scaffolding requirements included subcategories of domain support, task-based support, articulation support, metacognitive support,
and procedural support. For instance, in the Investigate stage, the conceptual scaffolding
requirements identified were:
1. Domain support: Students need to be reminded of the criteria for evaluation
2. Task-based support: Students need to know what question they are answering and what
search terms to use
3. Task-based support: Students will have access to “Hints” or “Clues” about where to look
for evidence of each criteria
4. Articulation support: Students need explicit areas to enter their comments and notes
5. Metacognitive support: Students need to be reminded of the steps of the overall process
(process map)
6. Procedural support; Students need to be reminded of their progress in this task (progress
bar toward quota)
  

This document was created as a final project in Professor Quintana’s master’s level class in the
School of Education titled “Principles of Software Design for Learning.” It was refined through
several rounds of group discussion between three collaborators: a doctoral student and a master’s
student from the School of Information, and a doctoral student from the School of Education.
Initial drawings of prototype screens were created and refined, and then mock-ups of screens
were created in Adobe Illustrator. A basic sitemap was created to show the overall structure of
the software and the workflow. The sitemap and screenshots were then integrated into the
storyboard. At each step the design documents were iteratively reviewed and refined. Feedback
from Professor Quintana was incorporated into the document before a final in-class presentation,
which provided further feedback.

83

Following the initial design document development, the researcher presented a mockup
of the storyboard screenshots to a master’s class in Information Literacy at the School of
Information, obtained their feedback, and incorporated the resulting feedback into a revised
design. Another round of review took place with a clinical professor of Library and Information
Sciences (LIS) and an informal group of school media librarians (SMLs) who reviewed the
designs and provided more feedback which was used to refine the storyboards. As potential
targeted users, the feedback from the SMLs was especially useful. Their feedback suggested that
the tool was a good match for the needs of classroom IL instruction and could potentially be
useful for teachers and librarians.
Grant funds from the Rackham Graduate Student Research Program enabled the
researcher to hire student programmers to build a working prototype. The InCredibility prototype
was comprised of two elements: the Notebook, a Firefox browser plug-in that students use to
search for sources and to answer the criteria questions during the Investigate stage, and a
dedicated website where students review their saved sources, and evaluate and synthesize them
during the Question and Solve stages. The tutorial utilizes elements of the CSCL cognitive tutor
model of introducing and reinforcing a structured sequential process, while the three activity
stages use a social constructivist model of encouraging active participation and knowledge
construction through engaging students in problem-solving activity and situating learning in a
social context by encouraging collaboration and peer-learning.
  

3.3.1.	  Structure	  of	  the	  prototype	  
An important scaffold in the prototype IC tool is the decomposition of complex tasks into
specific steps. The IC tool breaks down the process of online credibility evaluation into discreet
stages, allowing students to learn and practice individual skills in a structured sequence. See
Table 8 below for a description of the steps of the process that the student experience as they
proceed through the stages of the InCredibility workflow.
  

84

Table 8. InCredibility student workflow  
1. Register/login
2. Complete interactive tutorial
3. Investigate stage: Search online for sources on group topic. Save sources with online plug-in
(Notebook) and enter answers to credibility prompts (5Ws). Complete quota of sources. Review your
saved sources and edit responses, if desired.
4. Question stage: Review sources entered by other students. Agree or disagree with the responses
describing that source. Enter your comments on the source. Repeat to meet quota. Review your
evaluations and edit responses, if desired.
5. Solve stage: Compare the quality of paired sources, using higher-level credibility terminology.
Generate “in your own words” descriptions of credibility analysis online.

The registration/login stage is simple: students go to the InCredibility homepage and are
prompted to enter their login information, or if they are not yet registered, to create an account.
This login is also used for the Notebook, which saves all the users online activity to their account
in the database. The major stages of the online credibility evaluation tool are described in
sequence below.
The IC tool begins with a tutorial that introduces the basic credibility criteria for online
information in an interactive format. Rather than the non-interactive, passive style of
conventional IL tutorials, the online learning tool’s tutorial allows students to learn by trial and
error and provides feedback and tips based on their performance. It provides an initial static
webpage that highlights several elements of a website that students should investigate to
determine answers to the credibility questions:

•
•
•
•
•
•
•
•

Website URL
“About” link
Contact link
Date
Author name
Keywords
Main ideas
Advertising

85

These concepts are presented visually in the realistic context of an actual webpage, rather than
merely listed or described (see Figure 4).

Figure 4. Tutorial opening page

After introducing these basic sections of the website that should be used in evaluating credibility,
the tutorial moves to an interactive activity where the same example webpage is presented
without the highlights, and students are prompted to click on the appropriate element of the
webpage to answer each of the five credibility questions. The instructions read “Using this
example webpage, find evidence to answer the questions Who, What, Where, When and Why.
Select the section of the page that answers each question.” Each prompt states “Look for clues
about (5Ws question) and click on the evidence.” A Tip button also accompanies each question.
(See Appendix 5: Prototype Tutorial Questions, Tips and Answers). In Figure 5 the prompt
“Look for clues about WHO wrote this information and click on the evidence” appears. The
correct response to this question is to click on the author’s name field, which was highlighted on
the static page. The correct answers are hotspots identical to the static example page but without
the visible highlights, which only appear after the correct answer is revealed (either by student
selection of triggered by two incorrect responses). This tutorial provides scaffolding for
sensemaking by helping student build on their prior knowledge of web evaluation by expanding
it with greater detail.
86

Figure 5. Example tutorial question

The student may click on the “Tip” button to receive a reminder about what information to look
for (see Figure 6).
Figure 6. Tutorial tip text

If the student clicks on the correct hot spot, a popup message informs them that they are correct,
and highlights the correct answer field. Each question is reinforced by a re-statement of the
criteria after the question is completed (see Figure 7)

87

Figure 7. Tutorial correct answer

If the student clicks outside of the correct hot spot, a popup message informs them that they
should try again, and gives a tip (see Figure 8).
Figure 8. Tutorial wrong answer and tip

These tips provide scaffolding for sensemaking by embedding expert guidance. If the student
makes a second wrong response, a popup message informs them that they are incorrect, and
highlights the correct answer field along with repeating the tip (see Figure 9).

88

Figure 9. Tutorial second wrong answer

If the student answers the question correctly on the first try, they receive a congratulation
message which reinforces the criteria. If they answer incorrectly, a feedback message tells them
they are incorrect and provides tips. If they answer incorrectly again, they receive a second
incorrect message and then are shown the correct answer. If the student is already familiar with
this content, they should be able to proceed quickly through the tutorial by giving correct
answers on the first try. If they need to learn or need reinforcement, the tutorial gives them
practice. At the completion of each answer, the tutorial automatically advances to a new
question. After all questions are completed, the tutorial provides a link to the first stage of the
tool so that students can begin.
To provide overall structure for the learning experience, each student has an online
“Headquarters” page on the online learning tool’s site where they can monitor their progress and
performance (see Figure 10). This visual conceptual organizer serves as a process map. Their
progress in completing each activity is displayed. The system also issues time-based reminders
when students are close to the end and have not met goals. These features help students plan and
self-regulate their credibility evaluation process.

89

Figure 10. Headquarters page
Process map
Mapap

Progress monitor

To begin the Investigate stage, students search online for appropriate sources for their
research question. To guide them in this process, they use the Notebook, a Firefox browser plugin which is installed in the browser and can be opened by clicking on a small icon in the lower
right corner of the browser (similar to Zotero). When the icon is clicked (in the prototype, this
icon is a small graphic of a pencil next to the Zotero icon) the Notebook expands into a pane
resembling a tabbed notebook, with each tab bearing one of the 5 Ws questions and text entry
fields for entering data in response to the prompts (see Figure 11).

90

Figure 11. Notebook expanded

Notebook pane

Current 5Ws question
Mapap

Student responses
The highlighted tab in the Notebook shows which of the 5Ws the student is currently reviewing.
Since the plugin resides in the browser, students can open the Notebook as they search online for
information in the Investigate stage. When the Notebook is opened, the current webpage is
automatically saved to the database, with the URL and a screenshot of the page, which provides
scaffolding for process management by automating routine tasks. All entries to the Notebook are
also automatically saved. This information was displayed for review by the student, and for
evaluation by other students in the Question stage. When the Notebook is open, students are
prompted to enter comments on each source they evaluate. These prompts for each of the 5Ws
provide scaffolding for process management by decomposing the credibility evaluation task into
an ordered sequence of steps with question prompts, and as scaffolding for sensemaking by
making disciplinary strategies explicit. Each tab is decomposed into sub-steps scaffolded with
textual prompts, again providing scaffolding for process management (see Table 9 for the
original prototype Notebook question prompts). The structure of the 5Ws and the related
question prompts function as procedural prompts to guide learners step by step through the
process of credibility evaluation.

91

Table 9. Prototype Notebook question prompts
Tab

Question

WHO

Who is/are the author(s) of this source?

WHAT

What 3 keywords best describe this source?

WHERE

Where was this source published?

WHEN

When was this source published?

WHY

Why do you think this source was published?

OTHER

Please add your own personal comments to help you with this source later in your project.

After students have saved their sources and evaluation comments through the Notebook,
they can review their work on the Investigate page. On all of the InCredibility pages, a process
map is displayed along the top of the screen that highlights the student’s current stage in the
overall process through the stages of Investigate, Question, and Solve (see Figures 12, 13, and
14). A progress bar for each displays their current percentage completion of required quota of
tasks for that stage. This process map provides scaffolding for sensemaking by presenting a
visual conceptual organizer of the overall task process, and providing guidance to facilitate
planning and monitoring of the students’ current progress toward their quotas. If a student needs
to complete more tasks, an indicator is displayed (for example, “Find more sources”). Students
must complete a quota of actions before the end time of that stage (set by the instructor).
Figure 12. Investigate stage
Process map
Mapap

Progress bar
Saved website URL
and screenshot

5Ws responses from
Notebook

92

During the Question stage, students are prompted to evaluate the work of peers in
answering the 5Ws questions. A source donated by another student is randomly displayed, and
students review the quality of the responses entered about that source. For each of the 5W
questions, student agree or disagree with the entered responses, and explain why (see Figure 13).
These question prompts provide scaffolding through for articulation and reflection by providing
guidance for reflection, monitoring, and self-reflection on the student’s own performance. The
visual presentation of the two evaluations side-by-side also facilitates students’ self-reflection by
comparing their own evaluations to that of other students.
Figure 13. Question stage (part 1)

Question

5W tabs corresponding
to Notebook
Other student’s
responses
This student’s
evaluation of responses
Website screenshot

Each source is reviewed by multiple players. The evaluation questions in this stage provide
elaborative prompts to guide student in articulating their thoughts and elicit explanations of their
judgments.
At the end of the Question stage, students were instructed that the 5Ws questions that
they have learned correspond to the expert terminology of Authority, Relevance, Reliability,
Currency and Purpose (see Figure 14).

93

Figure 14. Question stage (part 2)

The text of these prompts scaffold students understanding of credibility criteria from their
original knowledge to a more advanced level (see Table 10).
Table 10. 5Ws correspondence to expert terminology
Clues about WHO wrote this information tell you about AUTHORITY, or if the author is qualified to
write about the topic
Clues about WHAT kind of information it is tell you about RELEVANCE, or if the information is useful
for your topic
Clues about WHERE this information comes from tells you about RELIABILITY, or if the information is
trustworthy
Clues about WHEN this information was written tells you about CURRENCY, or if information is
current, and whether currency is important
Clues about WHY this information was written tells you about PURPOSE, or if the site shows bias that
may influence the information

This introduction of expert terminology connected to the novice terminology of the 5Ws
provides scaffolding for sensemaking by bridging students’ knowledge from a simple conceptual
level to expert level. These definitions also appear in the Control Groups’ online worksheet, to
allow for comparison between groups.
94

During the Solve stage, students compare multiple sources and evaluate them. Through
comparing sources and weighing the relative importance of each criteria to the overall usefulness
of the source for their research questions, students learn that credibility is a multi-faceted
concept, and not a simple yes-or-no proposition. For each of the credibility criteria (in the
higher-level terminology introduced in the Question stage) they elect which of two side by side
sources are the best for the research topic (see Figure 15).
Figure 15. Solve stage
Two sources with
evaluations

Student votes on
which source is best
for each criteria

Student makes overall
assessment

These comparative questions ask the student to use critical thinking about their sources,
and determine for each individual criteria which of the two sources they think would be best, and
then to make an overall evaluation of which source is best for the research question. This step is
intended to help students understand that credibility is composed of multiple criteria, that some
criteria may be more important for some research topics than for others, and that an overall
evaluative judgment about the quality of sources should be made based on specific evidence.
After the student completes their quota of comparisons, they are presented with reflective
prompts (not shown in screenshot) that guide them through the metacognitive process of
reviewing and articulating their evaluation process and assessing their own evaluation decisions
95

(adapted from Herring, 2011 and Ge, Planas & Er, 2010). Table 11 lists the text of the reflective
prompts.
Table 11. Reflective prompts
Looking back on the process of evaluating the credibility of online information....
1. How did you decide whether a webpage was credible or not?
2. What specific criteria did you use to help you evaluate credibility?
3. What strategies did you use to evaluate credibility?
4. How confident were you in your evaluations of credibility?
5. What have you learned about evaluating credibility on the web?
Responses were open-ended text boxes, allowing students to articulate their own reflections on
their learning. These reflection prompts encourage students to engage in a self-monitoring
process, and promote reflection on the learning process at a meta-level that students do not
generally consider.
This step of the Question stage completes the InCredibility evaluation process. Once
students meet their quota, they recevie a congratualitions message, and then are able to review all
the evaluated sources.
3.3.2.	  Initial	  evaluation	  of	  prototype	  
During the design stages, the initial working prototype was informally evaluated by a
doctoral student from the School of Information and by a group of school media librarians
(SMLs) who have experience with IL instruction in K-12 schools. These evaluations were not
full pilot testing sessions, but presentations to experts who could provide informed feedback to
shape the design, and help detect potential problems. The doctoral student specializes in humancomputer interaction and provided feedback on the usability and understandability of the
prototype. This review produced several suggestions to modify the tutorial to include more
direction and clarification, as well as the recommendation to test the process of clicking on
elements of a webpage to answer the credibility questions, to see if students understand the
process and are able to perform it.
96

The SMLs strongly approved of the content and approach of the prototype, but they
expressed concern at the idea of having to use an additional technology in the classroom which
requires setting up and logging in. The SMLs were concerned about integrating the tool into their
teaching, and expressed preference for a tool that would be integrated into an existing learning
management tool such as Moodle. They also expressed concern about the use of the Firefox
browser, which is often not permitted by high school IT departments. This feedback from the
SMLs suggests that testing or using the InCredibility in high school classrooms may be
impossible. It also raised the possibility of incorporating the InCredibility tool into the existing
UM LMS, CTools, as it is open source and can be modified using the Learning Tools
Interoperability (LTI) specification. Doing so would make InCredibility instantly implementable
in any UM classroom that uses CTools, and potentially to any other LMS that employs the LTI
framework. The researcher hopes to be able to implement this functionality in the final version.

3.4.	  Pilot-­‐testing	  
The IC tool prototype was pilot-tested as part of the DBR process, before the final fullyfeatured tool is completed and ready for experimental testing. The focus of the pilot testing was
on assessing the functionality and understandability of the prototype tool and its interface, and
gathering feedback from subjects on their experience using the prototype. IRB exemption of the
pilot test was secured.
Pilot testing of the working prototype consisted of two phases, online and in-person:
1. Online pilot test (tutorial)
a. Usability questions
b. Content questions
c. Metacognition test
2. In-person pilot test (working prototype)
a. Walk-through
b. Think aloud
The online pilot test consisted of students taking the tutorial section of the online
credibility evaluation tool, and then answering survey questions about the tutorial’s usability and
its content, followed by completing an online metacognition test and answering questions about
97

the test’s understandability. The in-person pilot test consisted of students completing a
walkthrough of the complete working prototype, giving their “think aloud” commentary and
answering the researcher’s questions about its usability. The demographics of subjects in both
groups are listed below, and summaries of findings for both pilot tests.
3.4.1.	  Tutorial	  usability	  questions	  	  
  

In November 2012, students in a large, introductory undergraduate course were invited to

participate in an online pilot test, which consisted of completing the online tutorial, a survey
about its usability, and a trial of an online metacognition test. IRB exempt status was secured.
Students were offered extra credit in the class for participating. Fifty-six students completed the
online pilot test. Background demographic data was collected via the online survey, with the
distribution being sophomores (35%), followed by juniors (31%), freshmen (22%) and seniors
(13%). This mix of students is to be expected in an introductory level course that is not a
freshman requirement. The few seniors represented likely are only taking the course to meet
credit requirements. Subjects were asked to report their level of experience with searching for
information on the Internet. A majority (62%) reported “average experience,” followed by
“above average experience” (29%) and “a little experience” (9%). No subjects reported “not at
all experienced.” Again, this is an expected distribution, with most students self-reporting as
average with a smaller proportion showing greater confidence in their skills.
Next, subjects were asked “Have you previously received any formal instruction in
Information Literacy skills (library research skills, bibliographic instruction)?” The majority
responded “Yes” (60%) while 40% responded “No.” This level of previous training was higher
than expected. As a follow up, subjects were asked "If “Yes," where did you receive this
instruction?” For this question, 63% responded high school and 31% responded college. Two
subjects responded that they had training in both high school and college. When asked to
describe what they learned from this Information Literacy instruction, responses ranged from
“How to research scholarly articles and books” to “I learned more about analyzing information
on web pages, its legitimacy, and where the information came from as well as the content of
information and its relevance to my topic at hand (whatever that may be).” Thus, the range of
background and experience with IL training in this subject population was very broad.

98

The usability survey itself asked the following questions about the students’ experience
using the online tutorial:
•
•
•
•
•
•
•

Q6: Were the instructions for the tutorial clear?
Q7: Were there any questions on the tutorial that were confusing?
Q8: Were there any questions you had trouble answering?
Q9: Did anything in the tutorial not work the way you thought it would?
Q10: Did you use the Tip link that was available for each question?
Q11: If you were going to improve this tutorial, what would you change?
Q12: Please add any other comments you may have about the questions in this section.

All questions had a Yes/No response option and a prompt to add open text comments. A
summary of the responses are shown below:
•

•

•

•

•
•

Q6 Summary: 80% of responses indicated that the instructions for the tutorial were clear,
although the negative responses to this survey question showed that the introductory
instructions were not detailed enough.
Q7 Summary: A slight majority of participants (52%) indicated that they were not
confused by any questions, although nearly half indicated that they were confused. These
responses frequently mentioned that “Where” and “Why” were the most confusing
questions. This result supports the findings of Myhre (2012) that students have difficulty
evaluating Accuracy and Purpose for a website. Some students were also unfamiliar with
the About link on websites.
Q8 Summary: A majority of participants (55%) of the respondents indicated they had
trouble answering questions. Responses were similar to responses to Q2. While the
intention in this question was to explore any conceptual difficulties students may have
had in answering questions, the responses were mostly about technical issues. Some
students had difficulty with clicking on the correct answer fields, possibly due to the
small size of some targets.
Q9 Summary: A majority of respondents (75%) indicated that they did not experience
anything not working the way they thought it would. This suggests that the overall
concept and functionality of the tutorial worked effectively. As with Q3, some students
commented that they felt they had clicked the right answer but received an incorrect
response.
Q10 Summary: Over half of respondents (52%) did not use the tip button. Some students
did not even notice it.
Q11 Summary: Several students suggested better explanations of terms, including
definitions in the question prompts, and adding a review at the end of what has been
covered as reinforcement.
99

The comments on the tutorial showed a wide variety in responses from the participants.
Some students found it easy, while others struggled with the definitions and instructions. This
underlies the difficulty of building a learning tool that meets the need of a wide range of
students. Since some of the students had previously received IL instruction while others had not,
it may be difficult to keep students engaged. The participatory elements of the learning tool keep
the higher-skills students engaged, while the tips and help functions meet the needs of lowerskilled students.
Specific improvement to the tutorial that resulted from the findings of this pilot test
include:

•
•
•
•
•
•

Make introductory instructions more detailed
Add more explanation and examples of the 5Ws questions
Make the target areas of fields as larger and easier to click; also add a visual change to
the pointer when mousing over a target field
Make the Tip button more visible in size and color
Add the tip text directly to the 5Ws questions to help remind students what they are
looking for
Add a review at the end of what has been covered as reinforcement

3.4.2.	  Tutorial	  content	  questions	  
The next set of questions addressed the content of the tutorial, to investigate students’
understanding of online information evaluation:
•
•
•
•
•
•
•
•

Q12: How do you identify WHO wrote the information on a web page?
Q13. Beyond just the author’s name, how can you find out more information about the
author’s background and qualifications?
Q14. What other techniques can you use to evaluate the authority of this information?
Q13. Beyond just the author’s name, how can you find out more information about the
author’s background and qualifications?
Q14. What other techniques can you use to evaluate the authority of this information?
Q15. How do you determine WHAT main ideas a web page covers?
Q17: What other techniques can you use to evaluate the relevance of this information?
Q18. How do you identify WHERE the web page is hosted or published?
100

•
•
•
•
•
•
•

Q19. Beyond just the host/publisher’s name, how can you find out more information
about their background and qualifications?
Q21: How do you determine WHEN the information on a web page was posted?
Q22. How do you decide if the information is current enough for your topic?
Q23. What other techniques can you use to evaluate the currency of this information?
Q24. How do you determine WHAT the purpose of a web page is?
Q25. How do you decide if the information may be biased?
Q26: What other techniques can you use to evaluate the purpose of this information?

A summary of the responses to the tutorial content question are shown below:
•

•

•

•

•

•

•

Q12 Summary: Locating an author’s name is one of the easiest tasks in the tutorial. Only
a few respondents indicated awareness that the author may not always be listed, or may
not be an individual.
Q13 Summary: Several respondents indicated awareness of using Google to find out
more about an author’s background and credentials. This is a strategy that was not
included in the tutorial.
Q14 Summary: Respondents seemed unclear on the meaning of “authority” in this
context. They generally applied it to the overall site, rather than the author’s expertise in
the specific topic. Several mentioned relying on the domain name as an indicator of
authoritativeness. This is one of the higher level credibility terms that are introduced
through the prototype, so students will have a better understanding once they have used
the tool.
Q15 Summary: Respondents generally indicated a cursory approach to scanning an
article’s title, first paragraph and headings. While this is not a bad practice, there was
little indication of actually scanning the content of the webpage or synthesizing any
judgment of the entire contents. This may indicate satisficing by students when
evaluating information.
Q16 Summary: Determining usefulness also seemed to be a quick judgment for most
respondents. Only a few respondents mentioned actually reading the content and
assessing its usefulness for their research topic and argument, another indication of
satisficing.
Q17 Summary: Again, only a few respondents mentioned actually reading the content
and assessing its relevance for their research topic and argument. Several respondents
mentioned the presence of matching keyword, which in itself does not insure relevance to
a topic. Some respondents mentioned relying on other peoples’ comments on the article.
A few mentioned relying on intuition.
Q18 Summary: Most respondents relied on the URL and domain name to determine the
source of the information. Only some mentioned investigating the hosting or sponsoring
organization behind the site.
101

•

•

•

•

•

•

•

•

  

Q19 Summary: Some respondents indicated awareness of looking at the homepage for
more information about the site host or sponsor, and some mentioned the About page
(although this may have been a result of having taken the tutorial).
Q20 Summary: Responses to this question covered a wide variety of answers without
much agreement, perhaps suggesting that reliability may be an unfamiliar or unclear
concepts for students. Some mentioned checking sources, and some mentioned checking
for bias.
Q21 Summary: Checking the date of a webpage is very easy, along with finding the
author’s name. Most respondents were very clear about checking the date, and some
relied on it as a major indication of credibility, usefulness or relevance.
Q22 Summary: Most students indicated they just wanted the most current or recent
information, while some mentioned the topic itself as determining whether up-to-date
information is required
Q23 Summary: Some students mentioned comparing the information to other sources.
Some mentioned checking for recent comments or tags. Few students mentioned looking
for a “last updated” notice or copyright date.
Q24 Summary: There were many mentions of using the About page to determine
purpose. This is interesting because in the in-person testing (below), few of the subjects
were familiar with the About section of a website or had ever used it. It is unclear if
participants in the survey learned about this feature of websites from the tutorial, or were
familiar with it prior. Few responses mentioned identifying a specific purpose for a site
(educational, commercial, research, etc). Few responses mentioned determining the type
of source (blog, news, scholarly journal). IMore specific criteria for determining purpose
were added to the Notebook.
Q25 Summary: Many participants mentioned opinion or and objectivity. Several
mentioned language specifically as a criteria for judgment of bias. The responses suggest
a general awareness of evaluating bias as a recognized part of credibility judgment. This
criteria was reinforced through using the tool.
Q26 Summary: There were a variety of responses to this question without a general
theme. Some mentioned looking for bias, others mentioned language. A few mentioned
investigating the purpose of the hosting website, which is one of the strategies that the
tool helps to reinforce.
Participants gave a wide variety of answers to most questions, demonstrating varying

levels of skill and awareness of IL concepts. Most students do not show awareness of strategies
for evaluating sources, but rather use simple techniques such as skimming an article or Googling
the author. Most of the specific strategies mentioned in the responses were quick, perfunctory
heuristics that suggest students satisficing in their evaluations of credibility. Some mentioned
102

simply relying on “gut instinct” or intuition when making judgments of credibility. The IC tool
meets this need by introducing students to a step-by-step process of credibility evaluation
structured around a series of questions and the specific evidence they should use in making
evaluation judgments.
Searching the web to verify information was a very common response. Many students
mentioned relying on comments or “shares” of articles as a measure of credibility, relevance, or
currency. Asking friends or using social media to verify information were also mentioned,
clearly indicating that these students are in the Net generation. There were many mentions of the
About page in the content questions. Since some students indicated being unaware of this site
feature earlier in the survey (and during the in-person pilot, below), it is possible that this may be
one strategy that students learned from the tutorial.
This section of the tutorial pilot test did not produce specific changes to the prototype, as
the focus was on student learning of the content and their understanding of online information
evaluation.
3.4.3.	  Metacognition	  test	  	  
After subjects completed the questions regarding the tutorial, they were also asked to
complete the metacognition test developed by Raes et al. (2012), as this test was being
considered for use in the study at the time of the pilot testing. The aim of the pilot test was to
establish if students found the test understandable and if it was appropriate for use in the
experimental study. The 30-item test measures how students use metacognitive strategies when
searching for information online, and has been experimentally validated. Questions address
strategies for information problem solving strategies with questions such as “I ask myself
periodically if I am meeting my goals” and “I try to use strategies that have worked well in the
past.” Students were asked to respond on a 5-item Likert scale from ‘Strongly Disagree’ to
‘Strongly Agree’ for each of the questions (see Appendix 6 for full results of this survey). The
statements that received the highest agreement (averaging between “Agree” and “Strongly
Agree”) were: “I learn more information when I am interested in the topic” and “I try to use
strategies that have worked well in the past.” The statements with the lowest agreement
(averaging between “Disagree” and “Neither Agree or Disagree”) were “I think of several
strategies and choose the best one” and “I know how well I did after I finish.” While the results
103

of the metacognition test were not the focus of this pilot test, the results are interesting as an
example of its application. Generally, students reported little metacognitive awareness of their
online credibility evaluation practices.
After responding to the metacognition test questions, subjects were asked about the
understandability of the metacognition test questions. When asked “Were there any questions
that were confusing?” 95% percent of respondents answered “No.” Only 3 respondents who
answered “Yes” provided examples of specific questions that they found confusing. When asked
“Were there any questions in this section that you had trouble answering?” 87% answered “No.”
Of the 7 who responded “Yes,” several responses cited questions about using particular
strategies. Subjects were then asked “Please add any other comments you may have about the
questions in this section.” Although they had indicated generally that the questions were not
confusing and that they did not have trouble answering them, many respondents indicated a
different concern on Q12: many of the respondents indicated that the questions were repetitive
and similar, and that the survey felt too long. They also reported that the context of the questions
also did not relate directly to the evaluation tasks that were covered by the tutorial. As a result of
these responses, given the researcher’s concern about the understandability of the test questions
and the length of the question inventory, the decision was made not to use this metacognition test
in the experimental study. A simpler, shorter and more understandable test of metacognitive
awareness specifically related to the context of online credibility evaluation was developed (see
Section 4.4.2)
	  
3.4.4.	  Prototype	  pilot	  test	  	  
  

In the fall semester of 2012, students in a large, introductory undergraduate course were

invited to participate in an in-person pilot test, which consisted of a guided walkthrough of the
entire prototype. Students were offered extra credit in the class for participating. Eight students
completed the in-person pilot test. The test took about 45 minutes to complete. IRB approval was
secured. Background demographic data was collected via a manual form. The majority of
subjects were sophomores (50%), followed by juniors (25%), with freshmen and seniors tied at
12.5%. This distribution closely parallels that of the online pilot test. Subjects were asked to
report their level of experience with searching for information on the Internet. A strong majority
(75%) reported “above average experience” followed by “average experience” (25%). No
104

subjects reported “a little experienced” or “not at all experienced.” This distribution is skewed
higher than the online survey, and may reflect the self-selecting nature of students who were
willing and interested in volunteering for the study.
Next, subjects were asked “Have you previously received any formal instruction in
Information Literacy skills (library research skills, bibliographic instruction)?” A strong majority
responded “Yes” (75%) while 25% responded “No.” As a follow up, subjects were asked “If
"Yes," where did you receive this instruction?” For this question, 84% responded high school
and 16% responded college. Again, these distributions skewed higher than the online survey
demographics. While a majority of students responded that they had received IL training in high
school, although several students mentioned that the training they received in high school was
very simple, consisting of a one-shot session about types of sources that are credible or not.
Several mentioned that librarians and teachers told them “Don’t use Wikipedia.” Few responses
indicated that the students had any repeated practice of the skills or had applied them in the
actual setting of online information searching.
The testing sessions were audio recorded and screen capture recorded. Participants were
asked to “think aloud” as they completed the tasks. After the research gave an overview of the
project and introduced the IC tool, the students proceeded to use the tutorial and tool at their own
pace. I answered questions and addressed technical issues as necessary, but tried to give as little
guidance as possible. (The text of the pilot test questionnaire is included in Appendix 6). Several
repeated themes emerged from the in-person tests:

•
•
•
•
•
•
•

Most participants liked the functionality of the Notebook, the structure of the 5Ws and
the sequence of stages
Some students suggested that there should be a video intro to both the tutorial and the
tool functionality (the Notebook and the three stages).
Most participants automatically searched for more information about the author without
being prompted, reinforcing the findings from the online survey
Most participants said that they needed more and clearer instructions and definitions.
Most participants didn’t use the tips, or some did not notice the button at all
Some participants didn’t read the text of the tutorial feedback (which reinforce the tips),
just clicked to close the feedback box immediately
Some participants had trouble noticing when a new tutorial question appeared, due to the
speed
105

•

•

•

•

•

Several participants needed clarification that the comments they evaluated in the
Question section came from other students’ responses to the 5Ws questions in the
Notebook.
Most participants liked the comparison of two sources side by side in the Solve stage.
This seemed to be a new concept that they had not thought of before: the relative quality
of different sources on the same topic
Some participants were confused by the task in Solve. They were unsure if they were
judging based upon the comments added by other students or on their own judgment of
the quality of the source
Some participants had difficulty with the terminology “keywords,” which one student
defined as terms assigned by the author or site to an article. He indicated that “main
ideas” was a more familiar terminology for concepts determined by the reader that
summarize the content. Other subjects had difficulty with the term “source”
Few of the subjects were familiar with the About section of a website or had ever used it
On several occasions I pointed this out in the Tip text and participants had not seen it

Specific improvement to the tutorial that resulted from the findings of this pilot test include:
•
•
•
•

•
•
•
•

Make instructions clearer
Add a video introduction to the tutorial and the tool
Improve visibility of the Tip box
Move the placement of the “OK” (close) button on the feedback box to the bottom rather
than the top, to encourage students to read the feedback test and not just click to close the
box immediately
Slowing the rate that the feedback box appears, and the rate that new questions appear
clarification that the comments they evaluated in the Question section came from other
students’ responses to the 5Ws questions in the Notebook.
Clarify the task in Solve – judging based on their own judgment of the quality of the
source, not the other players’ comments
Change terms “keywords” to “main ideas” and “source” to information”

During this pilot test, a great deal of time was spent on the use of the Notebook, which generated
a number of useful improvements to the tool. Based on suggestions and feedback, the question
prompts in the Notebook were revised and expanded. Table 12 lists the revised prompts.
Table 12. Revised Notebook question prompts
Tab

Prototype Question

Revised Questions

106

WHO

Who is/are the author(s)

Who is/are the author(s)?

of this source?

What are the author’s qualifications?
What can you find out about their background?

WHAT

What 3 keywords best

What are the main topics?

describe this source?

What type of site is it?
(commercial/educational/governmental/news/opinion/scholarly)
How useful is this information for your topic?
(A little/Somewhat/Very)

WHERE

Where was this source

Where is this site hosted or published?

published?

What is the site’s domain name? (.com, .edu, .gov)
Are there links to supporting evidence?

WHEN

When was this source
published?

When was this webpage published or copyrighted?
Has it been updated?
How important is having current information for your topic? (A
little/Somewhat/Very)

WHY

Why do you think this
source was published?

COMMENT

  

Why do you think this site was created? (to educate/inform/
persuade/sell)
Do you see evidence of bias? (Yes/No/Not sure)
If yes, what is the evidence?
Deleted. Replaced with Home button linking to IC homepage

This pilot test reinforced the importance of clear and specific instructions and definitions.

Some students suggested that there should be a video intro to both the tutorial and the tool
functionality (the Notebook and the three stages). Rather than reading instructions these students
seem to like being shown how to do it, which supports the findings of the literature about Net
generation students. One student commented “video tutorials help me a lot.”
Overall, the participants responded favorably to the experience of using the tool and
understood its purpose. There did not seem to be any major issues with the functionality of the
Notebook, the concept of the 5Ws, and the sequence of stages. Several mentioned that they
found the tool to be effective and useful. Most participants liked the experience of comparing
two sources side by side in the Solve stage. One student mentioned that critiquing other people’s
work (as in the Question stage) helps him learn how to evaluate better.
  

One highlight of the pilot test was this very positive feedback:
107

“I really like it because it makes me go over the source, the text, really well. It
forces me to look for more information about the source and about the author, what
he’s really talking about. I think it’s a great way… because usually if I don’t do
this, I’ll probably skim over the… what he’s talking about, and probably jot down
what he bolded. So it really helps me consolidate a more specific idea of why I
would use this source and what information I’ll pull out of it… I think it really
helps me focus on an article more, instead of just skimming it. It definitely takes
more time but I feel like I’m getting so much more than what I would just do on a
skimming basis. It’s a more efficient way of extracting information from an article
than coming back and re-reading stuff again... (It’s) a fun way to approach an
instructional thing to do during class, ‘cause it’s on the Internet so I feel you’re
more engaged than if it were through a presentation.”(S4)

3.5.	  Chapter	  summary	  
  
  

This chapter described the design and development of the current prototype IC tool,

including the design methodology, theoretical models, initial design documentation, and the
creation of the current prototype. The chapter also discussed the online pilot test of the tutorial
and the in-person pilot test of the complete prototype. Results from the tutorial pilot test and
survey responses provided valuable feedback on the usability of the tool, including specific
questions that were added to the Notebook. Overall, the participants in both pilot tests responded
favorably to the experience of using the tool and understood its purpose. There did not seem to
be any major issues with the functionality of the Notebook, the concept of the 5Ws, and the
sequence of stages. Some specific suggestions from the feedback were incorporated into the
design of the IC tool. Several subjects mentioned that they found the tool to be effective and
useful, with most subjects expressing positive reactions to the skills practice they experienced
using the tool. This generally positive feedback, along with identification of specific
improvements to be made to the tool, provides great motivation for moving ahead with the
project.

108

Chapter	  4:	  Research	  design	  
This chapter provides an overview of the plan of research. Section 4.1 describes the
general research objective and the specific research questions addressed. Section 4.2 describes
the experimental design of randomly-assigned treatment and control groups, and the activities
carried out by subjects in the two conditions, and section 4.3 describes participant recruitment.
Section 4.4 describes the methods of data collection and the plan for data analysis is discussed in
Section 4.5.

4.1	  Purpose	  and	  research	  questions	  
Although there is a significant amount of literature on case studies of IL training there is
little empirical research on its effectiveness, beyond pre/post-tests and outcomes assessment
(Rockman, 2002; Orme, 2004). There has also been little research on the application of
scaffolding and metacognitive support to teaching students IL skills (Gorrell et al., 2009). These
gaps in the literature are addressed by the research through conducting an experimental study.
The purpose of the research is to investigate the effects of scaffolding and metacognitive
support on student learning of online credibility evaluation skills. The study tests if the IC tool
incorporating scaffolding and metacognitive support increases students’ knowledge of the expert
criteria that constitute credibility evaluations, the evidence-based source characteristics used in
making credibility evaluations, and the metacognitive strategies used while evaluating online
information. The expert credibility criteria that students will learn are the concepts of authority,
relevance, reliability, currency, and purpose (based on a scaffolded model of “who, what, where,
when and why”) and their definitions. The evidence-based source characteristics students will
learn to examine are evidence used for credibility evaluations such as author credentials, main
ideas, references/links, site domain, contact information, date, and About and purpose
statements. The metacognitive strategies students will learn are increased use of planning,
109

monitoring, and reflecting on their evaluation practices (see Section 4.5 for measurements of
these outcomes). Based on these objectives, the research questions addressed by this study are:
RQ1: Do students who use the online credibility evaluation learning tool demonstrate
greater understanding of expert credibility criteria in the process of evaluating
online sources compared to groups of students who use a tutorial and an online
form, or those who use only an online form?
RQ2: Do students who use the online credibility evaluation learning tool demonstrate
greater application of evidence-based source characteristics as the basis for their
credibility evaluations compared to groups of students who use a tutorial and an
online form, or those who use only an online form?
RQ3: Do students who use the online credibility evaluation learning tool demonstrate
greater metacognitive awareness compared to groups of students who use a tutorial
and an online form, or those who use only an online form?
These research questions were examined through an experimental study of college
undergraduates using two treatment groups and a control group to compare the performance of
subjects using the IC tool to the performance of subjects on similar tasks without the use of the
tool. The Treatment 1 (T1) group completed the online tutorial and use the three-stage IC tool to
conduct credibility evaluations, the Treatment 2 (T2) group completed the tutorial without using
the tool and using an online form to conduct credibility evaluations, and Control Group (CTRL)
used only the online form to conduct their credibility evaluations. After completing their
treatment/control group activities, all subjects completed two post-tests: a credibility criteria test
and a metacognition test (see sections 4.3.1 and 4.3.2).
Based on these research questions, it was hypothesized that the IC tool, with its step-bystep structured learning process that enlists scaffolding and question prompts, would be more
effective than the tutorial or online form methods. Specific hypotheses derived from the research
questions are:

•

H1: The Treatment 1 group will demonstrate greater use of specific credibility criteria
compared to Treatment 2
110

•

H2: The Treatment 1 group will demonstrate greater use of specific credibility criteria
compared to the Control group

•

H3: The Treatment 1 group will demonstrate greater use of evidence-based source
characteristics compared to Treatment 2

•

H4: The Treatment 1 group will demonstrate greater use of evidence-based source
characteristics compared to the Control group

•

H6: The Treatment 1 group will demonstrate greater metacognitive skills compared to the
Treatment 2

•

H7: The Treatment 1 group will demonstrate greater metacognitive skills compared to the
Control group

Both qualitative and quantitative data collection methods were used in the experiment.
Qualitative data included student responses entered to the Notebook credibility evaluation
prompts, evaluation comments on other students’ sources, and students’ responses to the
credibility criteria and metacognition post-tests. [Note: Quantitative logfile data was originally
planned to be used, however, limitations of time and resources made it impractical to capture
comparable logfile data for the T2 and CTRL groups, so this data source was not employed for
data analysis]. Table 13 summarizes the experiment’s data sources and analysis methods.
Table 13. Data analysis methods
Research question

Data source

Analysis method

RQ1

Credibility criteria responses

Coding and statistical

RQ2

Reflective prompts

Content analysis

RQ3

Metacognition post-test results

Scoring and statistical

Data collection is discussed in section 4.4 and data analysis is discussed in section 4.5.

	  

111

	  
4.2.	  Experimental	  design	  
This experiment tested the IC tool as a support for students’ online information
evaluation as part of a class research project. The experiment reproduced as much as possible the
real life conditions under which students would use the tool for academic research, instead of
testing in the artificial setting of a lab. This section describes the study’s experimental design, the
activities carried out by subjects in the three conditions, and the measurements used to identify
outcomes.
While many LIS studies use a pre-test/post-test research design, such design has poor
internal validity (Mitchell & Jolley, 2004). It is not a true experimental design, since only one
group of participants is tested, administered a treatment, and retested. The pre-test/post-test
design does not use randomized assignment of subjects, since there is only one group of subjects.
Because of this lack of randomization and experimental groups, the internal validity of this
design is compromised by the testing effect, in which subjects are primed on the nature of the
treatment by the pre-test, which may influence their performance on the post-test. In this design,
the influence of the pre-test cannot be separated from the effect of the treatment, since
improvements in scores may simply be the result of practice on the pre-test. In conditions where
randomized assignment is not possible, a pre-test/post-test design is acceptable. However,
randomized experiments are the best technique for determining which educational practices work
and for comparing the relative benefits of different treatments (Cook & Singha, 2006). In a true
experimental design with randomized assignment to treatment and control groups, the threat of
the testing effect is removed and internal validity of the design is improved, allowing for a more
reliable determination of the treatment’s effect (Mitchell & Jolley, 2004). Internal validity
insures that the treatment actually caused the measured effect, strengthening the quality of
statistical analyses used to evaluate the results (Shadish, Cook & Campbell, 2002). Causal claims
from randomized experiments are more credible than from other designs (Cook & Singha, 2006).
The use of randomized samples to ensure data accuracy and generalizability is needed in IL
studies (Metzger, Flanagin & Zwarun, 2003). For these reasons, the research design employs
randomized assignment of subjects to treatment and control conditions and uses a post-test only
design.
112

The T1 group completed the online tutorial and used the IC tool, consisting of the
browser-based Notebook and the website containing the three-stage evaluation activities
(described in Chapter 3). These subjects received the tool’s scaffolded guidance through tips and
question prompts and gain repeated practice in a step-by-step process of online credibility
evaluation. They received metacognitive support through the use of process maps, progress
monitors, and reflective questions, which helped them plan, monitor and reflect on their learning.
Since there can only be one control group in an experimental design, the group which used only
the tutorial but did not use the IC tool was designated as “Treatment 2” since they received a
partial intervention. T2 group subjects completed the same online tutorial as the T1 group but did
not use the IC tool. Instead, they used a static online form to answer the 5Ws questions about
their sources. The form provides the same credibility questions and criteria as the complete tool,
without the tips, prompts, or scaffolds. The form also provides the same introduction to the
higher-level credibility terminology from Part 2 of the Question Stage to provide equivalent
learning content (see Appendix 7: Online Form for Control Groups). Control group subjects did
not take the tutorial or use the IC tool, but only used the static online form to answer the 5Ws
questions about their sources. Due to the random assignment to condition, receiving identical
instruction, and completing equivalent tasks in an online learning environment, the T2 and
CTRL group subjects were hopefully unaware that they are experiencing different experimental
conditions from the T1 group.
All subjects worked online and received the same initial instructions, which reduced the
threat of treatment diffusion by minimizing the possibility of subjects realizing they were
assigned to different conditions, and increased the treatment integrity by providing quantifiable
evidence that all subjects performed similar tasks (Shadish, Cook & Campbell, 2002). Subjects
were randomly assigned to one of three groups: 1) the T1 group, which completed the tutorial
and use the three-stage IC tool to conduct credibility evaluations, 2) the T2 group, who
completed the tutorial but did not use the IC tool and instead used an online form to conduct
credibility evaluations, and 3) the CTRL group, which used only the online form (see Figure 16
for a diagram of the experimental design.) After completing their treatment/control group
activities, all subjects completed two post-tests: a credibility criteria test and a metacognition test
(see sections 4.1 and 4.2).
113

Figure 16. Experimental design

Three randomly-assigned groups of college undergraduates

Treatment 1

Treatment 2

Tutorial + tool

Tutorial + online form

Control Group

Online form only

Credibility and metacognition post-tests

The independent variable in this research design is exposure to the structured sequence of
scaffolded instruction and guided practice that constitutes the design of the IC tool. The
dependent variables are the students’ demonstrated understanding of the credibility criteria, their
use of evidence-based source characteristics to evaluate credibility of online information, and
their metacognitive awareness of the steps of the evaluation process.
The experimental study took approximately two weeks to complete, including subject
recruitment. Subjects in the T1 group were given two days to register and complete the online
tutorial, two days to complete each of the three stages of the IC tool, and two days to complete
the post-tests, for a total of ten days. Subjects in the T2 and CTRL groups followed the same
timeline, although they used the online form instead of using the three-stage tool. This timeline
was intended to equalize the tasks and study length between the three experimental groups. See
timeline in Figure 17 below:

114

Figure 17. Study timeline
2 days
(Tue-Wed)

2 days
(Thur-Fri)

Register &
complete tutorial

Investigate stage

2 days
(Sat-Sun)
Treatment 1 group
Question stage

2 days
(Mon-Tue)

2 days
(Wed-Thur)

Solve stage

Post-tests

Choose own
best sources

Post-tests

Choose own
best sources

Post-tests

Treatment 2 group
Register &
complete tutorial

Find & evaluate
sources

Review own
sources
Control Group

Register

Find & evaluate
sources

Review own
sources

Subjects completed the tasks outside of class on their own time using their own computers,
which allowed students to complete the required tasks at their own pace rather than under the
artificially constrained conditions of a laboratory experiment. This design was also intended to
recreate the conditions of an in-class assignment for which students would do their own online
research to find sources.
Subjects were randomly assigned using a random number generator to one of the three
experimental groups, using stratified assignment by year to insure equivalency in experience
level between the experimental groups. All groups were assigned the same research topic and
instructed to search the web for relevant sources on the topic “What is the effect of social media
on education?” This topic was relevant to the nature of the course in which subjects were
enrolled (see Chapter 5 for details on the final demographics and assignment of subjects).

4.3.	  Subject	  recruitment	  
Student subjects were recruited from a large undergraduate introductory course at the
University of Michigan (SI 110: Introduction to Information). This student population was
appropriate to the research goals because many are usually incoming freshman recently
graduated from high school, which is the target audience for the IC tool. A smaller percentage of
students in this class come from a mixture of other years in college, providing a variety of
subject backgrounds and skill levels. The class is large enough that an appropriate number of
115

subjects could be obtainable for each of the three conditions, with the desired goal of having 30
subjects in each group to ensure statistical validity and generalizability (Creswell, 2002). The
content of the class is broad and general, so specific knowledge or expertise is not required; this
is appropriate for this study because the IC tool is designed to be discipline-neutral. The research
topics in the class are generally related to current issues in information technology, and thus lend
themselves to online information searching.
Students were contacted via in-class announcements and through online messages
through the class Learning Management System. Potential subjects were offered extra credit in
the course for their participation in the study. They were encouraged to use their online research
as part of the study for their in-class writing assignments, to increase their motivation and the
relevance of the study.
IRB approval of the study was secured before the start of the experiment. An “exempt
from ongoing IRB review” status was received because the research was conducted in
established or commonly accepted educational settings, involving normal educational practices.

4.4.	  Data	  collection	  
All data in this study were collected online through automatic recording of all subject
responses to prompts and responses to post-test questions. Online assessments been shown to be
stronger predictors of learning outcomes than offline assessments such as questionnaires,
interviews, and self-report (Veenman, 2013). Correlations among online measures and learning
outcomes are higher than correlations among offline measures, suggesting that subjects do not do
what they prospectively say they will do and do not accurately recollect in retrospect what they
actually did (Veenman, 2013). There is also evidence that online assessments are more valid than
offline assessments of metacognitive skills (Veenman, 2013). Thus, online assessment measures
are used in this study.
Both quantitative and qualitative data was collected during this study, allowing for a
multimodal evaluation of the study outcomes. Qualitative data included student responses
entered to credibility evaluation prompts, comments on other students’ sources, responses to
reflective prompts, and definitions of the credibility criteria. Quantitative data on subject selfevaluation of their skills and metacognitive awareness was collected through the post-test. After
completing their treatment/control group activities, all subjects completed two post-tests: a test
116

of credibility criteria and a metacognition test. Both instruments were pilot tested prior to the
start of the experiment, and were modified as needed based on results from cognitive interviews
with pilot test subjects. The instruments are described below.
4.4.1.	  Credibility	  criteria	  test	  
The researcher originally intended to utilize a standardized IL skills test as a pre-test for
this study. There are several widely available IL skills test that are commonly used for
assessment of college students. Tests reviewed for this study are described in Table 14.
Table 14. Standardized IL tests
Name
Information Literacy Test
(ILT)

Developed by
James Madison
University

Description
60-item multiplechoice test

Standardized Assessment
of Information Literacy
Skills (SAILS)
iSkills
Research Readiness SelfAssessment (RRSA)

Kent State
University

45 questions (cohort),
55 questions
(individual)
14 tasks
50 tasks

ETS Corporation
Central Michigan
University

Source
www.madisonassessment.com/
assessment-testing/informationliteracy-test
www.ProjectSAILS.org
www.ets.org/iskills/about
rrsa.cmich.edu/twiki/
bin/view/RRSA/WebHome

All four of these tests are described as based on the ACRL Information Literacy Competency
Standards for Higher Education. ILT is described as reliable and validated (Cameron, Wise, &
Lottridge, 2007). These tests cover a broad array of topics from traditional bibliographic
instruction, including identifying magazines and journals, understanding call numbers, catalog
searching, and using library resources. The SAILS test groups the ACRL outcomes and
objectives into eight skill sets: Developing a Research Strategy, Selecting Finding Tools,
Searching, Using Finding Tool Features. Retrieving Sources, Evaluating Sources, Documenting
Sources, and Understanding Economic, Legal, and Social Issues. Many of the SAILS questions
are specifically related to libraries (i.e., “What are the best things to do when you need help with
library research?” with correct answers “Ask at the reference desk” and “Call the reference
desk”) and research skills such as using online catalogs and databases and understanding
citations. Evaluation of online sources is barely covered, although general questions about the
reliability of the Internet are asked. The iSkills test also focuses on information and
117

communication technologies (ICT) proficiency, such as web tools use, database management,
and typical office software. The RRSA’s website states that their objectives include explaining
the value of using libraries to students and motivating students to use libraries. Thus, much of the
material on these tests falls outside the scope of this study, while minimally covering the critical
evaluation of online sources, if at all. Additionally, the length of the questionnaires and the time
commitment for students to complete them made them prohibitive for this study. All these tests
are also fee-based and require subscription or per-user fees, making them cost-prohibitive. As a
result, the researcher determined that these assessments were not appropriate for this study, and
that a custom test needed to be developed that focused specifically on the credibility criteria and
evaluation skills that are the subject of the learning tool.
The custom credibility criteria post-test developed for this study asks 10 specific
questions about the criteria for evaluating the credibility of online information, based on the
expert terminology and definitions introduced in the Question stage (as opposed to the novicelevel terminology of the 5Ws). The open-ended questions ask students to demonstrate their
knowledge of both the criteria themselves and the strategies for evaluating each criteria. Table 15
lists the questions in the post-test.
Table 15. Credibility criteria post-test
1. How do you define the “authority” of information?
2. Why is it important to evaluate the authority of online information?
3. How do you define the “relevance” of information?
4. Why is it important to evaluate the relevance of online information?
5. How do you define the “reliability” of information?
6. Why is it important to evaluate the reliability of online information?
7. How do you define the “currency” of information?
8. Why is it important to evaluate the authority of online information?
9. How do you define the “purpose” of information?
10. Why is it important to evaluate the authority of online information?
The post-test was conducted as an online survey using the Qualtrics online survey software.

118

4.4.2. Metacognition test
As with the IL tests, several pre-existing metacognition tests were evaluated for use in
this study. The first test to be considered was developed by Schraw and Dennison (1994). The
Metacognitive Awareness Inventory (MAI) is a 52-item self-report questionnaire to assess
students’ knowledge about their own monitoring competence during a learning task (see
Appendix 8 for the complete inventory). The questions are specific to classroom-based
assignments (i.e., “I know how well I did once I finish a test” and “I know what the teacher
expects me to learn”). Subjects who took the MAI answered a series of questions by self-rated
their metacognitive abilities on100-point continuous scales from “Poor monitoring ability” to
“Excellent monitoring ability.” Students were also asked to rate their level of confidence in their
response to each question on a 100-point continuous scale from “0% confident” to “100%
confident.” Scores on pre-test confidence, test performance, and monitoring accuracy were
calculated. Monitoring accuracy was computed by taking the difference between each student’s
average confidence rating and their actual test score expressed as a proportion. This inventory
was found to reliably test metacognitive awareness (Schraw & Dennison 1991, p 464).
Subsequently, this inventory was modified by Raes, et al. (2012) to address online
information problem solving (i.e., “Once I finished searching the Internet, I asked myself how
well I had answered the information problem”). Because of the length of time necessary to
complete Schraw and Dennison’s original 52-item test, and to reduce the required effort from
test subjects, the researchers reduced the number of questions to 32 (see Appendix 12 for the
complete inventory). Responses were converted to a 4-point Likert scale with agree/disagree
responses instead of the original 100-point continuous scale, again to reduce the required effort
from test subjects. After the testing the question inventory, a one-way analysis of co-variance
(ANCOVA) was conducted with post-test scores as dependent variable, condition as independent
factor, and pre-test scores as covariate to measure differences in condition. The researchers
found that technology-enhanced scaffolding realized the highest learning gains.
When the Raes, et al. (2012) inventory was pilot tested, comments from students
repeatedly focused on the repetitiveness and length of the questions (see section 3.4.1). This
feedback suggests that the even longer 52-item Schraw and Dennison inventory would be even
more problematic for students. After reviewing these tests, it seemed that no test was available
119

that met the specific needs of this study, so a custom metacognition test was developed. A set of
12 questions was selected from the Raes et al. inventory, and the language of the questions was
customized specifically to the process of evaluating the credibility of online information. Based
on the metacognition literature, questions are conceptually grouped into three categories focusing
on planning, monitoring, and reflecting (see Table 16 below).
Table 16. Adapted metacognition test questions
Category
Planning

Monitoring

Reflecting

Question
I think about what information I need to evaluate the credibility of online
information
I ask myself questions about the topic before I begin evaluating the credibility
of online information
I think of several ways to find evidence for evaluating the credibility of
online information
I organize my time to best accomplish evaluating the credibility of online
information
I plan the steps of evaluating the credibility of information
I analyze the effectiveness of my evaluation strategies
I compare information from different websites when I evaluate them
I periodically review the evidence I find while evaluating the credibility of
online information
I try to find specific evidence to justify and support my evaluations
I try to look at the evidence from different perspectives when making
evaluations
I ask myself if there was a better way to find evidence after I finish
evaluating the credibility of online information
I ask myself if I found as much evidence as I could once I finish evaluating

Raes et al.’s (2012) model of 4-point Likert scale responses was employed (strongly disagree,
disagree, agree, strongly agree). This post-test was conducted as an online survey using the
Qualtrics survey software.

4.5.	  Data	  analysis	  
This study’s research questions were answered based on analyses of both qualitative and
quantitative data, which provided a multi-dimensional understanding of student behavior and
strengthened findings and conclusions regarding the impact of the online participatory learning
120

tool. Qualitative data (student responses entered in response to credibility evaluation prompts,
reflective prompts, and students’ final definitions of credibility criteria) were coded for the
presence of categories of credibility evaluations and sub-categories of evidence-based source
characteristics. Quantitative data based on scores for understanding the credibility criteria and for
the metacognitive post-test were conducted using the SPSS statistical software package.
The data analysis methods employed to answer this study’s research questions are
described below for each question:
RQ1: Do students who use the IC tool demonstrate greater understanding of expert credibility
criteria in the process of evaluating online sources compared to groups of students who
use a tutorial and an online form, or those who use only an online form?
For this study, “understanding” was defined as the ability to 1) accurately define the criteria and
2) articulate their importance. Subjects’ qualitative responses were scored based on a rubric
covering the components of each criteria’s definition and importance (see Table 17). The
highest-scoring responses demonstrated greater knowledge of the components of the expert
concept of credibility, their definition, and their importance.
Table 17. Coding rubric for student knowledge of credibility criteria
Criteria
Authority
Relevance
Reliability
Currency
Purpose

Definition
If the author is qualified to
write about the topic
If the information is useful for
the research topic

Importance
Anyone can post to the Internet, and
qualifications need to be verified
There is a lot of information on the
Internet, but you need to choose relevant
information for your research topic
If the information is trustworthy The sources of online information are not
always apparent, and need to be verified
If information is up-to-date
Up-to-date information is often most
accurate, although not for every topic
If the site shows bias
The purpose(s) of online information are
not always apparent, and may influence
its value

For example, a high-scoring answer to the test question “What does authority mean, and why is it
important to evaluate?” would be “Authority means that the author of the work has credentials
121

that show he/she is qualified to write about the subject. It’s important because anyone can post
information online, and you need to verify that they know what they are talking about.” A midscoring response would be “Authority means the author knows what he’s talking about. It’s
important to check if he has credentials.” A poor-scoring response would be “Authority means
who the author is. You should check their background.”

4.6.	  Qualitative	  Coding	  
In this study, students provided qualitative responses to open-ended question prompts in
the post-test. These responses were reviewed by coders and assigned numerical scores based on a
standardized coding rubric. The researcher developed the rubric for numerical scoring against
three levels of scoring based on demonstrated student comprehension of the topics. The three
levels of scoring equate to a "high/medium/low quality" quality level using a scale of 2/1/0 (see
complete rubric in Table 18 below). The rubric defines the terms or concepts that needed to be
present in the answer to achieve that score. A high quality answer (score of 2) showed clear
evidence that the student understood the concept by expressing that in the context of credibility
evaluation, “authority” means both identifying the author AND the author’s credentials or
qualifications. Responses did not need to use the specific language in the criteria as long as the
coder was confident that the student understand the concept. A medium quality answer (score of
1) showed some evidence that the student knows something about the concept by mentioning
either of these two elements of a definition, but didn't completely meet the criteria. These
answers sometimes required interpretation on the part of the coder. The coders were generous if
they felt that the student showed partial understanding. A low quality answer (score of 0) didn't
demonstrate any understanding of the question, and met none of the criteria.
Table 18. Coding rubric for student open-ended questions
Question
What does "authority" of
information mean?
Why is it important to evaluate
how authoritative online
information is?

Score
2
identify author AND
credentials/qualifications
anyone can post online AND
importance of verification

1
identify author OR
credentials/qualifications
anyone can post online OR
verification

0
neither
neither

122

What does "relevance" of
information mean?

content/topics are useful
/related to question

generic mention of
content/topics

neither

Why is it important to evaluate
how relevant online information
is?
What does "reliability" of
information mean?
Why is it important to evaluate
how reliable online information
is?
What does "currency" of
information mean?
Why is it important to evaluate
how current online information
is?
What does "purpose" of
information mean?

filter/select information
AND judge value

filter/select information OR
judge value

neither

trustworthiness AND
sources
Internet skepticism AND
need to verify/evaluate
sources
up to date/recent

trustworthiness OR sources

neither

Internet skepticism OR
verify

neither

time/when published (or
just "current")
useful/relevant/valid OR
depends on topic

neither

why the information was
written/published/presented
(goal/intent/bias)

generic mention of
"goal/intent/bias"

neither

Why is it important to evaluate
what the purpose of online
information is?

detect bias AND judge
value/credibility/usefulness

detect bias OR judge
value/credibility/usefulness

neither

useful/relevant/valid AND
depends on topic

neither

Responses for each question were scored according to this rubric for both the definition and
importance elements of each criteria. Table 19 below shows examples of the rubric applied to
potential response to the question “What does “currency” of information mean?” The first
example meets both of the requirements for the definition and receives a high score of 2. The
second example only mentions one of the criteria and receives a medium score of 1. The third
example meets neither criteria and receives a low score of zero.
Table 19. Potential responses and scores to definition responses
What does "currency" of information mean?

Score

The "currency" of information means that the information isn't outdated. Similarly to the
"relevance" of information, the "currency" means the information is current, pertinent, and
up to date.

2

The currency of information refers to the time a piece of information is created.

1

What type of information it is.

0

Table 20 below shows examples of the rubric applied to potential response to the question “Why
is it important to evaluate what the purpose of online information is?” The first example meets
123

both of the requirements for the explanation of importance and receives a high score of 2. The
second example only mentions one of the criteria and receives a medium score of 1. The third
example meets neither criteria and receives a low score of zero.

Table 20. Potential responses and scores to importance questions
Why is it important to evaluate what the purpose of online information is?

Score

By evaluating the purpose of online information, you can determine if there is a
bias to the source and how to then evaluate the source's credibility.

2

It is important to evaluate what the purpose of online information is because
one should make sure it is not biased.

1

The purpose dictates what kind of message the information is trying to send.

0

An overall score for understanding of the criteria was obtained by averaging the two
scores for the definition and importance of each criteria. Table 21 lists the pairs of answers that
were averaged together per criteria.
Table 21. Components of understanding score
Criteria
Authority

Component responses for understanding score
What does "authority" of information mean?
Why is it important to evaluate how authoritative online information is?

Relevance

What does "relevance" of information mean?

Reliability

Why is it important to evaluate how relevant online information is?
What does "reliability" of information mean?

Currency

Why is it important to evaluate how reliable online information is?
What does "currency" of information mean?
Why is it important to evaluate how current online information is?

Purpose

What does "purpose" of information mean?
Why is it important to evaluate what the purpose of online information is?

The average scores for understanding of each criteria were compared between groups to
determine differences in outcome between experimental conditions.

124

RQ2: Do students who use the IC tool demonstrate greater application of evidence-based
source characteristics as the basis for their credibility evaluations compared to groups of
students who use a tutorial and an online form, or those who use only an online form?
Subjects’ qualitative responses were scored based on a rubric covering the source characteristics
mentioned when describing the evaluation process. The highest-scoring responses demonstrate
knowledge of the specific evidence that should be examined when evaluating each criteria of
credibility. Specific components of a correct answer are listed in Table 22.
Table 22. Rubric for scoring post-test
Criteria
Authority
Relevance
Reliability
Currency
Purpose

Evidence
Author credentials, contact link
Main ideas, keywords, tags
URL/domain, host, references/sources
Copyright date, updates, importance of currency to topic
About link, purpose statement, advertising, biased language

For example, a high-scoring answer to the test question “How do you evaluate the authority of
online information?” would be “You can evaluate authority by looking for evidence such as the
author’s biography, credentials, and contact info to ask questions.” A mid-scoring responses
would be “You can evaluate authority by looking at the author’s credentials.” A poor-scoring
responses would be “The author should have a degree.” Subjects’ qualitative responses were
coded based on the credibility evaluation categories developed by Markey et al. (2014). This
model categorizes student credibility evaluations into three types: (1) Evidence-based, (2)
Projection, and (3) Intuition (see Table 23).
Table 23. Categories of credibility evaluations (Markey et al., 2014)
Category

Definition

Evidence-based

Cites specific evidence supporting evaluation

Projection-based

Speculates without evidence

Intuition-based

Makes unfounded assertions without evidence

125

These categories summarize the types of explicit justifications that students gave to support their
credibility judgments of sources. The Evidence-based category consists of justifications citing
specific criteria of the source, either internal factors such as author affiliation, date of
publication, length, format/genre, and cited references, or external evidence such as the database
or online repository from which the source was retrieved. The Projection category consists of
speculations on a source’s credibility, such as “well-written” or “not well-written,” that did not
cite specify criteria as evidence for their judgment. The Intuition category consists of broad
assertions without any supporting evidence, such as “seems credible enough” and “appeared
relevant.” (Note that these categories relate to Rieh’s 2002 model of evaluative vs. predictive
judgments.) Clearly, only the Evidence-based category shows reasoning and attempts to verify
credibility judgments through examining and evaluating characteristics of the source. This model
for categorizing the credibility judgments made by students, and the evidence cited to support
them, is a helpful tool in assessing the actual source evaluation practices of students.
For both RQ1 and RQ2 above, after coding was finalized, an interrater reliability analysis
using the Cohen’s Kappa statistic was performed to determine consistency between raters.
Cohen’s Kappa calculates that agreement between coders adjusted for that expected by chance. It
is the amount by which the observed agreement exceeds that expected by chance alone, divided
by the maximum which this difference could be (Landis & Koch, 1997).

RQ3: Do students who use the IC tool demonstrate greater metacognitive awareness compared
to groups of students who use a tutorial and an online form, or those who use only an
online form?
Scores on the Likert-scale metacognitive post-test (described in section 4.4.2) were analyzed
descriptively to compare the self-ratings between groups. Statistical tests were not appropriate
due to the similarly high scores from all subjects (see Chapter 5 for discussion of these results).
Responses were grouped by category of metacognition (planning, monitoring, reflecting) to
show differences between the groups in self-reported skills. Subjects’ quantitative scores on the
metacognitive post-test were automatically converted to a 4-point scale (strongly disagree = 1,
disagree = 2, agree = 3, strongly agree = 4). Subject scores were compared across groups for
evidence of different metacognitive awareness by treatment condition.
126

Note: Originally, subjects’ qualitative responses to the metacognitive prompts at the end
of the Solve stage (see section 3.3.2.) were intended to be coded for presence of key words or
phrases that show metacognitive awareness, based on the coding model in Ge, Planas, and Er
(2010) as either “superficial” or “deeper” reflection. However, based on the insufficient effort
from the subjects and lack of full responses, this analysis method was not used. See Chapter 5 for
discussion.

	  

127

Chapter	  5:	  Findings	  	  

This chapter describes the findings from the experimental study of the “InCredibility” tool
(IC tool) that were used to answer the following research questions:
RQ1: Do students who use the IC tool demonstrate greater understanding of expert credibility
criteria in the process of evaluating online sources compared to groups of students who
use a tutorial and an online form, or those who use only an online form?
RQ2: Do students who use the IC tool demonstrate greater use of evidence-based source
characteristics as the basis for their credibility evaluations compared to groups of students
who use a tutorial and an online form, or those who use only an online form?
RQ3: Do students who use the IC tool demonstrate greater metacognitive awareness compared
to groups of students who use a tutorial and an online form, or those who use only an
online form?
Before describing the study results, this chapter reviews the participant demographics from the
experimental groups, and the subjects’ contributed sources, as these were the basis for their
credibility evaluations.

5.1.	  Subject	  recruitment	  
As described in Section 4.3, subjects were recruited through a large undergraduate course
in the School of Information at the University of Michigan. Students enrolled in this class came
from a range of academic majors and years in school: of the 220 students there were 15%
freshmen, 51% sophomores, 21% juniors, and 12% seniors. The content of the course covered a
wide variety of internet-related topics (computers, networks, social media) that lent themselves
128

to students doing online research. An invitation to participate in the study was announced to the
entire class soliciting participation. In total, 193 responses were received (out of 220) indicating
that students were interested in participating in the study. When subjects responded to the initial
recruitment message, they indicated their interest in participating by completing a short
demographic survey. After completing the survey, they were instructed on how to begin the
study.

5.2.	  Randomized	  assignment	  of	  subjects	  
Subjects were randomly assigned to one of the three study groups: T1 group (T1) used
both the IC tool and the introductory tutorial, T2 group used the tutorial and an online form to
record their sources, and the CTRL group used only the online form. The goal of the subject
assignment was to have approximately 30 subjects per group to enable statistical validity and
generalizability (Creswell, 2002). To ensure that the group assignments reflected the original
population of the class, the total class registration was broken down by percentages per grade
level, with the aim of assigning similar proportions of each year in the experimental groups.
Randomized assignment with stratification was used to assign subjects to the experimental
groups by grade level, in order to ensure a comparable composition to the original distribution of
grade levels in the entire class: 15% freshmen, 51% sophomores, 21% juniors, and 12% seniors.
See Table 24 for the initial assignment of participants by grade level. Each experimental group
consisted of over 50% sophomores, reflecting the original class registration. This stratified
random assignment equalized the distribution of year levels between groups, and thus helped
CTRL for the level of experience between groups.
Table 24. Initial condition assignment (193 subjects)
Year

T1

T2

CTRL

Freshmen
Sophomore

18
48

20%
53%

10
28

20%
55%

10
28

20%
55%

Junior

15

16%

7

14%

7

14%

Senior

10

11%

6

12%

6

12%

Total

91

100%

51

100%

51

100%

129

After subjects were assigned to their respective experimental groups, they were contacted
via email with instructions on how to begin the study. Subjects were blind to their experimental
condition, and were asked to indicate their agreement to an honor code statement specifying that
they would work independently and not share their work with others. This statement was
intended to reduce the possibility that students in the class would talk to each other about the
study, or compare their experiences with each other and potentially realize that they were in
different conditions. Subjects were instructed to complete each of the three stages of the study
within the same time periods.
Although students were incentivized for their participation in the study, not all of the
students who initially responded to the recruitment phase successfully completed the experiment.
Out of the total 193 subjects who expressed interest in participating and completed the initial
survey, 84 subjects completed the study, resulting in a 44% completion rate. This large drop-off
in participation may reflect the fact that the study took more effort than many students expected
to invest, consisting of several steps over several days. Since it was an extra credit opportunity,
and not a required for-credit assignment, some participants may have opted not to complete it.
However, the final totals were close to the desired goal of having 30 subjects in each group to
enable statistical comparability. By treatment group, there were 33 subjects in T1, 25 subjects in
T2, and 26 subjects in CTRL (see Table 25). This small variation in the size of groups does not
effect the statistical analysis. The percentage of freshman in the final experimental groups was
higher than the original class distribution (15%), and the percentage of sophomores in the
experimental groups was lower than the original class distribution (51%) as a result of the
attrition of participants from the original assignments. Given the amount of attrition, this was still
considered to be fair approximation of the original class distribution.
Table 25. Final participant distribution (84 subjects)
Year
Freshmen
Sophomore

T1

T2

CTRL

9
12

27%
36%

7
11

28%
44%

8
10

31%
38%

Junior

6

23%

3

12%

4

15%

Senior

6

18%

4

16%

4

15%

Total

33

100%

25

100%

26

100%

130

The distribution of gender among the final study participants is shown in Table 26. Groups T1
(48% M, 52% F) and CTRL (46% M, 54% F) ended up with fairly equal gender representation
although the T2 group turned out to be skewed towards males (76% M, 24% F). Since the
subject assignment was randomized, the gender distribution overall was not considered as a
factor in the analysis of results.
Table 26. Final participant gender by group
Gender
Male
Female
Total

T1
16
17
33

T2
48%
52%
100%

19
6
25

CTRL
76%
24%
100%

12
14
26

46%
54%
100%

After the education level and gender questions, students were asked the following questions
regarding their background experience and knowledge:
•
•
•
•

How experienced are you with searching for information on the Internet?
Have you had any formal instruction in information literacy (how to do library
research)?
If yes, where did you have this information literacy/library research instruction?
Did this information literacy/library research instruction include how to evaluate the
quality of online information?

These questions sought to identify the subjects’ level of experience and skills with online
information seeking and credibility valuation, based on both prior experience and explicit IL
instruction. Prior experience with online searching and prior IL instruction could both be
potential confounding factors in this study, and might skew the results by allowing some students
to perform better regardless of their treatment condition since prior instruction could potentially
increase the subject’s skill level and performance. The responses to these questions were used in
evaluating the study results. Responses to the first question “How experienced are you with
searching for information on the Internet?” are shown by group in Table 27 below.

131

Table 27. Experience with searching the Internet
Experience level
Not at all experienced
A little experienced
Moderately experienced
Very experienced
Total

T1
0
4
16
13
33

T2
0%
12%
48%
39%
100%

0
4
12
9
25

CTRL
0%
16%
48%
36%
100%

0
3
14
8
25

0%
12%
56%
32%
100%

These results show that the groups are roughly equivalent in their distribution of experience, with
“moderately experienced” being the most frequent response for all groups at 48% (T1), 48%
(T2) and 56% (CTRL). The second most frequent response from subjects was “very
experienced” at 39% (T1), 36% (T2), and 32% (CTRL), which suggests that many subjects
considered themselves to be above average in their search skills. A small percentage of subjects
responded that that they were “a little experienced” at 12% (T1), 16% (T2), and 12% (CTRL). It
is interesting to note that none of the subjects reported themselves to be “not at all experienced,”
which may reflect either the universality of Internet use among these subjects, or alternately that
none of the subjects were willing to self-identify as inexperienced.
Responses to the second question ‘Have you had any formal instruction in information
literacy (how to do library research)?” are shown in Table 28.
Table 28. Prior IL instruction
Response

T1

T2

CTRL

Yes
No

18
15

55%
45%

14
11

56%
44%

15
10

60%
40%

Total

33

100%

25

100%

25

100%

While these results show that the distribution of prior IL instruction is roughly similar between
groups, the CNTRL group subjects had a somewhat higher percentage of prior IL training (60%)
than did the T1 (55%) and T2 groups (56%). While this difference is fairly small, in retrospect
the study would have been improved if this factor had been considered in the subject assignment
process as well as year level, in order to further equalize the experience level of subjects between
the groups.

132

The 47 subjects who responded “yes” they had received prior IL instruction (18 T1, 14
T2, and 15 CTRL) were given a follow-up question that asked “Did this information
literacy/library research instruction include how to evaluate the quality of online information?”
The intent of this question was to identify the extent to which these subjects had been explicitly
instructed in credibility evaluation skills, as opposed to traditional bibliographic and citation
instruction. Results of this follow-up question are shown below in Table 29.
Table 29. Prior IL instruction included quality evaluation
Response
Yes
No
Total

T1
12
6
18

T2
10
4
14

67%
33%
100%

71%
29%
100%

CTRL
12
80%
3
20%
15
100%

Of those subjects who reported receiving prior IL instruction, a majority of each group reported
that their instruction had included how to evaluate the quality of online information at 67% (T1),
71% (T2), and 80% (CTRL). The CTRL group reported a slightly higher level of agreement,
which corresponds with their reporting of a higher level of prior IL training. For each group, a
smaller percentage responded that their prior instruction had not included how to evaluate the
quality of online information at 33% (T1), 29% (T2), and 20% (CTRL). This suggests that their
prior IL instruction may have been very traditional bibliographic instruction, focused on
evaluating print materials and library databases. Alternately, students may not have remembered
or retained evaluation instruction, even if it was presented in their IL instruction.
A Pearson’s chi-square test was performed to determine if there significant differences
between the groups on these demographic variables: year in school, gender, experience with
searching the Internet, and prior IL instruction. The results showed no significant differences on
any of the demographic variables between the groups. (See Appendix 11: Study demographics
Chi Square Tests for complete results). This suggests that the stratified assignment was effective
in keeping the groups demographically comparable.

	  

133

5.3.	  Analysis	  of	  sources	  found	  by	  subjects	  
Before addressing the results of the experimental study of the IC tool, it is useful to
examine the sources that were contributed by the subjects. This helps to understand the types of
information sources that students were evaluating in making their credibility judgments, and also
gives a clear picture of the type of information seeking behaviors that these students employ
when conducting online research. Analysis of these sources revealed clear trends in the subjects’
searching and evaluating habits.
All study subjects were given the imposed query of “What is the effect of social media on
education?” as their research topic, in order to ensure comparability among the sources and to
allow the subjects to make comparisons between sources. This topic was appropriate to the
content of the class, which discussed many aspects of social media and their impact on many
impacts of modern life. Thus, the topic would potentially be of more relevance to the subjects
since it related directly to the content of the class, and sources that they found could potentially
be used in class assignments. If subjects were allowed unrestricted searching on their own topics,
the task of making comparative evaluations of the merits of two sources would have been very
difficult. Limiting all experimental groups to the same topic ensured that all contributed sources
would be broadly similar, and thus, subjects were able to focus their attention on the evaluation
of credibility criteria of each source.
5.3.1.	  Comparison	  of	  sources	  contributed	  by	  group	  
The three experimental groups searched on the same research topic, but in each group the
conditions were different. The T1 group used both the IC tutorial and the IC tool, which guides
students through the entire process of evaluating sources based on credibility criteria and using
source-based characteristics as evidence, comparing evaluations, and comparing sources to
decide on the best ones to use for their research topic. The IC tutorial gives an introduction to the
types of evidence students should use in their evaluation, but does not include scaffolding to help
practice and learn, and does not provide any support for metacognitive strategies. The CTRL
group used only an online form that only provides definitions of the credibility criteria.

134

Subjects in the T1 condition completed the IC tutorial and then were automatically
directed to the IC tool’s Headquarters page and were instructed as follows:
You	  are	  currently	  researching	  the	  topic	  "What	  is	  the	  effect	  of	  social	  media	  on	  
education?"	  Your	  task	  in	  the	  Investigate	  stage	  is	  to	  search	  online	  and	  find	  credible	  
information	  about	  your	  topic.	  When	  you	  find	  information	  you	  want	  to	  use,	  open	  the	  
Notebook	  in	  the	  lower	  right	  corner	  of	  the	  browser,	  and	  answer	  questions	  about	  the	  
credibility	  of	  the	  information	  

T1 subjects were able to search freely online and automatically save their chosen sources through
the Notebook plugin (see Chapter 3 for description of the functionality of IC). T2 subjects took
the IC tutorial, but did not use the Notebook plugin or the IC tool itself. Instead, they used the
Google Bookmark browser plugin to bookmark the sources that they found (see Chapter 4 for
description of the study design). CTRL subjects did not take the IC tutorial and used only the
online form.
The sources found and saved by subjects were either recorded automatically (T1) by the
IC tool or recorded by the subjects into the online form (T2 and CTRL). The URLs of all sources
found by subjects were reviewed by the researcher, titles were retrieved manually, and the type
of genre of each source was identified. Summary statistics were generated, including the multiple
use of identical sources. The comparison of these sources between the experimental groups is
described below.
Subjects in the T1 condition saved a total of 185 sources through their Notebooks.
Disregarding duplication among the results, there was a total of 85 unique sources. For the most
frequently contributed sources, titles, genre type, URL of the source, and the number of times
contributed are shown in Table 30. Note that the number of times contributed (N) includes
multiple contributions of the same source by different subjects. Of these 10 most frequently
saved sources, 50% were blogs on education or social media topics. Of the remainder, 3 were
commercial sites, 1 was a newspaper, and 1 was a research report from the ERIC database
(Education Resources Information Center) sponsored by the US Department of Education and
the Institute of Education Sciences. This last source is the only one that would be considered
scholarly for the purposes of this study. Note that there is duplication between the sources; they
were counted individually each time they were contributed by a subject.

135

Table 30. T1's most frequently contributed sources
Title

Type

Source

N

The 10 Best And Worst Ways Social Media
Impacts Education

Blog

edudemic.com

19

How Social Media Is Changing Education

Blog/Commercial

sproutsocial.com

11

Social Networking In Schools: Educators
Debate The Merits Of Technology In
Classrooms
Social Media In Higher Education: A
Literature Review And Research Directions

Blog

huffingtonpost.com

11

Research report

academia.edu

9

Why Social Media Can And Is Changing
Education
Social Networking: Teachers Blame Facebook
And Twitter For Pupils' Poor Grades
How Is Social Media Affecting Education?

Blog

connectedprincipals.com

8

Newspaper

telegraph.co.uk

8

Commercial
(video)
Blog

curiosity.discovery.com

6

educationandtech.com

6

Blog

socialmediatoday.com

5

Research report

eric.ed.gov

5

The Challenging Effect Of Social Media On
Education
5 Reasons Social Media Enables Genuine
Education
Teaching, Learning, And Sharing: How
Today's Higher Education Faculty Use Social
Media

Five of most frequently used sources for the T1 group were blogs. The first is Edudemic.com,
self-described as “one of the leading education technology sites on the web” featuring “a regular
flow of tools, tips, resources, visuals, and guest posts from dozens of authors around the world.”
Sharing links for a number of social media services (Twitter, Facebook, Google+, LinkedIn,
Pinterest) can be seen beneath the headline. The site also carries ads. Based on these
characteristics, the site was categorized as a blog. The second most frequently contributed blog
was the Huffington Post, a news aggregator and blog, which features social media links, user
comments, and advertising. Connectedprincipals.com is a group blog described as “the shared
thoughts of school administrators that want to share best practices in education” (the contributed
article is now archived and not available). Educationandtech.com is a personal blog maintained
by a teacher. Socialmediatoday.com describes itself as “an independent, online community for
professionals in PR, marketing, advertising, or any other discipline where thorough
understanding of social media is mission-critical.”
136

Screenshots of these blog results are shown below in Figure 18. Since research in student
credibility evaluation practices has shown that students often rely on the visual characteristics of
websites in evaluating credibility, considering the visual similarity of many of these sources and
the lack of traditional credibility markers it is important to bear in mind. As discussed in the
literature review, online information sources present challenges to credibility evaluation due to
the lack of traditional credibility cues. One goal of this study was to instruct students how to
identify the specific criteria they should use in evaluating the credibility of online sources, yet
these criteria may be challenging for students to identify in the types of sources that were used in
this study. Note that many of these sites are visually similar, and any differences in genre or
intended purpose of the site (professional, commercial, personal) may not be apparent to
students, suggesting that they may consider the content of each source to be equivalent as
information sources.
Figure 18. Blog search results

137

The second most frequently contributed source overall for the T1 group comes from the
commercial site SproutSocial.com, which sells social media services (see Figure 19). While it
could also be characterized as a blog, it is hosted by a for-profit company and was categorized as
a commercial source. This is arguably a grey area, though, since Huffington Post is also a
commercial entity. Sproutsocial.com, though, seems primarily to be selling its company services,
with a blog as a sideline (note that Sprout Social advertises on the Social Media Today site).
Note that this site is visually similar to the blogs shown above, and it would be difficult for many
students to determine the difference in purpose between this commercial blog and other
information sources. At first glance, it appears to be a news source, although it is unlikely to be
objective in its presentation of information since the company is directly involved in the industry
it is reporting on.
Figure 19. Commercial search results

Another one of the top 10 sources was a video titled “How is social media affecting
education?” from the website of the “Curiosity” TV show produced by the Discovery Channel, a
commercial cable television channel (see screenshot in Figure 20). This site features multiple
video clips apparently taken from a television show (preceded by ads), with each clip titled with
a provocative question (possibly these questions are submitted by users, although it is not clear
from the description of the site). A search box features the question prompt “What are you
curious about?” and presents a video clip of someone responding to the question “How is social
media effecting education?”

138

Figure 20. Video search result

The cited video clip shows an interview with a “marketing and product development officer”
from a corporation, and provides a transcript of the interview below the video. It is unclear how a
student might evaluate the credibility of this information. The site is apparently aimed at K-12
students, based on the broad array of question types listed under the Topics tab, and the informal
style of the text. However, it is difficult to determine how to classify the Curiousity site, because
no “About” statement is given, and its purpose and intended audience are unclear. The branding
of the Discovery Channel is prominent on the site, and the main page
(http://www.discovery.com/) advertises current episodes of the channel’s programming,
including “Fat N’ Furious,” “American Muscle,” and “Shark Week” which are clearly popular
entertainment. This overt marketing undermines the credibility of the site as an information
source, especially as the site describes Discovery Communications, LLC as “The World’s #1
Nonfiction Media Company.” The Discovery Channel notoriously produced a fake documentary
that claimed that an extinct prehistoric shark had been discovered alive (Davidson, 2013).
Animal Planet, a TV channel also owned by Discovery Communications, has produced a fake
documentary about the purported existence of mermaids (Davidson, 2013). Both of these
examples show that the company is not above producing false, misleading misinformation
masquerading as fact for the purposes of television ratings.
However, it is possible that students could view the Discovery Channel’s Curiosity site as
a credible source of information along the lines of a TV channel such as PBS, since the site
presents a list of “experts” who apparently answer the questions posed on the site. These experts
139

are shown under a link titled “Meet the Experts” (see Figure 21) and represent a varied mix of
backgrounds and training.
Figure 21. Experts presented by TV channel website

Prominently displayed are Richard Saul Wurman, an architect, author and creator of the TED
Conferece, and Elie Wiesel, Nobel Prize winning author, along with TV personality Dr. Mehmet
Oz, known for his appearances on the Oprah Winfrey TV show, and Deepak Chopra, a popular
author and proponent of alternative medicine. Note that two advertisements are also prominently
displayed in this section. Many other experts are listed below by topic, although what criteria for
determining their expertise might have been used is unclear. It is a widely varied group, ranging
from chefs to engineers to scientists to businessmen, which raises questions about what exactly
“expertise” is and how it is presented online. In the case of the Curiousity site, topical expertise
appears to be determined by any professional experience in the area, possibly moderated by the
extent to which the person is well known. An expert may well be just “someone who has
something to say” about the topic. Overall, this site with its apparently user-submitted questions,
its multi-media resources, and its broad application of the term “expertise,” is another example of
the hybrid genres emerging from the Internet, with a mix of commercial and educational content
and a reliance on branding to establish credibility.
For comparison, the research report included in the list of most used sources is a PDF of a
report by Pearson Learning Solutions and Babson Survey Research Group hosted on the ERIC
website. This source reports the findings of a survey of faculty on the topic of social media
usage, and supplies detailed data analysis and findings (see screenshot in Figure 22 below).
140

Figure 22. Research report search result 1

It also is designed to be printed, and uses a more professional visual format. This source
represents the most “academic” or expert-level source among the most frequently-contributed
sources by the T1 group, although it is not a scholarly peer-reviewed journal article.
Subjects in the T2 group contributed a total of 27 sources. Without duplicates, there were
18 unique sources. Titles, genre type, URL of the source, and the number of times contributed of
the sources donated more than once are shown in Table 31. Note that there is duplication
between the sources; they were counted individually each time they were contributed by a
subject.
Table 31. T2's most frequently contributed sources
Title
Social Media in Higher Education: A Literature
Review and Research Directions

Type
Research report

Source
academia.edu

N
5

Teaching, Learning, and Sharing: How today's
Higher Education Faculty Use Social Media
(PDF)
Teaching, Learning, and Sharing: How today's
Higher Education Faculty Use Social Media
(abstract & download link)

Research report

eric.ed.gov

3

Research report

eric.ed.gov

2

State and Local Education Agencies “Like” Social
Media

Blog

ed.gov

2

Effects of Student Engagement with Social Media
on Student Learning: A Review of Literature

Student paper

studentaffairs.com

2

141

The most frequently donated source of the T2 group is a literature review posted on the
repository site Academia.edu (see Figure 23). The site describes itself as:
“a platform for academics to share research papers. The company's mission is to
accelerate the world's research. Academics use Academia.edu to share their research,
monitor deep analytics around the impact of their research, and track the research of
academics they follow.” (http://www.academia.edu/about)
It is apparently a for-profit company, since they list their investors and amount of investment
capital they have received on their About page. Thus, it is unclear if this site could be considered
scholarly; since the researchers are academic, their work is likely to be scholarly, the papers
submitted do not seem to be peer-reviewed or vetted. In the case of this source, two of the
authors are professors and two are graduate students. However, since it consists only of a
literature review, it was classified as a research report rather than a scholarly journal article.
Figure 23. Research report search result 2

The second and third items from the T2 list of sources are the same research report that was
contributed by the T1 group and shown in Figure 21. However, the T2 group contributed both
the PDF version and the ERIC database listing containing the abstract and download link to the
report. This may suggest that these subjects did not read the report at all, and only perused the
abstract.

142

Another source that may be scholarly yet is difficult to determine is the article from
StudentAffairs.com (see Figure 24). This site describes itself as “one of the web's premier
resources for posting and finding a job for student affairs professionals.”
Figure 24. Article search result

The site hosts The Journal of Technology in Student Affairs, which describes itself as “The online magazine about technology and Student Affairs” and as “a forum for student affairs
generalists on issues relating to student affairs and technology.” This journal does not appear to
exist outside of the website. The issue of the journal containing the cited article states:
“In this issue, we have published the winning three articles from our recent
contest for graduate student authors. In an effort to allow our readers to see the
papers as they were submitted, we made very minor edits to the manuscripts.”
(http://studentaffairs.com/ejournal/Summer_2013/index.html)
Although the title of the journal sounds scholarly, and students may assume that it is scholarly,
there is no indication of peer-review or editorial vetting of the content. For this reason, this
source was categorized as a student paper. Note that the visual layout of this webpage is similar
to the layout of scholarly database, and it could be quite difficult for students to recognize the
difference.
Subjects in the CTRL group submitted a total of 27 sources in the post-test. Without
duplicates, there were 16 unique sources. Titles, genre type, URL of the source, and the number
of times contributed of the sources donated more than once are shown in Table 32. Note that

143

there is duplication between the sources; they were counted individually each time they were
contributed by a subject.

Table 32. CTRL's most frequently contributed sources
Title

Type

Source

N

Social Media in Higher Education: A
Literature Review and Research Directions

Research report

academia.edu

5

The 10 Best and Worst Ways Social Media
Impacts Education

Blog

edudemic.com

4

Overcoming Hurdles to Social Media in
Education
Social Networking In Schools: Educators
Debate The Merits Of Technology In
Classrooms
How Social Media Is Changing Education

Research report

educause.edu

3

Blog

huffingtonpost.com

2

Blog/Commercial

sproutsocial.com

2

The most contributed sources for the CTRL group include three of the sources also contributed
by the T1 group: the Acedemia.edu research report, the Edudemic.com blog post, and the
Huffington Post blog post. The third source is an article from the online journal EDUCAUSE
Review Online, which posts a mixture of peer-reviewed and not reviewed articles (see Figure
25). This particular article is written by an AVP of Marketing at the Pearson corporation,
discussing the findings from a survey conducted by at Pearson Learning Solutions. Since the
article presents original research results, it was categorized as a research report.
Figure 25. Research report search result 3

144

Several sources were contributed by subjects in all three of the experimental groups, all
of which appear in the most frequent results above. Titles, genre type, and URL of the shared
sources are shown in Table 34.
Table 33. Sources shared between groups
Title

Type

Source

Social Media in Higher Education: A Literature
Review and Research Directions
The Challenging Effect of Social Media on Education

Research report

academia.edu

Blog

educationandtech.com

The 10 Best and Worst Ways Social Media Impacts
Education
Social Networking In Schools: Educators Debate The
Merits Of Technology In Classrooms
How Social Media Is Changing Education

Blog

edudemic.com

Blog

huffingtonpost.com

Blog/Commercial

sproutsocial.com

Three of the five contributed articles are from blogs, and one is from a blog hosted on a
commercial social media company’s site. The other shared source is a research report hosted on
an academic sharing site or repository.
The similarity in titles of several of these sources is notable (How social media is
changing education, Why social media can and is changing education, How is social media
affecting education?, How today's higher education faculty use social media) suggest that
subjects may have relied on natural language queries when searching, that is, typed the exact
topic question into a search engine as their query. To examine the degree to which subjects relied
on this method for finding their sources, the researcher entered the exact query into Google and
recorded the first page of the search engine results. Four of the most frequently contributed
sources show up in in the first page of Google search results for the query “what is the effect of
social media on education” (see screenshot in Figure 26).

145

Figure 26. Google search engine results

Some of the other popular sources are found further down in this search results list. This fact
strongly suggests that many of the subjects in this study relied on the first page of results from a
Google search on the exact phrasing of the assigned research topic.

5.3.2.	  Summary	  of	  contributed	  sources	  	  
All sources contributed by subjects were classified by genre and totals calculated by
group and across the entire subject pool. Overall, the T1 group used more sources and more
diverse sources than the T2 or CTRL groups. Table 35 lists the summary data.

146

Table 34. Contributed sources by genre
Genre
Blog
Book
Commercial website
Commercial (video)
Conference Proceedings
Course Site
Dissertation
Educational website
Magazine
News Organization
Newspaper
Press Release
Research Report
Scholarly Article
Student Paper
Total

105
1
5
7
1
1
1
1
4
1
17
22
8
3
177

T1
59.3%
0.6%
2.8%
4.0%
0.6%
0.6%
0.6%
0.6%
2.3%
0.6%
9.6%
12.4%
4.5%
1.7%
100.0%

T2
14
51.9%

1

CTRL
8
29.6%

1

3.7%

2

7.4%

11
3
2
27

40.7%
11.1%
7.4%
100.0%

3.7%

1
8
3

3.7%
29.6%
11.1%

27

100.0%

Total
127
55.0%
1
0.4%
5
2.2%
8
3.5%
2
0.9%
1
0.4%
1
0.4%
1
0.4%
1
0.4%
1
0.4%
19
8.2%
1
0.4%
41
17.7%
14
6.1%
5
2.2%
231 100.0%

Blogs and research reports were the most frequently contributed genres of sources across all
groups. For the T1 group, blogs were 59.3% of 177 total sources, for the T2 group blogs were
51.9% of 27 total, and for the CTRL group blogs were 29.6% of 27 total. The second most
frequent genre was Research Report, with 12.4% of T1’s total, 29.6% of T2’s total, and 40.7% of
CTRL’s total. The third most contributed genre for T1 was Newspaper (9.6%), while for T2 and
CTRL it was Scholarly Article (11.1% for both groups). Scholarly Article came in fourth for T1
(4.5%). In the overall total of sources across all groups, blog was the overwhelming majority of
genres at 55.0%, followed by Research Report at 17.7% and Newspaper at 8.2%.
Overall, T1 contributed a wider variety of genres (13) than T2 (5) and CTRL (6). The
genres that were contributed by T1 but not by either other group were: Book, Commercial
website, Conference Proceedings, Course Site, Dissertation, Educational website, Magazine, and
News Organization. The inclusion of sources such as Book, Conference Proceedings, and
Dissertation by the T1 group show a greater familiarity and use of higher-quality scholarly
information. This suggests that the subjects who used the IC tool learned to make better
judgments about which sources to use based on their quality and credibility.

147

5.4.	  Data	  analysis	  
This experimental study used both qualitative and quantitative data analysis methods in
analyzing subject data, to provide a multi-dimensional understanding of student behavior and
answer the research questions regarding the impact of the online participatory learning tool.
Since the study employed randomized subject assignment to experimental groups, statistical tests
could be conducted on the results to demonstrate effects of the treatments.
Qualitative data (subject responses entered in response to open-ended credibility criteria
questions on the post-test and responses to open-ended reflective prompts) were analyzed
through coding against a rubric and textual analysis. Quantitative data (demographic data and
scores on criteria understanding responses) were analyzed using one-way analysis of variance
(ANOVA). Statistical tests were conducted using the SPSS statistical software package. Table 36
below lists the data analysis methods that were used to answer each research question:
Table 35. Data analysis methods
Research question

Data source

Analysis method

RQ1

Credibility criteria responses

Coding and statistical

RQ2

Reflective prompts

Content analysis

RQ3

Metacognition post-test results

Scoring and statistical

5.5.	  Understanding	  of	  expert	  credibility	  criteria	  	  
Research Question 1 asked: “Do students who use the IC tool demonstrate greater
understanding of expert credibility criteria in the process of evaluating online sources compared
to groups of students who use a tutorial and an online form, or those who use only an online
form?” The main finding for this research question is that the T1 group (which used the tutorial
and the online InCredibility tool) achieved significantly higher scores for understanding of the
five credibility criteria for in comparison to the T2 group (used only the tutorial) and the CRTL
group (used only the online form). Two of this study’s research hypotheses are:

148

•
•

H1: The T1 group will demonstrate greater understanding of expert credibility criteria
compared to T2
H2: The T1 group will demonstrate greater understanding of expert credibility criteria
compared to the CTRL group

Based on the findings that show that the T1 group demonstrated greater understanding of expert
credibility criteria than the T2 and CTRL groups, the hypotheses HI and H2 are supported by this
study. The process of finding these results, through coding of the subject’s open-ended responses
to credibility criteria questions and conducting ANOVA tests on the between-group differences,
is described below.
After students completed their use of the IC tool, they were presented with a post-test that
asked them about their understanding of credibility criteria by defining each criteria and
explaining its importance. As described in Chapter 4, subjects’ answers to the credibility
questions were coded according a rubric which assigned a score of 0-2 for each answer, with 0
indicating the response did not meet the standards of the rubric and 2 meaning it met them
satisfactorily (see Section 4.7 for details on the coding process). The rubric for scoring the
subject responses is shown in Table 37 below.
Table 36. Coding rubric for student criteria responses
Question
What does "authority" of
information mean?
Why is it important to evaluate
how authoritative online
information is?
What does "relevance" of
information mean?
Why is it important to evaluate
how relevant online information
is?
What does "reliability" of
information mean?
Why is it important to evaluate
how reliable online information
is?
What does "currency" of
information mean?

Score
2
identify author AND
credentials/qualifications
anyone can post online AND
importance of verification

1
identify author OR
credentials/qualifications
anyone can post online
OR verification

0
neither

content/topics are useful
/related to question

generic mention of
content/topics

neither

filter/select information AND
judge value

filter/select information
OR judge value

neither

trustworthiness AND sources

trustworthiness OR
sources
Internet skepticism OR
verify

neither

time/when published (or
just "current")

neither

Internet skepticism AND
need to verify/evaluate
sources
up to date/recent

neither

neither

149

Why is it important to evaluate
how current online information
is?
What does "purpose" of
information mean?

useful/relevant/valid AND
depends on topic

useful/relevant/valid OR
depends on topic

neither

why the information was
written/published/presented
(goal/intent/bias)

generic mention of
"goal/intent/bias"

neither

Why is it important to evaluate
what the purpose of online
information is?

detect bias AND judge
value/credibility/ usefulness

detect bias OR judge
value/credibility/
usefulness

neither

Examples of scores for responses at all three levels are shown for both the definition and
importance responses to the Authority criteria in Table 38 below. These examples were coded
based on the presence or absence of the specific terms stated in the rubric, and for the coder’s
evaluation of how much understanding of the criteria was evidenced in the response. A score of
2 means that the response met both of the criteria listed in the rubric, a score of 1 means that the
response met either of the criteria listed in the rubric, and a score of zero means that the response
met neither of the criteria listed in the rubric.
Table 37. Examples of scored criteria responses
What does "authority" of
information mean?
It's finding out who wrote the
information and what that person's
qualifications and background
are.

Score
2

The credibility of the author and the
website make up the authority. If a
person has higher authority, then the
information is more trustworthy.

1

The power or weight that that
information holds. How effectively
it can be used to do things like build
further information or persuade
based upon what is known.

0

Why is it important to evaluate how
authoritative online information is?
It's important to evaluate how
authoritative online information is
because it is possible for anyone to put
anything on the Internet and claim it to
be true. By evaluating if the author has
the authority to write about a specific
topic you are confirming that the
information is valid and useful.
It tells us whether the information is from
a reliable resource. So we can decide
whether to believe it or not.

Score

It is important because you need to make
sure the information you are consuming is
accurate and useful.

0

2

1

150

A Master’s student from the School of Information coded the responses along with the
researcher. Both coders initially met to review the coding rubric and jointly coded several
examples, then each made an independent first pass at coding. Next, the coders met again to
review the results and discuss any discrepancies. While there was general agreement on the
scoring of the definition responses, most discrepancies centered around how to score the
“importance” responses. Differences in coding arose from the approach of one coder to evaluate
strictly on whether specific words were stated in the responses, while the other coder interpreted
the overall intent of the answer. The two coders met to discuss these discrepancies, and the
coding rubric was subsequently expanded and defined more clearly to enable more consistent
coding based on interpreting the intent of the answer, that is whether it attempted to fully answer
the question rather than relying strictly on specific terminology. The coders then again coded
independently. Subsequently, an inter-rater reliability analysis using the Kappa statistic was
performed to determine consistency among the coders. The result was found to be Kappa = 0.534
(p<.001). Generally, a Kappa value of between 0.40 and 0.59 is considered moderate agreement
(Landis & Koch, 1977). While moderate agreement is not as strong as would be ideally desired,
given the shortness of many responses and the degree of interpretation sometimes demanded to
determine the intention behind responses, the researcher concluded that reliability of the coding
was high enough to proceed with analysis.
In order to answer Research Question 1, which defined “understanding” as composed of
the subjects’ ability to both define the criteria and express its importance, the scores for these
two responses to the open-ended prompts were averaged to produce a score for understanding of
each criteria. See Table 39 below for the understanding scores of all three groups.
Table 38. Understanding scores for credibility criteria
Group

Authority

Relevance

Reliability

Currency

Purpose

T1
T2
CTRL
Av.

1.03
1.02
0.88
0.98

1.40
1.33
1.33
1.35

0.94
0.68
0.84
0.82

1.45
1.15
1.16
1.25

1.32
1.10
0.93
1.12

These results show that the T1 group had higher understanding scores for all of the criteria
compared to both the T2 and CTRL groups. For T1, Currency received the highest understanding
151

score (1.45), followed by Relevance (1.40), Purpose (1.32), Authority (1.03), and Reliability
(0.94). The T2 group had the second-highest scores for Authority (1.02) and Purpose (1.10),
while the CTRL group had the second-highest scores for Reliability (0.84) and Currency (1.25),
and they tied on the Relevance (1.33). Overall, Relevance received the highest average score
across all three groups (1.35) while Reliability received the lowest average (0.82). These results
suggest that for most students, the concept of relevance is the most easily understandable, while
reliability is the least understandable. This may be because “relevance” is often easy to
determine at a surface level (“Does this information fit with my topic?”) while reliability (“Is this
information trustworthy?”) is much harder to determine because it requires critical thinking,
evaluation, and judgment based on evidence. Another factor may be that the word is unfamiliar
to some students, in comparison to the other terms that are more common. However, the fact that
all subjects still rated the lowest on understanding of reliability suggests that even after using the
IC tool, this topic is still challenging for students and needs greater emphasis in IL education,
especially in the online information environment.
To determine if the differences in outcomes between the groups was statistically
significant, a one-way ANOVA was used to test for differences among the three groups’ scores.
Results showed that the scores differed significantly across the three groups, F (2, 79) = 5.224, p
= .007. A Tukey post-hoc comparison of the three groups indicated that the T1 group (M = 1.23,
95% CI [1.11, 1.34]) had significantly higher scores than the T2 group (M = 1.03, 95% CI [0.93,
1.11]), p = .033, and the CTRL group (M = 0.99, 95% CI [0.85, 1.14]), p = .014. A comparison
between the T2 group and the CTRL group was not statistically significant at p < .05. (See
Appendix 12 for full test results.) These tests demonstrate that the differences between the results
for the T1 group and the T2 and CTRL groups are greater than what would be expected by
chance, and thus show an effect of the treatment condition on the learning outcomes.
Thus, the findings for Research Question 1 are that the T1 group (which used the IC
tutorial and the IC tool) achieved significantly higher scores for understanding of the credibility
criteria in comparison to the T2 group (used only the IC tutorial) and the CRTL group (used only
the online form). This means that the T1 group was better able to express the definitions of and
importance of the credibility criteria (the two components of understanding) than the other
groups. The difference between the T2 group and the CTRL group was not significant, showing
that the effect of the IC tutorial alone was not related to the difference in learning outcomes.
152

These results demonstrate that use of the IC tool increased students’ understanding of the expert
criteria that they should use in making credibility evaluations. These expert criteria (authority,
relevance, reliability, currency, and purpose) were scaffolded upon an initial model of “who,
what, where, when and why” questions which were subsequently expanded into more expert
terminology. Using the IC tool, subjects not only learned the definitions of these terms but also
learned to express their importance, which is an important component of learning. Since the T1
group had the experience of repeatedly practicing the application of these criteria to real-life
examples, and learned techniques for evaluating various elements of online sources for
credibility, they gained a greater understanding of online credibility criteria and may be better
able to apply them in the future. This explicit support for the understanding of credibility criteria
was not supplied to the T2 or CTRL groups, and thus the difference in outcomes can be
attributed to IC tool. These results show the advantage of a scaffolded model over simply
providing a checklist of questions or criteria and asking students to answer them without context,
especially in the online information environment where credibility evaluation can be challenging.

5.6. Evidence-­‐based	  source	  characteristics
Research Question 2 of this study asked: “Do students who use the IC tool demonstrate
greater application of evidence-based source characteristics as the basis for their credibility
evaluations compared to groups of students who use a tutorial and an online form, or those who
use only an online form?” Two of this study’s research hypotheses were:

•
•

H3: The T1 group will demonstrate greater use of evidence-based source characteristics
compared to T2
H4: The T1 group will demonstrate greater use of evidence-based source characteristics
compared to the CTRL group

The experimental results were not significantly different between groups, so a conclusion cannot
be made and the hypotheses H3 and H4 are not statistically supported. A larger sample size may
be necessary to draw a statistical conclusion. On a purely descriptive basis, however, the T1
group did show somewhat greater use of evidence-based source characteristics than the T2 and

153

CTRL groups. The process of finding these results, through textual analysis of the subjects’
open-ended responses to reflective prompts, is described below.
After the T1 subjects completed their use of the InCredibility tool, and the T2 and CTRL
subjects completed their parallel tasks using the online form, they were presented with a post-test
that included reflective questions regarding their evaluation process and assessing their own
evaluation decisions (adapted from Herring, 2011 and Ge, Planas & Er, 2010). The questions on
this post-test are shown in Table 40.
Table 39. Reflection prompts
Looking back on the process of evaluating the credibility of online information....
1. How did you decide whether a webpage was credible or not?
2. What criteria did you use to help you evaluate credibility?
3. What strategies did you use to evaluate credibility?
4. What have you learned about evaluating credibility on the web?

The questions were the same for each group, and were open-ended to allow students to
demonstrate their understanding in their own words.
Overall, the T1 group provided more answers (39) than T2 (25) and CTRL (26), however
answers given by T1 subjects were much shorter and less thoughtful than were those of T2 and
CTRL subjects for all the questions. Many T1 subjects gave terse responses in the form of a few
words or a phrase, while many T2 subjects gave longer and more complete responses that
showed more reflection and effort. See Table 41 for examples of the difference in the quality of
responses between groups. Total word counts demonstrate the terseness of T1 responses
compared to the responses of T2 and CRTL subjects. The T1 group gave 39 responses with a
total word count of 3,959 words. The T2 group, with only 25 responses, used nearly the same
number of words at 3,529. The CTRL group, with 26 responses, used a total of 5,162 words.
Thus, the T1 group used on average 101 words each, while the T2 group used on average 141
words each and the CTRL group used 198 words on average. Since the T12 group used fewer
words overall, between-group comparisons based on word count are unreliable.

154

Table 40. Length of responses between groups
Group

How did you decide whether a webpage was credible or not?

T1

by looking at the authors and their credentials, and date

T1

Whether more of the information was provided in the notes or not

T1

Author qualifications, domain ending (edu), currency

2

I looked to see the references, I googled the author, and I checked to see if I had heard of the
specific source already. (Such as the newspaper or magazine)

T2

Multiple ways. The greatest factor is if it is published in a peer reviewed journal, and if not I
checked to see who the author was and the relative credibility of the website. If the author
was a PhD this obviously led me to see it as more credible.
T2 I looked for information that contained the least amount of bias as possible. I also
researched the authors to find out their qualifications and how knowledgeable they are about
the subject.
CTRL I looked at who was writing the article or piece, and what background they had in the
subject area. I read the articles and looked for biases, deeming the most unbiased the more
credible articles. Direct quotations and references to other works (citing other credible
sources) were what I looked for.
CTRL I looked up the author and their level of expertise in the field they were discussing. I also
considered the accessibility of the site, and how restrictions on who could/could not post
information
CTRL I looked at whether or not it listed sources/references, was the published content peer
reviewed studies, or opinion pieces. The more reviewed studies the webpage had in
comparison to it's opinion pieces, the more credible it was to me.

This was an unexpected result. However, it seems likely that since the T1 group had
already been asked to complete many actions and give many responses to various prompts
throughout the use of the IC tool, that they may have become fatigued or uninterested or
unmotivated to provide thoughtful responses to these questions on the post-test. The overall
number of questions asked and time and effort demanded of the T1 group during the experiment
was much higher than for the other two groups, who were only required to search online and
enter notes about their sources into an online form. This lack of consistency in the response data
made it difficult to complete an analysis of word-count differences between the group. The
original research design involved applying a coding rubric based on the amount of specific
evidence-related terms supplied by subjects in their responses. While the responses to the
reflective prompts do often evidence some use of the expert terminology, the differences in
elaboration and depth between the T1 and the other two groups makes meaningful comparisons
between them impossible.
155

The research design also included coding the groups’ open-ended responses based on the
categories from Markey et. al. (2014) of Evidence-based, Projection-based, and Intuition-based
comments (see Chapter 4 for description of this coding methods). The criteria for these three
categories of coding are shown in Table 42 below. One additional category emerged through the
coding process based on the types of responses given by subjects. Some subjects gave
descriptive or narrative responses of how they evaluated credibility without mentioning specific
criteria. Those responses were coded as Description-based.
Table 41. Coding categories (adapted from Markey et al., 2014)
Code

Definition

Evidence-based

Cites specific evidence supporting evaluation

Projection-based

Speculates without evidence

Intuition-based

Makes unfounded assertions without evidence

Description-based

Descriptive or narrative responses

In some cases, there were elements of both Evidence and Projection or Description in their
responses, and these responses were coded as a combination of both categories. Some subjects
relied on repeating the terms used in the InCredibility tool, that is, by stating simply “who, what,
when, where, why” in their responses. These responses were coded as 5Ws (see examples of
coded responses in Table 43).
Table 42. Examples of coded reflection responses
Category
Evidence-based

Projection-based

Response
Author qualifications, domain ending (edu), currency
I based my decision on which source had the most reputable sources: for
example, a post by the Huffington Post was informative, but offered no
author / authors, so I did not view it as very reputable.
This source was the most credible because the author's qualifications were
the most legit and he had the most background and education in the field.
How much information it had and how relevant the information was.
Does there job having something to do with social media and/or education?
Does their opinion matter?
This source's author had more ethos than a few of the other authors, which
helped me narrow the sources down to this one and another one. I
ultimately picked this one because it seems to explore the topic more
thoroughly than the others.

Group
T1
T2
CTRL
T1
T2
CTRL

156

Description-based

Evidence/
Projection

Evidence/
Description

5Ws

I felt like this source really highlighted particular uses for social media.
Yes, social media is a great way to stay connected with friends, campus
safety, and tailgate events but we are talking about what educators want
from social media. This article highlights how social media can be used as
a strategy tool to fix the problem that everyone seems to have with
education today, which is mostly cost and return on investment. Colleges
can take this data and use it to address the some of the problem and get a
first hand look at what people are saying
The source (Discovery Channel) is familiar and the person speaking has
very high qualifications.
Firstly, the author seemed to be a credible source as she has worked for a
company directly interested in understanding and improving learning.
Also, the article was the most recent, and one of the few that had a distinct
list of sources.
When we were given the question "What is the effect of social media on
education?" I wanted to find an article that not only stated the negative
aspects but also ways that it can be used effectively. All of my sources did
provide some insight into how education was effected by the student use of
social media but the site I selected best answers the posed question. This
article discusses both sides of the conversation and ways in which social
medias can be beneficial, if used effectively. The author, Jeff Dunn, goes
in great depth in describing his evidence and ways to avoid problems that
may arise. The author provides many examples about the positives and
negatives and what he thinks will come next for our growing technology
society. He believes the best way to use social media to benefit education
is to manage the amount students use it.
This source was most credible because it incorporated several forms of
evidence in support of its claim. First, the author clearly defines "what is
social media" to inform her readers what the basis of the debate is about. It
is important to make this distinction in order for every reader to understand
her view on why social media enhances learning. Moreover, she gives the
specific example of one social media platform, Emodo, being incorporated
in a high school. This direct account and the findings that resulted from it
provide clear evidence of the success of social networking use in the
classroom.
By looking at if there was information on the who, what, where, when, and
why
To help me evaluate credibility, I answered "who, what, where, when, and
why."
I checked the Who, What, Where, When and Why - if the article didn't
successfully answer all of these points, then it lacked credibility.

T2

T2
CTRL

T2

CTRL

T1
T2
CTRL

The examples above show that responses coded as Evidence-based cited specific evidence
supporting the subject’s credibility evaluations. Responses coded as Projection-based speculated
about credibility without citing specific evidence to support the evaluation. Responses coded as
Description-based provided descriptive or narrative responses of the subjects actions or
behaviors but did not provide any supporting evidence for the basis of their evaluations.
157

Combinations of these categories were also possible. Some subjects combined both Evidence
and Projection in their responses by citing some evidence (“the person speaking has very high
qualifications”) while at the same time invoking Projection (“the source is familiar”). Some
subjects also combined the Evidence and Description by both mentioning specific evidence
(“The author provides many examples…”) and describing their actions or behaviors (“I wanted
to find an article that…”). Responses coded as 5Ws often just listed the five questions.
Thus, with the addition of a new category and two combined categories, the final
categories used in coding the open-ended responses were:

•
•
•
•
•
•
•

Evidence-based
Projection-based
Intuition-based
Description-based
Evidence/Projection
Evidence/Description
5Ws

Subject responses to the first two reflective prompts were coded according to this rubric, since
two these questions directly related to the criteria used in making evaluation evaluations:
Reflective prompt #1. How did you decide whether a webpage was credible or not?
Reflective prompt #2. What criteria did you use to help you evaluate credibility?

A second Master’s student from the School of Information coded these responses along with the
researcher. The coders initially met to review the rubric, jointly coded several examples, then
each made an independent first pass at coding, and then met again to review the results and
discuss any discrepancies. Most discrepancies centered around the distinction between the
Evidence and Evidence/Projection categories, which sometimes relied on the coder making a
judgment of how much projection was involved in the response. Reviewing and clarifying the
rubric helped to resolve most of the discrepancies in coding. Subsequently, an inter-rater
reliability analysis using the Kappa statistic was performed to determine consistency among the
coders. The result was found to be Kappa = 0.745 (p <.001). Generally, a Kappa value of
between 0.60 and 0.70 is considered substantial agreement (Landis & Koch, 1977). This strong
result allowed the researcher to feel confident in the reliability of the coding results.
158

After responses to both of the open-ended reflective prompts were coded, percentages of
each category out of the total number of responses for each group were calculated. Due to the
difference in sample sizes, and the small number of instances of some codes, statistical tests are
not appropriate and only descriptive analysis was used. Comparisons were made between groups
based on the percentages of each category of evaluation type. Coding results for the first
reflective prompt are shown in Table 44 below. The T1 group had the highest percentage use of
purely Evidence-based responses (74%) than the T2 group (60%) and the CTRL group (65%).
Projection-based responses were used by 5% of T1, 4% of T2 and 4% of T3. The T2 group used
28% Evidence/Projection and the CTRL group used 23% Evidence/Description, while the T1
group used a small percentage of Projection-based responses (5%) slightly larger than T2 and
CTRL (4%).

Table 43. Reflective prompt #1: How did you decide whether a webpage was credible?
Category
Evidence-based
Projection-based
Intuition-based
Description-based
Evidence/Projection
Evidence/Description
5WS
Total

29
2
1
0
0
0
7
39

T1
74.36%
5.13%
2.56%
0.00%
0.00%
0.00%
17.95%
100.0%

15
1
0
1
7
1
0
25

T2
60.00%
4.00%
0.00%
4.00%
28.00%
4.00%
0.00%
100.0%

CTRL
17
65.38%
1
3.85%
0
0.00%
0
0.00%
2
7.69%
6
23.08%
0
0.00%
26
100.0%

Coding results for the second reflective prompt are shown in Table 45. The T1 again group had
the highest percentage use of purely Evidence-based responses (87%) than the T2 group (72%)
and the CTRL group (76%). Projection-based responses were used by 5% of T1, 4% of T2 and
8% of T3. The T2 group used 8% Evidence/Projection, while the CTRL group used 8%
Projection and 4% Evidence/Projection.

Table 44. Reflective prompt #2: What criteria did you use to help you evaluate credibility?
Category
Evidence-based
Projection-based
Intuition-based

33
2
0

T1
86.84%
5.26%
0.00%

T2
18
72.00%
1
4.00%
0
0.00%

CTRL
19
76.00%
2
8.00%
0
0.00%

159

Description-based
Evidence/Projection
Evidence/Description
5WS
Total

0
0
0
3
38

0.00%
0.00%
0.00%
7.89%
100.0%

0
2
0
4
25

0.00%
8.00%
0.00%
16.00%
100.0%

0
1
0
3
25

0.00%
4.00%
0.00%
12.00%
100.0%

The responses to both of the reflective prompts were combined into a total count, shown in Table
46 below. The highest responses overall are highlighted.
Table 45. Total responses to prompts #1 and #2
Category
Evidence-based
Projection-based
Intuition-based
Description-based
Evidence/Projection
Evidence/Description
5WS
Total

63
4
1
0
0
0
10
77

T1
80.52%
5.19%
1.30%
0.00%
0.00%
0.00%
12.99%
100.0%

T2
33
2
0
1
9
1
4
50

66.00%
4.00%
0.00%
2.00%
18.00%
2.00%
8.00%
100.00%

CTRL
19
70.59%
2
5.88%
0
0.00%
0
0.00%
1
5.88%
0
11.76%
3
5.88%
25
100.00%

Overall, the T1 group showed the highest percentage of Evidence-based responses (80%)
compared to T2 (66%) and CTRL (70%). T2 showed the highest percentage of
Evidence/Projection, while CTRL showed the highest percentage of Evidence/Description. The
T1 group also showed the highest percentage of using the 5Ws, which might be simply a
shorthand for repeating what they had been taught, or lack of interest after answering a lot of
questions.
These findings are purely descriptive. Thus, the findings for Research Question 2 are
that the experimental results were not significantly different between groups, so a conclusion
cannot be made. A larger sample size may be necessary to draw a statistical conclusion. On a
descriptive basis, however, the T1 group (which used the tutorial and the IC tool) showed a
higher percentage of solely Evidence-based responses than the T2 group (used only the tutorial)
and the CTRL group (used only the online form). The T2 and CTRL groups showed a higher
percentage use of combined categories (Evidence/Projection or Evidence/Description) rather
than solely based on evidence. This suggests that the T1 group may have looked for and found
more evidence to support their evaluations, while the other groups relied more on projection and
160

description. Thus, the IC tool may have supported T1 subjects in learning that they should use
specific evidence in their credibility evaluations, instead of relying on projection-based or
description-based evaluations. The evidence-based source characteristics such as author
credentials, main ideas, references/links, site domain, contact information, date, and About and
purpose statements were demonstrated through the tutorial and then reinforced through the IC
tool’s Notebook question prompts which asked subjects to record the specific evidence that they
used in their evaluations. This explicit support for identifying specific evidence to be used as the
basis for credibility evaluations was not supplied to the T2 or CTRL groups, and thus the
descriptive difference in outcomes may be attributed to the IC tool.

5.7.	  Metacognitive	  awareness	  
Research Question 3 asked: “Do students who use the IC tool demonstrate greater
metacognitive awareness compared to groups of students who use a tutorial and an online form,
or those who use only an online form?” Two of this study’s research hypotheses are:

•
•

H5: The T1 group will demonstrate greater metacognitive awareness compared to the T2
H6: The T1 group will demonstrate greater metacognitive awareness compared to the
CTRL group

The experimental results were not significantly different between groups, so a conclusion cannot
be made and the hypotheses H5 and H6 are not statistically supported. A larger sample size may
be necessary to draw a statistical conclusion. On a purely descriptive basis, however, the T1
group showed greater metacognitive awareness than the T2 and CTRL groups. The process of
finding these results, through a test of metacognitive skills, is described below.
As part of the post-test, subjects responded to a series of statements regarding their
metacognitive strategies on a Likert scale of Strongly Disagree, Disagree, Agree, or Strongly
Agree (see section 4.4.2 for a description of the metacognition test). The statements are
categorized into three types of metacognition: Planning, Monitoring, and Reflecting (note: these
categories were not displayed to subjects). The test questions are shown in Table 47 below.

161

Table 46. Metacognitive statements by type
Type
Plan 1
Plan 2
Plan 3
Monitor 1
Monitor 2
Monitor 3
Monitor 4
Reflect 1
Reflect 2
Reflect 3
Reflect 4

Statements
I think about what evidence I need to evaluate the credibility of online information
I think of several ways to find evidence for evaluating the credibility of online
information
I ask myself questions about the topic before I begin evaluating the credibility of online
information
I analyze the effectiveness of my evaluation strategies
I compare information from different websites when I evaluate them
I periodically review the evidence I find while evaluating the credibility of online
information
I plan the steps of evaluating the credibility of online information before I start
I ask myself if I found as much evidence as I could once I finish evaluating
I ask myself if I used the best strategies once I finish evaluating
I try to consider multiple perspectives when making evaluations
I try to find evidence to justify and support my evaluations

The subjects’ responses on the Likert scale were converted into a numerical score of
Strongly Disagree = 1, Disagree = 2, Agree = 3, and Strongly Agree = 4 by the Qualtrics survey
software. Scores for each statement were averaged by group. These average scores range from a
low of 2.24 (slightly above “Disagree”) to a high of 3.42 (between “Agree and “Strongly
Agree”). Overall, most subjects rated themselves as 3 or 4. Fewer rated themselves a 2, and
almost none rated themselves as a 1 on any statement. Table 48 below lists the averages for each
metacognition statement by group.
Table 47. Average scores of metacognitive statements by group
Type
Plan 1
Plan 2
Plan 3
Monitor 1
Monitor 2
Monitor 3
Monitor 4

Statements
I think about what evidence I need to evaluate the
credibility of online information
I think of several ways to find evidence for
evaluating the credibility of online information
I ask myself questions about the topic before I begin
evaluating the credibility of online information
I analyze the effectiveness of my evaluation
strategies
I compare information from different websites when
I evaluate them
I periodically review the evidence I find while
evaluating the credibility of online information
I plan the steps of evaluating the credibility of online

T1

T2

CTRL

3.06

3.28

3.12

3.03

3.12

2.81

3.03

3.20

3.00

2.53

2.44

2.50

3.26

3.32

3.42

3.00

2.96

2.81

2.24

2.72

2.35

162

Reflect 1
Reflect 2
Reflect 3
Reflect 4

information before I start
I ask myself if I found as much evidence as I could
once I finish evaluating
I ask myself if I used the best strategies once I finish
evaluating
I try to consider multiple perspectives when making
evaluations
I try to find evidence to justify and support my
evaluations
Overall average

2.88

2.80

2.81

2.71

2.56

2.62

3.26

3.20

3.00

3.41

3.12

3.27

2.95

2.97

2.88

The overall averages for each group are quite similar, with T2 scoring slightly higher overall and
CTRL slightly lower and T1 scoring in the middle of the two. Between-group chi-square tests
were performed for each metacognitive statement, but none of the results were significantly
different.
Since a statistical test was not usable, simpler descriptive techniques were used to
understand trends in the data. For the results in Table 49, the highest score for each statement
was identified to compare each group’s average response per question (highest score is
highlighted). This revealed that the T2 group rated themselves higher than the other groups on
Plan statements, while the T1 group rated themselves higher on Reflect statements, with the T1
having two of the highest Monitor statements and T2 and CTRL one each.
Table 48. Highest average scores of metacognitive statements
Type
Plan 1
Plan 2
Plan 3
Monitor 1
Monitor 2
Monitor 3
Monitor 4

Statements
I think about what evidence I need to evaluate
the credibility of online information
I think of several ways to find evidence for
evaluating the credibility of online
information
I ask myself questions about the topic before I
begin evaluating the credibility of online
information
I analyze the effectiveness of my evaluation
strategies
I compare information from different websites
when I evaluate them
I periodically review the evidence I find while
evaluating the credibility of online
information
I plan the steps of evaluating the credibility of

T1
3.06

T2
3.28

CTRL
3.12

3.03

3.12

2.81

3.03

3.20

3.00

2.53

2.44

2.50

3.26

3.32

3.42

3.00

2.96

2.81

2.24

2.72

2.35

163

Reflect 1
Reflect 2
Reflect 3
Reflect 4

online information before I start
I ask myself if I found as much evidence as I
could once I finish evaluating
I ask myself if I used the best strategies once I
finish evaluating
I try to consider multiple perspectives when
making evaluations
I try to find evidence to justify and support
my evaluations

2.88

2.80

2.81

2.71

2.56

2.62

3.26

3.20

3.00

3.41

3.12

3.27

Since the Reflect tasks involved more a greater degree of level thinking and self-evaluation than
the Plan tasks, the T1 group might be seen as being slightly more self-aware of their own
evaluation processes. Overall, the T1 group had more of the highest scores per statement (6) than
the T2 group (4) and the CTRL group (1). This pattern suggests that the T1 subjects might have
been slightly more aware of their own metacognitive strategies.
For each experimental group, the average scores per statement were sorted from high to
low in order to compare the groups’ self-rating of their strengths and weaknesses (Table 50). For
the T1 group, the statement “I try to find evidence to justify and support my evaluations”
(Reflect 4) received the highest average score (3.41), followed by the statement “I compare
information from different websites when I evaluate them” (Monitor 2, average score 3.26).
Monitor 2 also received the highest score from both the T2 group (3.32) and the CTRL group
(3.42). The T2 group also gave high average scores to the statement “I think about what evidence
I need to evaluate the credibility of online information” (Plan 1, 3.28) and “I ask myself
questions about the topic before I begin evaluating the credibility of online information” (Plan 3,
3.20). The CTRL group also gave high scores to Reflect 4 (3.27) and Plan 1 (3.12). Since these
statements all capture some of the main aspects of the experiment, these scores suggest positive
outcomes from participation in the study for all groups.

164

Table 49. Group average metacognition scores from high to low
Question
Reflect 4
Monitor 2
Reflect 3
Plan 1
Plan 2
Plan 3
Monitor 3
Reflect 1
Reflect 2
Monitor 1
Monitor 4

T1
3.41
3.26
3.26
3.06
3.03
3.03
3.00
2.88
2.71
2.53
2.24

Question
Monitor 2
Plan 1
Plan 3
Reflect 3
Plan 2
Reflect 4
Monitor 3
Reflect 1
Monitor 4
Reflect 2
Monitor 1

T2
3.32
3.28
3.20
3.20
3.12
3.12
2.96
2.80
2.72
2.56
2.44

Question
Monitor 2
Reflect 4
Plan 1
Plan 3
Reflect 3
Plan 2
Monitor 3
Reflect 1
Reflect 2
Monitor 1
Monitor 4

CTRL
3.42
3.27
3.12
3.00
3.00
2.81
2.81
2.81
2.62
2.50
2.35

Interestingly, three of the metacognitive statements consistently scored the lowest across all
groups. The statements “I plan the steps of evaluating the credibility of online information before
I start” (Monitor 4) received the lowest average scores from T1 (2.24) and CTRL (2.35). The T2
group gave their lowest average response to the statement “I analyze the effectiveness of my
evaluation strategies” (Monitor 1, average 2.44). This statement also received the second-lowest
average scores from T1 (2.53) and CTRL (2.50). T2 gave their second-lowest score to the
statement “I ask myself if I used the best strategies once I finish evaluating” (Reflect 2, 2.56).
These are all important metacognitive strategies and the uniformly low scores suggest that
higher-level metacognition may still be a challenge for many of these students.
While there are intriguing descriptive trends in the average scores, overall the subjects
self-rated themselves very highly on this test. This tendency to over-estimate their own abilities
supports the literature that shows students in general are overly-confident in their skills and do
not make accurate judgments of their actual abilities. It is also possible that students don’t simply
over-estimate their skills, but that the concepts of the metacognitive questions could be too
abstract and difficult for students to answer accurately, since they may not be aware of their own
abilities or unable to realistically assess them; another possibility is that social desirability played
a role in influencing the answers, that is none of the students wanted to admit to be low-skilled.
To provide an empirical check on whether the subjects’ self-ratings of their
metacognitive skills related to the quality of the sources that they found, a comparison was
conducted of the average metacognitive score for each individual in the T1 group and the sources
165

they contributed. The 33 subjects in the T1 group were ranked by their average self-rating on the
metacognition test on a scale of Strongly Disagree (1), Disagree (2), Agree (3), or Strongly
Agree (4). They were then divided into three groups by low/medium/high scores. The scores for
the “low” group ranged from 2.45-2.82, the scores for the “medium” group ranged from 2.913.00, and the scores for the “high” group ranged from 3.07-3.09. Due to clustering of scores,
there were 14 subjects in the “low” group, 9 subjects in the “medium” group, and 10 subjects in
the “high” group. A total of 141 sources contributed by the 33 subjects in the T1 group were
identified by genre (see Table 51 below). Note that there is duplication between the sources; they
were counted individually each time they were contributed by a subject.

Table 50. T1 sources categorized by metacognition score
Genre
Blog
Book chapter
Commercial (video)
Dissertation
Magazine
Newspaper
Nonprofit research
Policy brief
Research report
Scholarly article
Student newspaper
Student research paper
Technical report
University PR site
Total

40

Low
63.49%

4

6.35%

26
1
1

Medium
60.47%
2.33%
2.33%

4
3
1

6.35%
4.76%
1.59%

5
3

11.63%
6.98%

5
4
1

7.94%
6.35%
1.59%

4
2
1

9.30%
4.65%
2.33%

1
63

1.59%
100.00%

43

100.00%

18

High
51.43%

1
2

2.86%
5.71%

1
1
8
2

2.86%
2.86%
22.86%
5.71%

1
1

2.86%
2.86%

35

100.00%

While blogs were the highest percentage of sources for all three groups, the high-score group
contributed the smallest percentage (51.43%) compared to the medium-score group (60.47%)
and the low-score group (63.49%). The groups show other differences in types of genres
contributed. The high-score group used a much higher percentage of research reports (22.86%)
than the medium-score group (9.30%) and the low-score group (7.94%). The high-score group
contributed some genres not contributed by the other groups: a dissertation, a policy brief, a
student research paper, and a technical report. All these sources would be considered of higher
166

potential credibility than many of the other genres shown. The medium-score group contributed
one unique source, a book chapter. The medium-score and low-score groups both contributed
commercial videos, newspapers, and student newspapers. The low-score group contributed one
genre not contributed by the other groups: a university PR site. Magazines were contributed by
both the high-score and low-score groups. Scholarly articles were contributed by all three
groups, which may suggest that many students understand that they should use scholarly
databases to find these sources. However, the differences in quality of the other contributed
sources suggests that the subjects who scored themselves the highest on the metacognitive test
tended to contribute better quality genres than the medium-score and low-score groups. This
result suggests that the metacognitive self-ratings, while overall still quite high, may actually
correlate to the ability of the subjects to identify and evaluate sources.
These findings are purely descriptive. Thus, the findings for Research Question 3 are
that the experimental results were not significantly different between groups, so a conclusion
cannot be made. A larger sample size may be necessary to draw a statistical conclusion. On a
descriptive basis, however, the T1 group showed somewhat greater metacognitive awareness
than the T2 and CTRL groups. The IC tool provided metacognitive support through the use of
process maps, progress monitors, and reflective questions, which helped subjects plan, monitor
and reflect on their learning. The metacognitive strategies of planning, monitoring, and reflecting
were supported by the IC tool through the decomposition of the complex task of credibility
evaluation into a sequence of steps, tracking progress in the completion of tasks, and prompts
which asked students to think about what they had learned. This explicit support for
metacognitive strategies was not supplied to the T2 or CTRL groups, and thus the descriptive
difference in outcomes may be attributed to IC tool.

	  
5.8.	  Student	  skills	  
In addition to the findings for the three research questions described above, this study
produced additional findings of interest related to students skill levels in four areas: subjects’
prior experience level with online searching and prior IL instruction, their self-reported
evaluation strategies, their self-reported learning, and their self-evaluation of their skills. The
findings in these four areas are discussed below.
167

	  
5.8.1.	  Experience	  level	  and	  prior	  IL	  instruction	  	  
As part of the preliminary demographic survey, all participants answered background
questions that included their level of experience with searching for information on the Internet
and their prior IL instruction (see section 5.2). After the completion of the study, this selfreported data was compared to the final scores assigned by the coders to subjects’ understanding
of the five credibility criteria (see section 5.5). The intention of this comparison was to determine
whether the subjects with higher self-reported levels of prior experience searching for
information on the Internet and prior IL instruction received higher scores. For each subject, an
overall average of their understanding scores was calculated. A Pearson’s chi square test was
performed to determine if either of the two demographic variables (Internet experience or prior
IL instruction) was related to the final scores. In both cases, no significant difference was found
in scores between subjects who reported greater experience or prior IL instruction and those who
did not. This suggests that students’ level of experience in searching the Internet does not
correspond to learning online credibility evaluation skills, so that even if students are highly
experience searchers they cannot be expected to have learned better credibility evaluation skills.
It also suggests that students who have received prior IL instruction have not learned better
online credibility evaluation skills. Two possible explanations for this are that either the IL
instruction that was received did not cover online credibility evaluation, or if it did, that students
simply did not retain what they learned from this instruction. In either case, these results suggest
that students cannot be expected to have “picked up” how to evaluate the credibility of online
information from their past experience with searching online or even from prior IL instruction.
Explicit instruction in techniques of online credibility evaluation is needed in order for students
to learn these important skills.
5.8.2.	  Self-­‐reported	  evaluation	  strategies	  	  
As part of the post-test, students were asked to respond to the reflective prompt “What
strategies did you use to evaluate credibility?” This question sought to elicit self-reported
descriptions of subjects’ behavioral strategies in evaluating credibility. Responses were openended and narrative, ranging in length from terse to descriptive, as with the responses discussed
168

above. The prompt was intended to elicit metacognitive reflection on the overall strategies that
subjects employed. Instead, the subjects instead tended to respond with brief descriptions of
particular actions or behaviors they employed.
Many responses mentioned checking the author’s background and looking for links to
other sources or citations, both in exploring the particular website for information on the author’s
background, or more generally performing background research. This was a common response
among all three groups.
T1: I looked online to find out about the credibility of the writer
T2: I looked them up online and researched.
CTRL: Internet research and website analyzing
Some responses characterized this specifically as using Google.
T1: I would look the person up on google (sic), linked in (sic) or other sites to find their
background information.
T2: I googled (sic) both the people and researched into their backgrounds.
Performing background research on the author outside of the site itself is an effective strategy
that was not specifically encouraged by the IC tool, but seemed to be a pre-existing skill
employed by some subjects.
Many responses mentioned a reliance on the domain name as a primary indicator of
credibility, which is often one of the primary criteria taught in K-12 library classes, and was also
reinforced by the IC tool.
T1: I determined whether the entire site was credible based on the domain name
T1: I looked at whether the link ended in .edu, .org, or .com
CTRL: My first criteria was to identify if the website was a .edu site of .com site.
CTRL: I checked what kind of webpage it was (.org/.gov/.com, etc.)
Although relying on the domain name alone is problematic as the entire basis for credibility
judgments, many subjects seemed to be aware of the importance of the domain name as a
criteria.
One specific strategy that was mentioned by a few subjects was the practice of skimming
or scanning a website for key evidence of criteria before making an evaluation.

169

T1: I looked at/skimmed each website to see which one seemed to be the most
professional or referred to the most outside sources.
T2: Scanning all information and then deciding whether the website should be deemed
credible and to what degree
CTRL: I scanned the document first, looking at titles and section headers and graphs.
This snap-judgment isn't perfect, but it's pretty effective in predicting the quality of
content.
Employing scanning or skimming is an appropriate strategy when making evaluation judgments,
although knowing which criteria to rely on is vitally important. The example comments seem to
rely on a “snap judgment” heuristic, since they mention impressions of professionalism or
organizational elements such as section headers rather than graphs. However, awareness of
scanning and skimming as strategies is valuable for students.
As with the responses to the reflective prompts discussed under RQ2, some subjects
simply responded that they employed the 5Ws as a strategy, without providing any more detail.
T1: Look into who, what, when, what, why
T2: I made sure each page I visited answered all five credibility questions. If it did not, I
deemed that page not credible.
CTRL: Went through the list of questions/criteria provided for the research activity, and
checked how many times the answer to those questions was "yes."
While not providing much any specificity on the way that they applied this strategy, the fact that
subjects may have learned the practice of applying a structured series of questions (and hopefully
their sub-questions) may indicate that they learned a new strategy for systematically evaluating
credibility.
While many of the responses mentioned similar basic criteria such as the author’s
background, some responses mentioned other criteria that were unexpected:
T2: Identify publisher, research author, view comments as an indicator of how popular
the company's or individual's popularity
T2: I looked up some of the blogs, to see their popularity, what audiences tended to read
them etc (sic)
These comments regarding “popularity” were a surprising finding. Online comments and
popularity of posts are traditionally considered markers of credibility in IL instruction, and could
even be considered the opposite since neither necessarily reflects the quality of the original
content but rather the degree to which it is discussed or shared. However, this factor also was
170

mentioned in response to the reflective prompts to the prompts discussed under RQ2. In response
to the “How did you decide” prompt, one subject responded: “(T)he buzz about this article is
significant – 108 shares on Facebook, 413 Tweets, 200 Emails and 23 Google+ connections”
(CTRL). In response to the “What criteria did you use” prompt, one subject responded “(W)ho
shared this link on facebook or twitter?”(sic) (T2). Apparently, some students include social
media popularity (“buzz”) as part of their credibility evaluations. Relying on popularity as a
measure of credibility may seem natural to today’s students, but it would certainly surprise a
librarian teaching IL skills. However, taking the devil’s advocate position, it could be considered
as a type of new gatekeeping function or endorsement, meaning that this information has been
“approved” through comments, shares, and Tweets by other readers, rather than being vetted by
editors, publishers, or the like. Interestingly, this type of metric is being adopted by scholarly
databases such as Scopus and PLOS. Scopus includes an “Altmetric” feature, which tracks “the
social or mainstream media mentions gathered for a particular paper as well as reader counts on
popular reference managers” using social media sites (e.g. Twitter, Facebook, Pinterest,
Google+), science blogs, and mainstream media outlets (Altmetric for Scopus, 2014). PLOS uses
a feature called “Article-Level Metrics” which includes social bookmarking and dissemination
activity, media and blog coverage, and discussion activity and ratings (Overview: Article-Level
Metrics, 2014). This acceptance of altmetrics by scholarly databases suggests that new forms of
gatekeeping and credibility criteria are emerging online.
While there were some surprises among the responses, overall, there was a great deal of
similarity in the responses given between the groups. Coding the responses along the Evidencebased/Projection-based/Intuition-based rubric did not produce much difference between groups,
as most responses included some type of evidence and none used Intuition-based evaluations.
Thus, the content analysis of these open-ended responses to these reflective responses did not
provide clear evidence of different strategies between groups, but instead provided an interesting
view of the evaluation behaviors of the subjects as a whole.

5.8.3.	  Self-­‐reported	  learning	  
As part of the post-test, students were also asked to respond to the reflective prompt
“What have you learned about evaluating credibility on the web?” This question was intended to
171

elicit self-reported descriptions of what subjects had learned from their experience with the
study. Responses were open-ended and narrative, ranging in length from terse to descriptive, as
with the responses discussed above. Many subjects responded that they had learned that
evaluating credibility is complex and involves multiple factors, and that making judgments
requires exerting effort. This was a common response between all three groups.
T1: I learned that it's important to take into consideration a lot of factors when evaluating
credibility on the web. I came across websites that seemed credible but upon further
investigation did not provide legitimate information.
T2: That there are countless factors that play a part in determining credibility on the web,
but by simply researching the author, citations, and host site you can determine the
credibility much easier and confidently.
CTRL: I learned that there is a lot that goes into the process of researching sources, and
that the process should be systematic involving many steps to narrow down your choices.
Some subjects expressed their surprise that evaluating credibility was challenging.
CTRL: It is a lot more difficult than I thought.
CTRL: It's not always as straightforward as it seems!
Many responses indicated that subjects had learned a process of asking questions about the
credibility of sources.
T1: I have learned that there are many questions you must ask yourself before siting (sic)
a website.
T2: I have learned that it is important to ask yourself many questions before picking a
source.
Along with asking questions, subjects also reported a more generalized awareness of critical
thinking, being skeptical, and not relying on first impressions.
T1: You cannot trust everything that is on the web despite how credible it may appear.
T2: Not everything you see or read online is credible and should be read as true. Anyone
can post something online and it may just be an opinion. You have to do a little bit of
investigating to determine if you should trust what is on a particular page.
CTRL: I have learned that not all websites are as credible as they may seem and to pick
the right ones.
In particular, a few subjects reported an awareness that they cannot rely on the first results from a
Google search.
172

T1: If you only take an article at face value, especially if it one that popped up in a
Google search, then you may be subjecting yourself to poor information.
T2: There are many factors to look into when evaluating a source on the web. Also, just
because it comes up on Google, doesnt mean its a legitimate and truthful source
CTRL: Don't just click the first five links that come up on a google (sic) search! To find a
good credible source you must narrow the search to exclude sources that are filled with
bias and unsupported claims.
The last response is encouraging, especially in light of the fact that the search results overall
seem to have come from the first sites listed in the Google search results.
Of course, not all subjects reported that they had learned anything. Some of the more
experienced searchers among the study subjects commented negatively when asked “What have
you learned?”
T1: Not too much. Most of it was given knowledge that I've picked up from previous
research experiences.
T2: Nothing from this, I do it all the time already and am about to be published.
Given the percentage of prior IL experience in the subject pool, as described above, it is not
surprising that a few of the subjects were already experience with evaluating online credibility.
Since many of the subjects commented on the unexpected complexity and difficulty of
online credibility evaluation, their new awareness of the importance of critical thinking, and the
types of questions they had learned to ask, there is qualitative evidence that at least some of these
subjects learned new skills for credibility evaluation from the experience of this study. While
there were not distinct differences between the responses of the three experimental groups,
overall this content analysis of the open-ended responses provided a qualitative overview of the
self-reported learning of the subjects as a whole.

5.8.4.	  Self-­‐evaluation	  of	  skills	  
As part of the post-test, subjects were given the opportunities to self-evaluate their skill
levels by rating their degree of confidence in their credibility evaluation skills and their
metacognitive strategies. As discussed in Chapter 4, the post-test included the following
questions:
173

•
•
•

Rate how confident you feel about evaluating the credibility of online information
Rate how well you feel you can evaluate the credibility of online information
Rate how challenging you find evaluating the credibility of online information

Each of these questions was answered by moving a slider along a scale marked “Not at all” on
the left end of the scale through “Somewhat” in the middle to “Very” on the right end of the
scale. When the subject placed the slider along the scale, the Qualtrics survey software recorded
the response on a scale of 0-100, with 0 meaning that the subject had placed the slider all the way
to the left to indicate “Not At all” and 100 meaning that the subject had placed the slider all the
way to the right to indicate “Very” in response to the statement. Note that this means for the
“challenging” question the value of responses is reversed, since a response of “very challenging”
is a negative response, as opposed to “very confident” being a positive response. Each subject
responded to all three questions. The average scores for each question by experimental group are
shown in Table 52 below.

Table 51. Post-test average self-evaluation scores
Group
T1
T2
CTRL
Overall

Confident
68.97
75.60
68.24
70.75

Well
73.09
73.24
71.32
72.60

Challenging
44.70
39.96
59.64
47.77

The average scores for both the confidence and well questions were over 70%, showing that
overall the subjects rated themselves very highly. The T2 group rated themselves higher than
both T1 and CTRL on both confidence and how well they feel they can evaluate the credibility
of online information, and rated themselves as finding it less challenging than the other groups (a
lower score meaning “not at all” challenging). The CTRL group was closer to the T2 group on
their confidence and well self-ratings, but rated themselves much higher on how challenging they
found it. This is interesting since it suggests that while the CTRL subjects, who did not use the
IC tutorial or the IC tool and only used the online form in conducting their credibility
evaluations, rated themselves as equally confident as the T1 group (T1 68.97 vs. CTRL 68.24)
174

they also rated the task as more challenging than the T1 group (T1 44.70 vs. CTRL 59.64). Using
the IC tool may have helped the T1 subjects to find the task less challenging. However, the
CTRL group reported equal levels of confidence, which clearly suggests that they over-estimated
their own confidence.
To investigate whether the student self-evaluations accurately reflected their actual
performance, a Pearson’s correlation test was performed on the self-report responses and the
overall understanding scores described in section 5.5. The intent of this test was to determine
whether the subjects were able to accurately self-evaluate their own skill levels by comparing the
self-evaluations to the objectively measured scores of their understanding of the credibility
criteria. For each subject, the self-reported score for each of the three self-evaluation questions
was paired with their average understanding score. In all three cases, no correlation was found
between the subjects’ self-evaluation scores and the coder’s understanding scores. Thus, the
subjects’ ratings of their own confidence in their ability to evaluate the credibility of online
information, how well they feel they can evaluate the credibility of online information, and how
challenging they find evaluating the credibility of online information had no statistical
correlation to the actual scores for their understanding of the credibility criteria.

5.9.	  Summary	  of	  findings	  
The findings from this study are based on the results from three experimental groups with
a total of 84 participants drawn from an introductory undergraduate course. Subjects were
assigned to three experimental conditions: use of the IC tutorial and IC tool, use of only the IC
tool and an online form, and use of only an online form. Assignment of the subjects to the three
experimental conditions was stratified by year in college to control for prior experience level.
The distribution of subjects by year in college and gender were roughly equivalent across the
final group participants, although the CTRL group had a slightly higher level of prior IL
instruction. Students in all three groups searched online for information on a common topic.
Both qualitative and quantitative data was collected from the experimental subjects to
answer three research questions. The findings for Research Question 1 of this study are that
significant improvement can be shown in the scores for understanding of the five credibility
criteria for the T1 group compared to the T2 and CTRL groups. The findings for Research
175

Question 2 of this study are that no significant difference was shown and a conclusion cannot be
made. On a purely descriptive basis, the T1 group gave a higher percentage of Evidence-based
responses and a lower percentage of Projection-based responses than the T2 and CTRL groups.
The findings for Research Question 3 of this study are that no significant difference was shown
and a conclusion cannot be made. On a purely descriptive basis, the T1 group showed slightly
greater metacognitive awareness than the T2 and CTRL groups.
Analysis of the search results show that subjects relied highly on blogs and research
reports as information sources and frequently used first items in the list of Google search results
on the exact phrasing of the research topic. Thus, students were shown to have satisficed in their
searching behavior and settled for the easiest and first results. Scholarly sources were little used.
Subjects also responded to reflective prompts that asked them about their strategies for
evaluating online credibility and what they had learned from the study. Although the responses
were not noticeably different between groups, both prompts provided interesting insights into the
experiences of the experimental subjects, describing various search strategies that they used and
the awareness that they gained about critical thinking and credibility evaluation online.

	  
5.10.	  Limitations	  of	  the	  study	  
There were several limitations in this study that may limit the generalizability of its
findings to broader populations of college students. Since Research Questions 2 and 3 did not
produce statistically significant results, a larger sample size may be necessary to detect an effect.
Although the study design relied upon random assignment to achieve comparability between
groups, the final participants in the CNTRL group subjects had a somewhat higher percentage of
prior IL training than did the T1 and T2 groups. Unintentionally, the experimental conditions
varied by group in the amount of effort required, which impacted the length and thoughtfulness
of some of the qualitative responses. Relying on self-report by subjects was a weakness of the
study design, both in the expectation that students would be motivated to answer numerous
questions about their evaluation strategies and to complete multiple tasks over the duration of the
study. Students’ responses to the metacognitive strategies survey were uniformly high and likely
represent overestimation of their own skill level. Since this study was a separate extra credit
activity not associated with a specific credit-bearing assignment, students may have put less than
176

usual effort into searching for and evaluating sources than they would for graded class
assignment. Although they received extra credit for participating, this research may not have
been viewed as a serious assignment. This may have affected the quality of the sources found
and the evaluations the subjects completed. Potential solutions to the limitations have been
discussed in the sections above.   

  

177

Chapter	  6:	  Discussion	  
The purpose of this experimental study was to investigate the extent to which a custombuilt online credibility evaluation learning tool incorporating scaffolding and metacognitive
support called “InCredibility” positively impacted: 1) students’ understanding of the expert
criteria that constitute credibility evaluations, 2) their application of evidence-based source
characteristics in making credibility evaluations, and 3) their metacognitive awareness of their
own learning and of online information evaluation strategies. The study used both qualitative and
quantitative data analysis methods (coding/scoring, statistical analysis, content analysis) to
provide a multi-dimensional understanding of student behavior and answer the research
questions regarding the impact of the learning tool. Subjects also provided open-ended responses
to reflective prompts describing their evaluation strategies, self-evaluation of their skills, and
what they learned from the study. In addition, the study analyzed the sources that students
contributed in response to a common research topic to examine the subjects’ search and
evaluation strategies.
This chapter summarizes the findings for each research question, the analysis of the
genres of sources that students contributed during the process of the study, and the subjects’ selfevaluation of their skills. Implications for future improvements in both the study design and the
design of the InCredibility tool itself are discussed, as well as for online IL instruction in general.
This chapter concludes with comments on the contributions of this study, and discusses future
directions for research.

6.1.	  Summary	  of	  Research	  Question	  findings	  
This study’s first Research Question asked: ”Do students who use the online credibility
evaluation learning tool demonstrate greater understanding of expert credibility criteria in the
process of evaluating online sources compared to groups of students who use a tutorial and an
178

online form, or those who use only an online form?” This research question was answered by
coding the subject’s open-ended responses to credibility criteria questions and performing a oneway ANOVA test on the between-group differences. In this study, “understanding” was defined
as the subjects’ ability to adequately define the credibility concepts of authority, relevance,
reliability, currency, and purpose, and to describe why these criteria are important to evaluate.
Understanding scores for each criteria were obtained by averaging coders’ scores for two
responses (definition and importance). Subject scores were then averaged for each experimental
group. The results show that the T1 group had higher understanding scores for all of the criteria
compared to both the T2 and CTRL groups, demonstrating that the use of the IC tool improved
the T1 group’s understanding of the each of the credibility criteria. The difference between the
T2 group and the CTRL group was not significantly different, showing that the effect of the IC
tutorial alone was not related to the difference in learning outcomes. These results demonstrate
that use of the IC tool incorporating scaffolding and metacognitive support increased students’
understanding of the expert criteria that they should use in making credibility evaluations. It also
suggests that an online IL learning tool that integrates the process of credibility evaluation into
the online research environment can be effective in helping students learn IL skills.
This study’s second Research Question 2 asked: “Do students who use the online
credibility evaluation learning tool demonstrate greater application of evidence-based source
characteristics as the basis for their credibility evaluations compared to groups of students who
use a tutorial and an online form, or those who use only an online form?” This research question
was addressed by coding of the subjects’ open-ended responses to reflective prompts in the posttest based on the categories from Markey et. al. (2014) of Evidence-based, Projection-based, and
Intuition-based comments. An additional category for Description-based comments was added
during the coding process, as well as combined codes for responses that included elements of
two categories. The results showed no statistically significant difference between the
experimental groups. On a purely descriptive basis, however, the T1 group gave a higher
percentage of solely Evidence-based source characteristics and a lower percentage of Projectionbased responses (or those that combined Projection) than the T2 group and the CTRL group.
Specifically, in response to one prompt “How did you decide whether a webpage was credible or
not?” the T1 group gave a higher percentage of Evidence-based responses (77%) than the T2
group (40%) and the CTRL group (65%), and in response to the second prompt “What criteria
179

did you use to help you evaluate credibility?” the T1 group gave a higher percentage of
Evidence-based responses (87%) than the T2 group (68%) and the CTRL group (76%). The T2
group used 8% combined Evidence/Projection and 4% Description and the CTRL group used 8%
Projection and 4% combined Evidence/Projection responses. These results suggest that the IC
tool may have helped improve the T1 group’s use of specific credibility criteria in making their
credibility evaluations. This suggests that use of the IC tool incorporating scaffolding and
metacognitive supports increased students’ application of evidence-based source characteristics
as the basis for their credibility evaluations.
This study’s third Research Question asked: “Do students who use the online credibility
evaluation learning tool demonstrate greater metacognitive awareness compared to groups of
students who use a tutorial and an online form, or those who use only an online form?” To
answer this question, a custom test of metacognitive skills was conducted as part of the post-test
for all three groups and results were compared between groups to determine any differences in
outcomes based on subject responses to a Likert-scale survey. The results showed that there was
no significant difference between the experimental groups. On a purely descriptive basis,
however, the T1 group reported more high scores for the metacognitive statements (6) than the
T2 group (4) and the CTRL group (1), suggesting that the IC tool may have helped T1 become
more aware of their own metacognition in relation to credibility evaluation, particularly in regard
to reflective strategies. However, it is important to note that overall, subjects in this study
consistently rated themselves very highly on all metacognition questions, which supports the
literature showing that students tend to overestimate their own skills when self-reporting (Gross
and Latham, 2007; Caspers & Bernhisel, 2007).

6.2.	  Additional	  findings	  
Along with the results for the three research questions described above, this study also
produced additional findings of interest regarding the quality of the sources that subjects
contributed, and their self-evaluation of their skills. These additional findings are discussed
below.

180

	  
6.2.1.	  Subjects’	  contributed	  sources	  	  
The online sources that were contributed by the study subjects were categorized by genre
to analyze which types were most frequently used. Blogs and research reports were the most
frequently contributed genres of sources across all groups. Blogs were 59.3% of T1’s sources,
51.9% of T2’s sources, and 29.6% of CTRL’s sources. Research Reports were 12.4% of T1’s
sources, 29.6% of T2’s sources and 40.7% of CTRL’s sources. In the overall total of sources
across all groups, Blog was the overwhelming majority of genres at 55.0%, followed by
Research Report at 17.7% and Newspaper at 8.2%. Several of the most frequently used sources
were found to be among the top results on the first page of results for a Google search on the
exact phrasing of the research topic.
As discussed in section 5.3.1, four of the most frequently contributed sources show up in
in the first page of Google search results for the query “what is the effect of social media on
education.” These results support themes of the LIS literature, which suggest that students often
rely on the top items in a list of search results as recommendations of the best sources and barely
go beyond the first few results pages (Lankes, 2008; Hargittai et al., 2010; Spink, Wolfram,
Jansen & Saracevic, 2001). College students overwhelming rely on Google to the exclusion of
many other academic search tools (Hargittai et al., 2010; Head & Eisenberg, 2011; Kim & Sin,
2011; Kolowich, 2011; Van Soyoc & Cason, 2006). The results also support the findings of the
LIS literature that indicate students often rely on satisficing, the tendency of information seekers
to accept the first satisfactory option vs. higher-quality alternatives, using the minimum amount
of effort judged necessary (Simon, 1956; Agosto, 2002; Thomas, 2004; Gibson, 2008; Warwick
et al., 2009). Research consistently finds that students rely on familiar sites and end searches as
soon as an acceptable result is found (Rieh & Hilligoss, 2008; Warwick et al., 2009; Connaway
et al., 2011). The fact that many subjects relied on the first search results found, and many
contributed the same sources, suggests that the subjects in this study satisficed rather than
expending effort to find sources that were of higher quality. However, the imposed nature of the
query, and the fact that this study was an extra credit opportunity rather than a personal
information need, may have affected the amount of effort that subjects invested in their search.
An unexpected finding of this research was that identifying the genre of the sources
contributed turned out to be more difficult than expected even for the researcher. Determining
181

the intended purpose and credibility of many blogs was complicated by their hybrid nature that
combined multiple existing information source categories (Herring et al., 2004; Scale & QuanHaas, 2012). Even defining the nature of a popular site like the Huffington Post (HP), one of the
most frequently used sources in this study, was difficult. The content of HP consists primarily of
aggregated content from other sites and blog posts by Hollywood actors, retired politicians, and
directors of charities, most of which would be classified as editorial in a traditional newspaper.
The layout and visual design of the site echoes that of a newspaper, as does the title, although the
site also prominently features social media sharing links and user commenting. HP does not give
any definition of itself (the “About Us” link is a staff list) so this study’s author relied on
definitions by other sites, from journalism and industry, to determine how to define the HP
website:

•

•

•

Columbia Journalism Review: “an online newspaper filled with celebrity bloggers and
virally disseminated aggregated content”
(http://www.cjr.org/cover_story/six_degrees_of_aggregation.php)
Crunchbase: “an online news aggregator and blog offering content that includes politics,
entertainment, world news and technology.”
(http://www.crunchbase.com/organization/huffingtonpost)
Mashable: “an American news website, content aggregator, and blog… featuring
columnists and various news sources.” (http://mashable.com/category/huffington-post)

These definitions point to the hybrid nature of this source. The genre of “news aggregator” is
itself an example of a new genre that has emerged on the Internet; it is defined by PC Magazine
as “A Web site that gathers news from different sources and other Web sites” (News Aggregator,
2014). This hybrid genre presents a challenge for students to judge the difference between an
online newspaper, an online news aggregator, and a news website, as variously described in the
definitions quoted above.
Other major news blogs pose similar evaluation challenges. Well-known media outlets
such as NPR, the Wall Street Journal, and the Economist all include blogs. In the case of the
Wall Street Journal, their blog titled “Digits” (http://blogs.wsj.com/digits) features coverage of
“hot topics,” user comments, and social media links. The content of these sites is similar to other
news blogs, but the WSJ branding is very prominent and clearly meant to give the blog
legitimacy. However, if these news blogs are considered equally credible to the traditional news
182

sites that host them, then alternative news blogs that feature similar content and format might be
considered equally credible as well. Two sources cited by subjects in this study’s experiment
provide examples. Socialmediatoday.com describes itself as “an independent, online community
for professionals in PR, marketing, advertising, or any other discipline where a thorough
understanding of social media is mission-critical” which suggests a degree of professional
credibility. InsideHigherEd.com describes itself as ‘the online source for news, opinion and jobs
for all of higher education… founded in 2004 by three executives with decades of expertise in
higher education journalism and recruitment.” These professionally-oriented blogs are hosted on
dot-com sites, which students are often instructed to consider as not credible by traditional IL
instruction. However, their professional experience may qualify them as experts on topics
in their field, even if their expertise is not scholarly. The traditional concept of author expertise is
challenged by the types of “experts” found on blogs: an AVP of Marketing at Pearson, a
corporate marketing and product development officer. These individuals might be considered
experts on their topics and thus have credibility, although perhaps not in the traditional sense of
IL instructors. Another factor in evaluating blog authors is that their credentials or backgrounds
may be difficult to locate on the site or completely absent. Thus, this discussion of specific blogs
and blog authors demonstrates the challenges to traditional IL evaluation posed by the hybrid
nature of new genres that have appeared on the Internet. The Internet is increasingly producing
such hybrid information genres that can often defy traditional evaluation techniques (Crowston et
al., 2010; Markey et al., 2014). This reality demonstrates the importance of IL instruction that is
customized to the online information environment and the importance of students understanding
what different genres consist of, their purpose, and how to critically evaluate their quality.
6.2.2.	  Subjects’	  self-­‐evaluation	  of	  their	  skills	  
As part of the post-test, subjects were given the opportunities to self-evaluate their skill
levels by rating their degree of confidence in their credibility evaluation skills and their
metacognitive strategies on the post-test. Subjects used a slider on a scale of 0-100 to reply to
questions asking them to rate themselves on how confident they feel about evaluating the
credibility of online information, how well they feel they can evaluate the credibility of online
information, and how challenging they find evaluating the credibility of online information. The
183

average scores for both the confidence and well questions were over 70%, showing that overall
the subjects rated themselves very highly. A Pearson’s correlation test was performed to
investigate whether the student self-evaluations accurately reflected their actual performance on
the overall scores for understanding of the credibility criteria. No correlation was found between
the subjects’ self-evaluation scores and the coder’s understanding scores, demonstrating that the
subjects’ self-evaluation of their skills had no statistical correlation to the actual scores for their
understanding of the credibility criteria.
These findings support the literature that shows students tend to overestimate their own
skills when self-reporting, and do not realize their own need for instruction (Gross & Latham,
2007; Caspers & Bernhisel, 2007). Low-skilled students also hold inflated views of their own
competence in information seeking, do not know their own weaknesses, and often overestimate
their abilities to find and evaluate online information (Manuel, 2002). The IC tool was designed
to address this issue by giving students repeated, structured practice in evaluating their own work
as well as the evaluating the quality of other students’ evaluations, and reflecting on their
strategies. Through this tool’s structured skills practice, students learned to evaluate their own
skill level more realistically and compare their own skills to others based on shared performance.

6.3.	  Implications	  for	  the	  InCredibility	  tool	  
This study tested the effectiveness of the IC tool, and produced a number of implications
for improving its design. The design of the IC tool was based on a synthesis of three related but
disconnected fields of research: IL instruction, credibility evaluation, and Computer-Supported
Collaborative Learning (CSCL). Underlying the online credibility evaluation learning tool’s
design is a 3-stage model that is the ACRL’s definition of information literacy—“a set of
abilities requiring individuals to ‘recognize when information is needed and have the ability to
locate, evaluate, and use effectively the needed information” (ALA, 2000). The tool represents
each step of “locate, evaluate, and use” through a division into three stages identified as
Investigate, Question, and Solve. The three stages of the tool give students repeated, structured
practice in evaluating their own work (Investigate) as well as the evaluating the quality of other
students’ evaluations (Question). Through this structured skills practice, students learn to
evaluate their own skill level more realistically and compare their own skills to others based on
184

shared performance. The tool provides scaffolding and metacognitive support through
incorporating a process map of the overall task, progress monitors, and reflective prompts.
One possible enhancement to the IC tool based on the findings from the study would be
the addition of rollover or pop-up texts to reinforce the definitions of the credibility criteria and
their importance during the process of using IC to evaluate sources. The current rubric used for
coding subject responses could be adapted as prompts in the Notebook and/or evaluation stages
of InCredibility. These added prompts would help reinforce the definitions and importance of the
criteria, and potentially increase student understanding of credibility criteria. Table 53 shows
examples of these adapted rubric prompts.
Table 52. Adapted rubric prompts
Criteria
Who/Authority

Definition prompt
Decide if the author is qualified
to write about the topic

Importance prompt
Anyone can post to the Internet so you need to
verify whether the author is qualified to write
on this topic

What/Relevance

Decide if the information is
useful for the research topic

There is a lot of information on the Internet so
you need to choose relevant information for
your research topic

Where/Reliability

Decide if the information is
trustworthy

The sources of online information are not
always apparent so you need to verify them

When/Currency

Decide if the information is upto-date

Up-to-date information is often most accurate,
(although not for every topic) so you need
to verify the date of publication

Why/Purpose

Decide if the site shows bias

The intended purpose of online information is
not always apparent and may influence its
value so you need to check for possible bias

These definition and importance prompts are currently included in the IC tool during the
Question stage, but are not repeated throughout. In the form of rollover or pop-up texts
integrated into the evaluation process, these added prompts would help reinforce the definitions
and importance of the criteria, and potentially increase student understanding of the credibility
criteria.
The IC tool could also be enhanced to further improve the learning outcome of greater
understanding and application of expert credibility criteria while evaluating online sources. One
185

possible enhancement to a future version of the tool would be to expand the use of the current
Tutorial tips about assessing evidence for credibility judgments throughout the InCredibility tool.
These tips could be adapted to pop-ups or rollovers that would reinforce the specific types of
evidence that students should look at for every source that they evaluate, potentially increasing
student application of evidence-based credibility criteria in their evaluations. Table 54 shows
examples of these adapted tip prompts.
Table 53. Adapted tip prompts
Criteria

Evidence prompt

Who/Authority

Look for author’s name, qualifications, and/or biography. Decide if the author
is qualified to publish information on this topic.

What/Relevance

Look for specific facts, keywords or tags that relate to your topic. Decide
what makes this source better than other sources for your question.

Where/Reliability

Look for the URL domain name that tells whether the site is commercial,
educational or non-profit. Decide if the source emanates from a reliable
source for your topic.

When/Currency

Look for dates at the top of an article or at the very bottom of the page.
Decide how important having current is information to your topic.

Why/Purpose

Look for an “About” section that describes the site’s purpose. Look for
evidence of objectivity or possible bias. Decide if advertising might influence
the content.

While these tips are currently included in the IC tool’s tutorial, they are not explicitly repeated in
the subsequent stages of the tool (Question and Solve). In the form of rollover or pop-up texts
integrated into the evaluation process, these added prompts would help reinforce the specific
types of evidence that students should look for, and potentially increase student application of
evidence-based credibility criteria in their evaluations.
Another enhancement to the IC tool would be adapt the current metacognitive post-test
questions as question prompts integrated into the process of using the tool. Specific
metacognitive prompts could be included to remind students to plan, monitor, and reflect on their
tasks during the process of evaluation. These integrated metacognitive question prompts would
prompt students to reflect on their own activities, increase their critical self-awareness, and
186

potentially increase in student awareness of their metacognitive strategies. Table 55 shows
examples of these adapted metacognitive prompts.
Table 54. Adapted metacognitive prompts
Category
Planning

Prompt
What information do you need to evaluate the credibility of this source?
What questions about the topic should you ask before you begin evaluating the
credibility of this source?
What are several ways to find evidence for evaluating the credibility of this source?

Monitoring

What steps will you take to evaluate the credibility of this source?
How effective are your strategies for evaluating this source?
Did you compare information from different websites when evaluating this source?
Did you periodically review the evidence you found while evaluating the credibility of
this source?

Reflecting

What specific evidence did you find to justify and support your evaluation?
Did you look at the evidence from different perspectives when evaluating this source?
Was there a better way you could have found evidence when you evaluated this
source?
Did you find several different types of evidence when you evaluated this source?

These integrated metacognitive questions would prompt students to focus on their own
evaluation strategies and increase their critical self-awareness. This could result in an increase in
student awareness of their metacognitive strategies. However, one potential drawback to this
approach is that it would significantly increase the number of questions that students would need
to answer when using the IC tool and thus increase the amount of effort required. Students might
not want to answer so many questions. Perhaps the questions could be randomized and only one
displayed for each source evaluated and saved.
Based on the overall low quality of subjects’ sources, enhancements could be made to a
future version of the IC tool to support students in finding higher quality sources. Displaying
suggested search terms or keywords as alternatives to searching on the exact topic phrase might
187

encourage students to try different variations of searches. Displaying the keywords used by other
students in their searches or even the results other students might help students realize how
common the most frequently used sources are and might motivate them to find more unique
sources. Rules could be added that an individual source can’t be used more than X times, or that
certain common search terms could not be used when searching. These enhancements would
guide students towards doing more than natural language search, encouraging them to go beyond
their inclination to satisfice by relying on the first few Google search results and help them
become better online searchers.
Another potential enhancement to the IC tool would be to increase the capabilities for
collaborative evaluation activities to help motivate participation and stimulate peer learning. In
today’s online information environment, credibility is often not determined by the individual, but
“within a community engaged in a larger conversation” (Lankes 2008, 114). Today’s students
often prefer a networked, participatory learning environment (Davidson & Goldberg, 2009;
Halse & Mallinson, 2009; Thomas & Brown, 2011). Traditional IL instruction models such as
one-shot classroom sessions may not connect effectively with today’s students who are used to
more social forms of learning (Costello et al., 2004; Manuel, 2002; Gibson, 2008; Leach &
Sugarman, 2005). Social learning software tools support participatory knowledge creation
through networking, socialization, communication and engagement with communities of
learning and are “increasingly being recognized as essential scaffolds and learning tools”
(McLoughlin & Lee, 2008, p. 649). Collaborative filtering and peer-review systems such as
recommender or reputation systems allow users to pool their intellectual and experiential
resources, transforming credibility evaluation into a collaborative rather than an individual effort
(Metzger, 2007). Not only the criteria for evaluation but also the processes by which credibility
evaluation is taught need to reflect the realities of online information seeking. For example,
practices of collaborative inquiry can encourage students to participate in online verification
strategies to assess author credentials (Metzger et al., 2003) or to share and compare sources
through organized, collaborative online searches (Todd, 2000). To reinforce the real-world value
of these practices, collaborative learning opportunities should be employed in the context of real
classroom assignments (Harris, 2008) and embrace the Internet as “a means of creating
communities and fostering collaboration” (Rieh & Danielson, 2007, p. 350). Social software
tools are “increasingly being recognized as essential scaffolds and learning tools” (McLoughlin
188

& Lee, 2008, p. 649) because their affordances support participatory knowledge creation through
networking, socialization, communication and engagement with communities of learning
(McLoughlin & Lee, 2008).
Although students were able to comment on each other’s evaluations in current the IC
tool, participation in these activities was low. Expanding the opportunities for group participation
in the information evaluation process might increase student engagement by enhancing social
interaction. Originally, the IC tool was intended to provide a question asking-and-answering
functionality that allowed the higher-skilled students (as based on tutorial performance) to
answer questions from the lower-skilled through threaded discussions. This planned feature was
eliminated during the development process due to time and budget constraints. Further
exploration of the use of collaborative, social, and participatory methods for online credibility
evaluation instruction is an intriguing possibility for future research.

6.4.	  Implications	  for	  study	  design	  
This study employed an experimental design with randomized assignment to heighten the
statistical validity and generalizability of the results. Randomized assignment with stratification
was used to assign subjects to the experimental groups by grade level, in order to ensure a
comparable composition to the original distribution of grade levels in the entire class, making the
study more ecologically valid. The stratified random assignment also equalized the distribution
of year levels between groups, and thus helped control for the level of experience between
groups. Subjects were randomly assigned to one of the three study groups: the Treatment 1 group
(T1) used both the IC tool and the introductory tutorial, the Treatment 2 group (T2) used the
tutorial and an online form to record their sources, and the control (CTRL) group used only the
online form. Each group worked independently on the same research topic: “What is the impact
of social media on education?” The timeline of the tasks was kept equal between the three
groups, although the amount of effort required may have been unintentionally higher for the T1
group. After competing the study, all three groups responded to a post-test that included
questions about the definition and importance of the credibility criteria, used to find the overall
understanding score for each subject, as well as self-evaluation questions about the subjects’
skills, and a Likert-scale survey of metacognitive strategies.
189

After conducting this study and analyzing the results, some issues with the study design
became evident. These issues along with proposed study redesign suggestions to address the
issues in any further iterations of the IC tool are described below:
Issue 1:
While the findings for Research Question 1 produced significant differences between the
experimental groups, the findings for Research Questions 2 and 3 did not.
Proposed Redesign:
A larger sample size might produce statistically significant differences between the
groups by producing greater variations in responses.
Issue 2:
The CNTRL group subjects had a somewhat higher percentage of prior IL training than
did the T1 and T2 groups, which may have effected the study outcomes by providing
those subjects with more prior experience and knowledge of the credibility evaluation
criteria and process and increasing their understanding scores.
Proposed Redesign:
Stratify the random subject assignment not only by year in college but also by prior IL
instruction, in order to ensure equal distribution of past experience across all the
experimental groups, to increase the validity of the study results.
Issue 3:
The study relied on self-report by students of their behavior in seeking evidence and
evaluating it. Students may have over-estimated how well they did, and the self-reported
results may not necessarily correlate to better quality results.
Proposed Redesign:
The subjects’ actual search outcomes could be qualitatively compared to an expert’s
search results, which would confirm or disconfirm the results of students’ self-reports.

190

Issue 4:
Since the T2 and CTRL did not use the IC tool, logfile data was not collected for those
experimental groups, which made it impossible to quantitatively compare their online
behaviors to the T1 group
Proposed Redesign:
Collect log-file data on the T2 and CTRL groups, which could include sites visited, links
clicked, time spent on sites, and the overall patterns of their searching and evaluation
behavior, be able to compare patterns of user actions and time spent on tasks between the
experimental groups
Issue 5:
It is likely that the experimental conditions varied in the amount of effort between the
experimental groups, by requiring the T1 group to perform more detailed and repetitive
tasks, and answer more questions, which is suggested by the dropoff in responses from
the T1 groups answers on the open-ended prompts, as opposed to the greater participation
in providing the open-ended responses from the T2 and CTRL groups
Proposed Redesign:
The amount of effort required across the experimental groups could be equalized, either
by reducing the total number of questions asked to the T1group, by reducing the total
quantity of tasks required for T1, or increasing the number of questions and tasks
assigned to the T2 and CTRL groups
Issue 6:
The responses to the open-ended prompts from the T1 group were overall briefer and less
thorough than the responses from the T2 and CTRL groups
Proposed Redesign:
The quality of participation while using the IC tool could increased by incentivizing more
in-depth responses to the open-ended prompts, either through making the IC tool part of a
graded in-class assignment, or by integrating the use of the tool into the syllabus instead
of being an extra credit option, so that student participation was tied to an in-class grade

191

Issue 7:
Subjects’ self-evaluations on the metacognitive test were overall very high, which meant
that differences between the groups were quite small and statistical analysis was not
possible, although the for RQ 3 showed a trend toward the T1 group’s having greater
metacognitive awareness
Proposed Redesign:
Instead of relying on a one-time test of metacognitive awareness at the end of the study,
measures for tracking students’ metacognitive strategies and skills could be integrated
throughout the study design, perhaps by integrating prompts about planning, monitoring,
and reflecting strategies into the IC tool’s interface and into the online form at all three
stages of the evaluation process (Investigate, Question, Solve)
A future study that employs the above proposed redesigns to addresses the issues described
would (1) produce data with greater comparability between the experimental groups to allow
greater confidence in comparisons of learning outcomes, (2) produce quantifiable data about
subjects’ behavior patterns that would allow for a comparison of metacognitive strategies, and
(3) incentivize greater participation and contribution of more thoughtful, in-depth and complete
answers to question prompts. These changes would produce new experimental results yielding
more detailed quantitative data that would allow for additional statistical testing, and also
yielding more extensive qualitative responses to help answer the research questions in greater
depth.
In hindsight, a disproportionate amount of the researcher’s time and effort was put into
designing and developing the IC tool, working with multiple coders to direct its development,
testing and bug-fixing, and pilot testing. Although this effort was necessary for the scale of the
IC tool, it resulted in less time being spent on designing the control condition, working through
details such as how to capture logfile data for T2 and CTRL groups, and pilot testing the T2 and
CTRL conditions with test subjects. Spending more time and effort on these steps could have
helped minimize some of the issues with the study design discussed above.

192

	  
6.5.	  Implications	  for	  online	  IL	  instruction	  
Along with supporting effective online IL instruction, the IC tool addresses some of the
challenges to teaching IL identified in the literature review:
1. The “faculty problem”: Faculty may not view librarians as educational partners but may
regard them as support staff and providers of support services (Owusu-Ansah 2004;
Manuel, Beck & Molloy 2005; McGuiness 2006). Some librarians feel that faculty are
either apathetic or outright obstructive towards efforts to collaborate on IL instruction
(McCarthy 1985). Faculty may feel that librarians are not qualified to be teachers
(Saunders 2012). They may also be unwilling to cede valuable in-class time to librarians
(Hardesty 1995; Breivik & Jones 1993; Owusu-Ansah 2004; Hrycaj & Russo 2007). A
tool that can be integrated into existing in-class assignments without requiring librarian
instruction sessions might increase faculty receptivity to adding IL instruction to their
course. Librarians could still be involved through supporting the assignments, and
perhaps through incorporating chat reference functionality into the tool so that students
could contact librarians with questions at the point of need during their research process.
2. Library Anxiety: Students may feel a sense of powerlessness when they begin an
information search that requires using the library, involving feeling lost, fearful of library
staff, and unable to navigate the library (Mellon, 1986). The IC tool addresses this barrier
by situating library instruction in the online context where students normally do their
research, rather than placing them physically in the library, or bringing a librarian
physically to the classroom. By integrating the credibility process into the context of a
real-life, in-class assignment, the IC tool situates credibility evaluation in a familiar
setting where students feel comfortable. This issue could also be addressed by building
on-demand chat reference into the structure of the IC tool, as mentioned above.
3. Lack of integrated IL curriculum: Only a small percentage of higher education
institutions with first-year experience programs include a required IL component (Boff &
Johnson 2002). Overall, IL is not a required component of most academic curriculum.
Inclusion of IL instruction in the curriculum often relies on the advocacy of individual
193

librarians (Weiner 2012), although librarians often lack political leverage within the
academic community, making it difficult for them to create change (McGuiness 2006).
These difficult conditions mean that broad integration of IL into undergraduate education
remains an aspiration rather than a fully realized ideal (McGuiness 2006). A tool that can
be integrated into existing in-class assignments could help achieve greater reach of IL
instruction to more students without requiring curricular changes and without relying on
departmental or college-wide approval.
4. Outmoded IL teaching methods: Delivery of library instruction through the traditional
lecture method is ineffective and does not engage today's students (Costello et al., 2004).
One-shot instruction sessions cannot provide students with the sustained practice required
to learn, apply and master IL competencies (Mokhtar et al. 2008, Mery et al. 2012).
Information literacy needs to be reinforced over a longer period of time with appropriate
scaffolding and guidance (Chu, et al., 2011). The traditional lecture-based course is
ineffective for Gen Y students, who prefer more active learning environments (Manuel,
2002). Students consider required, for-credit IL classes as their least preferred means of
getting library instruction, compared with individual instruction conducted at the point of
need while students are actively seeking information (Davidson, 2001). These students
prefer an online format for of IL instruction because it allows them to actually use the
skills that they are learning about (Anderson & May, 2010). The IC tool integrates IL
instruction into the real life context where students do their research, and makes
instruction more relevant by connecting it to actual assignments and online research
practices.
Based on the findings of this study and the review of the literature, several implications for
effective IL instruction regarding credibility evaluation of online sources emerged:

•

Subjects reported using appropriate information seeking skills such as skimming sources
to get the main idea and conducting Internet searches (“Googling”) to investigate an
author’s background. IL instruction should build on these types of pre-existing search
strategies when teaching online credibility evaluation skills, in order to connect with
students’ prior knowledge and scaffold them to more advanced skills such as techniques
194

for finding specific evidence to support their credibility evaluations (checking the
“About” link, searching for information about the author, Google Scholar, limiting
searches to scholarly sources). IL instruction today does not need to focus
•

Subjects in the study showed a strong indication of relying only on the first results of a
Google search. IL instructors should search with students on a class research topic,
discussing how and why Google generates those results, and why the results are not an
endorsement of credibility

•

Subjects in the study demonstrated little ability to accurately identify the genre of hybrid
online genres (such as identifying the Huffington Post as a newspaper because its layout
looks like a newspaper). IL instruction should teach students strategies for how to
evaluate the credibility of blogs and other hybrid Internet information sources, by
demonstrating actual examples of typical search results rather than preselected examplars
of traditional formats such as magazines and journal articles

•

Subjects in the study showed little understanding of how to evaluate the “semi-academic”
sources that are easily found online, such as independent research reports and non-peer
reviewed online-only journals. IL instruction should teach ways to evaluate these types of
sources, as well as understanding of how the scholarly research process works.

•

Subjects in the study readily accepted claims of expertise from all types of online authors
and content producers. IL instruction should address the varied forms of expertise that are
often invoked online, and how to evaluate them as evidence of credibility including
differentiating commercial content from educational content.

•

Subjects in the study cited popularity measures of social media (likes, shares, comments
or “buzz”) as measures of credibility. IL instruction should address these types of
popularity measures that students are familiar with and may interpret as measures of
credibility, and discuss how to evaluate them, particularly in light of the growing
acceptance of “altmetrics” in scholarly databases such as Scopus and PLOS.

These implications for online credibility evaluation instruction demonstrate the many challenges
that traditional IL instruction faces in the new online information environment. Given the time
and scheduling constraints often placed on IL instructors, and the difficulty of adding more
content to already limited instruction sessions, it is much more likely that these goals could be
195

met through integrating an online credibility evaluation tool into already existing classroom
assignments, rather than trying to cover all these topics as a stand-alone instruction session
divorced from students’ practical online research experience. Addressing these challenges
through the use of an online IL instruction and credibility evaluation learning tool offers a new
approach to traditional IL instruction that meets the needs of students faced with the difficulties
of evaluating online information, as well as the goals of IL instructors who seek to use teaching
methods and content that are relevant to today’s students, and to overcome the challenges to
integrate IL instruction into the classroom and reach greater numbers of students

6.6.	  Contributions	  	  
In today’s online information environment with its new hybrid genres and lack of
traditional credibility markers, IL and credibility evaluation skills cannot be taught effectively
using traditional library-based methods, or in an isolated, one-shot approach that does not
support repeated real-life practice and reflection. Although today’s students are fluent with and
reliant upon the Internet as their primary source of information, IL instruction is primarily
conducted in one-shot sessions or library-based classroom instruction that focuses on vetted,
authorized information sources. It is also divorced from the in-class context of students’ actual
research assignments. These traditional instructional approaches cannot provide students with the
engagement and sustained practice required to learn, apply and master IL competencies
(Mokhtar et al. 2008, Mery et al. 2012). Customizing credibility instruction to the online
information environment is particularly important because of the lack of conventional quality
control mechanisms and indicators of authority from traditional print-based formats (Rieh, 2002;
Gasser et al., 2012; Metzger et al., 2010). The markers of credibility in the print-based paradigm
that were traditionally maintained by professional gatekeepers such as editors and reviewers are
often lacking on the Internet, and web pages typically offer few reliable cues to credibility that
students can use in their evaluations (Burbules, 2001; Iding et al., 2008; Mackey & Jacobsen,
2011; Rieh & Danielson, 2007). Learning and practicing the techniques of online credibility
evaluation in the real-life context of academic research is important to make IL instruction
relevant to today’s students.
196

This research investigated the integration of IL instruction directly into the online
information environment where students actually do their research through the use of a browser
plugin (the Notebook) that allows students to evaluate sources and capture specific evidence for
their credibility judgments at the time of need. An important contribution of this research is the
demonstration of scaffolding and metacognitive support incorporated into the IC tool. To support
effective student learning, the IC tool employs scaffolding that decomposes a complex, highlevel task into simpler, easy-to-understand units (Quintana, et al. 2004). This scaffolded structure
breaks down the complex task of making credibility evaluations about online information
sources into a systematic process. The browser-based Notebook presents basic questions to be
answered, organized into a familiar visual format of a notebook with a “tab” for each of the five
questions to be answered. The student gains mastery over the process by completing each of the
smaller question-answering steps first, gathering together the evidence that will be later required
for making a credibility evaluation. At first, however, the student is only required to enter
answers to straightforward questions (Who is the author? What are their qualifications?).
Completing the questions in the Notebook for each source gives students practice in completing
the steps of a systematic process of gathering specific evidence to support credibility evaluations.
To support students in building cognitive links between prior knowledge (experience
using Google and Wikipedia to find answers) and new knowledge (the critical evaluation of
online information using evidence-based judgment), the IC tool scaffolds students from simple
concepts in a familiar context (questions about Who, What, Where, When, and Why) to the
higher-level concepts of Authority, Relevance, Reliability, Currency, and Purpose. The 5Ws
questions employ non-expert language that presumes no prior knowledge of credibility
evaluation. In the later stages, the 5Ws are mapped to more sophisticated credibility criteria
language, providing scaffolding to bridge the gap between students’ unsophisticated
understanding of online information evaluation and the more sophisticated models of evaluation
criteria used by experts.
The Notebook also scaffolds learning by automating routine tasks such as recording
URLs and taking notes into a single tool that is available at the point of need (while searching
online) and automatically gathers all the needed information into an easily-accessed format on
the IC website. Students can save and return to their notes about saved websites, and later will

197

use these notes as basis for their credibility evaluations. Automating these routine tasks saves
cognitive effort and allows students to focus on the important learning tasks.
To help students monitor their progress while completing their credibility evaluations, a
process map on the IC Headquarters page visually represents the three stages of the process
(Investigate, Question, and Solve) as a conceptual organizer. As students complete each stage,
these bars progressively fill in with a new color to indicate completion of the tasks. Time-based
reminders show students their progress and remind them of their goals. These features
decompose the complex task of evaluating online information into more manageable steps, help
students schedule and manage their time rather than waiting until the last minute, and help
students understand that credibility evaluation is a systematic process, not a one-time action.
To support articulation and reflection, the IC tool employs scaffolding that guides
students in reviewing their own understanding and making it explicit (Quintana, et al. 2004).
During the Investigate stage, students are prompted to enter comments on each source they
evaluate, explaining why they rated it as they did. At the end of the stage, they are prompted to
review their work and make any changes they feel necessary. A prompt asks “How confident in
your answers?” to encourage self-reflection. During the Question stage, other students can see
these comments and respond to them. Students receive comments on their own sources and make
comments on others. During the Solve stage, students are prompted to choose between two
sources and explain their rationale for choosing which is better, and then articulate their own
understanding of the credibility criteria that they used. These scaffolds help students make
explicit the steps they take in making credibility evaluations, and to review their own
understanding.
Scaffolds also support students in developing their metacognitive skills. The IC tool
provided metacognitive support through the use of process maps, progress monitors, and
reflective prompt, which helped subjects plan, monitor and reflect on their learning. Through
introducing explicit systematic structure to what had previously been a simplistic action, a
student’s self-regulation and self-evaluation processes are enhanced (Ge & Land, 2004; Pifarre
& Cobos, 2010). The IC tool supports students in the important metacognitive skills of planning
their tasks, monitoring their progress toward meeting goals, taking appropriate steps to solve
problems, and reflecting on past performance (Quintana et al., 2005, p. 2360). At each stage of
the IC tool, the unfamiliar and challenging process of evaluating the credibility of online
198

information is structured as a systematic process requiring planning, monitoring, and reflection.
As students complete each stage of the learning tool and gain repeated practice in each activity,
they learn how to regulate their online searching behavior and reflect on their own skills and
understanding and to reflect on their own thinking. By giving student repeated practice in
reviewing and commenting on the evaluations of others, the IC tool supports students in
externalizing and comparing their knowledge and beliefs with those of their peers (Sharma &
Hannafin, 2007).
The incorporation of scaffolding and metacognitive support into an online IL instruction
and credibility evaluation learning tool offers a new approach to traditional IL instruction that
meets the needs of students faced with the challenges of evaluating online information, as well as
the goals of IL instructors who seek to use teaching methods and content that are relevant to
today’s students, and to integrate IL instruction into the classroom and reach greater numbers of
students. The IC tool supports the ongoing movement in IL teaching philosophy and pedagogy
away from the concept of the traditional “sage on the stage” model and toward the contemporary
model of “guide on the side” (Doyle, 1994), embracing cognitive, constructivist and inquirybased models of learning (Stripling, 2010). Librarians have shifted their educational goals from
teaching students how to locate materials using specific library tools to teaching students how to
deal with information in any format located anywhere (Thompson, 2002), and the IC tool
supports integration of IL instruction and practice into the online information environment where
students do the majority of their research. The use of the IC tool supports the expansion of the
librarian’s role from simply teaching retrieval skills to incorporating “a more total research
environment in the course of finding and using information/knowledge” (Owusu-Ansah, 2004, p.
5). The explicit modeling and support of metacognitive strategies and exploring how these
techniques can support IL instruction is also a significant contribution to the field.
This study is unique in its application of metacognitive measurement to IL instruction.
No similar work was found in the IL literature, although scaffolding and metacognition have
been studied in other fields, e.g. science (Quintana, et al., 2004; Azevedo, 2005; Quintana, Zhang
& Krajick, 2005; Raes, 2012; Tanner, 2012), education and psychology (Kauffman, 2004; Iding,
2008; Pifarre & Cobos, 2010), educational media (Bannert, Hildebrand & Mengelkamp, 2009),
pharmacy (Ge, Planas & Er, 2010; Ge, 2013), and specific domains such as reading
comprehension and writing skills (Lin, 2011). However, there has been little research on the
199

application of scaffolding and metacognitive support to teaching students IL and credibility
evaluation skills (Gorrell et al., 2009; Bannert & Mengelkamp, 2013). This lack of attention to
metacognition is a significant gap in IL instruction, because IL can be seen as inherently
metacognitive in that it encourages individuals to become aware of their search and evaluation
skills and apply them to specific information needs (Booth, 2011). Both the Information Literacy
Competency Standards for Higher Education and the Guidelines for Information Literacy in the
Curriculum describe IL as essentially metacognitive (ACRL, 2000; MSCHE, 2003). Any
training in IL skills should not only equip students with guidelines to help them assess the
credibility of websites, but should also encourage them to reflect on the process of evaluation
(Madden et al., 2011). Since students do not spontaneously engage in metacognitive thinking
unless they are specifically encouraged to do so, it is important to include metacognitive support
in learning environments (Lin, 2001). Effective searching of the web is a complex process of
reasoning and decision-making (Todd, 2000), and strong self-regulation ability and
metacognitive awareness are necessary in order to be successful in web-based learning (Raes et
al., 2012). The use of metacognitive scaffolds can help students to develop strategies to be more
critical in their evaluation of the credibility of online information sources (Iding et al. 2008).
Online learning tools like IC can incorporate metacognitive scaffolding and explicit strategies for
planning, monitoring, and reflection into the process of learning IL skills. This research explores
integrating IL and metacognition instruction, and the results suggest the potential for greater
metacognitive awareness and skill development through the use of a scaffolded online learning
tool.

6.7.	  Significance	  
The Internet has transformed the nature of information literacy and credibility evaluation.
The new hybrid genres and forms of expertise that are emerging online no longer conform to the
traditional instructional model of print-based IL instruction. The checklist-based approach to
evaluating sources often employed by librarians does not simply transfer to the online
information environment, which requires more complex and challenging strategies. IL
instruction needs to be adapted to better suit the specific context of online information sources.
Since students tend to be fluent with finding information online and consider themselves to be
200

skilled searchers (regardless of their actual ability), they often do not see the relevance of IL
instruction that appears to be library-based and focused on traditional genres of information
(magazines, books). However, IL skills are even more important and necessary in the online
information environment than they have been before. To effectively reach these students and
help them understand the importance and value of IL skills in the inline information
environment, this dissertation’s research developed an innovative strategy for providing a new
kind of information literacy education customized for the needs of today’s students (Gross and
Latham, 2007). It explores the opportunity for IL instruction to incorporate the use of new
information technology and social media features to help students assess the credibility of online
information. This research sought to investigate how to teach IL in a way that supports how
“young people think and work” (Harris, 2008, p. 172) while taking into account the “newer
behaviors emerging in digital environments” (Rieh & Hilligoss, 2008, p. 50).
Not only the criteria for evaluation but also the processes by which credibility evaluation
is taught need to reflect the realities of online information seeking. Employing collaborative
learning opportunities in the context of real classroom assignments helps to reinforce the realworld value of these practices (Harris, 2008), embraces the Internet as “a means of creating
communities and fostering collaboration” (Rieh & Danielson, 2007, p. 350), and helps today’s
learners “use new technologies to participate in virtual communities where they share ideas,
comment on one another’s projects, and plan, design, implement, advance, or simply discuss
their practices, goals, and ideas together” (Davidson & Goldberg, 2009, p. 12). This study
explored a novel pedagogical technique for teaching online credibility evaluation to today’s
students, and showed that the custom-built online credibility evaluation tool improved students’
learning outcomes in their understanding of expert credibility criteria in the online information
environment. It has also contributed practical implications for IL study design, IL tool building,
and online IL instruction.
This research synthesized work from three distinct fields: IL instruction, online
credibility evaluation and Computer-Supported Collaborative Learning. Each of these fields has
a well-established research tradition but they are often segregated into disciplinary silos and do
not interconnect. This research explores areas of overlap and synergy between these fields, while
also producing an empirically tested learning tool. This tool embeds IL training in the online
information environment, links online credibility evaluation research with IL practice, employs
201

scaffolding and metacognitive support for learning, and incorporates interactivity and
participatory engagement. One significant contribution of this research is the development of a
new pedagogical approach to teach effective online IL skills to today’s students through
integrating these disparate approaches into a unified design. Thus, this research makes a
theoretical contribution to the field of IL research by suggesting that related findings and
perspectives from online credibility evaluation research and Computer-Supported Collaborative
Learning be synthesized with IL to explore the online information environment experienced by
today’s students.
The students in this study explained in their own words what they learned from the use of
the IC tool with its scaffolding and metacognitive support. Many subjects responded that they
had learned that evaluating credibility is complex and involves multiple factors, and that making
judgments requires exerting effort. One student stated: “I learned that it's important to take into
consideration a lot of factors when evaluating credibility on the web. I came across websites that
seemed credible but upon further investigation did not provide legitimate information.” Another
student said: “I learned that there is a lot that goes into the process of researching sources, and
that the process should be systematic involving many steps to narrow down your choices.” This
awareness of the challenges of online credibility evaluation also extends to a greater awareness
and appreciation of critical thinking skills: “You cannot trust everything that is on the web
despite how credible it may appear.” Some students reported learning the important lesson that
they should not rely on thirst results from a Google search: “just because it comes up on Google,
doesnt mean its a legitimate and truthful source.” Overall, students indicated that credibility
evaluation is challenging and requires effort: “It is a lot more difficult than I thought” and “It's
not always as straightforward as it seems!” These statements by students demonstrate that they
learned important IL concepts from using the IC tool, and that they gained knowledge about
credibility evaluation strategies that they can continue to use throughout their academic and
professional careers.
This research explored the question of how IL instructors, educators, and instructional
designers can help students think critically about online information. Given the easy availability
of unreliable and non-credible information on the Internet, it is crucial that we support critical
thinking in the online information environment for today’s students. IL instruction cannot rely on
older models of print-based formats and clearly identifiable exemplars of genres. The new hybrid
202

genres that are emerging on the Internet require unique approaches to credibility evaluation that
are specific to the types of evidence available online. Since traditional gatekeepers and indicators
of credibility are often not available in the online environment, IL instruction needs to be
customized to support new critical thinking practices. Overcoming students’ ingrained
tendencies to expect easy results from Web searching, to rely on the first results of a Google
search, and to employ only superficial evaluation of credibility, is a key challenge to IL
instruction today.
Overall, this research demonstrates that IL instruction needs to address the specific
challenges of online credibility evaluation, and that scaffolding and metacognitive support in the
form of an online learning tool can effectively integrate IL instruction into the online information
environment where students actually do their research.

6.8.	  Future	  research	  
There are several possible directions for further research based on the findings of this
study. The proposed improvements to the study design discussed in Section 6.6 provide
possibilities for interesting future work, for example, capturing detailed log-file data for all three
experimental groups to enable greater comparative analysis of potential differences in their
searching and evaluation behaviors. In particular, it would be interesting to empirically measure
the difference in subject behaviors based on metacognitive prompts. Recording detailed data on
the search behaviors, time on task, and patterns of evaluation for all the experimental groups in
the study would produce quantifiable data about subject behavior patterns that would allow
comparison of metacognitive strategies as a result of different metacognitive interventions. This
research could produce valuable insights into the effectiveness of metacognitive supports
supported by empirical data.
Another possible direction for future research would be through implementing
the design implications discussed in section 6.7. An expanded IC tool that supported greater
collaborative commenting and peer-review functionalities would address the current trend of
student research behavior in which credibility evaluation is a collaborative rather than an
individual effort. One example would be to incorporate the question asking-and-answering
203

functionality that was originally planned for the IC tool, allowing higher-skilled students (as
based on tutorial performance) to answer questions about sources from lower-skilled students
through threaded discussions. This social, participatory learning capability would scaffold the
interactivity that students are accustomed to in social media with learning opportunities tied
directly to the credibility evaluation tasks and integrated into the online learning environment.
Additional avenues for further research would involve exploring the implications for
effective IL instruction regarding online sources discussed in section 6.8, particularly focusing
on the critically evaluating the hybrid genres of information that currently exist and continue to
evolve online. Examples include teaching students strategies for evaluating (1) the credibility of
blogs and other hybrid Internet information sources, (2) the quality of “semi-academic” sources
such as independent research reports and non-peer reviewed online-only journals, (3) the varied
forms of professional expertise that are often invoked online, and (4) the popularity measures of
social media that students may interpret as measures of credibility (likes, shares, comments or
“buzz”).
Building on the implications from this study, further possible future research could
involve testing the IC tool with different subject populations in different in different academic
environments, such as high school or community college students. These different populations
might have different levels of prior IL instruction and levels of experience with searching the
Internet, and thus might provide different and potentially greater learning outcomes. Subject
populations from different socio-economic statuses other than an elite public university might
provide different outcomes. The IC tool could also be implemented in other public informationseeking settings such as public libraries where the emphasis is not on academic research but on
personal interests. Studying the use of the IC tool in these environments would likely produce
new insights into the online research practices of different populations and approaches to
supporting diverse information seeking practices.
The IC tool could also be applied to other types of scaffolded online learning tools in
areas other than credibility evaluation, such as collaborative online learning environments like
MOOCS or in collaborative research practices in libraries. The IC tool could also be integrated
into an existing LMS system such as U-M’s CTools or other systems such as Blackboard or
Moodle. This type of integration is made possible through employing the Learning Tools
Interoperability (LTI) specification. Doing so would make InCredibility instantly implementable
204

in any UM classroom that uses CTools, and potentially to any other LMS that employs the LTI
framework. Integrating the tool into an LMS would make it easily accessible to students through
a resource that they already use, and would connect it to the assignments and resources that they
already use as part of their class. Since online chat reference with librarians is also available
through CTools and other LMS systems, this would provide a complementary resource that also
supports academic research at the point it is most useful and relevant to students, when they are
doing actual online research for academic assignments.
While the IC tool was designed to be specific to online information sources and
credibility evaluation by college students, it could also be modified and extended to other
potential use cases and populations. Since identifying and evaluating the types of hybrid genres
that are emerging online has proven to be challenging, an IC-like tool could be customized to the
process of identifying and evaluating the quality of information sources such as blogs, non-peer
reviewed online journals, or “educational” sources such as TV documentaries. The process of
evaluating these types of genres would involve learning to identify specific evidence that should
be used in evaluating their reliability and credibility, such as what is the purpose of the site, who
owns the site, is it commercially motivated, to what degree is there editorial vetting of the site,
and what criteria the site has for who can write or post content. A genre-identification tool such
as this could help students overcome their natural instinct to accept websites at face value and
make simplistic, superficial judgments that are often influenced by visual layout and design.
Instead, they would learn how to critically evaluate genre as an important part of understanding
and using online information.
Another potential application of the IC tool design would be to identifying the nature of
expertise as it is invoked online. Sources such as the Huffington Post, the Discovery channel, and
social media blogs present a wide array of contributors as experts, but determining the degree to
which these contributors are actual experts in the subject area is challenging. The process of
evaluating expertise is more difficult than evaluating credibility, but it could involve searching
online to find evidence about the author’s background and qualifications, their other writing or
posting online, critical evaluations of the authors themselves from other online sources, and even
their scholarly output and academic citations. An expertise-identification tool such as this could
help students to understand that expertise is complicated and situational, and that they should not

205

automatically accept claims of expertise that are made online, but use critical thinking about who
is an acceptable expert for their information need.
Overall, there is great potential for future research to study the application of IL skills in
the online information environment where today’s students do their academic research and
personal information seeking. Learning effective IL skills is critical to success in today’s
economy and society, and online learning tools offer a unique opportunity to teach the “skills,
knowledge and expertise students should master to succeed in work and life in the 21st century”
(Partnership for 21st Century Skills, 2011). The online information environment presents many
challenges to IL instruction, but also offers opportunities. Utilizing the affordances of social
software tools in online learning presents opportunities to support participatory knowledge
creation through networking, socialization, communication and engagement with communities of
learning (McLoughlin & Lee, 2008). IL instruction must embrace the practices and skills of
today’s students and their reliance on the Internet as a primary source of information, and must
embrace the new hybrid genres of information and the resulting strategies for effective
credibility evaluation. New pedagogical methods are needed to teach these effective online IL
skills. This study investigated one potential avenue for online IL pedagogy, and there are many
more to explore. Insights gained from this study will be useful for IL instructors, librarians,
instructional designers, and researchers interested in promoting online information credibility
evaluation skills.

	  

206

Appendix	  1:	  Kapoun’s	  five criteria for evaluating Web pages (1998)

Evaluation of Web documents

How to interpret the basics

1. Accuracy of Web Documents
Accuracy
•
•
•

Who wrote the page and can you contact him
or her?
What is the purpose of the document and why
was it produced?
Is this person qualified to write this
document?

•
•

Make sure author provides e-mail or a
contact address/phone number.
Know the distinction between author and
Webmaster.

2. Authority of Web Documents
Authority
•
•
•

Who published the document and is it separate
from the "Webmaster?"
Check the domain of the document, what
institution publishes this document?
Does the publisher list his or her
qualifications?

•
•

What credentials are listed for the
authors)?
Where is the document published? Check
URL domain.

Objectivity
3. Objectivity of Web Documents
•
•
•

What goals/objectives does this page meet?
How detailed is the information?
What opinions (if any) are expressed by the
author?

4. Currency of Web Documents
•
•
•

When was it produced?
When was it updated?
How up-to-date are the links (if any)?

5. Coverage of the Web Documents

•

•

Determine if page is a mask for
advertising; if so information might be
biased.
View any Web page as you would an
infommercial on television. Ask yourself:
why was this written and for whom?

Currency
•
•
•

How many dead links are on the page?
Are the links current or updated regularly?
Is the information on the page outdated?

Coverage

207

•
•
•

Are the links (if any) evaluated and do they
complement the documents' themes?
Is it all images or a balance of text and
images?
Is the information presented cited correctly?

•

•
•

If page requires special software to view
the information, how much are you
missing if you don't have the software?
Is it free or is there a fee to obtain the
information?
Is there an option for text only, or frames,
or a suggested browser for better viewing?

Putting it all together
•
•
•
•
•

Accuracy. If your page lists the author and institution that published the page and provides a way of
contacting him/her and . . .
Authority. If your page lists the author credentials and its domain is preferred (.edu, .gov, .org, or
.net), and, . .
Objectivity. If your page provides accurate information with limited advertising and it is objective
in presenting the information, and . . .
Currency. If your page is current and updated regularly (as stated on the page) and the links (if any)
are also up-to-date, and . . .
Coverage. If you can view the information properly--not limited to fees, browser technology, or
software requirement, then . . .
You may have a Web page that could be of value to your research!

FROM: Kapoun, Jim. "Teaching undergrads WEB evaluation: A guide for library instruction."
C&RL News (July/August 1998): 522-523.
Converted to HTML by Paul McMillin, September 18, 1998
Minor textual corrections: 10 May 2010 [MOE]
Retrieved from: http://olinuris.library.cornell.edu/ref/research/webcrit.html
  

208

Appendix	  2:	  CRAAP	  Test	  	  

Evaluating Information – Applying the CRAAP Test
Meriam Library & California State University, Chico
When you search for information, you're going to find lots of it . . . but is it good information?
You will have to determine that for yourself, and the CRAAP Test can help. The CRAAP Test is
a list of questions to help you evaluate the information you find. Different criteria will be more
or less important depending on your situation or need.
Currency: the timeliness of the information
•
•
•

When was the information published or posted?
Has the information been revised or updated?
Is the information current or out-of date for your topic?
o Are the links functional? (For the Web)

Relevance: the importance of the information for your needs
•
•
•
•
•

Does the information relate to your topic or answer your question?
Who is the intended audience?
Is the information at an appropriate level (i.e. not too elementary or advanced for your
needs)?
Have you looked at a variety of sources before determining this is one you will use?
Would you be comfortable using this source for a research paper?

Authority: the source of the information
•
•
•
•
•
•

Who is the author/publisher/source/sponsor?
Are the author's credentials or organizational affiliations given?
What are the author's credentials or organizational affiliations given?
What are the author's qualifications to write on the topic?
Is there contact information, such as a publisher or e-mail address?
Does the URL reveal anything about the author or source?
o examples: .com (commercial), .edu (educational), .gov (U.S. government),
.org (nonprofit organization), or .net (network) (For the Web)

Accuracy: the reliability, truthfulness, and correctness of the content, and
•

Where does the information come from?
209

•
•
•
•
•

Is the information supported by evidence?
Has the information been reviewed or refereed?
Can you verify any of the information in another source or from personal knowledge?
Does the language or tone seem biased and free of emotion?
Are there spelling, grammar, or other typographical errors?

Purpose: the reason the information exists
•
•
•
•
•

What is the purpose of the information? to inform? teach? sell? entertain? persuade?
Do the authors/sponsors make their intentions or purpose clear?
Is the information fact? opinion? propaganda?
Does the point of view appear objective and impartial?
Are there political, ideological, cultural, religious, institutional, or personal biases?

Retrieved from: http://www.csuchico.edu/lins/handouts/eval_websites.pdf

	  
	  

210

Appendix	  3:	  Kathy	  Schrock’s	  5Ws	  
	  
THE FIVE W’S OF WEB SITE EVALUATION
WHO
Who wrote the pages and are they an expert? Is a biography of the author included? How can I
find out more about the author?
WHAT
What does the author say is the purpose of the site? What else might the author have in mind for
the site? What makes the site easy to use? What information is included and does this
information differ from other sites?
WHEN
When was the site created? When was the site last updated?
WHERE
Where does the information come from? Where can I look to find out more about the sponsor of
the site?
WHY
Why is this information useful for my purpose? Why should I use this information? Why is this
page better than another?

©2001-2009. Kathy Schrock. All rights reserved. Page may be reproduced for classroom use.
Retrieved from: http://kathyschrock.net/abceval/5ws.htm

211

Appendix	  4:	  University	  of	  Michigan	  Library	  5Ws	  
University of Michigan
Evaluating Information on the Web
The 5 Ws – Who, What, Where, When, and Why
Criteria

What to Look for

Who?
Who wrote the information? What are the
author’s credentials? Who sponsors/publishes
the site? Can I learn more about the sponsor of
the site?

Author’s name, credentials, a biography and
resume; phone or mailing address or means to
contact the author (not just email); Look at the
URL (.com, .edu, etc), and an “About” page

What?
What information in presented? What is the
purpose of the site? Is the information
objective, complete?

An “About/Purpose” section to see the author’s
stated purpose of the page; is the information
presented objectively, can you detect any bias?
Find out if other web sites refer to this site.

Where?
Where does the information come from? Are
there links to reliable external web sites? Can I
verify the source of the information?

Bibliographies, notes, or references; verify that
links go to credible web sites. You should be
able confirm info you find elsewhere too.

When?
Is the information presented actually current
for the topic at hand? Has it been updated
recently?

Last updated dates; whether the information
presented seems outdated compared to other
sources.

Why?
Why would you use this information over other Articulate what makes it better than other
information available? Does it fit your research sources of information on the web for your
goals?
purposes
212

Based on Kathy Schrock’s 5 Five Ws of Web Site Evaluation http://kathyschrock.net/abceval/5ws.htm
Retrieved from: http://www.lib.umich.edu/shapiro-undergraduate-library/diy-toolkit-modules-teachingresearch-concepts#Module_Four

213

Appendix 5: Prototype Tutorial Questions, Tips and Answers
The tutorial page shows a static screenshot of a page. A question displays at the top of the screen.
Each question begins “Look for clues about (question) and click on the evidence.” Clicking on
the correct area shows a highlight around the answer and displays “Right!” and the answer. A
“Tip” button shows a popup box with the first tip. Clicking on a wrong area shows a popup box
with “Try again” and the second tip. Clicking on a second wrong area shows the answer.
Question

Tip #1

Tip#2 (“Try again.”)

Answer (“Right!”)

WHO wrote this

Is an author’s name

Look for an author’s

Always look for author’s name,

information?

listed? Is there

name or biography. If the qualifications, and/or biography.

information about the

author is not named, is

Decide if they are qualified to

author’s expertise?

there an editor or group

publish this information.

name?
WHAT kind of

What topics does this

Look for words and topics Always look for specific facts,

information is it?

source cover? Is the

in the text that relate to

keywords or tags that relate to your

information useful for

your topic. Is the

topic. Decide what makes this

your research?

information useful?

source better than other sources for
your question.

WHERE does this

Who hosts or publishes

information come from? the site? Is there contact

Look for the URL domain Always look for the URL domain
name (.com, .edu, .org). Is name that tells whether the site is

info for the host or

there a “Contact” link?

publisher?

Are there links to credible profit. Decide if the source is
sources?

WHEN was information Is the information
written?

commercial, educational or nonreliable for your topic.

Look for a date when the Always look for dates at the top of

presented current? Has it information was posted or an article or at the very bottom of
been updated recently?

updated. Are there

the page. Decide how important

references in the text to

having current is information to

current events?

your topic.

214

WHY was this

What is the purpose of the Look for an “About” link. Always look for an “About” section

information written?

site? Is there evidence of Do any statements seem

that describes the site’s purpose.

bias?

Look for evidence of objectivity or

to be opinion? Does it

seem objective or biased? possible bias. Decide if advertising
Is there advertising related might influence the content.
to the content?

215

Appendix 6: Pilot study metacognition test results
Response options and numerical values:
Strongly Disagree = 1
Disagree = 2
Neither Agree nor Disagree = 3
Agree = 4
Strongly Agree = 5
Question	  
I	  learn	  more	  information	  when	  I	  am	  interested	  in	  the	  topic	  
I	  try	  to	  use	  strategies	  that	  have	  worked	  well	  in	  the	  past	  
I	  think	  about	  what	  I	  really	  need	  to	  evaluate	  before	  I	  begin	  
I	  consciously	  focus	  my	  attention	  on	  important	  information	  
I	  ask	  others	  for	  help	  when	  I	  don’t	  understand	  something	  
I	  try	  to	  translate	  new	  information	  into	  my	  own	  words	  
I	  ask	  myself	  questions	  about	  the	  subject	  before	  I	  begin	  
I	  can	  motivate	  myself	  to	  learn	  when	  I	  need	  to	  
I	  ask	  myself	  periodically	  if	  I	  am	  meeting	  my	  goals	  
I	  am	  a	  good	  judge	  of	  how	  well	  I	  understand	  something	  
I	  ask	  myself	  how	  well	  I	  accomplished	  my	  goals	  once	  I’m	  
finished	  
I	  know	  what	  kind	  of	  information	  is	  most	  important	  to	  
evaluate	  
I	  am	  good	  at	  organizing	  information	  
I	  focus	  on	  the	  meaning	  and	  significance	  of	  new	  
information	  
I	  ask	  myself	  if	  there	  was	  an	  easier	  way	  to	  do	  things	  after	  I	  
finish	  
I	  consider	  several	  alternatives	  strategies	  before	  I	  begin	  
I	  find	  myself	  using	  helpful	  strategies	  automatically	  
I	  find	  myself	  pausing	  regularly	  to	  check	  my	  comprehension	  
I	  organize	  my	  time	  to	  best	  accomplish	  my	  goals	  
I	  find	  myself	  analyzing	  the	  usefulness	  of	  strategies	  
I	  ask	  myself	  questions	  about	  how	  well	  I	  am	  doing	  while	  I'm	  
evaluating	  
I	  am	  good	  at	  remembering	  information	  

SD	  
1	  
0	  
0	  
0	  
2	  
0	  
1	  
1	  
0	  
0	  

D	  
NAD	  
0	  
1	  
1	  
1	  
3	  
11	  
3	  
12	  
7	  
6	  
7	  
10	  
6	  
9	  
7	  
10	  
6	  
9	  
7	  
9	  

A	  
13	  
19	  
27	  
30	  
23	  
23	  
29	  
24	  
34	  
31	  

SA	  
40	  
34	  
13	  
10	  
17	  
15	  
10	  
13	  
6	  
8	  

Mean	  
4.65	  
4.56	  
3.93	  
3.85	  
3.84	  
3.84	  
3.75	  
3.75	  
3.73	  
3.73	  

1	  

6	  

11	  

28	  

9	  

3.69	  

0	  
2	  

4	  
6	  

15	  
12	  

31	  
23	  

5	  
12	  

3.67	  
3.67	  

1	  

3	  

12	  

36	  

3	  

3.67	  

0	  
0	  
0	  
0	  
3	  
2	  

8	  
8	  
8	  
9	  
9	  
9	  

15	  
11	  
15	  
15	  
13	  
13	  

22	  
31	  
27	  
24	  
18	  
23	  

10	  
5	  
5	  
7	  
12	  
8	  

3.62	  
3.6	  
3.53	  
3.53	  
3.49	  
3.47	  

1	  
1	  

9	  
10	  

13	  
13	  

27	  
25	  

5	  
6	  

3.47	  
3.45	  

216

I	  periodically	  review	  information	  to	  help	  me	  evaluate	  
important	  criteria	  
I	  ask	  myself	  if	  I	  have	  considered	  all	  the	  options	  while	  I'm	  
evaluating	  
I	  have	  a	  specific	  purpose	  for	  each	  strategy	  I	  use	  
I	  think	  of	  several	  strategies	  and	  choose	  the	  best	  one	  
I	  know	  how	  well	  I	  did	  after	  I	  finish	  
I	  ask	  myself	  if	  I	  have	  considered	  all	  options	  after	  I	  finish	  
I	  know	  when	  each	  strategy	  I	  use	  will	  be	  most	  effective	  
I	  ask	  myself	  if	  I	  have	  learned	  as	  much	  as	  I	  could	  have	  once	  I	  
finish	  

2	  

7	  

15	  

27	  

4	  

3.44	  

1	  
0	  
0	  
0	  
2	  
1	  

11	  
10	  
14	  
15	  
14	  
16	  

14	  
16	  
13	  
11	  
16	  
17	  

23	  
26	  
22	  
26	  
19	  
20	  

6	  
3	  
6	  
3	  
4	  
1	  

3.4	  
3.4	  
3.36	  
3.31	  
3.16	  
3.07	  

4	  

16	  

15	  

17	  

3	  

2.98	  

217

Appendix	  7:	  Online	  Form	  for	  Control	  Groups	  
	  
This online form was used by the Control Group subjects. It provides the same credibility
questions and criteria as the Notebook, without the prompts and tips.
PART ONE
Enter the URL of the website:
Who is/are the author(s)?
What are the author’s qualifications?
What can you find out about their background?
What are the main topics?
What type of site is it? (commercial/educational/governmental/news/opinion/scholarly)
How useful is this information for your topic? (A little/Somewhat/Very)
Where is this site hosted or published?
What is the site’s domain name? (.com, .edu, .gov)
Are there links to supporting evidence?
When was this webpage published or copyrighted?
Has it been updated?
How important is having current information for your topic? (A little/Somewhat/Very)
Why do you think this site was created? (to educate/inform/ persuade/sell)
Do you see evidence of bias? (Yes/No/Not sure)
If yes, what is the evidence?
This section provides the conceptual scaffolding used in the tool.
PART TWO
The questions you’ve been asking about your sources are very important. Clues about
“Who, What, Where, When, and Why” answer questions which experts call Authority,

218

Relevance. Reliability, Currency, and Purpose
Clues about WHO wrote this information tell you about AUTHORITY, or if the author is
qualified to write about the topic
Clues about WHAT kind of information it is tell you about RELEVANCE, or if the
information is useful for your topic
Clues about WHERE this information comes from tells you about RELIABILITY, or if the
information is trustworthy
Clues about WHEN this information was written tells you about CURRENCY, or if
information is current, and whether currency is important
Clues about WHY this information was written tells you about PURPOSE, or if the site
shows bias that may influence the information
This section provides the reflective prompts used in the tool.
PART THREE
Looking back on the process of evaluating the credibility of online information....
1. How did you decide whether a webpage was credible or not?
2. What specific criteria did you use to help you evaluate credibility?
3. What strategies did you use to evaluate credibility?
4. How confident were you in your evaluations of credibility?
5. What have you learned about evaluating credibility on the web?

219

Appendix 8: Schraw and Dennison Metacognitive Awareness Inventory (1994)
Knowledge of Cognition
• Declarative knowledge (DK)
• Procedural knowledge (PC)
• Conditional knowledge (CK)
Regulation of Cognition
• Planning (P)
• Information management skills (IMS)
• Monitoring (M)
• Debugging strategies (DS)
• Evaluation (E)
I ask myself periodically if I am meeting my goals. (M)
I consider several alternative to a problem before I answer. (M)
I try to use strategies that have worked in the past. (PK)
I pace myself while learning in order to have enough time. (P)
I understand my intellectual strengths and weaknesses. (DK)
I think about what I really need to learn before I begin a task. (P)
I know how well I did once I finish a test. (E)
I set specific goals before I begin a task. (P)
I slow down when I encounter important information. (IMS)
I know what kind of information is most important to learn. (DK)
I ask myself if I have considered all options when solving a problem. (M)
I am good at organizing information. (DK)
I consciously focus my attention on important information. (IMS)
I have a specific purpose for each strategy I use. (PK)
I learn best when I know something about the topic. (CK)
I know what the teacher expects me to learn. (DK)
I am good at remembering information. (DK)
I use different learning strategies depending on the situation. (CK)
I ask myself if there was an easier way to do things after I finish a task. (E)
I have control over how well I learn. (DK)
I periodically review to help me understand important relationships. (M)
I ask myself questions about the material before I begin. (P)
I think of several ways to solve a problem and choose the best one. (P)
I summarize what I’ve learned after I finish. (E)
I ask others for help when I don’t understand something. (DS)
220

I can motivate myself to learn when I need to. (CK)
I am aware of what strategies I use when I study. (PK)
I find myself analyzing the usefulness of strategies while I study. (M)
I use my intellectual strength to compensate for my weaknesses. (CK)
I focus on the meaning and significance of new information. (IMS)
I create my own examples to make information more meaningful. (IMS)
I am a good judge of how well I understand something. (DK)
I find myself using helpful learning strategies automatically. (PK)
I find myself pausing regularly to check my comprehension. (M)
I know when each strategy I use will be most effective. (CK)
I ask myself how well I accomplished my goals once I’m finished. (E)
I draw pictures or diagrams to help me understand while learning. (IMS)
I ask myself if I have considered all options after I solve a problem. (E)
I try to translate new information into my own words. (IMS)
I change strategies when I fail to understand. (DS)
I use the organizational structure of the text to help me learn. (?)
I read instructions carefully before I begin a task. (P)
I ask myself if what I’m reading is related to what I already know. (IMS)
I reevaluate my assumptions when I get confused. (DS)
I organize my time to best accomplish my goals. (P)
I learn more when I am interested in the topic. (DK)
I try to break studying down into smaller steps. (IMS)
I focus on overall meaning rather than specifics. (IMS)
I ask myself questions about how well I am doing while learning something new. (M)
I ask myself if I have learned as much as I could have once I finish a task. (E)
I stop and go back over new information that is not clear. (DS)
I stop and reread when I get confused. (DS)

221

Appendix 9: Raes et al. adapted Metacognitive Awareness Inventory (2012)
	  
(Adapted from Schraw & Dennison, 1994)
(NOTE: The complete inventory of questions was provided courtesy of Annelies Raes (translated
from Dutch)
Knowledge of cognition
I try to use strategies that have worked in the past when searching the Internet for information
I am good at organizing the information I find on the Internet.
I know what kind of information is most important to find
I have a specific purpose for each strategy I use when searching for information on the Internet
I am good at remembering the information I found on the Internet
I can motivate myself to understand information I find on the Internet when I need to
I am a good judge of how well I understand the information that I find on the Internet
I find myself automatically using helpful strategies to find information on the Internet
I know when each strategy I use for finding information on the Internet will be most effective
I understand the information I find on the Internet better if I am interested in the topic (I learn
more if I am interested in the topic)
I use different learning strategies depending on the situation
I am aware of what strategies I use when I study
Regulation of cognition
I ask myself questions about the subject before I begin searching for information on the Internet
I think about what information I really need to find before I begin searching on the Internet
I think of several ways to find information on the Internet and choose the best one
I organize my time to best accomplish finding good information
I find myself analyzing the effectiveness of my searching strategies
I compare information from different Websites before I choose one
I periodically review to help me understand information I find online
I ask myself periodically if I am finding the best information
I ask myself questions about how well I am doing while searching for information on the
Internet.
I ask myself how well I accomplished my goals (answered my research question?) once I finish
searching the Internet
I ask myself if I have considered all options when searching for information
I find myself pausing regularly to check my comprehension when searching on the Internet
I know how well I did at finding information once I am done searching the Internet
I ask myself if there was a better way to find information after I finish searching the Internet
I ask myself if I have considered all options after I finish searching the Internet
222

I ask myself if I found as much information as I could once I finish searching

223

Appendix 10: Demographics Chi Square Tests
Case Processing Summary
Cases
Valid
N
Group * Year

Missing
Percent

82

N

100.0%

Total

Percent
0

N

0.0%

Percent
82

100.0%

Group * Year Crosstabulation
Count
Year
1

Group

Total

2

3

4

CTRL

7

10

4

3

24

T1

9

12

6

6

33

T2

7

11

3

4

25

23

33

13

13

82

Total

Chi-Square Tests
Value

df

Asymp. Sig. (2sided)

Pearson Chi-Square

.881

a

6

.990

Likelihood Ratio

.907

6

.989

N of Valid Cases

82
Case Processing Summary
Cases
Valid
N

Group * Gender

Missing

Percent
82

100.0%

N

Total

Percent
0

0.0%

N

Percent
82

100.0%

Group * Gender Crosstabulation
Count

224

Gender
1

Group

Total
2

CTRL

11

13

24

T1

16

17

33

T2

19

6

25

46

36

82

Total

Chi-Square Tests
Value

df

Asymp. Sig. (2sided)

Pearson Chi-Square

5.824a

2

.054

Likelihood Ratio

6.078

2

.048

N of Valid Cases

82

Case Processing Summary
Cases
Valid
N
Group * Experience

Missing

Percent
82

N

Total

Percent

100.0%

0

0.0%

N

Percent
82

100.0%

Group * Experience Crosstabulation
Count
Experience
2

Group

3

Total
4

CTRL

3

13

8

24

T1

4

16

13

33

T2

4

12

9

25

11

41

30

82

Total

Chi-Square Tests
Value

df

Asymp. Sig. (2sided)

Pearson Chi-Square

.442a

4

.979

Likelihood Ratio

.435

4

.980

N of Valid Cases

82
Case Processing Summary

225

Cases
Valid
N

Missing

Percent

Group * PriorIL

82

N

100.0%

Total

Percent
0

0.0%

N

Percent
82

100.0%

Group * PriorIL Crosstabulation
Count
PriorIL
1

Group

Total
2

CTRL

14

10

24

T1

18

15

33

T2

14

11

25

46

36

82

Total

Chi-Square Tests
Value

df

Asymp. Sig. (2sided)

Pearson Chi-Square

.081

a

2

.960

Likelihood Ratio

.081

2

.960

N of Valid Cases

82

	  

226

Appendix	  11:	  RQ1	  Chi	  Square	  Tests	  	  

Descriptives
Score
N

Mean

Std.

Std. Error 95% Confidence Interval for Mean

Deviation

Lower Bound

Minimum

Maximum

Upper Bound

1

33

1.2288

.32429

.05645

1.1138

1.3438

.25

1.70

2

25

1.0252

.21323

.04265

.9372

1.1132

.62

1.33

3

24

.9963

.34211

.06983

.8518

1.1407

.38

1.57

Total

82

1.0987

.31611

.03491

1.0292

1.1681

.25

1.70

ANOVA
Score
Sum of Squares
Between Groups

df

Mean Square

.945

2

.473

Within Groups

7.148

79

.090

Total

8.094

81

F

Sig.

5.224

.007

Post Hoc Tests
Multiple Comparisons
Dependent Variable: Score
Tukey HSD
(I) Group

(J) Group

Mean

Std. Error

Sig.

Difference (I-J)
1
2
3

95% Confidence Interval
Lower Bound

Upper Bound

.20359

*

.07976

.033

.0131

.3941

.23254

*

.08070

.014

.0398

.4253

1

-.20359

*

.07976

.033

-.3941

-.0131

3

.02895

.08596

.939

-.1764

.2343

1

-.23254

*

.08070

.014

-.4253

-.0398

2

-.02895

.08596

.939

-.2343

.1764

2
3

*. The mean difference is significant at the 0.05 level.

227

References	  	  
Agosto, D. E. (2002). Bounded rationality and satisficing in young people’s Web-based decision
making. Journal of the American Society for Information Science and Technology, 53(1),
16-27.
Akyol, Z. & Garrison, D.R. (2011). Assessing metacognition in an online community of inquiry.
Internet and Higher Education 14, 183-190.
Allen, M. (2008). Promoting critical thinking skills in online information literacy instruction
using a constructivist approach. College & Undergraduate Libraries, 15(1-2), 21-38.
Altmetrics for Scopus. (2014). Downloaded August 11, 2014 from
http://www.elsevier.com/journal-authors/authors-update/issue-3/altmetric-for-scopus
American Association of School Librarians. (2009). Standards for the 21st Century Learner.
Chicago: American Library Association.
American Association of School Librarians & Association for Educational Communications and
Technology. (1998). Information Power: Building Partnerships for Learning. Chicago:
American Library Association.
American Library Association. (1989). Presidential Committee on Information Literacy: Final
Report. Washington, D.C. : Association of College and Research Libraries.
Anderson, K., & May, F. A. (2010). Does the method of instruction matter? An experimental
examination of information literacy instruction in the online, blended, and face-to-face
classrooms. The Journal of Academic Librarianship, 36(6), 495-500.
Anderson, T. & Shattuck, J. (2012). Design-Based Research: A decade of progress in education
research? Educational Researcher, 41(1), 16–25.
Ardis, S. B. (2005). Instruction: Teaching or Marketing? Issues in Science and Technology
Librarianship. Retrieved from http://istl.org/05-spring/viewpoints.html
Arp, L. (1990). Information literacy or bibliographic instruction: semantics or philosophy. RQ,
30(1), 46-49.
228

Artz, A. F., & Armour-Thomas, E. (1992). Development of a cognitive-metacognitive
framework for protocol analysis of mathematical problem solving in small groups.
Cognition and Instruction, 9(2), 137-175.
Association of College and Research Libraries. (2000). Information Literacy Competency
Standards for Higher Education. Chicago: American Library Association.
Azevedo, R. (2005). Using hypermedia as a metacognitive tool for enhancing student learning?
The role of self-regulated learning. Educational Psychologist, 40(4), 199-209.
Azevedo, R., & Jacobson, M. J. (2007). Advances in scaffolding learning with hypertext and
hypermedia: a summary and critical analysis. Educational Technology Research and
Development, 56(1), 93-100.
Bannert, M., Hildebrand, M. & Mengelkamp, C. (2009). Effects of a metacognitive support
device in learning environments. Computers in Human Behavior 25, 829-835.
Barab, S., & Squire, K. (2004). Design-Based Research: Putting a stake in the ground. The
Journal of the Learning Sciences, 13(1), 1-14.
Barclay, D. (1993). Evaluating library instruction: Doing the best you can with what you have.
RQ, 33(2), 195-202.
Beaubien, A. K., Hogan, S. A., & George, M. W. (1982). Learning the library: concepts and
methods for effective bibliographic instruction. New York: Bowker.
Becker, N. J. (2003). Google in perspective: understanding and enhancing student search skills.
New Review of Academic Librarianship, 9(1), 84.
Behrens, S. J. (1994). A conceptual analysis and historical overview of Information Literacy.
College and Research Libraries, 55(4), 309-22.
Bell, P., Davis, E. A., & Linn, M. C. (1995). The knowledge integration environment: theory and
design. The first international conference on Computer Support for Collaborative Learning,
CSCL ’95 (pp. 14–21). Hillsdale, NJ, USA: L. Erlbaum Associates Inc.
Bennert, M. & Mengelkamp, C. (2013). In Azevedo, R. & Aleven, V. (Eds), International
Handbook of Metacognition and Learning Technologies. Springer International Handbooks
of Education, Vol. 26. New York: Springer.
Biddix, J. P., Chung, C. J., & Park, H. W. (2011). Convenience or credibility? A study of college
student online research behaviors. The Internet and Higher Education, 14(3), 175–182.

229

Bixler, B. A., & Land, S. M. (2010). Supporting college students’ ill-structured problem solving
in a web-based learning environment. Journal of Educational Technology Systems, 39(1).
Blakeslee, S. (2004). The CRAAP Test. LOEX Quarterly, Volume 31. Retrieved from
http://commons.emich.edu/cgi/viewcontent.cgi?article=1009&context=loexquarterly.
Bobish, G. (2011). Participation and pedagogy: Connecting the social web to ACRL learning
outcomes. The Journal of Academic Librarianship, 37(1), 54–63.
Boff, C., & Johnson, K. (2002). The library and first-year experience courses: A Nationwide
study. Reference Services Review, 30(4), 277-87.
Booth, C. (2011). Reflective teaching, effective learning: instructional literacy for library
educators. Chicago: American Library Association.
Brand-Gruwel, S., Wopereis, I., & Vermetten, Y. (2005). Information problem solving: Analysis
of a complex cognitive skill. Computers in Human Behavior, 21, 487–508.
Breivik, P. S., & Jones, D. L. (1993). Information Literacy: Liberal education for the Information
Age. Liberal Education, 79(1), 24-29.
Brown, A. (1987). Metacognition, Executive Control, Self-Regulation, and Other More
Mysterious Mechanisms. In F. E. Weinert & R. H. Kluwe. Metacognition, motivation and
understanding. Hillsdale, New Jersey: Lawrence Erlbaum. (pp. 65-116).
Brown, C., Murphy, T. J., & Nanny, M. (2003). Turning techno-savvy into info-savvy:
authentically integrating information literacy into the college curriculum. The Journal of
Academic Librarianship, 29(6), 386-398.
Bruce, C. (2000). Information Literacy research: Dimensions of the emerging collective
consciousness. Australian Academic & Research Libraries, 31(2), 91-109.
Bruce, C. (2004). Information literacy as a catalyst for educational change. A background paper.
In Danaher, Patrick A. (Ed.) "Lifelong Learning: Whose responsibility and what is your
contribution?", the 3rd International Lifelong Learning Conference, 13-16 June 2004,
Yeppoon, Queensland.
Bruning, R. H., Schraw, G.J., Norby, M.M. & Ronning, R.R. (2004). Cognitive psychology and
instruction. Upper Saddle River, N.J.: Pearson/Merrill/Prentice Hall.
Bruning, R. H.. (1994). The college classroom from the perspective of cognitive psychology. In
Prichard, K.W. and R. McLaren Sawyer (eds), Handbook of College Teaching: Theory and
Applications. Greenwood Press: Westport, CT.
230

Burbules, N.C. (2001). Paradoxes of the web: The ethical dimensions of credibility, Library
Trends 49(3), 441-453.
Cameron, L., Wise, S. L., & Lottridge, S. M. (2007). The Development and Validation of the
Information Literacy Test. College & Research Libraries, 68(3), 229–237.
Carnevale, A. P., Smith, N., & Melton, M. (2011). STEM. Washington, D.C.: Georgetown
University Center on Education and the Workforce.
Chu, S. K. W., Tse, S. K., & Chow, K. (2011). Using collaborative teaching and inquiry projectbased learning to help primary school students develop information literacy and information
skills. Library & Information Science Research, 33(2), 132-143.
Click, A., & Petit, J. (2010). Social networking and Web 2.0 in information literacy. The
International Information & Library Review, 42(2), 137-142.
Collins, A., Joseph, D., & Bielaczyc, K. (2004). Design Research: Theoretical and
methodological issues. Journal of the Learning Sciences, 13(1), 15.
Connaway, L. S., Dickey, T. J., & Radford, M. L. (2011). “If it is too inconvenient I’m not going
after it:” Convenience as a critical factor in information-seeking behaviors. Library &
Information Science Research, 33(3), 179-190.
Cook, T.D. & Singha, V. (2006). Randomized experiments in educational research. In Judith
Green, Gregory Camilli, & Patricia B. Elmore (eds). Washington DC: American
Educational Research Association.
Green, J. L., Camilli, G., and Elmore, P.B.(2006). Handbook of Complementary Methods in
Education Research. Washington, D.C.: American Educational Research Association.
Costello, B., Lenholt, R., Stryker, J. (2004). Using Blackboard in library instruction: Addressing
the learning styles of Generations X and Y. The Journal of Academic Librarianship, 30(6),
452-460.
Coupe, J. (1993). Undergraduate Library Skills: Two Surveys at Johns Hopkins University.
Research Strategies, 11(4), 188-201.
Creswell, J.W. (2002). Educational research: Planning, conducting, and evaluating quantitative
and qualitative research. Upper Saddle River, NJ: Pearson Education.
Crowston, K., Kwaśnik, B., & Rubleske, J. (2011). Problems in the use-centered development of
a taxonomy of web genres. In Mehler, A., Sharoff, S. & Santini, M. (eds), Genres on the Web
(pp. 69-84). Springer Netherlands.
231

CRAAP Test. Retrieved from http://www.csuchico.edu/lins/handouts/eval_websites.pdf
Cunningham, S., Carr, A., & Brasley, S. S. (2011). Uncovering the IL disconnect: Examining
expectations among librarians, faculty and students. In proceedings of ACRL National
Conference, Philadelphia (April 2, 2011).
Curtis, S.C. (2000). Listening to Generation X. Journal of Educational Media & Library
Science, (38), 21– 24.
Dalsgaard, C. (2006). Social software: E-learning beyond learning management systems.
European Journal of Open, Distance and E-Learning, 2006(2). Retrieved from
www.eurodl.org/?p=archives&year=2006&halfyear=2&article=228.
Daugherty, A. L., & Russo, M. F. (2011). An assessment of the lasting effects of a stand-alone
Information Literacy course: The students’ perspective. The Journal of Academic
Librarianship, 37(4), 319-326.
Davidson, C. N., & Goldberg, D. T. (2009). The future of learning institutions in a digital age.
John D. and Catherine T. MacArthur Foundation Reports on Digital Media and Learning.
Cambridge, MA: MIT Press.
Davidson, J. (2013, Aug 7). Discovery Channel provokes outrage with fake Shark Week
documentary. Time. Retrieved from http://entertainment.time.com/2013/08/07/discoverychannel-provokes-outrage-with-fake-shark-week-documentary/
Davidson, J.R. (2001). Faculty and student attitudes toward credit courses for library skills.
College and Research Libraries 62, 155-163.
Davis, E.A. (2000). Scaffolding students’ knowledge integration: Prompts for reflection in KIE.
International Journal of Science Education, 22(8), 819-837.
De Corte, E. (1996). Instructional psychology: Overview. In Corte, E. De., & Weinert, F. E.
(Eds.) International Encyclopedia of Developmental And Instructional Psychology.
Resources In Education. Oxford, Ox, UK: Pergamon.
Dede, C. (2005). Planning for Neomillennial learning styles. EDUCAUSE Quarterly, 28(1).
Design-Based Research Collective. (2003). Design-Based Research: An Emerging Paradigm for
Educational Inquiry. Educational Researcher, 32(1), 5-8.
Dewald, N.H. (1999). Transporting good library instruction practices into the Web environment:
An analysis of online tutorials. The Journal of Academic Librarianship, 25(1), 26-31.

232

Dillenbourg, P. (Ed.) (1999). What do you mean by “collaborative learning”? Collaborativelearning: Cognitive and Computational Approaches (pp. 1-19). Oxford: Elsevier.
Dillenbourg, P., Baker, M., Blaye, A. & O'Malley, C. (1996) The evolution of research on
collaborative learning. In E. Spada & P. Reiman (Eds.), Learning in Humans and Machine:
Towards an interdisciplinary learning science. (pp. 189-211). Oxford: Elsevier.
Doshi, A. (2006. How Gaming Could Improve Information Literacy. Computers in libraries 26,
(5): 14-17.
Doyle, C.S. (1994). Information Literacy in an Information Society: A concept for the
Information Age. Syracuse, NY: ERIC Clearinghouse on Information & Technology.
Doyle, T. & Hammond, J.L. (2006). Net cred: evaluating the internet as a research source.
Reference Services Review 34(1), 56-70.
Drabenstott, K. M. (2003). Do nondomain experts enlist the strategies of domain experts?
Journal of the American Society for Information Science and Technology, 54(9), 836-854.
Eisenberg, M. B. & Berkowitz, R. E. (1996). Information problem-solving: The Big Six
skills approach to library and information skills. Ablex, Norwood, NJ.
Eisenberg, M. B. (2010). Information Literacy: Essential skills for the Information Age.
DESIDOC Journal of Library & Information Technology, 28(2), 39-47.
Eisenberg, M., Lowe, C. A., & Spitzer, K. L. (2004). Information literacy: essential skills for the
information age. Westport, Conn.: Libraries Unlimited.
Farkas, M. (2012). Participatory technologies, pedagogy 2.0 and information literacy. Library Hi
Tech, 30(1), 82.
Fast, K. V., & Campbell, D. G. (2004). “I still like Google”: University student perceptions of
searching OPACs and the web. Proceedings of the American Society for Information
Science and Technology, 41(1), 138–146.
Fishman, B., Marx, R. W., Blumenfeld, P., Krajcik, J., & Soloway. (2004). Creating a
framework for research on systemic technology innovations. Journal of the Learning
Sciences, 13(1), 43.
Fitzpatrick, M. J., & Meulemans, Y. N. (2011). Assessing an Information Literacy Assignment
and Workshop Using a Quasi-Experimental Design. College Teaching, 59(4), 142–149.
Flanagin, A. J., & Metzger, M. J. (2008). Digital media and youth: Unparalleled opportunity and
unprecedented responsibility. In M. J. Metzger & A. J. Flanagin (Eds.), Digital media,
233

youth, and credibility (pp. 49-72). The John D. and Catherine T. MacArthur Foundation
Series on Digital Media and Learning. Cambridge, MA: MIT Press.
Flavell, J. H. (1979). Metacognition and cognitive monitoring: A new area of cognitive
developmental inquiry. American Psychologist, 34(10), 906-911.
Fogg, B. J., Soohoo, C., Danielson, D. R., Marable, L., Stanford, J., & Tauber, E. R. (2003).
How do users evaluate the credibility of Web sites?: a study with over 2,500 participants.
Proceedings of the 2003 conference on Designing for user experiences, DUX ’03 (pp. 1–
15). New York, NY, USA: ACM.
Garfield, E. (1979). 2001: An information society? Journal of Information Science, 1(4), 209 215.
Garrison, D. R., & Akyol, Z. (2013). Toward the development of a metacognition construct for
communities of inquiry. The Internet and Higher Education, 17(0), 84–89.
Gasser, U., Cortesi, S., Malik, M. & Lee, A. (2012). Youth and Digital Media: From Credibility
to Information Quality. Berkman Center Research Publication No. 2012-1.
Ge, X. (2001). Scaffolding students’ problem-solving processes on an ill-structured task using
question prompts and peer interactions. Unpublished doctoral dissertation, The
Pennsylvania State University, University Park, PA.
Ge, X. (2013). Designing learning technologies to support self-regulation during ill-structured
problem-solving processes. In Azevedo, R. & Aleven, V. (Eds), International Handbook of
Metacognition and Learning Technologies. Springer International Handbooks of Education,
Vol. 26. New York: Springer.
Ge, X., & Er, N. (2005). An online support system to scaffold real-world problem solving.
Interactive Learning Environments, 13(3), 139–157.
Ge, X., & Land, S. M. (2003). Scaffolding students’ problem-solving processes in an illstructured task using question prompts and peer interactions. Educational Technology
Research and Development, 51(1), 21-38.
Ge, X., & Land, S. M. (2004). A conceptual framework for scaffolding ill-structured problemsolving processes using question prompts and peer interactions. Educational Technology
Research and Development, 52(2), 5–22.

234

Ge, X., Chen, C.H. & Davis, K.A. (2005). Scaffolding novice instructional designers’ problemsolving processes using question prompts in a web-based learning environment. Journal of
Educational Computing Research, 33(2), 219-248.
Ge, X., Planas, L.G. & Er, N. (2010). A cognitive support system to scaffold students’ problembased learning in a web-based learning environment. Interdisciplinary Journal of Problembased Learning, 4(1):30-56.
Gibson, C. (2008). The History of Information Literacy. In C.N. Cox & E.B. Lindsay (Eds.),
Information Literacy Instruction Handbook. Chicago: Association of College and Research
Libraries.
Godwin, P. & Parker, J. (2008). Information Literacy meets Library 2.0. London: Facet
Publishing.
Goodin, M. E. (1991). The transferability of library research skills from high school to college.
School Library Media Quarterly, 20(1), 33-42.
Google Books Ngram viewer. (n.d.) Retrieved on December 31, 2011 from
www.books.google.com/ngrams/info.
Gorrell, G., Eaglestone, B., Ford, N., Holdridge, P., & Madden, A. (2009). Towards
“metacognitively aware” IR systems: An initial user study. Journal of Documentation,
65(3), 446–469.
Grassian, E. (2004). Building on bibliographic instruction: our strong BI foundation supports a
promising IL future. American Libraries, 35(9), 51-52.
Griffiths, J. R., & Brophy, P. (2005). Student Searching Behavior and the Web: Use of Academic
Resources and Google. Library Trends, 53(4), 539–554.
Gross, M., & Latham, D. (2007). Attaining information literacy: An investigation of the
relationship between skill level, self-estimates of skill, and library anxiety. Library &
Information Science Research, 29(3), 332-353.
Groves, R.M., et al. (2009). Survey methodology. Wiley series in survey methodology. Hoboken,
N.J.: Wiley.
Halse, M., & Mallinson, B. J. (2009). Investigating popular Internet applications as supporting elearning technologies for teaching and learning with Generation Y. International Journal of
Education and Development using Information and Communication Technology, 5(5).

235

Hardesty, L.L. (1995). Faculty culture and bibliographic instruction: An exploratory analysis.
Library Trends 44(2), 39–67.
Hargittai, E., Fullerton, L., Menchen-Trevino, E., & Thomas, K. Y. (2010). Trust online: Young
adults’ evaluation of web content. International Journal of Communication, 4, 468-494.
Harris, F.J.H. (2008). Challenges to teaching credibility assessment in contemporary schooling.
In M. J. Metzger & A. J. Flanagin (Eds.), Digital media, youth, and credibility (pp. 49-72).
The John D. and Catherine T. MacArthur Foundation Series on Digital Media and Learning.
Cambridge, MA: The MIT Press.
Head, A. (2007). Beyond Google: How do students conduct academic research? First Monday,
12 (8).
Head, A. J., & Eisenberg, M. B. (2010). Truth be told: How college students evaluate and use
information in the Digital Age. Project Information Literacy progress report. Retrieved
from: www.projectinfolit.org/pdfs/PIL_Fall2010_Survey_FullReport1.pdf
Herring, J. (2011). Improving Students’ Web Use and Information Literacy. London: Facet
Publishing.
Herrington, J., McKenney, S., Reeves, T. & Oliver, R. (2007). Design-based research and
doctoral students: Guidelines for preparing a dissertation proposal. In C. Montgomerie & J.
Seale (Eds.), Proceedings of World Conference on Educational Multimedia, Hypermedia and
Telecommunications 2007. Chesapeake, VA: AACE.
Hilligoss, B & Rieh, S.Y. (2008). Developing a unifying framework if credibility assessment:
Construct, heuristic, and interaction in context. Information Processing and Management
44, 1467-1484.
Hoadley, C. P. (2002). Creating context: design-based research in creating and understanding
CSCL. Proceedings of the Conference on Computer Support for Collaborative Learning:
Foundations for a CSCL Community, CSCL ’02 (pp. 453–462). International Society of the
Learning Sciences.
Holman, L. (2011). Millennial Students’ Mental Models of Search: Implications for Academic
Librarians and Database Developers. The Journal of Academic Librarianship, 37(1), 19–27.
Hrycaj, P. & Russo, M. (2007). Reflections on surveys of faculty attitudes toward collaboration
with librarians. Journal of Academic Librarianship 33(6), 692-696.

236

http://www.madisonassessment.com/assessment-testing/information-literacy-test (accessed July
23, 2012)
Hung, D. (2001). Design principles for web-based learning: Implications from Vygotskian
thought. Educational Technology 41(3), 33-41.
Iding, M., Auernheimer, B. & Crosby, M.E. (2008). Towards a metacognitive approach to
credibility. In Proceedings of the 2nd ACM workshop on Information credibility on the web.
October 30, 2008, Napa Valley, CA.
Iding, M.K., Crosby, M.E, Auernheimer, B. & Klemm, B.E. (2009). Web site credibility: Why
do people believe what they believe? Instructional Science 37:43-63.
International Society for Technology in Education. (n.d.). NETS For Students. Retrieved October
29, 2012, from http://www.iste.org/standards/nets-for-students.
Ito, M., Horst, M., boyd, d., Herr-Stephenson, B., Lange. P.G., Pascoe, C.J. & Robinson, L.
(2008). Living and learning with new media: Summary of findings from the digital youth
project. The John D. and Catherine T. MacArthur Foundation Reports on Digital Media and
Learning.
Jackson, R. (2008). Information literacy and its relationship to cognitive development and
reflective judgment. New Directions for Teaching and Learning, 114, 47–61.
Julien, H. & Barker, S. (2009). How high school students evaluate scientific information: A basis
for information literacy skills development. Library and Information Science Research,
31(1), 12-17.
Kapitzke, C. (2003). Information literacy: A review and poststructural critique. Australian
Journal of Language and Literacy, 26(1), 53–66.
Kapoun, J. (1998). Teaching undergrads web evaluation: A guide for library instruction. C&RL
News 59 (7), 522-523.
Kauffman, D. F. (2004). Self-regulated learning in Web-based environments: Instructional tools
designed to facilitate cognitive strategy use, metacognitive processing, and motivational
beliefs. Journal of Educational Computing Research, 30(1), 139-161.
Kauffman, D. F., Ge, X., Xie, K., & Chen, C. H. (2008). Prompting in Web-based environments:
Supporting self-monitoring and problem solving skills in college students. Journal of
Educational Computing Research, 38(2), 115-137.

237

Kim, M. C., & Hannafin, M. J. (2011). Scaffolding problem solving in technology-enhanced
learning environments (TELEs): Bridging research and theory with practice. Computers &
Education, 56(2), 403-417.
Kim, K. & Sin, S.-C. J. (2011). Selecting quality sources: Bridging the gap between the
perception and use of information sources. Journal of Information Science, 37(2), 178 –188.
Knapp, P.B. (1956). A suggested program of college instruction in the use of the library. Library
Quarterly, 26:1/4, 224-231.
Kohavi, R., Longbotham, R., Sommerfield, D. & Henne, R. (2009). Controlled Experiments on
the Web: Survey and Practical Guide. Data Mining and Knowledge Discover, 18(1), 140–
181.
Kolowich, S. (2011). What students don’t know. Inside Higher Ed. Retrieved from:
www.insidehighered.com/news/2011/08/22.
Koschmann, T. D. (1996). CSCL, theory and practice of an emerging paradigm. Mahwah, N.J. :
L. Erlbaum Associates.
Kuhlthau, C. C. (1991). Inside the search process: Information seeking from the user’s
perspective. Journal of the American Society for Information Science, 42(5), 361-71.
Kuhlthau, C. C. (1999). Literacy and learning for the information age. In B. K. Stripling (Ed.),
Learning and libraries in an information age: Principles and practice. Englewood, CO:
Libraries Unlimited.
Kuhlthau, C. C., Maniotes, L. K., & Caspari, A. K. (2007). Guided Inquiry: Learning in the 21st
Century. Westport, Conn.: Libraries Unlimited.
Lai, M., & Law, N. (2006). Peer scaffolding of knowledge building through collaborative groups
with differential learning experiences. Journal of Educational Computing Research, 35(2),
123-144.
Lance, K. C., Rodney, M. J., & Hamilton-Pennell, C. (2000). Measuring Up to Standards: The
Impact of School Library Programs & Information Literacy in Pennsylvania Schools.
Retrieved from:
www.eric.ed.gov/ERICWebPortal/contentdelivery/servlet/ERICServlet?accno=ED446771
Landis, J.R. & Koch, G.G. (1977). The measurement of observer agreement for categorical data.
Biometrics 33:159-174.

238

Langford, L. (1998). Information Literacy: A clarification. School Libraries Worldwide 4(1), 5972.
Lankes, R.D. (2008). Trusting the Internet: New approaches to credibility tools. In M. J. Metzger
& A. J. Flanagin (Eds.), Digital media, youth, and credibility (pp. 49-72). The John D. and
Catherine T. MacArthur Foundation Series on Digital Media and Learning. Cambridge,
MA: The MIT Press.
Large, A. (2005). Children, teenagers, and the Web. Annual Review of Information Science and
Technology, 39, 347–392.
Lazar, J., Meiselwitz, G., & Feng, J. (2007). Understanding web credibility: a synthesis of the
research literature. Found. Trends in Human-Computer Interaction, 1(2), 139–202.
Leckie, G.J. (1996). Desperately seeking citations. The Journal of Academic Librarianship
22(3), 201-208.
Lajoie, S.P. & Lu, J. (2012). Supporting collaboration with technology: Does shared cognition
lead to co-regulation in medicine. Metacognition & Learning, 7(1), 45-62.
Lin, X. (2001). Designing metacognitive activities. Educational Technology Research and
Development, 49(2), 23–40.
Lippincott, J. K. (2005). Net Generation Students and Libraries. In D. G. Oblinger and J. L.
Oblinger (Eds.), Educating the Net Generation. EDUCAUSE (e-book). Retrieved from:
www.educause.edu/Resources/NetGenerationStudentsandLibrar/160467
Lipponen, L. (2002). Exploring foundations for computer-supported collaborative learning. In
Proceedings of the Conference on Computer Support for Collaborative Learning:
Foundations for a CSCL Community (pp. 72–81). International Society of the Learning
Sciences. Retrieved from http://dl.acm.org/citation.cfm?id=1658616.1658627.
Lorenzo, G., & Dziuban, C. (2006). Ensuring the Net Generation Is Net Savvy. EDUCAUSE.
Retrieved from: www.educause.edu/ELI/EnsuringtheNetGenerationIsNetS/156766
Ludvigsen, S. R., & Mørch, A. I. (2010). Computer-Supported Collaborative Learning: Basic
concepts, multiple perspectives, and emerging trends. International Encyclopedia of
Education, 290-296.
Mackey, T.P., & Jacobson, T.E.. (2011). Reframing Information Literacy as a metaliteracy.
College & Research Libraries,72 (1): 62–78.

239

Madden, A. D., Ford, N., Gorrell, G., Eaglestone, B., & Holdridge, P. (2012). Metacognition and
web credibility. The Electronic Library 30(5), 671–689.
Magerko, B., Heeter, C., & Medler, B. (2010). Different Strokes for Different Folks: Tapping
Into the Hidden Potential of Serious Games. In Van Eck, R. (Ed.) Gaming and cognition:
Theories and perspectives from the learning sciences, 255-80. Hershey, PA : Information
Science Reference.
Mann, T. (1993). The principle of least effort. In Library research models: A guide to
classification, cataloguing and computers (pp. 91-101). New York: Oxford University
Press.
Manuel, K. (2002). Teaching Information Literacy to Generation Y. Journal of Library
Administration. 36:1/2, 195-207.
Manuel, K., Beck, S. E., & Molloy, M. (2005). An ethnographic study of attitudes influencing
faculty collaboration in library instruction. The Reference Librarian, 43, 89-90.
Marcum, J.W. (2002). Rethinking Information Literacy. The Library Quarterly 72 (1), 1-26.
Markless, S. (2009). A New Conception of Information Literacy for the Digital Environment in
Higher Education. Nordic Journal of Information Literacy in Higher Education, 1(1).
Markless, S., & Streatfield, D. (2007). Three decades of information literacy: redefining the
parameters. In S. Andretta (Ed.), Change and challenge  : information literacy for the 21st
century. Auslib Press.
Markey, K., Leeder, C., & Rieh, S. Y. (2012). Through a game darkly: student experiences with
the technology of the library research process. Library Hi Tech, 30(1), 12–34.
Markey, K., Leeder, C. & Rieh, S.Y. (2014). Designing Information Literacy Games Students
Will Want to Play. New York: Rowman & Littlefield.
McCarthy, Constance. 1985. The Faculty Problem. Journal of Academic Librarianship, 11, 142145.
McDevitt, T. (Ed.). (2011). Let the Games Begin! Engaging Students in Field-tested Interactive
Information Literacy Instruction. New York: Neal-Schuman.
McGuinness, C. (2006). What faculty think: Exploring the barriers to Information Literacy
development in undergraduate education. The Journal of Academic Librarianship, 32(6),
573-582.

240

McLoughlin, C. & Lee, M. J. W. (2008). Mapping the digital terrain: New media and social
software as catalysts for pedagogical change. Proceedings ascilite Melbourne 2008.
Mellon, C. (1986). Library anxiety: A grounded theory and its development, College & Research
Libraries 47(2), 160–165.
Meola, M. (2004). Chucking the checklist: A contextual approach to teaching undergraduates
Web-site evaluation. portal: Libraries and the Academy, 4(3), 331-344.
Mery, Y., Newby, J., & Peng, K. (2012). Why one-shot Information Literacy sessions are not the
future of instruction: A Case for Online Credit Courses. College & Research Libraries
73(4):366-377.
Metzger, M. J. (2007). Making sense of credibility on the Web: Models for evaluating online
information and recommendations for future research. Journal of the American Society for
Information Science and Technology, 58(13), 2078-2091.
Metzger, M. J., Flanagin, A. J., & Medders, R. B. (2010). Social and heuristic approaches to
credibility evaluation online. Journal of Communication, 60(3), 413-439.
Metzger, M. J., Flanagin, A. J., & Zwarun, L. (2003). College student Web use, perceptions of
information credibility, and verification behavior. Computers & Education, 41(3), 271-290.
Middle States Commission on Higher Education. (2003). Developing research & communication
skills: Guidelines for Information Literacy in the curriculum. Philadelphia: MSCHE.
Mitchell, M.L. & Jolley, J.M. (2004). Research Design Explained. Belmont, CA:
Wadsworth/Thomson Learning.
Mizrachi, D. (2010). Undergraduates’ academic information and library behaviors: preliminary
results. Reference Services Review, 38(4), 571–580.
Mokhtar, I. A., Majid, S., & Foo, S. (2008). Information literacy education: Applications of
mediated learning and multiple intelligences. Library & Information Science Research,
30(3), 195-206.
Morris, R., Hadwin, A.F., Gress, C.L.Z., Miller, M., Fior, M., Church, H., & Winne, P.H. (2010).
Designing roles, scripts, and prompts to support CSCL in gStudy. Computers in Human
Behavior, 26(5), 815-824.
Myhre, S.K. (2012). Using the CRAAP test to evaluate websites. Presented at the 17th Annual
Technology, Colleges, and Community Worldwide Online Conference, April 17 2012.

241

National Governors Association Center for Best Practices, Council of Chief State School
Officers. (2010). Common Core State Standards for English Language Arts and Literacy in
History/Social Studies, Science, and Technical Subjects. Washington D.C.: National
Governors Association Center for Best Practices, Council of Chief State School Officers.
National Leadership Council for Liberal Education and America’s Promise. (2007). College
Learning for the New Global Century. Washington DC: Association of American Colleges
and Universities.
National Research Council. (2012). Education for Life and Work: Developing Transferable
Knowledge and Skills in the 21st Century. Washington, DC: The National Academies Press.
Retrieved from https://download.nap.edu/catalog.php?record_id=13398.
News Aggregator. (2014) In PC Magazine Encyclopedia. Retrieved July 29, 2014 from
http://www.pcmag.com/encyclopedia/term/47946/news-aggregator.
Oblinger, D.G. & Hawkins, B.L. (2006). The Myth About Student Competency. EDUCAUSE
Review 41(2), 12–13.
OCLC. (2002). How academic librarians can influence students’ Web-based information
choices. Dublin, Ohio: OCLC.
Oliver, K. M. (2000). Methods for developing constructivist learning on the Web. Educational
Technology, 40(6), 5-18.
Orme, W. A. (2004). A study of the residual impact of the Texas Information Literacy Tutorial
on the information-seeking ability of first year college students. College & Research
Libraries, 65(3), 205-215.
Onwuegbuzie, A.J. & Jiao, Q.G. (2000). I’ll go to the library later: The relationship between
academic procrastination and library anxiety. College & Research Libraries 61(1), 45-54.
Onwuegbuzie, A. J., Jiao, Q. G., & Bostick, S. L. (2004). Library anxiety: Theory, research, and
applications. Landham, Md.: Scarecrow Press, Inc.
Overview: Article-Level Metrics. (2014). Downloaded August 11 from http://article-levelmetrics.plos.org/alm-info/.
Owusu-Ansah, E. K. (2004). Information Literacy and higher education: Placing the academic
library in the center of a comprehensive solution. The Journal of Academic Librarianship,
30(1), 3-16.

242

Palfrey, J., & Gasser, U. (2008). Born Digital: Understanding the First Generation of Digital
Natives New York: Basic Books.
Palincsar, A.S. & Brown, A.L. (1984). Reciprocal teaching of comprehension-fostering and
comprehension-monitoring activities. Cognition and Instruction, 2, 117-175.
Pan, B., Hembrooke, H., Joachims, T., Lorigo, L., Gay, G. & Granka, L. (2007). In Google We
Trust: Users’ Decisions on Rank, Position, and Relevance. Journal of Computer-Mediated
Communication 12, 801-823.
Parker-Gibson, N. (2005). From the womb to the web. The Reference Librarian, 44(91-92), 83102.
Partnership for 21st Century Skills. (2011). Framework for 21st Century Learning. Retrieved
from http://www.p21.org/overview/skills-framework.
Perry, N. E., & Winne, P. H. (2006). Learning from Learning Kits: gStudy Traces of Students’
Self-Regulated Engagements with Computerized Content. Educational Psychology Review,
18(3), 211–228.
Pifarre, M., & Cobos, R. (2010). Promoting metacognitive skills through peer scaffolding in a
CSCL environment. International Journal of Computer-Supported Collaborative Learning,
5(2), 237-253.
Pintrich, P. R., Smith, D. Garcia, T. & McKeachie, W. J. (1991). A Manual for the Use of the
Motivated Strategies for Learning Questionnaire. National Center for Research to Improve
Postsecondary Teaching and Learning, University of Michigan, Ann Arbor.
Pintrich, P.R., Wolters, C.A., & Baxter, G.P. (2000). Assessing metacognition and self-regulated
learning. In G. Schraw & J.C. Impara (eds.), Issues in the measurement of metacognition.
University of Nebraska-Lincoln.
Quintana, C., Reiser, B. J., Davis, E. A., Krajcik, Joseph, Fretz, E., Duncan, R. G., Kyza, E., &
Edelson, D. (2004). A scaffolding design framework for software to support science inquiry.
Journal of the Learning Sciences, 13(3), 337-386.
Quintana, C., Zhang, M., & Krajcik, J. (2005). A framework for supporting metacognitive
aspects of online inquiry through software-based scaffolding. Educational Psychologist,
40(4), 235-244.
Raes, A., Schellens, T., De Wever, B. & Vanderhoven, E. (2012) Scaffolding information
problem solving in web-based collaborative inquiry. Computers & Education, 59 (1), 82-94.
243

Reece, G. J. (2005). Critical thinking and cognitive transfer: Implications for the development of
online information literacy tutorials. Research Strategies, 20(4), 482-493.
Reiser, B. (2004). Scaffolding complex learning: The mechanisms of structuring and
problematizing student work. Journal of the Learning Sciences, 13(3), 273-304.
Ren, W.H. (2000). Library instruction and college student self-efficacy in electronic information
searching. The Journal of Academic Librarianship, 26(5), 323-328.
Rheingold, H. (2012). Net smart: how to thrive online. Cambridge, Mass.: MIT Press.
Rieger, O.Y., Horne, A.K., & Revels, I. (2004). Linking course web sites to library collections
and services. The Journal of Academic Librarianship, (30) 3, 205-211.
Rieh, S. Y. (2002). Judgment of information quality and cognitive authority in the web. Journal
of the American Society for Information Science and Technology, 53, 145-161.
Rieh, S. Y., & Danielson, D. R. (2007). Credibility: A multidisciplinary framework. Annual
Review of Information Science and Technology, 41(1), 307-364.
Rieh, S. Y. & Hilligoss, B. (2008). College students’ credibility judgments in the informationseeking process. In M. J. Metzger & A. J. Flanagin (Eds.), Digital media, youth, and
credibility (pp. 49-72). The John D. and Catherine T. MacArthur Foundation Series on
Digital Media and Learning. Cambridge, MA: The MIT Press.
Rockman, I. F. (2002). Strengthening connections between information literacy, general
education, and assessment efforts. Library trends, 51(2), 185-198.
Rosenberg, V. (1974). The scientific premises of information science. Journal of the American
Society for Information Science, 25(4), 263-269.
Rosenshine, B. & Meister, C. (1992). The use of scaffolds for teaching higher-level cognitive
strategies. Educational Leadership, 49(7), 26-33.
Rowlands, I., Nicholas, D., Williams, P., Huntington, P., Fieldhouse, M., Gunter, B., & Withey,
R. (2008). The Google generation: the information behaviour of the researcher of the future.
Aslib Proceedings, 60(4), 290-310.
Sandoval, W. A., & Bell, P. (2004). Design-Based Research methods for studying learning in
context: Introduction. Educational Psychologist, 39(4), 199-201.
Saunders, L. (2012). Faculty Perspectives on Information Literacy as a Student Learning
Outcome. The Journal of Academic Librarianship, 38(4), 226–236.

244

Sawyer, R. K. (2006). The Cambridge handbook of the learning sciences. Cambridge  ; New
York: Cambridge University Press.
Scardamalia, M., Berieter, C., Mclean, R.S., Swallow, J. & Woodruff, E. (1989). Computersupported intentional learning environments. Journal of Educational Computing Research,
5(1), 51-68.
Schraw, G. & Dennison, R.S. (1994). Assessing metacognitive awareness. Contemporary
Educational Psychology, 19, 460-475.
Schein, C., Conway, L., Harner, R., Byerley, S. & Harper, S. 2011. Bridging the Gap: Preparing
High School Students for College Level Research. Colorado Libraries 36(1). Retrieved
from http://coloradolibrariesjournal.org/content/bridging-gap-preparing-high-schoolstudents-college-level-research
Schrock, K. (2013). Personal communication, March 13, 2013.
Schrum, L., & Berenfeld, B. (1997). Teaching and learning in the Information Age: A Guide to
educational telecommunications. Boston: Allyn and Bacon.
Scriven, M. & Paul, R. (1987). Critical thinking. Presented at the 8th Annual International
Conference on Critical Thinking and Education Reform, Summer 1987.
Selegean, J.C., Thomas, M.L. & Richman, M.L. (1983). Long-range effectiveness of library use
instruction. College and Research Libraries 44(6), 476-480.
Shadish, W.R., Cook, T.D., & Campbell, D.T. (2002). Experimental and quasi-experimental
research designs for generalized causal inference. Boston: Houghton Mifflin.
Shapiro, J. J., & Hughes, S. K. (1996). Information Literacy as a liberal art: Enlightenment:
proposals for a new curriculum. Educom Review, 31(2), n.p.
Sharma, P., & Hannafin, M. J. (2007). Scaffolding in technology-enhanced learning
environments. Interactive Learning Environments, 15(1), 27-46.
Shavelson, R. J., Phillips, D. C., Towne, L., & Feuer, M. J. (2003). On the science of education
design studies. Educational Researcher, 32(1), 25 -28.
Simon, H. A. (1956). Rational choice and the structure of the environment. Psychological
Review, 63(2), 129-138.
Smalley, T. N. (2004). College success: High school librarians make the difference. Journal of
Academic Librarianship 30(3), 193–198.

245

Smith, F.A. (2007). Games for teaching Information Literacy skills, Library Philosophy and
Practice, 9(2).
Spink, A., Wolfram, D., Jansen, M. B., & Saracevic, T. (2001). Searching the web: The public
and their queries. Journal of the American society for information science and technology,
52(3), 226-234.
Špiranec, S., & Zorica, M. B. (2010). Information Literacy 2.0: hype or discourse refinement?
Journal of Documentation, 66(1), 140-153.
Stahl, G., Koschmann, T. & Suthers, D. (2006). Computer-Supported Collaborative Learning: A
historical perspective. In: R.K. Sawyer (Ed.) Cambridge handbook of the learning sciences.
Cambridge University Press, Cambridge.
Stamatoplos, A. & Mackoy, R. (1998). Effects of library instruction on university students’
satisfaction with the library: A longitudinal study. College and Research Libraries, 59(4),
322-333.
Stripling, B. (2010). Teaching students to think in the digital environment: Digital Literacy and
digital inquiry. School Library Monthly, 26(8), 16-19.
Sundar, S.S. (2008). The MAIN Model: A heuristic approach to understanding technology
effects on credibility. In M. J. Metzger & A. J. Flanagin (Eds.), Digital media, youth, and
credibility (pp. 49-72). The John D. and Catherine T. MacArthur Foundation Series on
Digital Media and Learning. Cambridge, MA: The MIT Press.
Sundin, O. (2005). Negotiations on information-seeking expertise: A study of web-based
tutorials for information literacy. Journal of Documentation 64(1), 24-44.
Sundin, O., & Francke, H. (2009). In search of credibility: pupils’ information practices in
learning environments. Information Research, 14(4) paper 418.
Swanson, T. (2005). Teaching students about information: Information literacy and cognitive
authority. Research Strategies, 20(4), 322-333.
Tanner, K. D. (2012). Promoting student metacognition. CBE-Life Sciences Education, 11(2),
113-120.
Tapscott, D. (2009). Grown up digital: How the Net Generation is changing the world. New
York: McGraw-Hill.
Taylor, R.S. (1986). Value-added processes in information systems. Norwood, NJ: Ablex
Publishing.
246

Thomas, D., & Brown, J. S. (2011). A new culture of learning: Cultivating the imagination for a
world of constant change. Lexington, KY: CreateSpace.
Thomas, N.P. (2004). Information Literacy and information skills instruction: Applying research
to practice in the school library media center. Westport, Conn.: Libraries Unlimited.
Thompson, G. B. (2002). Information Literacy accreditation mandates: What they mean for
faculty and librarians. Library Trends, 51(2), 218-241.
Todd, R. (2000). Negotiating the Web: Language, critical literacies and learning. Information
Technology, Education and Societies, 1(1), 81-89.
Todd, R. J. (2008). Youth and their virtual networked words: Research findings and implications
for school libraries. School Libraries Worldwide, 14(2).
Todd, R. J., Lamb, L., & McNicholas, C. (1992). The power of Information Literacy: Unity of
education and resources for the 21st century. Paper presented at the Annual Meeting of the
International Association of School Librarianship, Belfast, Northern Ireland.
Tuominen, K. (2007). Information literacy 2.0. Signum, 35(5).
Tuominen, K., Savolainen, R., & Talja, S. (2005). Information Literacy as a sociotechnical
practice. The Library Quarterly, 75(3), 329-345.
United States 21st Century Workforce Commission. (2000). A nation of opportunity: Building
America’s 21st century workforce. Retrieved from http://digitalcommons.ilr.cornell.edu/
cgi/viewcontent.cgi?article=1003&context=key_workplace.
van Meegen, A., & Limpens, I. (2010). How Serious Do We Need to Be? Improving Information
Literacy Skills through Gaming and Interactive Elements. LIBER Quarterly, 20(2), 270-288.
Retrieved from http://www.religionandgender.org/index.php/lq/article/view/7993/8316
Van Scoyoc, A. M. (2003). Reducing library anxiety in first-year students. Reference & User
Services Quarterly, 42(4).
Veenman, M.V.J. (2013) Assessing metacognitive skills in computerized learning environments.
In Azevedo, R. & Aleven, V. (Eds), International Handbook of Metacognition and Learning
Technologies. Springer International Handbooks of Education, Vol. 26. New York:
Springer.
Vygotsky, L. (1978). Mind in Society. Cambridge, M.A.: Harvard University Press.

247

Walraven, A., Brand-Gruwel, S., & Boshuizen, H. P. A. (2009). How students evaluate
information and sources when searching the World Wide Web for information. Computers
& Education, 52(1), 234-246.
Walton, G., & Hepworth, M. (2011). A longitudinal study of changes in learners’ cognitive states
during and following an information literacy teaching intervention. Journal of
Documentation, 67(3), 449–479.
Wang, F., & Hannafin, M. J. (2005). Design-based research and technology-enhanced learning
environments. Educational Technology Research and Development, 53(4), 5-23.
Wang, R. (2006). The lasting impact of a library credit course. portal: Libraries and the
Academy, 6(1), 79-92.
Warwick, C., Rimmer, J., Blandford, A., Gow, J. & Buchanan, G. (2009). Cognitive economy
and satisficing in information seeking: A longitudinal study of undergraduate information
behavior. Journal of the American Society for Information Science and Technology, 60(12),
2402-2415.
Wathen, C. N., & Burkell, J. (2002). Believe it or not: Factors influencing credibility on the
Web. Journal of the American Society for Information Science and Technology, 53(2), 134144.
Weiner, S. A. (2012). Institutionalizing Information Literacy. The Journal of Academic
Librarianship 38(5), 287–293.
Wilson, B. G. (1996). What is a Constructivist learning environment? In B. G. Wilson (Ed.),
Constructivist learning environments: Case studies in instructional design. Englewood
Cliffs NJ: Educational Technology Publications.
Wilson, P. (1983). Second-hand knowledge; An inquiry into cognitive authority. Westport, CT:
Greenwood Press.
Wise, S.L., Cameron, L., Yang, S. & Davis, S.L. (2009). “The Information Literacy Test (ILT)
Test Manual.” Available online at http://www.madisonassessment.com/
uploads/ILT%20Test%20Manual%202010.pdf.
Wong, G., Chan, D., & Chu, S. (2006). Assessing the enduring impact of library instruction
programs. The Journal of Academic Librarianship, 32(4), 384-395.
Wood, D., Bruner, J. S., & Ross, G. (1976). The role of tutoring in problem solving. Journal of
Child Psychology and Psychiatry, 17(2), 89-100.
248

Woodard, B. S. (2003). Technology and the constructivist learning environment: Implications for
teaching information literacy skills. Research Strategies, 19(3-4), 181-192.
Zhang, M., & Quintana, C. (2012). Scaffolding strategies for supporting middle school students’
online inquiry processes. Computers & Education, 58(1), 181–196.
Zipf, G. K. (1949). Human behavior and the principle of least effort: an introduction to human
ecology. New York: Addison-Wesley.
Zurkowski, P.G. (1974). The information service environment: Relationships and priorities.
Washington DC: National Commission on Libraries and Information Science.

249

