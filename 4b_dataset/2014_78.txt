Sarah Knox Morley
Candidate

Educational Psychology
Department

This dissertation is approved, and it is acceptable in quality and form for publication:
Approved by the Dissertation Committee:
Jay Parkes, Ph.D., Chairperson
George Comerci, M.D.
Terri Flowerday, Ph.D.
Kathleen Keating, M.L.S.

i

INITIAL DEVELOPMENT OF A MEDICAL INFORMATION
LITERACY QUESTIONNAIRE
by
SARAH KNOX MORLEY
B.S. Elementary Education, Wheelock College, 1974
M.L.S., University of Arizona, 1980

DISSERTATION
Submitted in Partial Fulfillment of the
Requirements for the Degree of
Doctor of Philosophy
Educational Psychology
The University of New Mexico
Albuquerque, New Mexico
May, 2014

ii

UMI Number: 3630372

All rights reserved
INFORMATION TO ALL USERS
The quality of this reproduction is dependent upon the quality of the copy submitted.
In the unlikely event that the author did not send a complete manuscript
and there are missing pages, these will be noted. Also, if material had to be removed,
a note will indicate the deletion.

UMI 3630372
Published by ProQuest LLC (2014). Copyright in the Dissertation held by the Author.
Microform Edition © ProQuest LLC.
All rights reserved. This work is protected against
unauthorized copying under Title 17, United States Code

ProQuest LLC.
789 East Eisenhower Parkway
P.O. Box 1346
Ann Arbor, MI 48106 - 1346

ACKNOWLEDGEMENTS
As the saying goes, it takes a village… The denizens of my particular village
include the following people and groups who cheered me on and helped in measureable
and immeasurable ways.
To Joe Sparkman, Senior Program Manager of the Graduate Medical Education
Office at the University of New Mexico, without whose knowledge about all things GME
(local and national) I still would be searching for the necessary documents. His prompt
response to requests for annual reports, latest demographics and the like was greatly
appreciated as is his friendship.
Lori Sloane, HSLIC Data Manager, introduced me to the REDCap survey tool
and held my hand through the initial steps of the survey development. Kevin Wesley,
REDCap administrator, took over from Lori for the behind the scene management and
never seemed to cringe when I sent yet another e-mail his way.
I couldn’t have developed the questionnaire without the librarians, physicians and
researchers who provided me with ideas and questions for the items included in the
questionnaire. Thank you to Laura Cartwright, Gail Hannigan, Ingrid Hendrix, Kathy
Kerdolff, Molly Knapp, Dana Ladd, Robin Matsuyama, Teresita McCarty, and Chuck
Wiggins for your assistance. No items, no instrument.
Likewise, the physicians and librarians who took part in the validation panel were
instrumental in moving the project forward. The time and energy these volunteers put
into ranking the items and providing thoughtful comments was not insignificant. Their
input was crucial but most rewarding was the enthusiasm shown for the entire project.
This encouragement meant more than I can express.
A special thank you is due to Teri McCarty for sharing with me her passion for
assessment. Who knew item writing could be fun?
Committees can make or break you or the dissertation. My committee members
were unfailingly supportive and enthusiastic throughout the process. Terri Flowerday,
Ph.D. always gave me the necessary encouragement and (says) she couldn’t wait to read
the manuscript. Kathleen Keating, MLS, and George Comerci, M.D. appreciated the
importance of the project from the perspectives of library science and medicine and
always cheered me on. I was fortunate to have Jay Parkes, Ph.D., with his tremendous
knowledge about measurement as my Committee Chair. He taught me much about the
dissertation process and guided me through the statistical maze. More importantly, he let
me find my way without once ever wondering out loud if I was truly going to finish.

iii

Initial Development of a Medical Information Literacy Questionnaire
Sarah Knox Morley
B.S. Elementary Education
Master of Library Science, M.L.S.
Ph.D. Educational Psychology
ABSTRACT
Purpose
The purpose of this dissertation was to develop and pilot test an instrument measuring
information literacy (IL) competence in resident physicians. Originating from the library
science literature, information literacy is defined as a broad set of skills and abilities
necessary to locate, evaluate and use information ethically and legally. This important
skill set is incorporated into general competency requirements for post graduate residency
programs, however no standardized instrument currently exists to measure resident
physician IL knowledge and skills.
Method
The author constructed a questionnaire of sixty-nine multiple-choice items to assess skills
covering five IL domains. Evidence of test content was evaluated by a panel of twenty
physicians and five health sciences librarians. A draft instrument was administered to a
convenience sample of resident physicians at the University of New Mexico during a two
week period in 2014. Psychometric properties of the tested items were evaluated using
item analyses. Scores from the pilot test were analyzed to examine the structure of the
draft instrument.
Results
The internal consistency reliability coefficient using Cronbach’s alpha was good (α =
.872). Data from the item analyses for each item was used to guide the item retention
process. Each item was reviewed for corrected item-total correlation value to gauge level
of item discrimination and P-values for item difficulty. Cronbach’s alpha-if-item-deleted,
CVR scores established by the validity panel, and the test blueprint were also considered.
Based on the analyses, 32 items (46%) were eliminated from the original pool of 69 items
tested. The remaining 37 items now constitute a revised instrument (α = .858). Although
Principal Component Analysis (PCA) with Varimax rotation was conducted on the
revised instrument, the large amount of variance unaccounted for (45%) made it
impractical to interpret or label the extracted components.
iv

Conclusions
Further investigation is needed before the revised instrument is used as an assessment
tool.

v

TABLE OF CONTENTS
Committee Signature Page………………………………………………….i
Title Page……………………………………………………………………ii
Acknowledgements…………………………………………………………iii
Abstract……………………………………………………………………..iv
Table of Contents…………………………………………………………...vi
Tables and Figures…………………………………………………………..ix
CHAPTER 1 INTRODUCTION…………………………………………1
Introduction…………………………………………………………………1
Background and Significance……………………………………………….2
Literature Review…………………………………………………………...4
Information Literacy………………………………………………………...4
Information Literacy Standards…………………………………….6
Information Literacy in Medicine…………………………………………..9
Evidence-based Medicine…………………………………..9
Graduate Medical Education……………………………………………….12
ACGME Competency Standards…………………………………………..13
Information Literacy Assessment…………………………………………..15
Library Science……………………………………………………..15
Graduate Medical Education……………………………………….16
Information Literacy Instruments…………………………………..17
Development of The Medical Information Literacy Questionnaire………...23
Blueprint creation……………………………………………………23
Method………………………………………………………………………24
Participants…………………………………………………………..24
vi

Analysis…………………………………………………………….24
Summary……………………………………………………………………25
CHAPTER 2 METHODS………………………………………………...26
Introduction………………………………………………………………...26
Statement of Instrument Purpose…………………………………………...26
Phase I: Initial Item Generation…………………………………………….27
Item Writer Qualifications………………………………………….27
Item Writer Recruitment……………………………………………28
Demographic Characteristics……………………………………….29
Analyses……………………………………………………………30
Phase II: Item Validation Panel…………………………………………….30
Subject Matter Expert Eligibility Criteria………………………….30
SME Recruitment…………………………………………………..31
Psychometric Processes…………………………………………….31
Demographic Characteristics……………………………………….32
Analyses…………………………………………………………….34
Results………………………………………………………………35
Phase III: Draft Instrument Pilot Test………………………………………35
Purpose……………………………………………………………..35
Sample and Participant Selection…………………………………..35
Data Collection……………………………………………………..36
Questionnaire Description………………………………………….37
Analyses……………………………………………………………38
Summary……………………………………………………………………40

vii

CHAPTER 3 RESULTS………………………………………………….41
Data Collection……………………………………………………………..41
Demographic Characteristics……………………………………………….42
Psychometric Analyses……………………………………………………..43
Initial Item Analyses………………………………………………..43
Item Selection………………………………………………………………48
Initial Evidence of the Internal Structure of the Instrument………………..51
Summary……………………………………………………………………54
CHAPTER 4 DISCUSSION………………………………………………55
Discussion…………………………………………………………………...55
Implications for Future Research……………………………………………60
Conclusion…………………………………………………………………..62
REFERENCES……………………………………………………………63
APPENDICES…………………………………………………………….90
Appendix A: MILQ Test Blueprint: A detailed listing of specific topics in
the information literacy domain…………………………….90
Appendix B: Specifications for the MILQ………………………………..92
Appendix C: Validation Panel Demographic Form……………………….93
Appendix D: Informed Consent Cover Letter…………………………….95

viii

TABLES AND FIGURES
Table 1

ACRL Information Literacy Competency Standards
For Higher Education………………………………………5

Table 2

CVR Participant Demographic Characteristics…………….33

Table 3

General Demographic Profile and Academic
Characteristics of Survey Participants……………………...42

Table 4

MILQ Initial Item Analyses………………………………...46

Table 5

MILQ Final Form Item Analyses and
Principal Components Analyses……………………………50

Figure 1

Scree Plot…………………………………………………...53

ix

Chapter 1
Introduction

In today’s data driven, information rich society, the meaning of the term
“literacy” has moved beyond the simple ability to read and write to one incorporating
various types of “literacies” across multiple domains. At the most basic level, core
reading and writing literacy skills are necessary to function in daily life. However, to be
considered a truly “literate” person, individuals must go beyond the literal translation of
words and be able to seek out information, measure the reliability and validity of
information within the context from which it was received as well as have the ability to
communicate information to others. These additional skills generally describe the
construct known as “Information Literacy”.
As defined by the field of library and information science, information literacy is
a broad set of skills and abilities necessary to locate, evaluate, and use information.
Information literacy is said to encourage higher level critical thinking and problemsolving skills, and encourages life-long learning. Competency standards developed by
the Association of College and Research Libraries (ACRL) (Association of College and
Research Libraries & American Library Association, 2000) serve as a framework for
information literacy instructional and assessment practices across the educational
continuum.
Information literacy skills are especially pertinent for those in information
intensive fields such as medicine. Medicine has become increasingly complex and
specialized, providing challenges to physicians as they care for patients. In order to

1

provide good patient care, physicians are expected to keep up to date with the latest
medical knowledge. In this environment, it is critical physicians have the ability to locate
current literature in their field, evaluate information they find, use the information to
support clinical decisions and patient care, and communicate that information to various
audiences. While the term “information literacy” is not widely recognized in the medical
field, information literacy skills exemplify the steps found in the research process and in
“evidence-based medicine” (EBM), a well understood concept in healthcare, and skills
that have been incorporated into the general competency requirements for postgraduate
residency programs (Accreditation Council of Graduate Medical Education, 2009).
Whether called Information Literacy or Evidence-Based Medicine, these skills are
recognized as important skills for all learners by the Association of College and Research
Libraries (ACRL), the American Association of Medical Colleges (AAMC), the
Accreditation Council for Graduate Medical Education (ACGME) and other major health
professional associations (Accreditation Council of Graduate Medical Education, 2009;
Association of American Medical Colleges, 1998; Association of College and Research
Libraries & American Library Association, 2000; Eldredge, Morley, Hendrix, Carr, &
Bengtson, 2012) .
Background and Significance
However, despite this recognition, neither specific training nor assessment of these
skills has been explicitly prescribed thus providing an opportunity for further research.
Whereas information literacy tests have been developed for use with undergraduate
college students, information literacy tests for use in graduate student populations have
been less well investigated, and none of the currently published instruments target

2

medical or health sciences populations. Information literacy is relevant to graduate
medical education. An instrument capable of measuring information literacy skills in the
medical setting has practical implications for identifying gaps in physician trainee
knowledge. The ability to measure information literacy proficiency in this population
would provide formative feedback to residency program directors and health sciences
librarians. This feedback would be applicable as an assessment tool to document resident
information competencies or as a diagnostic tool for revising instruction to enhance
resident education. Therefore, the purpose of this study was to develop, field test, and
evaluate an information literacy measurement tool designed for graduate medical
education trainees.
Specific aims of this project were to
Aim 1.

Generate an initial pool of information literacy items;

Aim 2.

Establish validity evidence for the draft items;

Aim 3.

Design and pilot test the draft instrument; and

Aim 4.

Evaluate psychometric properties of the tested items.

Drawing upon the ACRL standards, ACGME competencies, and a blueprint
developed from previously conducted research, a pilot study was conducted with medical
education subject matter experts to validate test items. Items approved by the panel were
incorporated into a draft instrument and tested in a resident physician population.
Psychometric properties of the instrument were examined using item analyses.
This study is important not only because there is no information literacy
instrument available for healthcare providers but because the development of a survey
instrument that reliably measures residents’ information literacy skills will provide a

3

means by which graduate medical education residency programs can identify areas of
additional or continued education needs related to ACGME competencies.

Literature Review
Information Literacy
Derived from the Latin word littera, meaning letter, literacy has been defined as
“the quality or state of being literate, especially the ability to read and write” (Webster's
Third New International Dictionary of the English Language, 1993). At this most basic
level, reading and writing are thought to make it possible for people to function in society
with the potential of furthering one’s goals and abilities (Bhola, 1994; Committee on
Performance Levels for Adult Literacy, 2005). Although these core skills are recognized
as a foundation of literacy, the ability to decode symbols is not viewed by all as true
literacy. Freire (1983) argued the additional steps of critical reflection, interpretation, and
communication with others are necessary before one can be considered a literate person
(Freire, 1983). Freire and others consider literacy a multidimensional concept that is an
essential skill across multiple domains and one which has been described as a world
view, social practice, and way of knowing (Bhola, 1994; Bruce, 1998; Committee on
Performance Levels for Adult Literacy, 2005; Freire, 1983; Mackey, 2002; OwusuAnsah, 2003).
More recently multiple literacies have emerged to enhance and expand what it
means to be “literate” in the modern era. “Literacy has multiple conceptions, which
range from a focus on the most fundamental survival skills to more complex definitions
that encompass the skills needed to thrive in a variety of contexts.” (Committee on
4

Performance Levels for Adult Literacy, 2005). The types of literacy referred to in the
previous statement include, but are not limited to: adult literacy, civic literacy, computer
literacy, cultural literacy, digital literacy, economic literacy, environmental literacy,
family literacy, health literacy, information literacy, information and communications
technology literacy, media literacy, network literacy, oral literacy, political literacy,
scientific literacy, visual literacy, and workplace literacy (Bhola, 1994; Kirsch, 2001;
Mackey, 2002; Owusu-Ansah, 2003; Shanbhag, 2006). It is not difficult to imagine how
some of these literacies overlap with or are dependent upon one another but each has its
origin in different domains or fields and each has its own particular focus.
One such literacy, information literacy, is broadly defined by the field of library
and information science as “a set of abilities requiring individuals to ‘recognize when
information is needed and have the ability to locate, evaluate, and use effectively the
needed information.’” (Presidential Committee on Information Literacy & American
Library Association, 1989, para.3). This construct includes finding and using quality print
and electronic resources effectively and ethically. The overarching concept of
information literacy addresses the comprehensive research process from generating
hypotheses to retrieving, analyzing and evaluating information that can then be used or
communicated to others (Grafstein, 2007; Owusu-Ansah, 2004). Information literacy is a
broad set of skills and abilities identified as a critical skill necessary for all students
throughout the education continuum (Baker & Boruff-Jones, 2009; Barbour, Gavin, &
Canfield, 2004; Thompson, 2002; Virkus, 2003) and is thought to encourage higher level
critical thinking, problem-solving, and life-long learning (Barbour et al., 2004; Grafstein,
2007; Grassion & Kaplowitz, 2011).

5

Information Literacy, a term in use since the mid 1970’s (Zurkowsky, 1974), has
its roots in library instruction. Also known as library user education or bibliographic
instruction, traditional library instruction in an educational setting had orientation to and
awareness of the library and its resources as its focus. The earliest type of instruction,
library tours and one-on-one or group instruction on how to use the card catalog or a
printed index to locate materials in the library was based on a narrow set of skills in a
print environment. Typically, instruction was taught by a librarian and was separate from
the curriculum (C. S. Bruce, 1997; Thompson, 2002). Moving away from the more
narrowly defined “library skills” to a broader concept of “information skills” the
information literacy movement first advanced by the 1988 Model Statement of
Objectives for Academic Bibliographic Instruction (Arp, 1987) and later explicated in the
Information Literacy Competency Standards for Higher Education (Association of
College and Research Libraries & American Library Association, 2000).
Information Literacy Standards
These standards outline knowledge, skills, and abilities in five content areas deemed
important for students at the college level (see Table 1.) The five standards (“know”,
“access”, “use”, “evaluate” and “ethical/legal”) encompass 22 performance indicators
with 87 associated learning outcomes. The outcome statements, based on Bloom’s
Taxonomy of Learning Objectives (Bloom, Englehart, Furst, Hill, & Krathwohl, 1956),
combine both technical and intellectual skills that fall into three cognitive levels:




Level 1: knowledge/remembering
Level 2: comprehension/understanding & application
Level 3: analysis & synthesis / creating & evaluating

6

Table 1.
ACRL Information Literacy Competency Standards for Higher Education
Standards
Performance Learning
Indicators
Outcomes
1 Determine nature and extent of information
4
17
needed (Know)
2 Access needed information effectively and
5
22
efficiently (Access)
3 Evaluate information and sources critically
7
25
(Evaluate)
4 Uses information effectively to accomplish
3
10
a specific purpose (Use)
5 Understands ethical, legal, and
3
13
socioeconomic issues surrounding
information and information technology
(Ethical/Legal)
Note. Adapted from Association of College and Research Libraries, & American Library Association. (2000). The
information literacy competency standards for higher education. Retrieved November 15, 2008, from
http://www.ala.org/ala/mgrps/divs/acrl/standards/informationliteracycompetency.cfm

The ACRL Standards, endorsed by the Middle States Commission on Higher
Education (Middle States Commission on Higher Education, 2006), were developed as a
framework for instructional and assessment practices for information literacy in
undergraduate and graduate education. The ACRL Model Statement, revised in 2001,
delineated learning objectives that tie to specific performance indicators and outcomes in
the ACRL competency standards.
Information literacy describes both technical and intellectual skills, combining
critical thinking and facility with information technology (Atton, 1994; Baker & BoruffJones, 2009; Kasowitz-Scheer & Pasqualoni, 2002; Lih-Juan ChanLin & Chwen-Chwen
Chang, 2003; McCarthy & Pusateri, 2006; Walton & Archen, 2004). As a result of this
paradigm shift, today’s library education and training attempts to be more in-depth and

7

discipline specific (Grafstein, 2002; Manuel, 2004). The literature confirms this trend in
the area of business (Cooney, 2005; Fiegen, Cherry, & Watson, 2002; Roldan & Yuhfen
Diana Wu, 2004; Rutledge & Maehler, 2003), the basic sciences (Aydelott, 2007; Brown
& Krumholz, March 2002; Garritano, 2010; J. A. Porter et al., 2010; J. R. Porter, 2005;
Ward & Hockey, 2007), and the liberal arts (McCarthy & Pusateri, 2006; Sjoberg &
Ahlfeldt, 2010; Studstill & Cabrera, 2010). The allied health professions also are wellrepresented in the literature (Baker & Boruff-Jones, 2009; Cobban & Seale, 2003; Cobus,
2008; Flood, Gasiewicz, & Delpier, 2010; Ford, Foxlee, & Green, 2009; Kipnis & Frisby,
2006; Morris, 2005; Nail-Chiwetalu & Ratner, 2006; Peterson-Clark et al., 2010;
Peterson-Clark, Aslani, & Williams, 2010; Powell & Case-Smith, 2010; Shanahan, 2006;
Spang, Marks, & Adams, 1998; N. Thompson, Lewis, Brennan, & Robinson, 2010).
Nursing, in particular, understood and adopted the concept early on (Cheek & Doskatsch,
1998; Dorner, Taylor, & Hodson-Carlton, 2001; Fox, Richter, & White, 1989; Fox,
Richter, & White, 1996; Francis & Fisher, 1995; Kerfoot, 2002; Rosenfeld, SalazarRiera, & Vieira, 2002; Verhey, 1999) and has continued to promote information literacy
in relation to evidence-based practice (Courey, Benson-Soros, Deemer, & Zeller, 2006;
Jacobs, Rosenfeld, & Haber, 2003; Klem & Weiss, 2005; Moch, Cronje, & Branson,
2010; Pravikoff, 2006; Ross, 2010; Shorten, Wallace, & Crookes, 2001; Tanner, Pierce,
& Pravikoff, 2004).
Ideally, information literacy skills should be taught throughout the undergraduate
and graduate curricula in higher education. However, some research suggests that many
students earning a 4-year degree graduate with less than adequate information literacy
skills (Educational Testing Service, 2006; Gross, 2005; Maughan, 2001). The truth of the

8

matter is information literacy instruction is not fully integrated into most college curricula
(Brown & Krumholz, March 2002; Burrows, Moore, Arriaga, Paulaitis, & Lemkau, 2003;
Kaplan & Whelan, 2002; K. Kingsley et al., 2011; Kingsley & Kingsley, 2009; Rader,
2002).
Information Literacy in Medicine
Evidence-based Medicine
In the field of healthcare, the corollary to information literacy is evidence-based
practice (Kaplan & Whelan, 2002) involving “the process of acquiring, systematically
reviewing, appraising and applying research findings to aid the delivery of optimum
clinical care to patients.”(Das, Malick, & Khan, 2008, p. 493). Initially introduced in the
early 1990’s, evidence-based medicine (EBM) is a five step clinical decision making
process whereby the health care provider 1) develops a structured clinical question; 2)
performs a literature search to locate the best available evidence on that topic; 3)
systematically reviews and critically appraises the evidence; 4) applies the findings in a
clinical context; and 5) evaluates the process (Sackett, 2000). Informatics, another related
concept often used in conjunction with evidence-based medicine, is “concerned with the
optimal use of information, often aided by the use of technology, to improve individual
health, health care, public health, and biomedical research.” (Hersh, 2009, "Discussion",
para.3).
Medical schools employ various teaching models (Azer, 2011; Parmelee &
Michaelsen, 2010; Schmidt, Rotgans, & Yew, 2011; Searle et al., 2003) and many have a
particular emphasis integrating one or more topics throughout the curriculum or provide a
particular track for those interested (Anderson & Kanter, 2010; Gotterer, O'Day, &

9

Miller, 2010). Most undergraduate medical schools now incorporate evidence-based
medicine instruction into the curriculum through a variety of means (e.g. lecture,
laboratory, computer assisted instruction, etc.) however only 22 schools report EBM as
an independent course (Association of American Medical Colleges, 2012). An increasing
number of medical schools emphasize research in their curricula or offer a “scholarly
concentration”. Some schools offer a month long research elective, others require six
months of research with a capstone project including both written and oral presentations,
while still others elect to provide students with a longer, more in-depth concentration
(Green et al., 2010). Of 157 medical schools, 84 institutions report a research
requirement with 10 of those schools requiring a written thesis (Association of American
Medical Colleges, 2012). Information literacy is a necessary foundation for both research
and evidence-based medicine.
Rating medical school instruction as inadequate, appropriate, or excessive, most
graduates responding to a 2013 survey reported evidence-based medicine training as
appropriate (Association of American Medical Colleges, 2013). Also rated as
appropriate were interpretation of clinical data and research reports (86.9%), ability to
conduct a systematic literature review (83.7%), and decision analysis (86.9%). When
asked about confidence in their knowledge and skills, 49.4% (agree) and 36.7% (strongly
agree) self-reported their ability to carry out sophisticated searches of medical
information databases along with 49.1% (agree) and 31.3% (strongly agree) who were
confident they could critically review published research (p.22-23). When asked about
preparation for residency, the respondents agreed (53.9%) or strongly agreed (40.2%) that
they had basic skills in clinical decision making and application of evidence based

10

information to medical practice. It should be noted here this is self-reported data. There
is no standardized instrument presently in place to verify actual knowledge or skills in
these areas, although the Committee to Evaluate the USMLE Program (CEUP) has
proposed adding assessments to address medical student ability to access, evaluate, and
apply information with the new test (Kies & Shultz, 2010). The USMLE Step 3
Foundations of Independent Practice (FIP) examination will contain content related to
interpreting medical literature and will comprise “newer item formats based on scientific
abstracts and pharmaceutical advertisements” and will take place beginning 2014
(USMLE, 2013).
Given the educational requirements for acceptance to and graduation from
medical school one must assume physicians are, in the broadest sense, literate. The
practice of medicine, increasingly complex and specialized, provides challenges to
physicians as they care for patients. In this environment, it is critical therefore that
physicians are also information literate – that is, able to locate current literature in their
field, evaluate the information they find, and able to use and communicate that
information. By the time a student graduates from medical school and moves into postgraduate training he or she should be capable of and comfortable with locating,
evaluating, and incorporating information into daily practice. However, as previously
described, students experience different exposure to information literacy in the college
years and to research, evidence-based medicine, and medical informatics during medical
school (Association of American Medical Colleges, 2010; Citrome & Ketter, 2009;
Krause, Roulette, Papp, & Kaelber, 2006; McGowan, Passiment, & Hoffman, 2007;
Scott, Shaad, Mandel, Brock, & Kim, 2000; van Dijk, Hooft, & Wieringa-de Waard,

11

2010) thus leading to varying information literacy skill levels among physician trainees
during the post-graduate years.
Graduate Medical Education
Post-graduate training (aka “residency”) is the second phase of a physician’s
education. Medical school graduates take a licensure exam to become physicians but,
depending upon the medical specialty they have chosen, must then spend between three
and six years in the post-graduate years (PGY) learning to become competent
practitioners. A residency in a primary specialty such as Family Medicine, Internal
Medicine, Psychiatry or Pediatrics takes three years to complete while the surgical
specialties require more years of training. Post-graduate medical training programs in the
U.S. are governed by the Accreditation Council for Graduate Medical Education
(ACGME). The most recent census lists 3,399 unique institutions providing post
graduate education to 109,482 physicians in 91,384 core programs and 18,098
subspecialty programs (Accreditation Council for Graduate Medical Education, 2013).
The ACGME has a standard set of guidelines and competencies for all residency
programs (Accreditation Council of Graduate Medical Education, 2009) with additional
requirements set forth by Residency Review Committees (RRC) for individual medical
specialties (Accreditation Council for Graduate Medical Education, 2011a).
Before 1910 medical education followed a model vastly different from today.
Education in the basic sciences was a very minor aspect of training and “students” paid
physicians to be an apprentice, caring for patients while living in the hospital (hence the
term “residents”) (Joyner, 2004). This changed with the Flexner Report (Flexner, 1910)
which was the impetus for regulating undergraduate medical education training. More

12

recently, graduate medical education has evolved from a process-based to a competencybased model that includes outcomes-based assessment (Baum & Axtell, 2005; Carraccio,
Wolfsthal, Englander, Ferentz, & Martin, 2002; Leach, 2002; Long, 2000; Marple, 2007;
Meyers et al., 2007). While there continues to be an aspect of “apprenticeship”, the major
focus of postgraduate training is resident education. This education is both didactic and
experiential with individual residency programs now expected to develop program goals
and objectives, integrate learning objectives into the curricula, and document the
educational outcomes of their trainees (Goroll et al., 2004; Joyner, 2004; Lurie, Mooney,
& Lyness, 2009)
ACGME Competency Standards
In 1999 the Accreditation Council for Graduate Medical Education (ACGME)
developed a ten year plan to define six general competencies (1999-2002), define and
develop assessment tools (2002-2006), and integrate the competencies and assessment
into learning and clinical care (2006-2011) (Lurie et al., 2009). The core competencies set
forth by the ACGME fall into six general domains: Patient Care, Medical Knowledge,
Practice-Based Learning and Improvement (PBLI), Interpersonal and Communication
Skills, Professionalism, and Systems-Based Practice (Accreditation Council of Graduate
Medical Education, 2009; Rousseau, Saucier, & Cote, 2007). Skills learned in one
competency domain may be applied to other domains. For example, the understanding
and incorporation of specific content knowledge found in Medical Knowledge is related
to the skills development described in the Practice-based Learning and Improvement
domain and key to appropriate and effective Patient Care. Likewise, it is assumed the
competent physician will exhibit Professionalism across all domains.

13

While some groups have adapted the ACRL competency standards to better meet
the needs of their discipline (Anthropology and Sociology Section Instruction and
Information Literacy Committee Task Force on IL Standards, Association of College and
Research Libraries, & American Library Association, 2008; Psychology Information
Literacy Working Group, Education and Behavioral Sciences Section, & Association of
College and Research Libraries, 2010; Task Force on Information Literacy for Science
and Engineering Technology, Association of College and Research Libraries, &
American Library Association, 2008), other potential stakeholders, such as those in the
medical education field, are either unaware of the information literacy standards or have
not been incorporated into the discussion.
Most directly related to the concepts of information literacy, evidence-based
medicine and informatics is the ACGME competency for Practice-based Learning and
Improvement (PBLI) (Kaplan & Whelan, 2002; Moskowitz & Nash, 2007; Ogrinc et al.,
2003; van Dijk et al., 2010; Wood, Kronick, & Association of Medical School Pediatric
Department Chairs, Inc, 2008). Within this domain, resident physicians are expected to:





Locate, appraise, and assimilate evidence from scientific studies related to
their patients’ health problems;
Apply knowledge of study designs and statistical methods to the appraisal
of clinical studies and other information on diagnostic and therapeutic
effectiveness
Use information technology to manage information, access on-line
medical information; and support their own education
Facilitate the learning of students and other health care professionals

Although this language generally corresponds to that found in the information literacy
competency standards, clearly written, measurable indicators are lacking. While ACRL

14

standards can be adapted to represent the needs of the medical community neither of
these professional bodies has specified how the standards are to be assessed.
Information Literacy Assessment
Information assessment instruments and tools described in the library science and
medical literature exhibit similar traits. In general, the majority of available instruments
assessments are frequently tied to specific courses or institutional resources thus
precluding generalization to another setting; rely on self-reported data; are missing
elements of the domain under investigation (i.e., plagiarism); and, even when objective
measures are used, reliability and validity data frequently are not reported. While the
medical education literature does address assessment for post-graduates, most college
level instruments are intended to measure undergraduate student skills.
Library Science Assessment
The library science literature reports numerous studies related to the assessment
of information literacy skills and evaluation of information literacy instruction.
Information literacy tools in one review of 70 college-level assessments were categorized
as six types: attitudinal, freshman skills, institution-based, pre/post- test, subject or
discipline oriented, and technology skills and observed that most were not based on the
ACRL standards (Neely, 2006). Further investigation by this author in 2011 showed none
of the reported instruments listed by Neely were health related and many were no longer
available on the websites referenced. Another more current review provides information
regarding the methods used in information literacy testing while also addressing the
question of instrument reliability and validity (Walsh, 2009). From a sample of case
studies (N=91) Walsh found multiple-choice questionnaires were most often used (34%),

15

followed by analysis of bibliographies using scoring rubrics (19%), quizzes (15%)
containing both multiple-choice and short answer questions, and self-assessment (11%).
Less often employed were portfolios (9%), essay (7%), simulation (2%), observation
(2%), or final course grade (1%). Within this review, only 9 studies were found to contain
reliability and validity data (3 multiple-choice, 2 Quiz/Test, 2 simulation, 1 each portfolio
and self-assessment).
Graduate Medical Education Assessment
The ACGME “Outcome Project” was initiated to provide a compendium of
competency assessment methods and examples of instruments. Closely mirroring
methods found in the Walsh review (Walsh, 2009), the Toolbox of Assessment
Methods©, a joint initiative of the ACGME and the American Board of Medical
Specialties (ABMS), lists thirteen methods of evaluation used to assess skills found
within each of the six general competencies. These methods include: Record Review,
Chart-Stimulated Recall; Checklist; Global Rating; Standardized Patient; OSCE
(Observed Structured Clinical Examination); Portfolios; Exam MCQ; Exam Oral;
Procedure or Case Logs; and Patient Survey and is accompanied by a grading scale
suggesting methods most appropriate for assessing each of the required skills
(Accreditation Council for Graduate Medical Education, 2011b). A 2009 review of
ACGME competency assessment methods by Lurie, Mooney, and Lyness (2009)
concluded that multiple dimensionality across competencies, problematic issues with
methodology and samples, in addition to the overall lack of psychometric data, created
concerns about the ability to accurately measure necessary skills. Despite these

16

challenges, the authors remarked upon the need for continued effort toward effective
measurement.
During the postgraduate training years, residency program directors and other
medical educators may assume a level of expertise on the part of the resident physician
that is not there or they themselves may not have the knowledge or proficiency necessary
to model, teach, or assess the necessary skills (Holmboe et al., 2011; Maggio & Posley,
2011; Swiatek-Kelley, 2010). If this is the case, then one must ask the question “How do
medical educators know what information literacy knowledge, skills, and abilities
resident physicians have?”
Information literacy Instruments
The instruments described in the following section are commercially available or
open-source, encompass validated and non-validated tools, and were considered (in
whole or in part) for use in this dissertation project.
1. Information Literacy Test (ILT) (Cameron, Wise, & Lottridge, 2007; Wise,
Cameron, Yang, & Davis, 2010) The ILT, measuring ACRL Standards One, Two,
Three, and Five, is a validated, web-based multiple-choice test (N=60) for use
with undergraduate college students. The ILT is available for purchase and may
be used to assess individual student information literacy competence or as a
program evaluation tool. Lower order knowledge skills make up two-thirds of the
items with higher order application skills making up the remainder. Validity
evidence was gathered from results of four studies and a validation panel. In one
study a significant positive correlation (r=.45) was found when comparing ILT
scores of 524 sophomores to their freshman scores on an earlier version of the

17

test. A comparison of sophomore and freshman student scores in two other
studies illustrated improved scores in students who had been provided information
literacy instruction (Wise, Cameron, Yang, & Davis, 2010). Finally, freshman
ILT scores (years 2008 and 2009) from four universities and five community
colleges compared against JMU freshman scores from 2004 showed incoming
students, regardless of institution type, have similar levels of information literacy
skills. A validity panel of three librarians rated the 60 items in relation to the
ACRL standards. Inter-rater agreement was good. All raters were in agreement on
70% of the items (N=42) while at least two concurred on 98% (N=59). Internal
reliability is high (Cronbach’s alpha = 0.88).

2. Standardized Assessment of Information Literacy Skills (Project SAILS®) Kent
State University (2001) http://www.projectsails.org Based on outcomes and
objectives from the ACRL Standards (excluding Standard Four), the SAILS test is
a web-based, information literacy knowledge assessment tool. Questions are
grouped into skill sets and each is referenced to the applicable ACRL standard,
performance indicator, outcome, and objective. The test, accessible for a fee,
employs a multiple-choice format and provides either individual or cohort
benchmarking data to institutions. The instrument was externally validated
through comparison of participant scores on the SAILS with their SAT/ACT
scores. Test developers found high scoring students on SAT/ACT also achieved
high SAILS scores. Performance on the SAILS test also was compared against
performance data from the JMU Information Literacy Test (ILT). Item reliability

18

estimates were greater than .80 and inter-rater reliability analyses on item
difficulty across skill sets was deemed satisfactory (ranging from .65 to .80).
Instrument item level difficulty is geared toward the undergraduate college
student population and is, therefore, not available for testing primary, secondary,
or graduate school students.

3. ICT Literacy Assessment (iSkills™) (Katz, 2007) Covering seven content areas,
this instrument assesses information and communication technology (ICT) and is
available for purchase from the Educational Testing Services (ETS). “ICT literacy
is using digital technology, communication tools and/or networks to access,
manage, integrate, and create information in order to function in a knowledge
society.” (Candy, 2002). The iSkills™ exam is a performance-based instrument
assessing skills related to web use (i.e., e-mail, instant message, browser use,
bulletin board postings, and search engines), database management (i.e., database
search queries and file management), and software (i.e., word processing,
spreadsheets, presentation and graphics programs). The instrument is appropriate
for students (10th grade through college) and adults in the workplace. Questions
simulate real-world scenarios; there are no multiple-choice items in this test.
Internal reliability is reported to be approximately.88 (Cronbach’s alpha).
Although aligned with the ACRL Standards, the ETS instrument differs in content
from other information literacy tests. Here the focus is on competence with
technology and less related to other information literacy domain skills as
delineated by the ACRL.

19

4. Beile Test of Information Literacy for Education (B-TILED) (Beile, 2008).
Devised to measure information literacy knowledge in undergraduate students in
an education program, this instrument initially began as items developed for the
Project SAILS test bank. The original items, aligned with ACRL objectives and
National Educational Technology Standards for Teachers (NETS*T), were
reduced in number (N=22) and pilot tested as the Information Literacy
Assessment Scale for Education (ILAS-ED) (Beile, 2005). Reliability coefficient,
using Kuder Richardson 20 test, was deemed only “adequate” (.675) due to small
number of test items. Item difficulty ranged from .32 to .89. Item accuracy (mean
score of 2.67) was evaluated by five content experts on a scale of 0 (low) to 3
(high). Results from a subset of eight questions from the measure were evaluated
for validity against a performance exam with comparable questions. Results
indicated a high correspondence between tests, however the sample size was
small (N=10).
Further revision of the ILAS-ED led to the development of the B-TILED, a freely
available, multiple-choice test containing 25 content questions (covering concept
identification, search strategies, document types, search tools, and using results)
specific to the discipline of education and 21 demographic questions. Reliability
and validity data for the revised instrument has not been published.

5. Information Competency Proficiency Exam (ICPE) Bay Area Community
Colleges (The Bay Area Community Colleges information competency
assessment project, 2004). The ICPE is a two part proficiency exam whose
purpose is to assess information literacy skills in a community college population.
20

Based on national and local information literacy standards this test serves as a
means to earn credit for an information competency requirement. Part A (N=47)
incorporates questions using multiple-choice, matching and short answer formats;
Part B has three performance-based activities each with sub-parts (N=12).
Detailed test specifications are provided: practices used in item development,
organization and formatting, instructions and comments, suggestions for scoring,
level of performance, and timing, as well as charts mapping competency
outcomes to test items. In addition, the developers make available documents for
scoring the rubric, answer key, scoring sheets and a scoring manual for Parts A &
B. Scores from this instrument have not been statistically analyzed therefore no
reliability or validity evidence has been reported.

6. Berlin Questionnaire (Fritsche, Greenhalgh, Falck-Ytter, Neumayer, & Kunz,

2002). Two psychometrically equivalent sets of questions (N=15) were
developed, validated, and field tested for this evidence based medicine
instrument. The validation phase included administering the 15 item instrument
to a group of EBM experts (N=43) and a control group of medical students
(N=20) with no prior knowledge. Pre- and post-test data was obtained from
participants (N=203) attending a 3 day intensive EBM course. Experts scored
high at 0.81; pre-test scores for course participants were moderate at 0.42; and the
control group at 0.29 scored lowest. Cronbach’s α was 0.75 (set 1) and 0.82 (set
2). There was an educationally significant increase (57%) in the post scores of
course attendees. This instrument measured critical appraisal skills and interpreted

21

quantitative measures. Other skills such as literature searching and question
formulation were not assessed.

7. Fresno Test of Competence in Evidence Based Medicine (Ramos, Schafer, &
Tracz, 2003). This performance based measure was specifically developed to
assess an EBM course taught in a Family Practice residency program at the
University of California. Utilizing two clinical scenarios test takers answer 17
open ended questions related to four key EBM domains - developing a focused
clinical question; searching and critiquing the literature; identifying article
relevance and validity; and calculation skills. Standardized grading rubrics and
criteria were developed and revised using expert opinion and expected responses.
Each item is awarded points (0-24). Raw scores are summed across items to yield
a possible range of 0 - 212 with higher values indicating greater level of
knowledge. Cut score for passing was assigned based on the authors’ professional
judgment. Test results from the development dataset showed very good internal
consistency (Cronbach’s alpha = 0.88); no negative or weak items (item
discrimination index = 0.47 to 0.75). Validity dataset inter-rater reliability (2
scorers) ranged from 0.72 to 0.96 for individual items and 0.97 for total scores.
While found to be appropriate for Family Medicine physicians, both novice and
expert, the Fresno Test is intended to measure the effectiveness of a specific EBM
training course and has not been validated in other medical disciplines.

While the instruments described above address information literacy assessment,
none were appropriate for the project envisioned here, although individual questions were
22

considered, given publisher permission and appropriate attribution. This study was
intended for postgraduates, not the undergraduate population most tested with these
instruments. Ease of instrument administration and scoring was a consideration thus
eliminating performance-based testing. More importantly, only two are validated for use
in a medical education setting and none encompass the full domain as defined by the
ACRL Standards. For these reasons, an instrument capable of easily measuring
information literacy skills in a graduate medical education population was developed,
tested, and evaluated for evidence of reliability.
Development of the Medical Information Literacy Questionnaire (MILQ)
Blueprint Creation
The first step in instrument development is the creation of a test blueprint (also
known as a table of specifications). The test blueprint provides the framework for
classifying learning objectives, ensuring proper emphasis is given to each objective,
guides item development, and is used to improve validity of test scores (Anderson &
Morgan, 2008;Thorndike, 1997). The operational definition of the content domain to be
measured is represented by the test blueprint.
The test blueprint for this dissertation was developed using data from a prior
research study conducted at the University of New Mexico Health Sciences Center
(Morley, 2009). The specific purpose of the study was to investigate the content validity
of the ACRL information literacy standards in a graduate medical education setting using
a Q-sort methodology. Card sorting, an exploratory technique used to elicit information
about how individuals or groups think about and categorize topics (Morse & Field, 1995;

23

Rugg & McGeorge, 2005), was used to capture expert judgment about core information
literacy competencies necessary for resident physicians to know or be able to do.
Method
Participants
Sixteen UNM Health Sciences Center physicians (38% Female, 62% Male)
representing eleven specialties took part in the study. Study participants included those
with responsibility for residency training programs (56%) and those working closely with
resident physicians in their role as educators or attending physicians (44%). These subject
matter experts (SMEs) were invited to rank content and behavioral objectives of the
construct known as information literacy. As defined by the ACRL, each of the five broad
competency standards includes a number of associated performance indicators which are
further articulated by 87 specific learning outcomes or objectives, herein referred to as
information literacy statements. Working individually, the subject experts ranked each
information literacy statement as “essential”, “useful but not essential”, “not
necessary”, or “unsure” as they pertained to resident physician knowledge and ability.
Analysis
Data from the card sort were analyzed using a statistical descriptor of expert
consensus known as the Content Validity Ratio (CVR) formula (Ford & Wroten, 1982;
Ford & Wroten, 1984; Lawshe, 1975; Pray & Popovich, 1985). Twenty-nine statements
rated as “essential” had CVR values .49 or greater which constitutes the cut score
established by Lawshe (1975, p. 568) and were thus retained for inclusion in the blueprint
(Appendix A). (Note: information on this technique is more fully described in the
Chapter 2 Phase II Validation Panel.)

24

The test blueprint used in the study described herein was based upon information
literacy competencies identified by physicians as essential to resident competence.
Various forms of this test blueprint (see Appendices A and B) were employed throughout
the dissertation study to guide item writing, verify a match between item content and
targeted objective, and measure item balance across the standards (Li & Sireci, 2005;
Rothman, Slattery, Vranek, & Resnick, 2002).
Summary
The previous sections describe the information literacy construct and its
alignment with graduate medical education competencies and evidence-based medicine.
The literature shows physicians and accrediting bodies consider information literacy an
important skill set for resident physicians to master. In addition, although information
literacy assessment tools exist, no validated instruments covering all aspects of the
information literacy domain were found specifically designed for the graduate medical
education population. To address this deficiency, the author designed, validated, and field
tested items for a new information literacy instrument called the Medical Information
Literacy Questionnaire (MILQ). Chapter 2 describes in detail the study design and
methods employed in this project.
This study is centered on the ACRL Information Literacy Competency Standards
for Higher Education and draws upon research and ideas from educational and
psychological measurement, evidence-based medicine, graduate medical education, and
information sciences.

25

CHAPTER 2
METHODS
This chapter presents a detailed description of the procedures employed in
constructing, pilot testing and assessing the reliability, validity, and internal structure of a
newly developed instrument entitled the Medical Information Literacy Questionnaire
(MILQ). Following established instrument development processes (American
Educational Research Association, American Psychological Association, & National
Council on Measurement in Education, 1999), the study was executed in four phases.
Phase I: The first phase of the study focused on generating items from which the
eventual MILQ would be constructed. This item generation was guided by a
previously developed test blueprint (Morley, 2009) and employed a number of item
writers.
Phase II: Initial validity evidence for the pool of draft items was gathered from a
volunteer panel of subject matter experts (SMEs) in the second phase of the study. A
statistical descriptor of expert consensus, the Content validity Ratio (CVR) method
(Lawshe, 1975), was employed in the analyses.
Phase III: Validated items from Phase II were field tested using cross-sectional
survey methodology in a sample of resident physicians from the University of New
Mexico.
Phase IV: In the final phase of the study, psychometric properties of the tested items
were evaluated using item analyses. Scores from the field test were analyzed to
examine the structure and reliability of the draft instrument.
Statement of Instrument Purpose
The MILQ is intended for use by residency program directors and health sciences
librarians as a diagnostic tool to measure resident physician information literacy skills.
The MILQ, a criterion-referenced test (Glaser, 1963;Berk, 1986), is intended to measure
maximum performance, as judged by the correct answer, at the time of testing and not as
a predictor of future IL skills or abilities. Scores from this instrument will provide
26

necessary data for formative evaluation of IL instruction and the assessment of skills
found within the required ACGME general competencies. Information from
administration of this instrument will be used to improve curriculum and instructional
programs thus enhancing resident education. Program directors may also elect to report
test results as verification of resident ability to meet ACGME competencies (e.g.,
Practice Based Learning and Improvement) or, in the case of increased IL training, as
documentation of resident education.
In order to meet the purpose, certain things need to be true of the final instrument.
The instrument should be a web-based, best-answer, multiple-choice test employing case
vignettes or application of knowledge stems (Case & Swanson, 2001). The test is meant
to be completed in one sitting. While time for completion depends upon the number of
items included in the final form, ideally the test should take less than 30 minutes.
Phase I: Initial Item Generation
Items representing the IL construct as defined by the ACRL Standards (“know,
“access”, “use”, “evaluate”, and “ethical/legal”) were developed by the researcher and
a volunteer group of subject matter experts (N=11), that included librarians, medical
educators, physicians, and researchers. Item writing took place between October 1, 2013
and December 10, 2013. After review, items either were revised or eliminated from
consideration for inclusion in Phase II validation activities.
Item-writer qualifications
Because the proposed instrument is intended as an information literacy
assessment tool in graduate medical education, it was important to obtain volunteers who
were familiar with the clinical setting, knowledgeable about the skills necessary to

27

address physician’s information needs, and have experience teaching information literacy
skills to resident physicians or medical students. Initially, volunteers considered for the
item writing phase of the study were health sciences librarians with a minimum of two
years’ experience teaching evidence-based medicine or information literacy to resident
physicians or medical students. Item writing experience was preferred but not mandatory.
Item-writer recruitment
Using convenience and snowball sampling methods, regional and local health
sciences librarians were invited to participate in the item writing phase. The goal was to
accrue a minimum of five health sciences librarian volunteers to assist with item writing.
An initial call for volunteers was made to education librarians at the 2013 annual meeting
of the South Central Chapter of the Medical Library Association (SCC/MLA). A followup e-mail describing the study process was sent to the SCAMeL Education listserv
following the conference. Because the initial call and subsequent follow-up email
elicited only one response, the recruitment plan was revised. Individuals known to the
researcher were targeted and the potential pool was expanded to include other
professionals with experience in medical education and those with item writing expertise.
Once confirmation was received from persons willing to take part in the exercise,
a second email was sent containing a copy of the detailed version of the blueprint
(Appendix A). Each item writer was asked to submit as many questions or ideas for
questions as they wished. Participants were encouraged to complete and return all
questions within two to three weeks of receiving the instructions.
Using an expanded version of the test blueprint as the framework, standardized
item writing rules were employed to create items for the initial pool (Anderson &

28

Morgan, 2008; Case & Swanson, 2001; Haladyna, Downing, & Rodriguez, 2002;
Haladyna, 2004; Thorndike, 1997). Three types of multiple-choice formats were used in
developing items for this project.
1. Conventional multiple choice- consisting of explicitly stated questions
(stem) and three to five answer options.
2. Extended matching – a group of questions with one list of options for all
the items.
3. Context-dependent item set – material (i.e., picture, table, or abstract)
followed by a set of questions whose answer options are predicated upon
information found within the material.
Content for the items was informed by material found in textbooks and journal
articles, evidence-based medicine and health sciences library courses, curricular
materials, currently published information literacy tests, and items from a UNM School
of Medicine test bank. Each item was developed using scenarios, vignettes, or other
materials considered relevant to a resident physician. Answer options covered
information resources commonly used by a majority of residents (e.g., PubMed Medline)
regardless of where they went to medical school. To avoid bias universally recognized
online resources were included as answer options while specific resources licensed by the
University of New Mexico were avoided.
Demographic Characteristics
Eleven volunteers provided questions or ideas for questions. The group was
predominately women (N=10) and health sciences librarians (N=7). Other volunteers
included a doctoral student in Public Health, an associate professor with expertise in

29

health literacy item writing and instrument development, and an assistant professor from
Psychiatry with expertise in assessment. The only male in the group, a professor of
Internal Medicine, granted permission to use a series of his previously developed test
items.
Analyses
A total number of 147 items were developed during the item generation phase.
An assessment expert from the UNM School of Medicine reviewed the proposed items
for adherence to item writing best practice and provided comments and suggested
changes. In many cases several iterations were necessary before items were deemed
acceptable. Items were revised as necessary or eliminated. Items were deleted from the
initial pool if the stem and/or answer option were out of scope, lacked clarity, were
ambiguously or poorly worded and difficult to revise, or did not fit the MCQ format.
Furthermore, in domains where there were large numbers of overlapping questions, the
weakest questions were eliminated. Ninety items (61%) from the original pool were
approved for inclusion in the Phase II validation panel. These items were conventional
multiple-choice questions (N=25), matching (N=16); and context-dependent item set
(N=28) made up of 7 separate scenarios/sets of materials.
Phase II: Item Validation Panel
To establish some initial validity evidence that the items did, indeed, map to the
test blueprint and adhered to best-practice recommendations, a volunteer panel of subject
matter experts (SMEs) reviewed the item pool for representativeness of the draft items to
the content domain and significance to resident physicians. Data gathered from the
validation panel were used to determine item inclusion in the draft instrument tested in

30

Phase III pilot with resident physicians. The validation panel investigation took place
December 13, 2013 through January 12, 2014.
Subject Matter Experts Eligibility Criteria
Experienced health sciences librarians (as defined in Phase I) and physicians were
eligible to take part in the validation panel process. Physician participants were expected
to have demonstrated experience working with resident physicians in a clinical or
educational setting.
SME Recruitment
An e-mail describing the study and the validation panel process was sent to a
purposive sample of thirty-five health sciences librarians (n=6) and physicians (n=29).
Three of the invited librarians contributed to the item development phase and ten of the
physicians previously took part in the card sort study (Morley, 2009) that resulted in the
test blueprint used in the present study (see Appendix B).
Psychometric Process
Once agreement was received, individuals received a follow-up email thanking
them for their participation. Included in the email were instructions for completing the
required tasks along with the necessary attachments: a copy of the detailed blueprint
(Appendix A), a demographic form (Appendix C), and five separate Word documents
containing multiple-choice items covering each of the five domains. Domains 1 and 2
contained eight items each; Domain 3 and 5 contained thirty-five items each; and Domain
4 contained four items.
Using the Content Validity Ratio method (Lawshe, 1975) panelists rated each
item for relevance to the specific domain from the blueprint to which it aligned. Panelists

31

were invited to provide information as to the suitability and accuracy of the items. Also,
as part of the review process, SMEs were asked to agree or disagree with the researcher’s
placement of each question within a particular domain.
Panelists completed a questionnaire ranking their agreement or disagreement with
items from the item pool. Utilizing a three-point rating scale (“essential”, “useful but not
essential”, or “not necessary”), panelists provided a relevance rating for each draft item.
According to Kline ( 2009) scales containing 5-9 points may be optimal in terms of
reliability and ability to discriminate between the scale values while Sierci (Sireci, 1998a)
explains that the use of even numbered scales prevents the excessive use of neutral
points; see also (Ritter & Sue, 2007). However, because the purpose of the validation
panel was to gather opinion about item inclusion or exclusion, the Lawshe (1975) CVR
three–point scale was deemed more appropriate than a wider spread scale.
To finalize the questions to be included in an instrument, a determination must
also be made whether or not each item is representative of the content category in which
it was placed. Therefore, in order to show evidence of validity, panelists were asked to
verify assignment of each item to the blueprint (“yes” or “no”). Responses from the
completed forms were entered into an Excel spreadsheet.
Demographic Characteristics
Of the thirty-five experts invited to lend their professional expertise to the project,
twenty-eight (80%) agreed to participate; four (11%) declined to participate; and three
(9%) did not respond to the invitation or a subsequent follow-up e-mail. Of those who
initially agreed two (7%) failed to return their forms by the deadline and a third person
did not follow the instructions for ranking the items thus making their contribution

32

ineligible for inclusion. Twenty-five persons (out of 28) completed the CVR
questionnaire, a response rate of 89%. There were similar numbers of men and women
on the panel (Female 52%); Fifty percent of the physicians identified themselves as a
residency program director, assistant/associate program director, or former program
director. Demographic forms for three physicians were not returned. Apart from
academic and departmental status, gathered from the institutional website, and gender,
missing data were otherwise excluded from the analysis. See Table 2 for details of the
demographic characteristics for this panel of volunteers.
Table 2.
CVR Participant Demographic Characteristics
Participant Demographic Characteristics
Ethnicity
White
Hispanic
Asian/Pacific Islander
Gender
Female
Male
Degree
Master level or above Librarian
M.D.
Resident Program Responsibility
Program Director
Assistant Program Director
Former Program Director
Physician Specialty
Internal Medicine
Pediatrics
Emergency Medicine
Anesthesiology
Neurosurgery
Pathology
Psychiatry
Surgery

n1

Valid
Percent2

20
2
1

80%
8%
2%

13
12

52%
48%

5
20

20%
80%

5
1
4

25%
5%
20%

9
4
2
1
1
1
1
1

45%
20%
10%
5%
5%
5%
5%
5%

1 n≠ 28 due to missing values from non‐response.
2 Valid percent refers to frequencies excluding missing values.

33

Analyses
In order to determine which items showed acceptable evidence of validity, data
gathered from the participants were statistically analyzed using the content validity ratio
(CVR) method developed by Lawshe (1975). The Content Validity Ratio (CVR), a
statistical descriptor of expert consensus, has been used in a number of research studies
for instrument development and content validation (Alotaibi & Youssef, 2013; Ford &
Wroten, 1982; Pray & Popovich, 1985; Wallace, Blake, Parham, & Baldridge, 2003).
Panel members individually rated each item based upon whether, in their judgment, the
item represented an ‘essential’ resident physician skill or knowledge. The content validity
ratio for each item was calculated using the formula CVR= (ne-N/2) / N/2.
Utilizing a one-tailed significance test at alpha=.05, and based on a panel of 25
raters, each item should receive a minimum CVR score of .37 in order to be considered
for inclusion in the draft instrument for the pilot study. It should be noted here that not
every SME rated all the items (range 18-25). To allow for missing data, computation was
based on the total number of panelist responses for each individual item and not the total
sample size. Because the published guidelines do not provide scores for every
permutation of panel size the interpretation of the cut scores was based on the span of
numbers provided in Lawshe’s table (Lawshe, 1975, p.568). For example, a CVR score
of .49 is the recommended cut score based on fifteen raters. For 20 raters the
recommended CVR is .42. Following Lawshe’s guidelines, items were retained if the
minimum CVR values fell within a permissible range.

34

Results
The validity panel of 20 physicians and 5 librarians rated ninety multiple-choice
items assessing IL knowledge and skills divided into five domains of knowledge. A
content validity ratio (CVR) score was calculated for each item. Of the ninety original
items, sixty-seven (74%) met or exceeded the minimum CVR score recommended by
Lawshe. Despite lower than minimum CVR scores, two marginally ranked items were
retained in Domain 4 to ensure sufficient domain coverage.
Phase III: Pilot Test Draft Instrument
Purpose
Based on the results from the Phase II validation panel, an electronic version of
the instrument was created using the REDCap (Research Electronic Data Capture) online
survey tool. The 69 items retained from Phase II were pilot tested in a resident physician
population using a web-based questionnaire. This phase took place January 29, 2014
through February 12, 2014.
Sample and Participant Selection
Using a convenience sample, individuals eligible to participate in the study were:
(1) employed by the UNM Health Sciences Center as a resident physician, (2) in postgraduate year one through six, and (3) had an institutional GroupWise email address.
Based upon these criteria there were 472 residents from 14 residency programs in the
population. No eligible participant who wished to take part was excluded from the study.
Using a saturation sampling technique (Ritter & Sue, 2007), the sampling frame
consisted of email addresses from the official UNM HSC e-mail system (GroupWise
version 8.0, Novell Corp.). An alphabetic listing by email address was provided as an

35

Excel spreadsheet to the researcher by the UNM Health Sciences Center GroupWise
account manager.
To introduce and gather support for the project, information about the study was
presented to the Associate and Assistant Deans for Graduate Medical Education at an
operations committee meeting (GMEC). A follow-up email was sent to the program
directors for each residency program. An explanation of the survey and request for
participation was communicated by e-mail, in-person, or a presentation during a
residents’ meeting. No compensation or incentives were offered for participation. This
study was approved by the University of New Mexico institutional review board.
Data Collection
Study data were collected and managed using REDCap electronic data capture tools
hosted at the University of New Mexico (DHHS/NIH/NCRR #8UL1TR000041).
REDCap (Research Electronic Data Capture) is a secure, web-based application designed
to support data capture for research studies, providing 1) an intuitive interface for
validated data entry; 2) audit trails for tracking data manipulation and export procedures;
3) automated export procedures for seamless data downloads to common statistical
packages; and 4) procedures for importing data from external sources (Harris et al.,
2009).
All current UNM resident physicians at the time of the survey were sent an email
invitation to participate in the study (Appendix D). A hyperlink connected directly to the
survey was included in the email. By clicking on the link, the resident agreed to
participate. Participants were assured their responses would be kept confidential. E-mail
identifiers within the survey tool were used for invitations and reminders only. Data

36

downloaded from the survey tool into Excel did not include email addresses or other
unique identifiers. Each survey response was given a sequential record_id (i.e., 1, 2, etc.)
Questionnaire Description
The initial MILQ that was field-tested consisted of 69 multiple-choice questions
measuring the information literacy construct contained within five domains or subscales.


Subscale one (six items) determining the nature and extent of the information
need



Subscale two ( five items) information access



Subscale three ( 23 items) evaluating information



Subscale four (four items) information use



Subscale five (31 items) addressing ethical and legal issues related to information
and information resources
Respondents were instructed to read each item then choose the BEST answer

option for the item.
Because the length of the questionnaire could reasonably be expected to lead to
survey fatigue, two parallel forms of the instrument were created. The intent of having
two forms was to capture as many responses as possible for each question in the event the
respondent failed to complete the entire questionnaire. Each form contained the same
questions but was ordered differently by domain. Form A (N=235) started with domain
one, question one and continued through to the final question of domain five. Form B
(N=237) began with questions from domain five, followed by domain three, four, two,
and ending with domain one questions. Email addresses for the sample were divided into
two subsets by choosing every other address. Each subset was then copied and pasted
into the REDCap survey tool as an invitation list for Form A or Form B.

37

Each online survey was assigned a unique identifier so that follow-up email could be
sent to non-responders. The literature indicates varying response rates for email surveys
(Dillman, 2007; Kaplowitz, Hadlock, & Levine, 2004). In one study surveying health
educators, survey response rate was shown to increase following one reminder but only
slightly more with a second reminder (Kittleson, 1997). UNM residents are surveyed
throughout the year on a variety of topics (e.g. ACGME duty hours, faculty and annual
program evaluations). Based upon the literature and UNM specific data, one email
reminder was sent on day four to those in the sampling frame who had not responded.
Reminder emails to those who only partially completed their survey (N=16) were sent on
day eight. A second reminder to the larger group of non-responders was sent on day nine
following the initial survey launch. Residency program directors and chief residents were
enlisted to assist with general reminders to the residents in their programs prior to the
initial launch and before a third and final email reminder was sent on day twelve. The
survey remained open for 14 days.
Analyses
Participant responses from the field test were exported from the REDCap survey
tool into a Microsoft Office 2007 Excel spreadsheet then imported into the SPSS
statistical package (IBM SPSS Statistics, version 22.) Initial evaluation was conducted
through item analyses. The purpose of item analyses is to assess overall test performance
and individual items. This evaluation entails quantitative and interpretative review and
was used to reduce the test length by eliminating poorly performing items. Item means,
standard deviations, alpha-if-item-deleted, corrected item-total correlation (item
discrimination), and P values (item difficulty) were computed and examined. Data from

38

this analysis were used in conjunction with the blueprint to examine items for retention in
a final form.
Using exploratory factor analytic techniques, test score data were analyzed to
investigate the internal structure of the instrument. Exploratory factor analysis (EFA) was
chosen as the model of analysis because it allows exploration of underlying construct
dimensions and is frequently used in instrument development (DeVellis, 2003; Henson &
Roberts, 2006; Kieffer, 1999; Pett, Lackey, & Sullivan, 2003; Tabachnick & Fidell,
2007). Four common elements of the EFA methodology used in this study for decision
making and reporting are described below.
To determine factors appearing to represent the construct under study, first the
Principal Axis Factor (PAF) extraction method was employed followed by Principal
Component Analysis (PCA) extraction method. These methods are used to compute
confidence intervals and a wide range of goodness of fit indices as well as statistical
significance testing of factor loading and correlation among factors (DeVellis, 2003).
An oblique method of rotation was used with the PAF extraction method to look
for correlation between the factors. Three factor matrices (pattern, structure, and
correlation) resulting from the oblimin rotation were examined. Orthogonal rotation
(Varimax) was conducted as part of the PCA extraction method to confirm the results
when the oblimin rotation displayed factors with close to zero correlation (Reise, Waller,
& Comrey, 2000).
The following tests were run to assess internal consistency: Cronbach’s alpha,
alpha-if-item-deleted, Eigenvalues (>1), scree test, and factor saturation.

39

Summary
Multiple-choice items, developed by a volunteer group of librarians, physicians
and researchers, underwent a validation process utilizing an expert panel of physicians
and librarians. Items meeting a minimum CVR threshold score were included in an
initial form of the instrument tested with resident physicians at the University of New
Mexico. Item analyses and factor analyses were conducted. Information gained through
the process led to the elimination of poorly functioning items and the development of an
item pool which was then pilot tested with a sample of resident physicians. Results from
the pilot test are fully described in Chapter 3.

40

Chapter 3
RESULTS
This chapter presents the results of the procedures utilized in the initial study
conducted with resident physicians at the University of New Mexico of the preliminary
Medical Information Literacy Questionnaire (MILQ). These procedures were used to
identify and eliminate weak or problematic items leading to a reduced set of items for a
final form of the instrument. Finally, these results were examined to see if they provided
any validity evidence of the internal structure of the MILQ. Reported here are the results
from the following procedures:
1. Initial item analyses;
2. Item selection for retention;
3. Evaluation of the final form;
4. Initial evidence for the internal structure of the MILQ.
Data Collection
Data for all four stages were collected from the administration of a questionnaire
consisting of an initial item pool of 69 items and a demographic form. Using a web-based
online survey tool (REDCap), questionnaires were sent out via email January 29, 2014 to
all current resident physicians at the University of New Mexico (n=472). Five were
returned as undeliverable. The total response rate (68 residents; 15%) was low. The
survey closed on February 12, 2014.

41

Demographic Characteristics
Not everyone answered all questions. Consequently there are two samples, herein
referred to as “completers” and “non-completers”. The demographic characteristics
representing each of the samples are reported in table 3.
Table 3.
General demographic profile and academic characteristics of survey participants
Participant Demographic
NonPercent2
Completers
Characteristic
completers
N=22
n1
Ethnicity
White
46
69%
15
Hispanic
8
12%
1
Asian or Pacific Islander
7
10%
4
Black
1
1%
0
Native American/ Alaskan
1
1%
Native
0
Prefer not to answer
4
6%
2
Gender
Female
38
57%
12
Male
29
43%
10
Post-graduate Year (PGY)
PGY 1
20
30%
9
PGY 2
20
30%
5
PGY 3
18
27%
4
PGY 4
4
6%
3
PGY 5
4
6%
1
PGY 6
1
1%
0
Specialty
Pediatrics
14
21%
5
Internal Medicine
8
12%
2
Psychiatry
8
12%
2
Pathology
7
10%
3
Surgery
6
9%
3
Emergency Medicine
5
7%
2
Family & Community Medicine
5
7%
0
Neurological Surgery
3
5%
1
Orthopedics
3
5%
2
Radiology
3
5%
1
Obstetrics & Gynecology
2
3%
0
Anesthesiology
1
1%
1
Dermatology
1
1%
0
Neurology
1
1%
0
1

n ≠ 68 due to missing values from non-response.
Valid percent refers to frequencies excluding missing values.

2

42

Percent

68%
5%
18%
0
0
9%
54%
46%
41%
23%
18%
14%
4%
0
23%
9%
9%
13%
13%
9%
0
5%
9%
5%
0
5%
0
0

Thirty-six participants (“non-completers” 53%) partially completed the survey
answering between six and sixty-eight of the total items with a mean of 19.8 (SD = 13.7).
Ten persons (15%) answered demographic questions only. Fifty-seven percent of the
total respondents were female and the majority of respondents self-reported their
ethnicity as white (67%) which closely matches that of the entire UNM resident
population at 46% and 66% respectively. Overall, participants represented 14 specialties
and post-graduate year one through six.
From the original sample of 68, Twenty-two participants (“completers” 32%) who
completed all questions represented ten medical specialties, were predominantly white
(68%), and closely split between female (54%) and male (46%). No resident from postgraduate year six completed the survey. The results described in this chapter come from
the sample of completers and do not include data from the non-completer sample.
Psychometric Analyses
Initial Item Analyses
Performing item analyses on the scores from the tested items provided the level of
difficulty for each item; how well the item was able to discriminate between participants
with high and low competencies; and the degree to which each item contributed to the
reliability of the overall scores. The statistical indicators for the scores from the initial
pool of sixty-nine items are displayed in Table 4.

43

The internal consistency reliability coefficient, calculated for all 69 items using
Cronbach’s alpha, was good (α = .872) for this instrument with this sample (DeVellis,
2003).
Item difficulty (range .09 to .97), represented as P-value in Table 4, is the
percentage of participants who selected the correct answer option. On a scale of .0 to 1.0,
items with a P-value less than .30 may be considered very difficult or, at the lower end of
the scale, flawed in some way. Items earning a P-value of greater than .8 would be
classified as easy or very easy. In both cases easy and difficult items merit further review.
The item discrimination index for individual items in this instrument is reflected
as corrected item-total correlation coefficients (range = -.02 to .84) and is shown in Table
4. These scores indicate the relationship between an item score and the total test score
and are used to provide information about how well the item discriminates between
individuals with higher and lower levels of knowledge or ability. Positive high scores
indicate individuals with higher total scores selected the correct answer option while
negative or low scores signify these same individuals chose one of the distractors instead
of the correct answer option. Low values indicate poor discrimination, that is, a poor
ability to distinguish individuals with high levels of knowledge from those with low
levels of knowledge. Negative are an indication the item may be flawed and should be
reviewed. The answer may be keyed incorrectly or something in the wording,
presentation, or content may explain the discrepancy. Ebel and Frisbie (Ebel & Frisbie,
1991) suggest items with values > .40 indicate very good discrimination, .30 to .39 are
reasonably good, and .20 to .29 signal marginal discrimination and need revision or
elimination of the item.
44

The value listed in the alpha-if-item-removed column of Table 4 communicates
whether and how much the Cronbach’s alpha would increase if the item in question was
removed from the instrument. It is a useful indicator of the contribution of the item to
overall score reliability. Little variation in this statistic was observed for these items
(range .864 to .880) with a total alpha of .872.
The means, standard deviations (SD), and frequency distributions of each of the
above convey information about the quality of the overall set of items. The mean P-value
of .64 indicates that, on average, 64% of respondents got each item correct. It suggests
that overall the items were neither too easy nor too difficult. The range of difficulties
demonstrates a mix of easy, moderately difficult, and difficult items, all important for
examinee motivation. Too many easy items would not be sufficiently challenging to
maintain interest, though too many difficult items would be demoralizing. A range of .60
to .80 is considered acceptable for multiple choice examinations (Lord, 1952). The
corrected item-total correlation mean (.28) is close to the .30 threshold for good
discrimination, although 33.32% of items fell below that threshold. This indicates
minimally acceptable average levels of item discrimination. The values in the CVR
column characterize the analysis of the overall reviewer ratings for each item (range 0.24
to 1.0). With 91% at or above the recommended threshold (4.2), most have excellent
scores to support evidence of test content validity. These summary values constitute
benchmarks against which the shorter form can be assessed.

45

Table 4
MILQ Initial Item Analyses
Items (69)
CVR
ff

= retained in final
form
ff

Q1
Q2 ff
Q3 ff
Q4
Q5 ff
Q6 ff

0.42
0.57
0.65
0.39
0.67
0.74

Q1 ff
Q2
Q3 ff
Q4 ff
Q5 ff

0.50
0.55
0.67
0.58
0.67

Q1 ff
Q2 ff
Q3 ff
Q4 ff
Q5 ff
Q6
Q7 ff
Q8 ff
Q9 ff
Q10
Q11
Q12 ff
Q13 ff
Q14 ff
Q15 ff
Q16 ff
Q17 ff
Q18 ff
Q19 ff
Q20 ff
Q21
Q22 ff
Q23

0.58
0.58
0.91
0.57
0.58
0.67
0.74
0.50
0.44
0.83
0.50
0.75
0.75
0.65
0.82
0.92
0.48
0.55
0.39
0.33
0.83
0.91
1.00

Q1 ff
Q2
Q3
Q4 ff

0.92
0.42
0.24
0.24

P Value
(Item
Difficulty)
Domain 1
.82
.09
.59
1.00
.68
.77
Domain 2
.59
.77
.59
.41
.73
Domain 3
.73
.55
.64
.59
.64
.82
.91
.96
.91
1.00
.32
.91
.64
.91
.91
.82
.50
.55
.91
.59
.68
.82
.96
Domain 4
.96
.96
.59
.96

Corrected
Item- Total
Correlation

Cronbach’s
alpha if
Item
Deleted

.31
.870
.28
.871
.13
.873
Zero variance
-.21
.878
.39
.869
.17
-.17
.34
.17
.39

.872
.877
.870
.873
.869

.49
.867
.26
.871
.14
.873
.28
.871
.12
.873
-.42
.879
.40
.869
.39
.870
.84
.865
Zero variance
.21
.872
.84
.865
.51
.867
.84
.865
.26
.871
.76
.864
.11
.873
.39
.869
.52
.868
.23
.872
-.24
.878
.55
.867
.75
.868
.75
.75
.19
.39

46

.868
.868
.872
.870

Items (69)
ff

= retained in final
form

Q1
Q2 ff
Q3
Q4
Q5
Q6
Q7
Q8 ff
Q9
Q10 ff
Q11
Q12 ff
Q13 ff
Q14 ff
Q15
Q16
Q17
Q18 ff
Q19
Q20
Q21
Q22
Q23
Q24
Q25
Q26 ff
Q27
Q28
Q29
Q30
Q31
Means and SD’s
Negative
0-.99
1 - 1.99
2-2.99
3-3.99
4-4.99
5-5.99
6-6.99
7-7.99
8-8.99
9-9.99

CVR

P Value
(Item
Difficulty)

Corrected
Item- Total
Correlation

Cronbach’s
alpha if
Item
Deleted

Domain 5
0.76
.86
.75
.865
0.76
.59
.28
.871
0.84
1.00
Zero variance
0.75
.86
.15
.872
0.75
.91
.15
.872
0.67
.68
.45
.868
0.76
.77
.41
.869
0.67
.45
.52
.867
0.58
.27
.28
.871
0.74
.73
.41
.869
0.67
.82
.08
.873
0.75
.50
.38
.869
0.67
.59
.46
.868
0.50
.64
.33
.870
0.83
.91
.84
.865
0.71
.73
.18
.872
0.43
1.00
Zero variance
0.48
.96
.75
.868
0.45
.27
.22
.871
0.36
.32
.21
.872
0.45
.86
.33
.870
0.67
.68
.49
.867
0.92
.50
-.29
.880
0.83
.96
-.22
.874
0.67
.82
.45
.868
0.74
.82
.39
.869
0.83
.91
.67
.867
0.65
.96
.75
.868
0.64
.96
-.22
.874
0.55
.59
-.02
.876
0.89
.73
.15
.872
.60 (1.8)
.64 (2.2)
.28 (2.6)
.872
Frequency Categories (f and %)
8 (11.6%)
2 (2.9%)
5 (7.2%)
11 (15.9%)
2 (2.9%)
2 (2.9%)
10 (14.5%)
4 (5.8%)
2 (2.9%)
11 (15.9%)
8 (11.6%)
7 (10.1%)
8 (11.6%)
14 (20.3%) 7 (10.1%)
4 (5.8%)
14 (20.3%) 11 (15.9%)
1 (1.4%)
13 (18.8%) 13 (18.8%)
7 (10.1%)
8 (11.6%) 14 (20.3%)
4 (5.8%)
6 (8.7%)
11 (15.9%)
47

Item Selection
In an attempt to comprehensively cover all relevant aspects of the IL construct, at
the same time expecting item attrition, the initial pool of items was purposively large
(Nunnally & Bernstein, 1994) . The ultimate goal of this project was to develop a
streamlined instrument containing items of sufficient quality and breadth while retaining
the integrity of the blueprint (described in Chapter 2). The item retention process was
based on both statistical and interpretive considerations. Inspecting data from Table 4
above, each item was reviewed for corrected item-total correlation value to gauge level of
item discrimination and P-values for item difficulty. Additionally, to help with the
decision making, Cronbach’s alpha-if-deleted values and CVR scores, established by the
Phase 2 validity panel, were noted. The guidelines used for evaluating test items for this
process followed general recommendations found in the literature (DeVellis, 2003; Ebel
& Frisbie, 1991; Haladyna, 2004).
Taking a holistic approach to selecting items for the final form, decisions were
based on the following criteria. Threshold scores for consideration consisted of P-values
between .30 to .70 and values >.20 for the corrected item-total correlation. The stem and
answer options for those questions slated for possible elimination were evaluated. A
global assessment of the blueprint was also an important component of the review.
Consideration was given to how item removal might affect the match of the proportion of
items in each domain on the final form to the test blueprint proportions.
Using the above considerations, a total of 32 items (46%) were eliminated from
the original pool of 69 items tested. First to be eliminated were items reporting zero
48

variance (N=4) which indicates that all respondents gave the same answer (P-value =
1.0). Items with negative corrected item-total correlation values (N=8) were scrutinized
next as these indicated possible flaws in the item stem or the answer options. Seven items
in this group were removed due to issues related to an internal flaw while also exhibiting
either a low end CVR score or a P-value >.70 indicating an easy item. One such item
made a statement about use of a genetics table in a presentation then asked whether or not
a citation was needed. The stem was not in the form of a question and the answer options
were actually true/false in nature. An additional fifteen items were removed based on Pvalue < .30 (N=2) or >.82 (N=8) or DI <.22 (N=5). A set of four items was removed
because they duplicated similar items elsewhere in the questionnaire. In this case, two
sets of questions regarding the legal use of an image were asked. The stem and answer
options were the same but the images and the source from which they originated differed
(i.e., government web site and electronic textbook). An additional two items were
dropped because values for both corrected item-total correlation and P-value were low
indicating a difficult question without the benefit of discriminating between groups of test
takers. In order to preserve appropriate percentages in the blueprint, several items were
retained despite less than optimal item characteristics.
After removing 32 flawed items from the pool, the remaining 37 items now
constitute a revised instrument. The scale-dependent item analyses (corrected item-total
correlation and alpha-if-item-deleted) and the reliability coefficient were recalculated as
seen in Table 5.

49

Table 5.
MILQ Final Form Item Analyses and Principal Component Analysis
Items (37)

CVR

P Value
(Item
Difficulty)

Q1
Q2
Q3
Q5
Q6

0.42
0.57
0.65
0.67
0.74

.82
.09
.59
.68
.77

Q1
Q3
Q4
Q5

0.50
0.67
0.58
0.67

.59
.59
.41
.73

Q1
Q2
Q3
Q4
Q5
Q7
Q8
Q9
Q12
Q13
Q14
Q15
Q16
Q17
Q18
Q19
Q20
Q22

0.58
0.58
0.91
0.57
0.58
0.74
0.50
0.44
0.75
0.75
0.65
0.82
0.92
0.48
0.55
0.39
0.33
0.91

.73
.55
.64
.59
.64
.91
.96
.91
.91
.64
.91
.91
.82
.50
.55
.91
.59
.82

Q1
Q4

0.92
0.42

.92
.96

Q2
Q8
Q10
Q12
Q13
Q14
Q18
Q26

0.76
0.67
0.74
0.75
0.67
0.50
0.48
0.74

.59
.45
.73
.50
.59
.64
.96
.82

Means
and SD’s

.63
(1.6)

.69
(2.2)

Corrected
Item-Total
Correlation

Cronbach’s
alpha if Item
Deleted

Domain 1
.24
.29
.07
-.18
.33
Domain 2
.27
.29
.14
.42
Domain 3
.42
.36
.25
.22
.14
.39
.46
.84
.84
.54
.84
.33
.68
.19
.47
.49
.30
.55
Domain 4
.68
.46
Domain 5
.21
.48
.30
.43
.41
.28
.68
.37
.39
(2.2)

50

Component
1

2

3

4

.857
.856
.863
.868
.858

-.018
.037
.091
-.163
-.052

.492
.057
.502
-.001
.604

.117
.065
-.122
-.456
.273

.083
.488
.427
.151
.047

.857
.857
.861
.853

.483
.484
.446
.491

.009
.158
.151
.226

-.155
-.158
-.033
.251

.099
-.074
-.398
-.198

.853
.855
.858
.859
.861
.855
.855
.847
.847
.850
.847
.856
.848
.860
.852
.853
.856
.851

.526
.156
.252
.076
-.094
.700
.823
.833
.833
.283
.833
.655
.606
-.007
.444
.297
.349
.554

.110
-.008
-.272
.115
.436
-.304
-.382
.339
.339
.138
.339
-.106
.253
.224
.096
.787
.241
.095

.147
.221
-.254
.577
.061
.010
.307
.349
.349
.015
.349
-.077
.183
-.420
-.287
-.172
-.197
.355

.169
.577
.676
.116
.046
.216
.087
.152
.152
.663
.152
-.042
.348
.540
.589
-.015
.173
.296

.852
.855

.327
.823

.850
-.382

.176
.307

.123
.087

.859
.852
.856
.853
.853
.857
.852
.855

.164
.018
-.016
.000
.083
.071
.327
-.007

.143
.082
.364
.161
.159
.514
.850
.577

.619
.523
-.007
.714
.804
-.389
.176
.391

-.163
.703
.478
.355
.211
.293
.123
-.110

.858

-

-

-

-

Frequency Categories (f and %)
Items (37)

CVR

P Value
(Item
Difficulty)

Corrected
Item- Total
Correlation

Negative
0-.99
1 - 1.99
2-2.99
3-3.99
4-4.99
5-5.99
6-6.99
7-7.99
8-8.99
9-9.99

1 (2.7%)
2 (5.4%)
4 (10.8%)
10 (27.8%)
7 (18.9%)
8 (21.6%)
1 (2.7%)
4 (10.8%)

1 (2.7%)
7 (18.9%)
5 (13.5%)
9 (24.3%)
5 (13.5%)
7 (18.9%)
3 (8.1%)

1 (2.7%)
1 (2.7%)
3 (8.1%)
9 (23.3%)
7 (18.9%)
7 (18.9%)
3 (8.1%)
1 (2.7%)
2 (5.4%)
3 (8.1%)
-

Cronbach’
s alpha if
Item
Deleted
-

-

-

-

-

Analysis of the initial study data showed internal consistency was good for the
initial 69 items tested (α=0.872) and remained good (α=0.858) when the instrument was
reduced to 37 items. The P-value mean increased slightly from .64 to .69, at 2.2 the SD
remained the same. Once less than optimal items were removed, the corrected item-total
correlation mean improved markedly. Previously .28, the revised mean of .39 denotes
good discrimination for the overall score set which is precisely what was desired. In the
revised instrument, the percentage of items below the CVR value threshold dropped from
8.6% to 4.3% further evidence of test content validity for this set of items.

Initial Evidence of the Internal Structure of the Instrument
Once the pool of items was reduced to maximize both item characteristics and
match to the blueprint (Table 6), principal component analyses were used to examine the
structure of the latest version of the instrument.

51

Table 6.
Test Blueprint for the MILQ Final Form

Content Areas
Determine nature and extent of
information needed
Access needed information effectively
and efficiently
Evaluate information and sources
critically
Uses information effectively to
accomplish a specific purpose
Understands ethical, legal and socioeconomic issues surrounding
information and information
technology
Total

Blueprint
%

No. of Items in
Final Form

Final Form %

21%

5

13%

11%

4

11%

48%

18

49%

3%

2

5%

17%

8

22%

100%

37

100%

Factor analytic techniques, used in instrument development, determines whether
or not the items adequately reflect the structure indicated by the blueprint. The question
to be answered in this section is whether the analysis suggests evidence of validity based
on the internal structure of the revised instrument.
An exploratory factor analysis using principal axis factoring with an oblique
rotation returned no results because the correlation matrix was defined as “not positive
definite” indicating zero or negative eigenvalues. This outcome most likely was due to
the extremely low respondent-to-item ratio (22 respondents for 37 items).
A principal components analysis with oblique rotation was next attempted, which
returned eleven components. Given low inter-component correlations, PCA with varimax
rotation was used. It also produced 11 components with eigenvalues greater than 1. A
52

visual inspection of the scree plot (Figure 1) displayed a break after the fourth
component.
Figure 1
Scree Plot

The analysis was run once more, this time constraining the criteria to four
components. The four-component solution (Table 5) explained a total of 54.8% of the
variance, with Component 1 contributing 19.18%, Component 2 contributing 13.71%,
Component 3 contributing 11.15%, and Component 4 contributing 10.80%. Component
loadings are shown in Table 5.

53

These results indicate evidence to suggest some items represent elements of the IL
construct however the large amount of variance unaccounted for (45%) makes it
impractical to interpret or label the extracted components.
Summary
The purpose of the analyses conducted on scores from the initial test was to explore
psychometric properties of the initial instrument. After computing and examining item
score P-values, the corrected item-total correlation matrix, CVR values, and Cronbach’s
alpha, individual items were reviewed with consideration to preserving the test blueprint
proportions between domains in order to revise the instrument. A final pool of 37 items
was developed. This revised instrument underwent further evaluation utilizing factor
analytic techniques to see if any components could be identified. These analyses confirm
the need for more empirical investigation.

54

Chapter 4
DISCUSSION
The purpose of this research study was to develop, test, and refine an information
literacy measurement tool designed for postgraduate medical education trainees. This
chapter provides an overview of the project with results and outcomes discussed in
relation to the specific aims outlined in Chapter 1 and in context of the research literature.
Also found in this chapter are concluding remarks regarding implications of the study and
recommendations for future research.
The following research aims guided this study:
Aim 1.

Generate an initial pool of information literacy items;

Aim 2.

Establish validity evidence for the draft items;

Aim 3.

Design and pilot test the draft instrument; and

Aim 4.

Evaluate psychometric properties of the tested items.

All Aims of the study were met.
The creation of items for this study was one of the more difficult and time
consuming aspects of this project. Item writing is a special skill requiring knowledge and
practice and one not regularly taught in higher education. Best practice guidelines for
item writing are available (Case & Swanson, 2001; Haladyna et al., 2002; Moss, 2001;
Nitko & Brookhart, 2007; Norcini, Swanson, Grosso, & Webster, 1985) however one
cannot fully appreciate the challenge until writing that first (good) question. Item writing
for professional exams is most often conducted in a group setting over a period of time.
Protected time to concentrate solely on item writing was not available to either this writer
or the volunteers who generously assisted in this effort and, except in one instance, the
item writers never met face to face. This is not to say the items produced for the
55

instrument were not good; they are. One of the strengths of this instrument lies in the
diversity of the item writers. Another is the incorporation of context-dependent sets in
which higher thinking skills are tested. Rather the point to be made here is that a panel
brought together for the sole purpose of generating items would have expedited the
process and might have produced a more cohesive, targeted, and larger pool of items.
This should be a consideration for anyone attempting this type of project.
As recommended, various sources of validity evidence (i.e., subject matter
experts, test format, test administration, and proposed use of the test scores) were
examined (American Educational Research Association et al., 1999). The IL construct as
described in the literature (Association of College and Research Libraries & American
Library Association, 2000) was used as the conceptual framework for developing the test
blueprint. Utilization of subject matter expert judgment in the development of the
blueprint and for reviewing the draft items lends credence to the importance and
appropriateness of the test content and the relationship to the blueprint. Developing a test
blueprint utilizing a small pool of subject matter experts (N=16) from one institution
could affect generalizability. However ACGME accredited programs have standardized
requirements throughout the United States, making it unlikely that these experts’
knowledge and experience differs in important ways from others in the U.S. Likewise,
the “expert” eligibility criteria and selection process used for the item development and
validity panels may be called into question by others. While there is some controversy
surrounding what constitutes an “expert” (Hasson, Keeney, & McKenna, 2000) expert
panels are commonly used to develop, review, and evaluate test specifications and
content (American Educational Research Association et al., 1999; Rubio, Berg-Weger,

56

Tebb, Lee, & Rauch, 2003; Sireci, 1998b; Sumsion, 1998; Wallace et al., 2003). The pool
of physician educators and health sciences librarians was judged sufficient to find the
“competent, diverse, and representative sample” necessary for this project. The role of
these experts in conjunction with the CVR statistical methodology was important to the
development of the initial instrument. Items with acceptable CVR values included in the
instrument reflect SME consensus about the ability of the items to capture the IL
construct.
Although it could be argued the multiple-choice format is not the only, or perhaps
even the most effective, method to measure competence, the proposed instrument is
meant to provide a snapshot of information literacy that can be used as a formative
assessment by health sciences librarians and GME Program Directors. The MILQ is not
intended as a performance-based assessment, which is more costly and time intensive
than the MC format. Instead the MILQ is a competency-based measure that could be
used as a surrogate for actual performance and one ideally to be used in conjunction with
other assessment methods.
Utilizing an online survey tool was expected to provide easy access to the
questionnaire for both study participants and the test creator. Certainly the researcher
found the tool trouble free when sending out the initial survey and subsequent reminders
and for importing response data into SPSS. What is not known is how study participants
accessed the survey (i.e., mobile device or desktop computer) or how the survey behaved
on a personal device such as a table or smart phone. Although REDCap is said to be
compatible with a variety of devices, many items contained images or were text heavy
which may have affected participant willingness to take or complete the questionnaire.

57

Cognitive testing prior to launching the survey did not take place and would have been
prudent.
The sample for the pilot test was made up solely of resident physicians from one
university therefore the findings may not be generalizable to another institution.
However, resident physician experiential learning and competencies are governed by the
ACGME making it unlikely there would be significant differences between UNM
residents and those elsewhere in the United States. Similarly, although the use of
convenience samples can be problematic, the UNM resident physician population shows
evidence of diversity in gender, ethnicity, and prior education thus demonstrating a mixed
population similar to other graduate medical education programs in the United States
(Accreditation Council for Graduate Medical Education, 2013). For 2013-2014, the UNM
Graduate Medical Education Office reported a total of 562 resident physicians and
fellows in 50 programs. Of the total number 29% were educated at the UNM School of
Medicine, 50% graduated from other US medical schools and 21% were foreign
graduates; male (54%), female (46%), Asian (18%), Hispanic (12%), Native American
(2%), African American (2%), and Caucasian (66%).
While the initial test design, deployment and management were completed
successfully, the resulting participant response rate was disappointingly low. While it is
impossible to know specifically why residents did not respond to the survey, several
possible or likely reasons for the minimal response are considered here. First and
foremost, resident physicians are incredibly busy people who have multiple and
competing duties including clinical responsibilities, educational requirements,
supervision of medical students and other residents, as well as research or quality

58

improvement projects, among other obligations. Additionally, multiple times throughout
the year residents are expected to complete surveys related to: satisfaction with multiple
aspects of their residency program, evaluation of program faculty, number and type of
procedures achieved, etc. thus leading to survey fatigue. In January and February, around
the time of the initial test, trainees from thirteen residency programs completed detailed
annual program evaluation surveys and the largest residency program (Internal Medicine)
was launching a pilot program with major revisions to their call schedule. Two or three
faculty members also suggested the fact there was no compensation for study
participation may have had an adverse effect on responses.
Finally, the instrument was unavoidably lengthy. In the overall scheme of
instrument development, the number of items was appropriate. Without a large pool of
items to test, the researcher may not retrieve the data necessary to review the properties
of the instrument and upon which decisions are made. The purpose of instrument
development is to examine which items measure the construct being studied, eliminate
flawed items, and produce the smallest set of items that will measure the construct.
Having said that, the number of items participants were expected to complete for this
study appears to have been too many and may provide an explanation for the number of
“non-completers” in the overall sample. While it was expected demographic
characteristics (Table 2) could be used to compare scores across different groups (e.g.,
postgraduate year) due to the extremely small sample size, no conclusions can or should
be inferred.
Realistically the sample of participants was not large enough to perform factor
analysis on the scores gathered however collecting pilot data from this small number of
59

participants did allow for examination of internal consistency among items. It is
important to note that reducing the number of items by half from the initial test did not
affect the quality of the newly revised instrument. While the CVR data and the test
blueprint display compelling evidence of test content validity for this set of items.
Based on the item analyses conducted on 69 items, it was possible to identify
those items contributing most to the homogeneity of the measurement tool. Inspection of
frequencies, item difficulty, and corrected item-total correlation provided information
needed to consider which items did not enhance reliability and could be eliminated.
Reduced to 37 items, the corrected item-total correlation for the final form was improved
over the initial form of the instrument. This improvement, at .39, denotes good
discrimination for the overall score set, and is well within conventional standards. The
final form exhibits an appropriate level of difficulty (P=.69) has an acceptable alpha
(α=.858) and maps relatively well to the blueprint (Table 6).

Implications for Future Research
Instrument development is an iterative process. Certainly working on this study
has proven this to be true. The analyses performed on scores from the initial study
confirm the need for more empirical investigation. Additional work needs to be
performed before the MILQ can be called ready for use as an assessment tool with
resident physicians. This section discusses next steps to move the project forward toward
that goal.
Conducting distractor analysis is the first activity to undertake. Although the Pvalue and corrected item-total correlation provided useful information neither addressed

60

how distractors (incorrect response options) contribute to item performance. Looking at
all answer options, not just the correct answer option, will contribute to an additional
level of item review. The data from this analysis would allow the researcher to see the
percentage of respondents per option, leading to a better understanding of whether the
distractor is worded clearly, without multiple interpretations, and is plausible. The
analysis also would offer insight into which respondents (high or low knowledge) are
choosing which option choices.
Once the analysis is finalized, distractors may be immediately rewritten or it may
be further indication the item should be removed entirely. This may mean the current 37
item instrument needs to be revised. If this is the case, some items previously removed
may be placed back into the instrument after consideration to CVR value, corrected totalitem correlation, and Cronbach’s alpha. Item analyses will be re-run for the newly
revised instrument and checked against the blueprint.
Although the item to blueprint ratio is appropriate for most of the domains (see
Table 6) there are areas needing improvement. Additional items should be written to
better cover Domain 1which currently has only five items. These five items represent
13% of content but should be 21%.
Because it is difficult to draw any conclusions based on such a small sample of
responses, larger samples are needed to investigate the reliability of the measure. Given
the open nature of the IRB approval, a revised instrument could be tested in summer 2014
with the incoming resident group (N=~125). Hopefully an instrument with a reduced
number of items will be more palatable to respondents thus increasing the response rate.

61

A health sciences librarian has expressed interest in testing the instrument with residents
at her institution. This would entail new IRB approvals but will be explored further.
Conclusion
The purpose of this study was the initial development of an instrument
capable of measuring information literacy competence in the postgraduate medical
setting. The intent was to add to the knowledge base of information literacy and graduate
medical education assessment. The aims of this study have been met. The current version
of the instrument needs testing before being finalized. The steps taken throughout this
project have continued the effort toward more effective measurement in both the library
science and graduate medical education. Physician educators working with postgraduate
trainees provided positive feedback and enthusiastically supported this project making it
imperative work in this area continues.

62

REFERENCES
Accreditation Council for Graduate Medical Education. (2011a). Residency review
committees. Retrieved November 1, 2011, from
http://www.acgme.org/acWebsite/navPages/nav_comRRC.asp
Accreditation Council for Graduate Medical Education. (2011b). Toolbox of assessment
methods. Retrieved March 10, 2011, from
http://www.acgme.org/outcome/assess/toolbox.asp
Accreditation Council for Graduate Medical Education. (2013). Data resource book
academic year 2012-2013. Retrieved March 9, 2014, from
https://www.acgme.org/acgmeweb/Portals/0/PFAssets/PublicationsBooks/20122013_ACGME_DATABOOK_DOCUMENT_Final.pdf
Accreditation Council of Graduate Medical Education. (2009). Program director guide to
the common program requirements. Retrieved March 5, 2009, from
http://www.acgme.org/acwebsite/navpages/nav_commonpr.asp
Alotaibi, G., & Youssef, A. (2013). Development of an assessment tool to measure
students' perceptions of respiratory care education programs: Item generation, item
reduction, and preliminary validation. Journal of Family & Community Medicine,
20(2), 116-122. doi:10.4103/2230-8229.114770; 10.4103/2230-8229.114770
American Educational Research Association, American Psychological Association, &
National Council on Measurement in Education. (1999). Standards for educational
63

and psychological testing. Washington, D.C.: American Educational Research
Association, American Psychological Association, & National Council on
Measurement in Education.
Anderson, P., & Morgan, G. (2008). Developing tests and questionnaires for a national
assessment of educational achievement. Washington DC: The World Bank.
Anderson, M. B., & Kanter, S. L. (2010). Medical education in the united states and
canada, 2010. Academic Medicine : Journal of the Association of American Medical
Colleges, 85(9 Suppl), S2-18. doi:10.1097/ACM.0b013e3181f16f52
Anthropology and Sociology Section Instruction and Information Literacy Committee
Task Force on IL Standards, Association of College and Research Libraries &
American Library Association. (2008). Information literacy standards for
anthropology and sociology students. Retrieved October 10, 2011, from
http://www.ala.org/ala/mgrps/divs/acrl/standards/anthro_soc_standards.cfm
Arp, L. (1987). Model statement of objectives for academic bibliographic instruction:
Draft revision. College & Research Libraries News, (May), 256-261.
Association of American Medical Colleges. (1998). Contemporary issues in medicine:
Medical informatics and population health. Retrieved February 9, 2009, from
https://members.aamc.org/eweb/DynamicPage.aspx?Action=Add&ObjectKeyFrom=
1A83491A-9853-4C87-86A4F7D95601C2E2&WebCode=PubDetailAdd&DoNotSave=yes&ParentObject=Centr
alizedOrderEntry&ParentDataObject=Invoice%20Detail&ivd_formkey=6920279264

63d7-4ba2-bf4e-a0da41270555&ivd_prc_prd_key=4F099E49-F328-4EEC-BB7BAE073EA04F6B
Association of American Medical Colleges. (2010). 2010 CQ medical school graduation
questionnaire: All schools summary report. Retrieved January 18, 2011, from
https://www.aamc.org/data/gq/allschoolsreports/
Association of American Medical Colleges. (2012). Curriculum reports. Retrieved March
5, 2012, from
https://www.aamc.org/initiatives/medaps/curriculumreports/265940/curriculumdetail
sreports.html
Association of American Medical Colleges. (2013). Medical school graduation
questionnaire: 2013 all schools summary report. Retrieved March 8, 2014, from
https://www.aamc.org/download/350998/data/2013gqallschoolssummaryreport.pdf
Association of College and Research Libraries, & American Library Association. (2000).
The information literacy competency standards for higher education. Retrieved
November 15, 2008, from
http://www.ala.org/ala/mgrps/divs/acrl/standards/informationliteracycompetency.cf
m
Atton, C. (1994). Using critical thinking as a basis for library user education. Journal of
Academic Librarianship, 20(5/6), 310-313.

65

Aydelott, K. (2007). Using the ACRL information literacy competency standards for
science and engineering/technology to develop a modular critical-thinking-based
information literacy tutorial. Science & Technology Libraries, 27(4), 19-42.
doi:10.1300/J122v27n04̱03
Azer, S. A. (2011). Introducing a problem-based learning program: 12 tips for success.
Medical Teacher, 33(10), 808-813. doi:10.3109/0142159X.2011.558137
Baker, S. S., & Boruff-Jones, P. D. (2009). Information literacy. Radiologic Technology,
80(4), 374-376.
Barbour, W., Gavin, C., & Canfield, J. (2004). Integrating information literacy into the
academic curriculum. Boulder, CO: EDUCAUSE Center for Applied Research.
Retrieved from www.educause.edu/ecar
Baum, K. D., & Axtell, S. (2005). Trends in North American medical education. The
Keio Journal of Medicine, 54(1), 22-28.
The Bay Area Community Colleges information competency assessment project. (2004).
Retrieved September 22, 2010, from http://www.topsy.org/ICAP/ICAProject.html
Beile, P. (2005). Development and validation of the information literacy assessment scale
for education (ILAS-ED) ERIC.
Beile, P. (2008). Beile test of information literacy for education. Retrieved November 2,
2011, from

66

http://ilassessments.pbworks.com/w/page/7760872/Beile%20Test%20of%20Informa
tion%20Literacy%20for%20Education
Berk, R. A. (1986). A consumer's guide to setting performance standards on criterionreferenced tests. Review of Educational Research, 56(1), 137-172.
Bhola, H. S. (1994). A sourcebook for literacy work: Perspectives from the grassroots.
London: UNESCO.
Bloom, B., Englehart, M., Furst, E., Hill, W., & Krathwohl, D. (1956). Taxonomy of
educational objectives: The classification of educational goals. handbook I:
Cognitive domain.. New York: Longmans.
Brown, C., & Krumholz, L. R. (March 2002). Integrating information literacy into the
science curriculum. College & Research Libraries, 63(2), 111-123.
Bruce, C. S. (1997). The seven faces of information literacy. Adelaide, Australia: Auslib
Press.
Bruce, B. (1998). New literacies. Journal of Adolescent & Adult Literacy, 42(1), 46-49.
Burrows, S., Moore, K., Arriaga, J., Paulaitis, G., & Lemkau, H. L.,Jr. (2003).
Developing an "evidence-based medicine and use of the biomedical literature"
component as a longitudinal theme of an outcomes-based medical school
curriculum: Year 1. Journal of the Medical Library Association : JMLA, 91(1), 3441.

67

Cameron, L., Wise, S. L., & Lottridge, S. M. (2007). The development and validation of
the information literacy test. College & Research Libraries, 68(3), 229-236.
Candy, P. (2002). Lifelong learning and information literacy. (White paper). Prague:
UNESCO. Retrieved from
http://www.nclis.gov/libinter/infolitconf&meet/papers/candy-paper.pdf
Carraccio, C., Wolfsthal, S. D., Englander, R., Ferentz, K., & Martin, C. (2002). Shifting
paradigms: From flexner to competencies. Academic Medicine : Journal of the
Association of American Medical Colleges, 77(5), 361-367.
Case, S. M., & Swanson, D. B. (2001). Constructing written test questions for the basic
and clinical sciences. Philadelphia, PA: National Board of Medical Examiners.
Cheek, J., & Doskatsch, I. (1998). Information literacy: A resource for nurses as lifelong
learners. Nurse Education Today, 18(3), 243-250.
Citrome, L., & Ketter, T. A. (2009). Teaching the philosophy and tools of evidence-based
medicine: Misunderstandings and solutions. International Journal of Clinical
Practice, 63(3), 353-359. doi:10.1111/j.1742-1241.2009.02014.x
Cobban, S. J., & Seale, L. N. (2003). A collaborative approach for improving information
literacy skills of dental hygiene students. International Journal of Dental Hygiene,
1(1), 49-56. doi:10.1034/j.1601-5037.2003.00005.x

68

Cobus, L. (2008). Integrating information literacy into the education of public health
professionals: Roles for librarians and the library. Journal of the Medical Library
Association : JMLA, 96(1), 28-33. doi:10.3163/1536-5050.96.1.28
Committee on Performance Levels for Adult Literacy. (2005). In Hauser R. M., Edley C.
F. J., Koeniz J. A., Elliott S. W. and National Research Council (Eds.), Measuring
literacy: Performance levels for adults. Washington, DC: National Academy of
Sciences.
Cooney, M. (2005). Business information literacy instruction: A survey and progress
report. Journal of Business & Finance Librarianship, 11(1), 3-25.
doi:10.1300/J109v11n01-02
Courey, T., Benson-Soros, J., Deemer, K., & Zeller, R. A. (2006). The missing link:
Information literacy and evidence-based practice as a new challenge for nurse
educators. Nursing Education Perspectives, 27(6), 320-323.
Das, K., Malick, S., & Khan, K. S. (2008). Tips for teaching evidence-based medicine in
a clinical setting: Lessons from adult learning theory. part one. Journal of the Royal
Society of Medicine, 101(10), 493-500. doi:10.1258/jrsm.2008.080712
DeVellis, R. F. (2003). Scale development: Theory and application (2nd ed.). Thousand
Oaks, CA: Sage Publications.
Dillman, D. A. (2007). Mail and internet surveys : The tailored design method. Hoboken
N.J.: Wiley.

69

Dorner, J. L., Taylor, S. E., & Hodson-Carlton, K. (2001). Faculty-librarian collaboration
for nursing information literacy: A tiered approach. RSR: Reference Services Review,
29(2), 132-140.
Ebel, R. L., & Frisbie, D. A. (1991).
Essentials of educational measurement. (5th ed.). Engelwood Cliffs, NJ: Prentice
Hall.
Educational Testing Service. (2006). 2006 ICT literacy assessment preliminary findings.
Retrieved August 5, 2011, from
http://www.ets.org/Media/Products/ICT_Literacy/pdf/2006_Preliminary_Findings.p
df
Eldredge, J. D., Morley, S. K., Hendrix, I. C., Carr, R. D., & Bengtson, J. (2012). Library
and informatics skills competencies statements from major health professional
associations. Medical Reference Services Quarterly, 31(1), 34-44.
doi:10.1080/02763869.2012.641839
Fiegen, A. M., Cherry, B., & Watson, K. (2002). Reflections on collaboration: Learning
outcomes and information literacy assessment in the business curriculum. Reference
Services Review, 30(4), 307-318.
Flexner, A. (1910). Medical education in the United States and Canada. Washington,
DC: Science and Health Publications, Inc.

70

Flood, L. S., Gasiewicz, N., & Delpier, T. (2010). Integrating information literacy across
a BSN curriculum. The Journal of Nursing Education, 49(2), 101-104.
doi:10.3928/01484834-20091023-01; 10.3928/01484834-20091023-01
Ford, J. K., & Wroten, S. P. (1982). A content validity ratio approach to determining
training needs. 90th Annual Convention of the American Psychological Association,
Washington, DC. , ED 223 937
Ford, J. K., & Wroten, S. P. (1984). Introducing new methods for conducting training
evaluation and for linking training evaluation to program redesign. Personnel
Psychology, 37(4), 651-665. doi:10.1111/j.1744-6570.1984.tb00531.x
Ford, P. J., Foxlee, N., & Green, W. (2009). Developing information literacy with first
year oral health students. European Journal of Dental Education : Official Journal
of the Association for Dental Education in Europe, 13(1), 46-51. doi:10.1111/j.16000579.2008.00536.x
Fox, L. M., Richter, J. M., & White, N. (1989). Pathways to information literacy. The
Journal of Nursing Education, 28(9), 422-425.
Fox, L. M., Richter, J. M., & White, N. E. (1996). A multidimensional evaluation of a
nursing information-literacy program. Bulletin of the Medical Library Association,
84(2), 182-190.
Francis, B. W., & Fisher, C. C. (1995). Multilevel library instruction for emerging
nursing roles. Bulletin of the Medical Library Association, 83(4), 492-498.

71

Freire, P. (1983). The importance of the act of reading. Journal of Education, 165(1), 511.
Fritsche, L., Greenhalgh, T., Falck-Ytter, Y., Neumayer, H. H., & Kunz, R. (2002). Do
short courses in evidence based medicine improve knowledge and skills? validation
of berlin questionnaire and before and after study of courses in evidence based
medicine. BMJ (Clinical Research Ed.), 325(7376), 1338-1341.
Garritano, J. R. (2010). Trends in chemical information literacy and collection
development. Science & Technology Libraries, 29(3), 235-257.
Glaser, R. (1963). Instructional technology and the measurement of learing outcomes:
Some questions. American Psychologist, 18(8), 519-521. doi:10.1037/h0049294
Goroll, A. H., Sirio, C., Duffy, F. D., LeBlond, R. F., Alguire, P., Blackwell, T. A., . . .
Residency Review Committee for Internal Medicine. (2004). A new model for
accreditation of residency programs in internal medicine. Annals of Internal
Medicine, 140(11), 902-909.
Gotterer, G. S., O'Day, D., & Miller, B. M. (2010). The emphasis program: A scholarly
concentrations program at Vanderbilt University School of Medicine. Academic
Medicine : Journal of the Association of American Medical Colleges, 85(11), 17171724. doi:10.1097/ACM.0b013e3181e7771b
Grafstein, A. (2002). A discipline-based approach to information literacy. Journal of
Academic Librarianship, 28(4), 197-204.

72

Grafstein, A. (2007). Information literacy and technology: An examination of some
issues. Portal: Libraries and the Academy, 7(1), 51-64.
Grassion, E. S., & Kaplowitz, J. R. (2011). Information literacy instruction: Theory and
practice. New York: Neal-Schuman.
Green, E. P., Borkan, J. M., Pross, S. H., Adler, S. R., Nothnagle, M., Parsonnet, J., &
Gruppuso, P. A. (2010). Encouraging scholarship: Medical school programs to
promote student inquiry beyond the traditional medical curriculum. Academic
Medicine : Journal of the Association of American Medical Colleges, 85(3), 409418. doi:10.1097/ACM.0b013e3181cd3e00
Gross, M. (2005). The impact of low-level skills on information-seeking behavior
implications of competency theory for research and practice. Reference & User
Services Quarterly, 45(2), 155-162.
Haladyna, T. M. (2004). Developing and validating multiple-choice test items (3rd ed.).
Mahwah, N.J.: Lawrence Erlbaum Associates.
Haladyna, T. M., Downing, S. M., & Rodriguez, M. C. (2002). A review of multiplechoice item-writing guidelines for classroom assessment. Applied Measurement in
Education, 15(3), 309-334.
Harris, P. A., Taylor, R., Thielke, R., Payne, J., Gonzalez, N., & Conde, J. G. (2009).
Research electronic data capture (REDCap)—A metadata-driven methodology and
workflow process for providing translational research informatics support. Journal

73

of Biomedical Informatics, 42(2), 377-381.
doi:http://dx.doi.org/10.1016/j.jbi.2008.08.010
Hasson, F., Keeney, S., & McKenna, H. (2000). Research guidelines for the delphi
survey technique. Journal of Advanced Nursing, 32(4), 1008-1015.
Henson, R. K., & Roberts, J. K. (2006). Use of exploratory factor analysis in published
research: Common errors and some comment on improved practice. Educational and
Psychological Measurement, 66(3), 393-416. doi:10.1177/0013164405282485
Hersh, W. (2009). A stimulus to define informatics and health information technology.
BMC Medical Informatics and Decision Making, 9, 24. doi:10.1186/1472-6947-9-24
Holmboe, E. S., Ward, D. S., Reznick, R. K., Katsufrakis, P. J., Leslie, K. M., Patel, V.
L., . . . Nelson, E. A. (2011). Faculty development in assessment: The missing link in
competency-based medical education. Academic Medicine : Journal of the
Association of American Medical Colleges, 86(4), 460-467.
doi:10.1097/ACM.0b013e31820cb2a7
Jacobs, S. K., Rosenfeld, P., & Haber, J. (2003). Information literacy as the foundation
for evidence-based practice in graduate nursing education: A curriculum-integrated
approach. Journal of Professional Nursing : Official Journal of the American
Association of Colleges of Nursing, 19(5), 320-328.

74

Joyner, B. D. (2004). An historical review of graduate medical education and a protocol
of Accreditation Council for Graduate Medical Education compliance. The Journal
of Urology, 172(1), 34-39. doi:10.1097/01.ju.0000121804.51403.ef
Kaplan, R. B., & Whelan, J. S. (2002). Buoyed by a rising tide: Information literacy sails
into the curriculum on the currents of evidence-based medicine and professional
competency objectives. Journal of Library Administration, 36(1), 219-235.
Kaplowitz, M. D., Hadlock, T. D., & Levine, R. (2004). A comparison of web and mail
survey response rates. Public Opinion Quarterly, 68(1), 94-101.
Kasowitz-Scheer, A., & Pasqualoni, M. (2002). Information literacy instruction in higher
education: Trends and issues. (ERIC Publications No. EDO-IR-2002-01).
Washington, DC: Office of Educational Research and Improvement.
Katz, I. R. (2007). Testing information literacy in digital environments: ETS's iSkills
assessment. Information Technology and Libraries, 26(3), 3-12.
Kerfoot, K. M. (2002). Out with computer literacy, in with information literacy: The
nurse manager's challenge for the 21st century. Seminars for Nurse Managers, 10(2),
114-116.
Kieffer, K. M. (1999). An introductory primer on the appropriate use of exploratory and
confirmatory factor analysis. Research in the Schools, 6(2), 75-92.

75

Kies, S., & Shultz, M. (2010). Proposed changes to the United States Medical Licensing
Examination: Impact on curricula and libraries. Journal of the Medical Library
Association : JMLA, 98(1), 12-16. doi:10.3163/1536-5050.98.1.007
Kingsley, K., Galbraith, G. M., Herring, M., Stowers, E., Stewart, T., & Kingsley, K. V.
(2011). Why not just google it? an assessment of information literacy skills in a
biomedical science curriculum. BMC Medical Education, 11, 17. doi:10.1186/14726920-11-17
Kingsley, K. V., & Kingsley, K. (2009). A case study for teaching information literacy
skills. BMC Medical Education, 9, 7. doi:10.1186/1472-6920-9-7
Kipnis, D. G., & Frisby, A. J. (2006). Information literacy and library attitudes of
occupational therapy students. Medical Reference Services Quarterly, 25(4), 11-20.
doi:10.1300/J115v25n04_02
Kirsch, I. (2001). The international adult literacy survey (IALS): Understanding what
was measured. (Research No. RR-01-25). Princeton, N.J.: Educational Testing
Service. . (ETS RR-01-25)
Kittleson, M. J. (1997). Determining effective follow-up of e-mail surveys. American
Journal of Health Behavior, 21(3), 193-196.
Klem, M. L., & Weiss, P. M. (2005). Evidence-based resources and the role of librarians
in developing evidence-based practice curricula. Journal of Professional Nursing :

76

Official Journal of the American Association of Colleges of Nursing, 21(6), 380-387.
doi:10.1016/j.profnurs.2005.10.004
Kline, R. B. (2009). Becoming a behavioral science researcher: A guide to producing
research that matters. New York: Guilford Press.
Krause, N. D., Roulette, G. D., Papp, K. K., & Kaelber, D. (2006). Assessing medical
informatics confidence among 1st and 2nd year medical students. AMIA ...Annual
Symposium Proceedings / AMIA Symposium.AMIA Symposium, , 989.
Lawshe, C. H. (1975). A quantitative approach to content validity. Personnel Psychology,
28(4), 563-575. doi:10.1111/j.1744-6570.1975.tb01393.x
Leach, D. C. (2002). Building and assessing competence: The potential for evidencebased graduate medical education. Quality Management in Health Care, 11(1), 3944.
Li, S., & Sireci, S. G. (2005). Evaluating the fit between test content, instruction, and
curriculum frameworks: A review of methods for evaluating test alignment. ( No.
Center for Educational Assessment MCAS Validity Report No. 9 (CEA-558)).
Amherst, MA: University of Massachusetts.
Lih-Juan ChanLin, & Chwen-Chwen Chang, B. (2003). Web-based library instruction for
promoting information skills. Journal of Instructional Psychology, 30(4), 265-275.

77

Long, D. M. (2000). Competency-based residency training: The next advance in graduate
medical education. Academic Medicine : Journal of the Association of American
Medical Colleges, 75(12), 1178-1183.
Lord, F. M. (1952). The relationship of the reliability of multiple-choice test to the
distribution of item difficulties. Psychometrika, 18, 181.
Lurie, S. J., Mooney, C. J., & Lyness, J. M. (2009). Measurement of the general
competencies of the Accreditation Council for Graduate Medical Education: A
systematic review. Academic Medicine : Journal of the Association of American
Medical Colleges, 84(3), 301-309. doi:10.1097/ACM.0b013e3181971f08
Mackey, M. (2002). Literacies across the media: Playing the text. New York: Routledge
Falmer.
Maggio, L. A., & Posley, K. A. (2011). Training the trainers: Teaching clinician
educators to provide information literacy skills feedback. Journal of the Medical
Library Association : JMLA, 99(3), 258-261. doi:10.3163/1536-5050.99.3.014
Manuel, K. (2004). Generic and discipline-specific information literacy competencies:
The case of the sciences. Science & Technology Libraries, 24(3), 279-308.
Marple, B. F. (2007). Competency-based resident education. Otolaryngologic Clinics of
North America, 40(6), 1215-25, vi-vii. doi:10.1016/j.otc.2007.07.003

78

Maughan, P. (2001). Assessing information literacy among undergraduates: A discussion
of the literature and the University of California-Berkeley assessment experience.
College and Research Libraries, 62(1), 71-85.
McCarthy, M., & Pusateri, T. P. (2006). Teaching students to use electronic databases. In
S. F. Davis (Ed.), Handbook of the teaching of psychology. (pp. 107-111). Malden:
Blackwell Publishing. doi:10.1002/9780470754924.ch18
McGowan, J. J., Passiment, M., & Hoffman, H. M. (2007). Educating medical students as
competent users of health information technologies: The MSOP data. Studies in
Health Technology and Informatics, 129(Pt 2), 1414-1418.
Meyers, F. J., Weinberger, S. E., Fitzgibbons, J. P., Glassroth, J., Duffy, F. D., Clayton,
C. P., & Alliance for Academic Internal Medicine Education Redesign Task Force.
(2007). Redesigning residency training in internal medicine: The consensus report of
the alliance for academic internal medicine education redesign task force. Academic
Medicine : Journal of the Association of American Medical Colleges, 82(12), 12111219. doi:10.1097/ACM.0b013e318159d010
Middle States Commission on Higher Education. (2006). Characteristics of excellence in
higher education: Eligibility requirements and standards for accreditation (12th
ed.). Philadelphia, PA:
Moch, S. D., Cronje, R. J., & Branson, J. (2010). Part I. Undergraduate nursing evidencebased practice education: Envisioning the role of students. Journal of Professional
Nursing, 26(1), 5-13. doi:10.1016/j.profnurs.2009.01.015
79

Morley, S. K. (2009). Information literacy in medicine: A card sort activity (UNM HRRC
09-100)
Morris, D. (2005). E-learning in the common learning curriculum for health and social
care professionals: Information literacy and the library. Health Information and
Libraries Journal, 22 Suppl 2, 74-80. doi:10.1111/j.1470-3327.2005.00613.x
Morse, J. M., & Field, P. A. (1995). Qualitative research methods for health
professionals (2nd ed.). Thousand Oaks, CA: Sage Publications.
Moskowitz, E. J., & Nash, D. B. (2007). Accreditation council for graduate medical
education competencies: Practice-based learning and systems-based practice.
American Journal of Medical Quality : The Official Journal of the American College
of Medical Quality, 22(5), 351-382. doi:10.1177/1062860607305381
Moss, E. (2001). Multiple choice questions: Their value as an assessment tool. Current
Opinion in Anaesthesiology, 14(6), 661-666.
Nail-Chiwetalu, B., & Ratner, N. B. (2006). Information literacy for speech-language
pathologists: A key to evidence-based practice. Language, Speech, and Hearing
Services in Schools, 37(3), 157-167. doi:10.1044/0161-1461(2006/018)
Neely, T. Y. (2006). Information literacy assessment: Standards-based tools and
assignments. Chicago, IL: American Library Association.
Nitko, A. J., & Brookhart, S. M. (2007). Educational assessment of students (5th ed.).
Upper Saddle, NJ: Pearson Merrill Prentice Hall.
80

Norcini, J. J., Swanson, D. B., Grosso, L. J., & Webster, G. D. (1985). Reliability,
validity and efficiency of multiple choice question and patient management problem
item formats in assessment of clinical competence. Medical Education, 19(3), 238247.
Nunnally, J. C., & Bernstein, I. H. (1994). Psychometric theory (3rd ed.). New York:
McGraw-Hill.
Ogrinc, G., Headrick, L. A., Mutha, S., Coleman, M. T., O'Donnell, J., & Miles, P. V.
(2003). A framework for teaching medical students and residents about practicebased learning and improvement, synthesized from a literature review. Academic
Medicine : Journal of the Association of American Medical Colleges, 78(7), 748756.
Owusu-Ansah, E. (2003). Information literacy and the academic library: A critical look at
a concept and the controversies surrounding it. Journal of Academic Librarianship,
29(4), 219-230.
Owusu-Ansah, E. (2004). Information literacy and higher education: Placing the
academic library in the center of a comprehensive solution. Journal of Academic
Librarianship, 30(1), 3-16.
Parmelee, D. X., & Michaelsen, L. K. (2010). Twelve tips for doing effective team-based
learning (TBL). Medical Teacher, 32(2), 118-122. doi:10.3109/01421590903548562

81

Peterson-Clark, G., Aslani, P., & Williams, K. A. (2010). Pharmacists' online information
literacy: An assessment of their use of internet-based medicines information. Health
Information and Libraries Journal, 27(3), 208-216. doi:10.1111/j.14711842.2010.00891.x
Pett, M. A., Lackey, N. R., & Sullivan, J. J. (2003). Making sense of factor analysis: The
use of factor analysis for instrument development in health care research. Thousand
Oaks, CA: Sage Publications.
Porter, J. A., Wolbach, K. C., Purzycki, C. B., Bowman, L. A., Agbada, E., & Mostrom,
A. M. (2010). Integration of information and scientific literacy: Promoting literacy in
undergraduates. CBE Life Sciences Education, 9(4), 536-542. doi:10.1187/cbe.1001-0006
Porter, J. R. (2005). Information literacy in biology education: An example from an
advanced cell biology course. Cell Biology Education, 4(4), 335-343.
doi:10.1187/cbe.04-12-0060
Powell, C. A., & Case-Smith, J. (2010). Information literacy skills of occupational
therapy graduates: Promoting evidence-based practice in the MOT curriculum.
Medical Reference Services Quarterly, 29(4), 363-380.
doi:10.1080/02763869.2010.518923
Pravikoff, D. S. (2006). Mission critical: A culture of evidence-based practice and
information literacy. Nursing Outlook, 54(4), 254-255.

82

Pray, W. S., & Popovich, N. G. (1985). The development of a standardized competency
examination for doctor of pharmacy students. American Journal of Pharmaceutical
Education, 49, 1-9.
Presidential Committee on Information Literacy, & American Library Association.
(1989). Final report. Chicago, IL: American Library Association.
Psychology Information Literacy Working Group, Education and Behavioral Sciences
Section & Association of College and Research Libraries. (2010). Psychology
information literacy standards. Retrieved March 20, 2012, from
http://www.ala.org/acrl/standards/psych_info_lit
Rader, H. B. (2002). Information literacy 1973-2002: A selected literature review.
Library Trends, 51(2), 242-259.
Ramos, K. D., Schafer, S., & Tracz, S. M. (2003). Validation of the Fresno test of
competence in evidence based medicine. BMJ: British Medical Journal, 326(7384),
319.
Reise, S. P., Waller, N. G., & Comrey, A. L. (2000). Factor analysis and scale revision.
Psychological Assessment, 12(3), 287-297. doi:10.1037/1040-3590.12.3.287
Ritter, L. A., & Sue, V. M. (2007). Conducting the survey. New Directions for
Evaluation, (115), 47-50. doi:10.1002/ev.235
Roldan, M., & Yuhfen Diana Wu. (2004). Building context-based library instruction.
Journal of Education for Business, 79(6), 323-327.
83

Rosenfeld, P., Salazar-Riera, N., & Vieira, D. (2002). Piloting an information literacy
program for staff nurses: Lessons learned. Computers, Informatics, Nursing : CIN,
20(6), 236-41; quiz 242-3.
Ross, J. (2010). Information literacy for evidence-based practice in perianesthesia nurses:
Readiness for evidence-based practice. Journal of Perianesthesia Nursing : Official
Journal of the American Society of PeriAnesthesia Nurses / American Society of
PeriAnesthesia Nurses, 25(2), 64-70. doi:10.1016/j.jopan.2010.01.007
Rothman, R., Slattery, J. B., Vranek, J. L., & Resnick, L. B. (2002). Benchmarking and
alignment of standards and testing. ( No. CSE Technical Report 566). Los Angeles,
CA: University of California.
Rousseau, A., Saucier, D., & Cote, L. (2007). Introduction to core competencies in
residency: A description of an intensive, integrated, multispecialty teaching program.
Academic Medicine : Journal of the Association of American Medical Colleges,
82(6), 563-568. doi:10.1097/ACM.0b013e3180555b29
Rubio, D. M., Berg-Weger, M., Tebb, S. S., Lee, E. S., & Rauch, S. (2003). Objectifying
content validity: Conducting a content validity study in social work research. Social
Work Research, 27(2), 94-104.
Rugg, G., & McGeorge, P. (2005). The sorting techniques: A tutorial paper on card sorts,
picture sorts and item sorts. Expert Systems: International Journal of Knowledge
Engineering and Neural Networks, 22(3), 94-107. doi:10.1111/j.14680394.2005.00300.x
84

Rutledge, D. P., & Maehler, A. (2003). An assessment of library education contributions
to business student learning: A case study. Journal of Business & Finance
Librarianship, 9(1), 3-19.
Sackett, D. L. (2000). Evidence-based medicine : How to practice and teach EBM (2nd
ed.). Edinburgh ; New York: Churchill Livingstone.
Schmidt, H. G., Rotgans, J. I., & Yew, E. H. (2011). The process of problem-based
learning: What works and why. Medical Education, 45(8), 792-806.
doi:10.1111/j.1365-2923.2011.04035.x; 10.1111/j.1365-2923.2011.04035.x
Scott, C. S., Shaad, D. C., Mandel, L. S., Brock, D. M., & Kim, S. (2000). Information
and informatics literacy: Skill, timing, and estimates of competence. Teaching and
Learning in Medicine, 12(2), 85-90. doi:10.1207/S15328015TLM1202_5
Searle, N. S., Haidet, P., Kelly, P. A., Schneider, V. F., Seidel, C. L., & Richards, B. F.
(2003). Team learning in medical education: Initial experiences at ten institutions.
Academic Medicine : Journal of the Association of American Medical Colleges,
78(10 Suppl), S55-8.
Shanahan, M. C. (2006). Information literacy skills of undergraduate medical radiation
students. Radiography, 13, 187-196.
Shanbhag, S. (2006). Alternative methods of knowledge production: A step forward in
information literacy as a liberal art. Library Philosophy and Practice, 8(2) Retrieved
from http://webpages.uidaho.edu/~mbolin/shanbhag.htm

85

Shorten, A., Wallace, M. C., & Crookes, P. A. (2001). Developing information literacy:
A key to evidence-based nursing. International Nursing Review, 48(2), 86-92.
doi:10.1046/j.1466-7657.2001.00045.x
Sireci, S. G. (1998a). Gathering and analyzing content validity data. Eduational
Assessment, 54(4), 299-321.
Sireci, S. G. (1998b). The construct of content validity. Social Indicators Research, 45(13), 83-117.
Sjoberg, L. M., & Ahlfeldt, S. L. (2010). Bridging the gap: Integrating information
literacy into communication courses. Communication Teacher, 24(3), 131-135.
doi:10.1080/17404622.2010.489193
Spang, L., Marks, E., & Adams, N. (1998). Health sciences information tools 2000: A
cooperative health sciences library/public school information literacy program for
medical assistant students. Bulletin of the Medical Library Association, 86(4), 534540.
Studstill, R., & Cabrera, P. (2010). Online primary sources in religious studies: Active
learning exercises for information literacy instruction. Journal of Religious &
Theological Information, 9(3), 84-112. doi:10.1080/10477845.2010.527252
Sumsion, T. (1998). The delphi technique: An adaptive research tool. British Journal of
Occupational Therapy, 61(4), 153-156.

86

Swiatek-Kelley, J. (2010). Physician information seeking behaviors: Are physicians
successful searchers? ProQuest Information & Learning). Dissertation Abstracts
International Section A: Humanities and Social Sciences, 71(4-). (2010-99190-076).
Tabachnick, B. G., & Fidell, L. S. (2007). Using multivariate statistics (5th ed.). Boston,
MA: Allyn & Bacon/Pearson Education.
Tanner, A., Pierce, S., & Pravikoff, D. (2004). Readiness for evidence-based practice:
Information literacy needs of nurses in the United States. Studies in Health
Technology and Informatics, 107(Pt 2), 936-940.
Task Force on Information Literacy for Science and Engineering Technology,
Association of College and Research Libraries & American Library Association.
(2008). Information literacy standards for science and engineering technology.
Retrieved October 10, 2011, from
http://www.ala.org/ala/mgrps/divs/acrl/standards/infolitscitech.cfm
Thompson, G. B. (2002). Information literacy accreditation mandates: What they mean
for faculty and librarians. Library Trends, 51(2), 218-241.
Thompson, N., Lewis, S., Brennan, P., & Robinson, J. (2010). Information literacy: Are
final-year medical radiation science students on the pathway to success? Journal of
Allied Health, 39(3), e83-9.
Thorndike, R. M. (1997). Measurement and evaluation in psychology and education (6th
ed.). Saddle River, NJ: Prentice-Hall, Inc.

87

USMLE. (2013). Announcement. Retrieved January 29, 2014, from
http://www.usmle.org/announcements/?ContentId=121
van Dijk, N., Hooft, L., & Wieringa-de Waard, M. (2010). What are the barriers to
residents' practicing evidence-based medicine? A systematic review. Academic
Medicine : Journal of the Association of American Medical Colleges, 85(7), 11631170. doi:10.1097/ACM.0b013e3181d4152f
Verhey, M. P. (1999). Information literacy in an undergraduate nursing curriculum:
Development, implementation, and evaluation. The Journal of Nursing Education,
38(6), 252-259.
Virkus, S. (2003). Information literacy in Europe: A literature review. Information
Research, 8(4, paper no. 159) Retrieved from http://informationr.net/ir/84/paper159.html
Wallace, L. S., Blake, G. H., Parham, J. S., & Baldridge, R. E. (2003). Development and
content validation of family practice residency recruitment questionnaires. Family
Medicine, 35(7), 496-498.
Walsh, A. (2009). Information literacy assessment: Where do we start? Journal of
Librarianship and Information Science, 41(1), 19-28.
Walton, M., & Archen, A. (2004). The web and information literacy: Scaffolding the use
of web sources in a project-based curriculum. British Journal of Educational
Technology, 35(2), 173-186. doi:10.1111/j.0007-1013.2004.00379.x

88

Ward, H., & Hockey, J. (2007). Engaging the learner: Embedding information literacy
skills into a biotechnology degree. Biochemistry and Molecular Biology Education :
A Bimonthly Publication of the International Union of Biochemistry and Molecular
Biology, 35(5), 374-380. doi:10.1002/bmb.79; 10.1002/bmb.79
Webster's third new international dictionary of the english language (1993). (unabridged
ed.). Springfield, MA: Merriam-Webster, Inc.
Wise, S. L., Cameron, L., Yang, S., & Davis, S. L. (2010). The information literacy test
(ILT): Test manual. Harrisonburg, VA: The Center for Assessment & Research
Studies.
Wood, E., Kronick, J. B., & Association of Medical School Pediatric Department Chairs,
Inc. (2008). A pediatric residency research curriculum. The Journal of Pediatrics,
153(2), 153-4, 154.e1-4. doi:10.1016/j.jpeds.2008.02.026
Zurkowsky, P. G. (1974). The information services environment: Relationships and
priorities. ( No. ED 100391). Washington, DC: National Commission on Libraries
and Information Science.

89

APPENDIX A
MILQ Test Blueprint: Detailed Listing of Specific Topics in the IL Domain
I.

Determine nature and extent of information needed (21%)
a. Explores general information sources to increase familiarity with the topic
b. Defines or modifies the information need to achieve a manageable focus
c. Identifies key concepts and terms that describe the information need
d. Identifies the value and differences of potential resources in a variety of
formats (e.g. multimedia, database, website, data set, audio/visual, book)
e. Identifies the purpose and audience of potential resources (e.g. popular vs.
scholarly, current vs. historical)
f. Reviews the information need to clarify, revise, or refine the question

II.

Access needed information effectively and efficiently (11%)
a. Selects efficient and effective approaches for accessing the information
needed from the investigative method or information retrieval system
b. Identifies keywords, synonyms and related terms for the information
needed
c. Identifies gaps in the information retrieved and determines if the search
strategy should be revised

III.

Evaluate information and sources critically (48%)
a. Reads the text and selects the main ideas
b. Restates textual concepts in his/her own words and selects data accurately
c. Identifies verbatim material that can be then appropriately quoted
d. Examines and compares information from various sources in order to
evaluate reliability, validity, accuracy, authority, timeliness, and point of
view or bias
e. Analyzes the structure and logic of supporting arguments or methods
f. Recognizes prejudice, deception, or manipulation
g. Determines whether information satisfies the research or other information
need
h. Uses consciously selected criteria to determine whether the information
contradicts or verifies information from other sources
i. Draws conclusions based upon information gathered
j. Determines probable accuracy by questioning the source of the data, the
limitations of the information gathering tools or strategies, and the
reasonableness of the conclusions
k. Integrates new information with previous information or knowledge
90

l.
m.
n.
o.

IV.

V.

Selects information that provides evidence for the topic
Investigates differing viewpoints encountered in literature
Determines whether to incorporate or reject viewpoints encountered
Determines if original information needs has been satisfied or if additional
information is needed

Uses information effectively to accomplish a specific purpose (3%)
a. Communicates clearly and with a style that supports the purposes of the
intended audience
Understands ethical, legal and socio-economic issues surrounding
information and information technology (17%)
a. Uses approved passwords and other forms of ID for access to information
resources
b. Preserves the integrity of information resources, equipment, systems, and
facilities
c. Legally obtains, stores, and disseminates text, data, images, or sounds
d. Demonstrates an understanding of what constitutes plagiarism and does
not represent work attributable to others as his/her own

*Based upon Association of College and Research Libraries, American Library
Association. The information literacy competency standards for higher education.
http://www.ala.org/ala/mgrps/divs/acrl/standards/informationliteracycompetency.cfm.
Updated 2000. Accessed November 15, 2008.

91

APPENDIX B
Specifications for the Medical Information Literacy Questionnaire
Domain of Knowledge

Domain Content Area

Percent
of Test

Specific content to be tested

Information Need

21%

Identify and assess resource value and purpose

Information Access

11%

Design and appraise search strategies

Information
Evaluation
Information Use

48%

Synthesize and evaluate information

3%

Communicate appropriately

Information Ethics

17%

Appropriate use of resources; understanding of
copyright and plagiarism

92

APPENDIX C
Validation Panel Demographic Form
1.

Gender:
Male
Female

2.

Ethnicity:
American Indian, Alaskan Native
Asian/Pacific Islander
Black
Hispanic
Non-Hispanic White

3.
4.

Age: _____
Please indicate your academic background: (check all that apply)
Ed.D.
MD
MLS/MLIS/MILS
Ph.D.
RN/BSN
Other (please indicate) ___________________________________

5.

What is your medical specialty? _____________________________

6.

What is your job title? _____________________________________

7.

Are you currently a Program Director for a Graduate Medical Education residency program?
Yes
No

8.

Are you currently an Assistant Program Director for a Graduate Medical Education residency
program?
Yes
No

9.

If not currently a director or assistant director, have you been a Program Director for a Graduate
Medical Education residency program in the past?
Yes
No

10. Who do you primarily teach or supervise?
Resident Physicians
Medical Students
Other healthcare professional students (please indicate) ____________________________

93

11. On average, what percentage of time do you spend with the following groups?
Resident Physicians _____
Medical Students _____
Other healthcare professional students _____
12. How many years have you been teaching?
Resident Physicians _____
Medical Students _____
Other healthcare students _____
13. Employment:
Institution _________________________________
City & State _______________________________

Questions regarding information skills self-assessment
Please describe your skill level in the following areas:

Some
Skills

Good
Skills

Expert
Skills

14. Defining the topic
15. Identifying keywords or subject headings
16. Teaching evidence-based medicine (EBM)
17. Incorporating EBM into daily practice
18. Using a database to identify articles
19. Searching PubMed
20. Assessing resident information skills
21. Assessing medical student information skills
22. Use information under US copyright law
23. Knowledge about plagiarism
Thank you for your assistance with this project. Please return this demographic sheet and your CVR rating
sheets to:
Sarah Morley smorley@salud.unm.edu
OR
Sarah Knox Morley
Clinical Services Librarian
UNM Health Sciences Library and Informatics Center
MSC 09 5100
1 University of New Mexico
Albuquerque, NM 87131-0001

94

APPENDIX D

95

