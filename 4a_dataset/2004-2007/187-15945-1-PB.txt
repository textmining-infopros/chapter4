DESIDOC Bulletin of Information Technology, Vol. 27, No. 4, July 2007, pp. 51-58
© 2007, DESIDOC

Zipf’s Law in a Random Text from English
With a New Ranking Method
Anurag Saxena*, Monika Jauhari** and B.M. Gupta***
*Indira Gandhi National Open University, New Delhi-110 068
E-mail:anurags@ignou.ac.in
**Bundelkhand University, Jhansi-284 128
***National Institute of Science, Technology and Developmental Studies
Dr K.S.Krishnan Marg, New Delhi-110 012
E-mail:bmgupta1@yahoo.com

ABSTRACT
Zipf’s law has attracted infometricians time and again. There have been many
studies, which have explored the application of Zipf’s law to various areas. However,
there are a few parameters, which largely affect a study. These parameters are
the power law embedded in Zipf’s law, the ranking method, the type of text taken
for the study and the behaviour of extreme regions in the Zipf’s curve. This paper
tries to address all these points by taking a random text in English language from
computer science literature. The selected text is called random because of its
highly specific nature of technical words. The paper studies the properties of this
text and compares the product of rank and frequency for three ranking procedures.
It also analyses the performance of data in the extreme regions of the Zipf’s curve.
It is observed that ranking procedure and type of text have definite bearings on
the performance of Zipf’s curve.
Keywords: Zipf’s law, zipf’s curve, infometrics, power law, computer science

1. INTRODUCTION
Zipf’s law postulates that the frequency
of occurrence of any word as a function of
rank follows a power law with exponent close
to unity. It has been applied to many areas
like natural languages, monkey-typing texts,
web-access statistics, informetrics, finance
and business and ecological systems, etc.
Received 9 February 2007

There is evidence of differences on whether
the power law embedded in Zipf’s law is
actually a Yule distribution1 lognormal distribution2
or stretched exponential distribution 3. There
have been many applications of the law in
natural languages, like English 4 , Chinese 5 ,
Voyanich manuscript 6 , etc. However, there
are few applications of the law to random
texts. Li showed that the Zipf’s law is applicable
51

to random texts provided it has a very different
word structure and length distribution than
a natural language 7 .
To investigate more into this area, the
authors have selected a random text and
have tried to find clues on the distribution
of rank and frequency. An attempt has been
made to evolve a new ranking method, based
on tied-ranks, and a comparison has been
made with the random rank method, deployed
by Zipf8and maximum rank method, deployed
by Chen & Leimkuhler9. According to Mandelbrot,
“The monkey language is, in the terminology
of fractal geometry, self-similar and grows
on infinite trees (any branch of the tree will
be identical to the tree itself), thus needing
an infinite dictionary” 10. A natural language
like English, on the other hand, is a massively
geared down system that economises on
entropy in a number of ways, e.g., the
interdependence or redundancy of words that
seems necessary in order to make a text
‘meaningful’.
Most letter combinations (an uncountable
set) in English are non-words. However, the
random text taken for analysis in this paper
is called ‘random’ only because though it is
in English, it follows a very subject specific
usage of words, e.g., use of hyphenated
words. Hence, in this paper, the random
text used, differs from monkey typing text
by only one virtue, i.e., every word in this
random text has a definite meaning.

2. METHODOLOGY
To study the application of Zipf’s law
and the performance of the new ranking method
on random texts, the authors have taken a
text from a computer science book, Operating
System—Concepts and Design by Milan
Milenkovic, Second Edition, 1997 ( Tata McGraw
Hill, New Delhi). The authors have counted
the frequency of occurrence of each unique
word in the text, and found 1775
unique or different words out of a total of 10,
043 words in the full text. It was observed
that the words of less than nine characters
in length were extensively used. However,
one striking characteristic of computer science
52

literature was the use of hyphenated words,
which made length of the words vary over a
large range. One can easily see from the
Table 1 that after words having 13 characters,
there are a series of hyphenated words.
Use of hyphenated words can be taken
as a special characteristic of the text taken,
i.e., the computer science literature. It would
thus be interesting to investigate the rank
and frequency relationship as propounded
by Zipf and other scientists in such a text.
The authors have intentionally kept the
hyphenated words as they are. One can
also see that hyphenated words are typical
in describing the very specific nature of the
meaning they convey in the concerned literature.
Some of them are the commands given to
the computer to perform specific tasks.
All unique words were arbitrarily ranked
according to their frequency of occurrence
in a decreasing order. Words, which shared
the same frequency, were arranged alphabetically
and different ranks were assigned to each
of them according to Zipf's approach of randomranks. Thus, the words ‘able’ got the rank(r)
868 and the word ‘writes’ got the rank(r)
1775. The two words contributing one occurrence
each are assigned random ranks 868 and
1775, respectively according to Zipf's random
rank approach. This leads to steps for large
values of rank. This is one of the disadvantages
with the random rank method. Chen and
Leimukuhler (1987) had overcome this problem,
by using the maximum rank for all the words
with the same rank. Also their method helped
in preserving the convertibility between frequencyrank distribution and frequency-count distribution
and vice-versa, which was not possible in
random rank approach.
Another method proposed by us is based
on the concept of ‘ties’, which means, that
if two observations are tied, i.e., they have
the same frequency then they should be
assigned the ranks according to the average
of their random ranks. This was done in
order to stabilise the product R x g(r), especially
in the last rank-range. Here, R is the word
rank and g (r) is the rank frequency, i.e., the
number of words of the same rank.
DESIDOC Bull. Inf. Technol., 2007, 27(4)

Table 1. Description of words according to length and frequency

Word example
A

Length
1

Frequency
205

AN

2

1765

CAD

3

1580

AREA

4

1100

LOGIN

5

730

DESIGN

6

856

ADDRESS

7

1076

LANGUAGE

8

844

INTERVALS

9

775

CONCURRENT

10

423

UTILIZATION

11

285

ABSTRACTIONS

12

165

COMMUNICATION

13

84

USER-SPECIFIED

14

37

CHANGE_PASSWORD

15

40

REMOTE-PROCEDURE

16

54

MEMORY-MANAGEMENT

17

7

PROGRAMMER-DEFINED

18

5

ADDRESS-TRANSLATION

19

4

LOWER-PRIORITY-BASED

20

3

COMPUTATION-INTENSIVE

21

2

TRANSACTION-PROCESSING

22

1

APPLICATION-PROGRAMMING

23

1

3. ANALYSIS AND RESULTS
The authors had expected that the new
ranking procedure based on ‘ties’ would be
able to minimise the dispersion of the product
R x g(r) in all the rank range due to a simple
logic that the maximum rank would always
be greater than the average rank. A preliminary
analysis of the product R x g(r) is as given
in Table 2. It can be seen from the Table 2,

on the basis of data given in Appendix I,
that the R x g(r) is distributed with fairly
less variability but for the rank-range (1-10).
This is due to the fact that observation
with rank 1 is a clear outlier. If we delete
that observation from our calculation of standard
deviation then the variability substantially
reduces and comes down to 104.61 instead
of 227.45.

Table2. Rank frequency reletionship in different rank methods

Rank
range

R x g(r) by maximal rank method

R x g(r) by tied rank method

Max

Min

Std. Dev

Max

Min

Std. Dev

1-10

1240

553

227.45

1377

553

227.4

11-51

1485

1239

57.79

1501

1239

62.85

52-99

1548

1352

56.23

1503

1352

46.15

108-228

1596

1512

30.99

1503

1456

16.29

276-1775

1775

1656

40.47

1538

1321.5

83.79

DESIDOC Bull. Inf. Technol., 2007, 27(4)

53

Also, an interesting observation is that
method of tied rank shows the same variability
in the rank range 1-51, performs better in
the rank range 52-228, and performs badly
in the rank range 276-1775 when compared
to the maximal rank method. In Table 3
standard error (S) is the standard error of
the estimate, which quantifies the spread
of data points around the regression curve
and correlation coefficient (r) is the square
root of the normalised difference between
the spread around mean and spread around
the fitting function. As the regression model
better describes the data, the correlation
coefficient will approach unity. It can be
seen that the random texts taken from the
computer science literature do exhibit Zipflike distribution with the slope of the linear
regression touching unity. However, there
is a marked difference in the performance
of maximal rank and tied rank verses random
rank of Zipf.

As far as the distribution of rank and
frequency (Fig. 1) are concerned, it is found
that the relation is a shifted power distribution
(Mandelbrot Zipf’s law) of the form
g(r) = a(r+b) c
where the coefficients are estimated as
a = 3301.44, b = -2.99, c = -1.23, and
S = 14.551777760 and r = 0.99110795.
The authors have applied the fit on the
Good’s data (Fig. 2) used by Chen and
Leimkuhler 9 to check whether that also
behaved in the similar manner. The fit behaved
in the following manner:
g(r) = a(r+b) c
The coefficient are estimated as
a = 216.13, b = 0, c = -0.66, and S =
10.91206292 and r = 0.95879084.

Table 3. Comparison of different models

Statistical measure

Ranking Procedure
Chen
99.14
1718.16
5.77
1
1775
a =2.99
b = -0.91
0.039
0.997

Zipf
223.76
1393.93
16.052
1
1775
a = 3.05
b = -0.96
0.057
0.995

Stdandard deviation
Mean
% c.v
Min. rank
Max. rank
For linear fit y=a+bx
parameters
Standard error (s)
Correlation coefficient (r)

Tied
86.47
1393.93
6.20
1
1321.50
a = 3.03
b = -0.93
0.045
0.997

S = 14.55177775
r = 0.99110795

S = 10.91206292
r = 0.95879084

608.20

280.40

233.68

F re q u e n c y

Frequency

506.85

405.50

304.15

140.25

202.80

93.53

101.45

46.82

0.10

0.1

242.3

484.6

726.8

969.1

1211.3

1453.6

Tied rank

Figure 1. Plot of tied-rank vs frequency for
the random text from computer science
literature.
54

186.97

0.10

0.1

375.5

751.0

1126.4

1501.8

1877.3

2252.7

Maximal rank

Figure 2. Plot of maximum rank vs frequency
for the Good’s data.

DESIDOC Bull. Inf. Technol., 2007, 27(4)

It can been seen that the power distribution
(Mandelbrot Zipf’s law) is fitting this type of
data fairly well but with a slight modification
in the form and parameters for different texts.
Besides this, the authors plotted the log
rank with log frequency (Figs. 3-6) to see
how the ranking methods fare. It could be
seen very clearly that both the Maximal
rank method and Tied rank method perform
better than the Random rank method of Zipf.
Also, the fits of the rank range at the end.
The purpose of analyzing Good’s data here
was just to give a picture that it did not fit
the Zipf’ law properly. The exponent in the
fit of Good’s data comes out to be
-0.61, which was not close to –1 as propounded
by Zipf.

4. DISCUSSION
From the Figs 1 to 6 it is evident that
the lower tail (containing lower ranks) of the
plot of log rank versus log frequency behaved
in the best possible manner in the case of

Maximal rank. The scatter in tied rank method
was better than that in random rank method
but not better than that in the maximal rank
method. The question that naturally arises
is whether the ranking method had a bearing
on the type of text in question.
The analysis of Good’s data done by
Chen and Leimkuhler was revisited in the
earlier section. It is evident from the figure
6 that the curve of log rank vs log frequency
was not linear, specifically for region I, as
defined by Chen and Leimkuhler (region I
comprises higher ranks). This was a departure
from their corollary 1 which says: “In
region I Zipf-curve is linearly decreasing iff
b=0”. The same concept if applied to our
data gave the result: “ Curve is linearly decreasing
even if b is not equal to zero”.

5. CONCLUSIONS
There are two basic issues, which come
out of this exercise. Firstly, random texts

S = 0.04455752
r = 0.99673558

3.02

3.02

2.51

2.51

Frequency

F req u en cy

S = 0.05692424
r = 0.99515619

2.01

1.51

2.01

1.51

1.01

1.01

0.50

0.50

0.00

0.0

0.6

1.2

1.8

2.4

3.0

0.00

3.6

0.0

0.6

1.1

1.7

2.3

Figure 3. Plot of log rank vs frequency for
random rank methods.

S = 0.08594092
r = 0.98096261
2.65

2.51

2.21

2.01

1.76

Frequency

Frequency

S = 0.03946140
r = 0.99744050

1.51

1.32

1.01

0.88

0.50

0.44

0.0

0.6

1.2

1.8

3.4

Figure 5. Plot of log rank with log frequency
for tied rank method.

3.02

0.00

2.9

Log rank

Log rank

2.4

3.0

3.6

Log rank

Figure 4. Plot of log rank with log frequency for
maximal rank method.
DESIDOC Bull. Inf. Technol., 2007, 27(4)

0.00

0.0

0.6

1.2

1.8

2.4

3.0

3.6

Log rank

Figure 6. Plot of log rank with log frequency for
Good’s data.
55

do follow Zipf’s law, however, the exponent
varies from text to text. The method of random
rank performs inferiorly to the maximal rank
method and the tied rank method as proposed
by the authors. However, there is a need for
further investigation in this area to ascertain
whether the ranking method has a bearing
on the type of text in question. Secondly,
the analysis of Good’s data forces us to
raise some doubts about the generalisations
of regions and the Mandelbrot-Zipf law (Chen
and Leimkuhler 1987) which says “In
region I, the Zipf-type curve is linearly decreasing
iff b=0”. However, in region I of the plot of
Good’s data the curve is not linearly decreasing
even if b=0.

scales. Euro. Physical J., 1998, B(2),
525-39.
4. Miller, G.A. & Newman, E.B. Tests of a
statistical explanation of the rank-frequency
relation for words in written English. Amer.
J. Psycho., 1958, 71, 209-18
5. Rousseau, R. & Zhang, Qiaoqiao. Zipf's
data on the frequency of Chinese words
revisited. Scientometrics, 1992, 24(2),
201-20.
6. Landini, G. Zipf's laws in the Voynich
manuscript, 1997.

7. Li, W. Random texts exhibit Zipf's-lawlike word frequency distribution. IEEE
Trans. Inform. Theory, 1992, 38(6), 1842REFERENCES
845.
1. Martindale, Colin & Konopka, Andrzej
8. Zipf, G.K. Human behaviour and the principle
K. Oligonucleotide frequencies in DNA
of least effort. Addison-Wesley, 1949.
follow a Yule distribution.
Comp. & Chem.,
1996, 20(1), 35-38.
9. Ye-Sho, Chen & Ferdinand F, Leimkuhler.
Analysis of Zipf’s law : An index approach.
2. Perline, Richard. Zipf's law, the central
Infor. Proces. Manage., 1987, 23(3), 171limit theorem, and the random division
82.
of the unit interval. Physical Rev., 1996,
54(1), 220-23.
10. Mandelbrot, B. An information theory of
the statistical structure of language. In
3. Laherrere, Jean & Sornette, D. Stretched
Proc. Symposium on Applications of
exponential distributions in nature and
Communication Theory. Butteworths,
economy: 'Fat tails' with characteristic
London, 1953. pp. 486-500.

56

DESIDOC Bull. Inf. Technol., 2007, 27(4)

Appendix I

World Rank and Rank Frequencies by Maximum Rank
and Tied Rank Method
Rank (range)

g (r)

r (max)

g (rmax)

r (tied)

g (rt)

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
26
27
28
29
30
31
32
33
34
36
39

553
545
375
259
238
204
184
155
153
124
121
118
105
103
99
92
79
77
76
68
59
58
57
54
53
47
45
43
42
41
40
39
37
36

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
25
26
27
28
29
30
31
32
33
35
38
39

553
545
375
259
238
204
184
155
153
124
121
118
105
103
99
92
79
77
76
68
59
58
57
54
53
47
45
43
42
41
40
39
37
36

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
24
26
27
28
29
30
31
32
33
34.5
37
39

553
545
375
259
238
204
184
155
153
124
121
118
105
103
99
92
79
77
76
68
59
58
57
54
53
47
45
43
42
41
40
39
37
36

DESIDOC Bull. Inf. Technol., 2007, 27(4)

57

World Rank and Rank Frequencies by Maximum Rank
and Tied Rank Method

58

Rank (range)

g (r)

r (max)

g (rmax)

r (tied)

g (rt)

40
41
42
45
46
47
48
49
52
53
60
61
64
67
70
78
81
87
90
97
100
109
122
129
139
159
176
194
229
277
339
431
569
868

35
33
32
31
30
29
28
27
26
25
24
23
22
21
20
19
18
17
16
15
14
13
12
11
10
9
8
7
6
5
4
3
2
1

40
41
44
45
46
47
48
51
52
59
60
63
66
69
77
80
86
89
96
99
108
121
128
138
158
175
193
228
276
338
430
568
867
1775

35
33
32
31
30
29
28
27
26
25
24
23
22
21
20
19
18
17
16
15
14
13
12
11
10
9
8
7
6
5
4
3
2
1

40
41
43
45
46
47
48
50
52
56
60
62
65
68
73.5
79
83.5
88
93
98
104
115
125
133.5
148.5
167
184.5
211
252.5
307.5
384.5
499.5
718
1321.5

35
33
32
31
30
29
28
27
26
25
24
23
22
21
20
19
18
17
16
15
14
13
12
11
10
9
8
7
6
5
4
3
2
1

DESIDOC Bull. Inf. Technol., 2007, 27(4)

